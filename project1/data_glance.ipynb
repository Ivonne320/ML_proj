{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"./data/dataset_to_release\", sub_sample=True)\n",
    "x_train_processed = x_train.copy()\n",
    "x_train_processed = fillna_with_mean(x_train_processed)\n",
    "x_train_processed = standardize(x_train_processed)\n",
    "x_train_processed = add_bias(x_train_processed)\n",
    "# add a column of ones\n",
    "y_train_processed = y_train.copy()\n",
    "y_train_processed = process_y(y_train_processed)\n",
    "print(np.isnan(x_train_processed).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6563, 50)\n",
      "[1.99026100e+01+0.00000000e+00j 8.83824945e+00+0.00000000e+00j\n",
      " 5.96839095e+00+0.00000000e+00j 5.27963201e+00+0.00000000e+00j\n",
      " 4.74515991e+00+0.00000000e+00j 4.43052927e+00+0.00000000e+00j\n",
      " 3.52373980e+00+0.00000000e+00j 3.43205788e+00+0.00000000e+00j\n",
      " 3.35808778e+00+0.00000000e+00j 3.10854220e+00+0.00000000e+00j\n",
      " 3.03653907e+00+0.00000000e+00j 2.99031044e+00+0.00000000e+00j\n",
      " 2.78777022e+00+0.00000000e+00j 2.55074742e+00+0.00000000e+00j\n",
      " 2.47028671e+00+0.00000000e+00j 2.44021943e+00+0.00000000e+00j\n",
      " 2.19214395e+00+0.00000000e+00j 2.13182074e+00+0.00000000e+00j\n",
      " 2.03081449e+00+0.00000000e+00j 1.94761577e+00+0.00000000e+00j\n",
      " 1.90587386e+00+0.00000000e+00j 1.90074749e+00+0.00000000e+00j\n",
      " 1.84871689e+00+0.00000000e+00j 1.79490247e+00+0.00000000e+00j\n",
      " 1.72151037e+00+0.00000000e+00j 1.70503860e+00+0.00000000e+00j\n",
      " 1.66410595e+00+0.00000000e+00j 1.59415556e+00+0.00000000e+00j\n",
      " 1.55583249e+00+0.00000000e+00j 1.51367271e+00+0.00000000e+00j\n",
      " 1.48232496e+00+0.00000000e+00j 1.45364001e+00+0.00000000e+00j\n",
      " 1.42456100e+00+0.00000000e+00j 1.36648729e+00+0.00000000e+00j\n",
      " 1.28838204e+00+0.00000000e+00j 1.28088773e+00+0.00000000e+00j\n",
      " 1.26293883e+00+0.00000000e+00j 1.23366464e+00+0.00000000e+00j\n",
      " 1.20287943e+00+0.00000000e+00j 1.18070297e+00+0.00000000e+00j\n",
      " 1.16178608e+00+0.00000000e+00j 1.14616794e+00+0.00000000e+00j\n",
      " 1.12061072e+00+0.00000000e+00j 1.10950778e+00+0.00000000e+00j\n",
      " 1.08350906e+00+0.00000000e+00j 1.07646728e+00+0.00000000e+00j\n",
      " 1.05554128e+00+0.00000000e+00j 1.05321590e+00+0.00000000e+00j\n",
      " 1.03823334e+00+0.00000000e+00j 1.02203772e+00+0.00000000e+00j\n",
      " 1.00963670e+00+0.00000000e+00j 1.00113326e+00+0.00000000e+00j\n",
      " 9.70794193e-01+0.00000000e+00j 9.60895094e-01+0.00000000e+00j\n",
      " 9.49355579e-01+0.00000000e+00j 9.44646207e-01+0.00000000e+00j\n",
      " 9.36177593e-01+0.00000000e+00j 9.04062331e-01+0.00000000e+00j\n",
      " 8.93694448e-01+0.00000000e+00j 8.88771981e-01+0.00000000e+00j\n",
      " 8.67513965e-01+0.00000000e+00j 8.62090191e-01+0.00000000e+00j\n",
      " 8.49627474e-01+0.00000000e+00j 8.48153560e-01+0.00000000e+00j\n",
      " 8.37094764e-01+0.00000000e+00j 8.22452087e-01+0.00000000e+00j\n",
      " 8.06207368e-01+0.00000000e+00j 8.03165326e-01+0.00000000e+00j\n",
      " 7.84949445e-01+0.00000000e+00j 7.82073717e-01+0.00000000e+00j\n",
      " 7.68979969e-01+0.00000000e+00j 7.60054149e-01+0.00000000e+00j\n",
      " 7.37249561e-01+0.00000000e+00j 7.24684556e-01+0.00000000e+00j\n",
      " 7.05995497e-01+0.00000000e+00j 7.02021410e-01+0.00000000e+00j\n",
      " 6.75131413e-01+0.00000000e+00j 6.62099530e-01+0.00000000e+00j\n",
      " 6.55361052e-01+0.00000000e+00j 6.32061058e-01+0.00000000e+00j\n",
      " 6.28758967e-01+0.00000000e+00j 6.09086185e-01+0.00000000e+00j\n",
      " 6.04671673e-01+0.00000000e+00j 5.80548998e-01+0.00000000e+00j\n",
      " 5.71361901e-01+0.00000000e+00j 5.54823298e-01+0.00000000e+00j\n",
      " 5.38647398e-01+0.00000000e+00j 5.22823660e-01+0.00000000e+00j\n",
      " 5.07687276e-01+0.00000000e+00j 4.87234865e-01+0.00000000e+00j\n",
      " 4.73952190e-01+0.00000000e+00j 4.61229360e-01+0.00000000e+00j\n",
      " 4.51991907e-01+0.00000000e+00j 4.49093735e-01+0.00000000e+00j\n",
      " 4.35105735e-01+0.00000000e+00j 4.25292115e-01+0.00000000e+00j\n",
      " 4.12906331e-01+0.00000000e+00j 4.03614451e-01+0.00000000e+00j\n",
      " 3.90408236e-01+0.00000000e+00j 3.76460099e-01+0.00000000e+00j\n",
      " 3.66600090e-01+0.00000000e+00j 3.62335532e-01+0.00000000e+00j\n",
      " 3.54063600e-01+0.00000000e+00j 3.38682384e-01+0.00000000e+00j\n",
      " 3.27475467e-01+0.00000000e+00j 3.18895677e-01+0.00000000e+00j\n",
      " 3.14253560e-01+0.00000000e+00j 3.10758767e-01+0.00000000e+00j\n",
      " 2.83843112e-01+0.00000000e+00j 2.71906557e-01+0.00000000e+00j\n",
      " 2.67269502e-01+0.00000000e+00j 2.52598122e-01+0.00000000e+00j\n",
      " 2.36194317e-01+0.00000000e+00j 2.15188669e-01+0.00000000e+00j\n",
      " 2.10643984e-01+0.00000000e+00j 2.01027817e-01+0.00000000e+00j\n",
      " 1.92893456e-01+0.00000000e+00j 1.84619024e-01+0.00000000e+00j\n",
      " 1.73658079e-01+0.00000000e+00j 1.56787496e-01+0.00000000e+00j\n",
      " 1.55867960e-01+0.00000000e+00j 1.28674262e-01+0.00000000e+00j\n",
      " 1.28296837e-01+0.00000000e+00j 1.18430792e-01+0.00000000e+00j\n",
      " 1.09106550e-01+0.00000000e+00j 1.04958345e-01+0.00000000e+00j\n",
      " 9.69807249e-02+0.00000000e+00j 9.68784892e-02+0.00000000e+00j\n",
      " 7.87662605e-02+0.00000000e+00j 7.66675956e-02+0.00000000e+00j\n",
      " 7.41500265e-02+0.00000000e+00j 6.54773078e-02+0.00000000e+00j\n",
      " 5.74067293e-02+0.00000000e+00j 5.19976737e-02+0.00000000e+00j\n",
      " 3.02555764e-02+0.00000000e+00j 2.87312560e-02+0.00000000e+00j\n",
      " 2.64294392e-02+0.00000000e+00j 2.33711590e-02+0.00000000e+00j\n",
      " 2.23138643e-02+0.00000000e+00j 2.03007310e-02+0.00000000e+00j\n",
      " 2.00483910e-02+0.00000000e+00j 1.66032983e-02+0.00000000e+00j\n",
      " 1.41315991e-02+0.00000000e+00j 1.33251290e-02+0.00000000e+00j\n",
      " 1.09944662e-02+0.00000000e+00j 9.56904895e-03+0.00000000e+00j\n",
      " 9.36061383e-03+0.00000000e+00j 8.92365799e-03+0.00000000e+00j\n",
      " 7.43537555e-03+0.00000000e+00j 6.40068635e-03+0.00000000e+00j\n",
      " 4.92045450e-03+0.00000000e+00j 4.40097533e-03+0.00000000e+00j\n",
      " 4.22524780e-03+0.00000000e+00j 3.45838287e-03+0.00000000e+00j\n",
      " 3.10731528e-03+0.00000000e+00j 2.00471879e-03+0.00000000e+00j\n",
      " 1.79821505e-03+0.00000000e+00j 1.39190278e-03+0.00000000e+00j\n",
      " 1.05217557e-03+0.00000000e+00j 1.04388984e-03+0.00000000e+00j\n",
      " 7.94734809e-04+0.00000000e+00j 5.63083804e-04+0.00000000e+00j\n",
      " 3.20604189e-04+0.00000000e+00j 7.40844811e-05+0.00000000e+00j\n",
      " 1.37225932e-07+0.00000000e+00j 8.59978214e-16+0.00000000e+00j\n",
      " 1.80586124e-16+6.32533384e-17j 1.80586124e-16-6.32533384e-17j\n",
      " 9.17453738e-17+1.38569349e-17j 9.17453738e-17-1.38569349e-17j\n",
      " 0.00000000e+00+0.00000000e+00j 0.00000000e+00+0.00000000e+00j\n",
      " 0.00000000e+00+0.00000000e+00j 0.00000000e+00+0.00000000e+00j\n",
      " 0.00000000e+00+0.00000000e+00j]\n",
      "[[ 0.        +0.j  0.        +0.j  0.        +0.j ...  0.        +0.j\n",
      "   0.        +0.j  0.        +0.j]\n",
      " [-0.0047891 +0.j  0.01030592+0.j -0.01481581+0.j ...  0.01139433+0.j\n",
      "  -0.00940604+0.j  0.00173796+0.j]\n",
      " [ 0.006958  +0.j  0.00186257+0.j  0.00299892+0.j ...  0.02322025+0.j\n",
      "   0.07781513+0.j  0.04124557+0.j]\n",
      " ...\n",
      " [ 0.19652198+0.j  0.00564162+0.j  0.0300704 +0.j ... -0.00908683+0.j\n",
      "  -0.02468311+0.j -0.01709661+0.j]\n",
      " [ 0.19572309+0.j  0.00428719+0.j  0.02848796+0.j ... -0.01018733+0.j\n",
      "  -0.02998879+0.j -0.0191753 +0.j]\n",
      " [ 0.02545889+0.j  0.0337035 +0.j  0.0058581 +0.j ...  0.05882587+0.j\n",
      "   0.02296973+0.j  0.02709241+0.j]]\n",
      "[1.17056338e-01+0.00000000e+00j 5.19817810e-02+0.00000000e+00j\n",
      " 3.51028326e-02+0.00000000e+00j 3.10519268e-02+0.00000000e+00j\n",
      " 2.79084523e-02+0.00000000e+00j 2.60579658e-02+0.00000000e+00j\n",
      " 2.07247229e-02+0.00000000e+00j 2.01854996e-02+0.00000000e+00j\n",
      " 1.97504477e-02+0.00000000e+00j 1.82827562e-02+0.00000000e+00j\n",
      " 1.78592729e-02+0.00000000e+00j 1.75873812e-02+0.00000000e+00j\n",
      " 1.63961497e-02+0.00000000e+00j 1.50021104e-02+0.00000000e+00j\n",
      " 1.45288842e-02+0.00000000e+00j 1.43520448e-02+0.00000000e+00j\n",
      " 1.28929996e-02+0.00000000e+00j 1.25382113e-02+0.00000000e+00j\n",
      " 1.19441474e-02+0.00000000e+00j 1.14548177e-02+0.00000000e+00j\n",
      " 1.12093145e-02+0.00000000e+00j 1.11791640e-02+0.00000000e+00j\n",
      " 1.08731483e-02+0.00000000e+00j 1.05566411e-02+0.00000000e+00j\n",
      " 1.01249886e-02+0.00000000e+00j 1.00281106e-02+0.00000000e+00j\n",
      " 9.78736703e-03+0.00000000e+00j 9.37595682e-03+0.00000000e+00j\n",
      " 9.15056134e-03+0.00000000e+00j 8.90260043e-03+0.00000000e+00j\n",
      " 8.71822997e-03+0.00000000e+00j 8.54952070e-03+0.00000000e+00j\n",
      " 8.37849378e-03+0.00000000e+00j 8.03693573e-03+0.00000000e+00j\n",
      " 7.57756311e-03+0.00000000e+00j 7.53348568e-03+0.00000000e+00j\n",
      " 7.42791998e-03+0.00000000e+00j 7.25574511e-03+0.00000000e+00j\n",
      " 7.07468322e-03+0.00000000e+00j 6.94425333e-03+0.00000000e+00j\n",
      " 6.83299449e-03+0.00000000e+00j 6.74113705e-03+0.00000000e+00j\n",
      " 6.59082339e-03+0.00000000e+00j 6.52552193e-03+0.00000000e+00j\n",
      " 6.37261159e-03+0.00000000e+00j 6.33119565e-03+0.00000000e+00j\n",
      " 6.20812026e-03+0.00000000e+00j 6.19444367e-03+0.00000000e+00j\n",
      " 6.10632437e-03+0.00000000e+00j 6.01107057e-03+0.00000000e+00j\n",
      " 5.93813447e-03+0.00000000e+00j 5.88812186e-03+0.00000000e+00j\n",
      " 5.70968396e-03+0.00000000e+00j 5.65146284e-03+0.00000000e+00j\n",
      " 5.58359368e-03+0.00000000e+00j 5.55589571e-03+0.00000000e+00j\n",
      " 5.50608793e-03+0.00000000e+00j 5.31720341e-03+0.00000000e+00j\n",
      " 5.25622515e-03+0.00000000e+00j 5.22727388e-03+0.00000000e+00j\n",
      " 5.10224578e-03+0.00000000e+00j 5.07034609e-03+0.00000000e+00j\n",
      " 4.99704716e-03+0.00000000e+00j 4.98837839e-03+0.00000000e+00j\n",
      " 4.92333657e-03+0.00000000e+00j 4.83721630e-03+0.00000000e+00j\n",
      " 4.74167368e-03+0.00000000e+00j 4.72378205e-03+0.00000000e+00j\n",
      " 4.61664613e-03+0.00000000e+00j 4.59973266e-03+0.00000000e+00j\n",
      " 4.52272236e-03+0.00000000e+00j 4.47022553e-03+0.00000000e+00j\n",
      " 4.33610133e-03+0.00000000e+00j 4.26220080e-03+0.00000000e+00j\n",
      " 4.15228191e-03+0.00000000e+00j 4.12890849e-03+0.00000000e+00j\n",
      " 3.97075614e-03+0.00000000e+00j 3.89410968e-03+0.00000000e+00j\n",
      " 3.85447762e-03+0.00000000e+00j 3.71743972e-03+0.00000000e+00j\n",
      " 3.69801861e-03+0.00000000e+00j 3.58231399e-03+0.00000000e+00j\n",
      " 3.55635023e-03+0.00000000e+00j 3.41447376e-03+0.00000000e+00j\n",
      " 3.36044025e-03+0.00000000e+00j 3.26316918e-03+0.00000000e+00j\n",
      " 3.16803132e-03+0.00000000e+00j 3.07496469e-03+0.00000000e+00j\n",
      " 2.98594071e-03+0.00000000e+00j 2.86565074e-03+0.00000000e+00j\n",
      " 2.78752926e-03+0.00000000e+00j 2.71270049e-03+0.00000000e+00j\n",
      " 2.65837081e-03+0.00000000e+00j 2.64132533e-03+0.00000000e+00j\n",
      " 2.55905552e-03+0.00000000e+00j 2.50133714e-03+0.00000000e+00j\n",
      " 2.42849069e-03+0.00000000e+00j 2.37384090e-03+0.00000000e+00j\n",
      " 2.29616912e-03+0.00000000e+00j 2.21413375e-03+0.00000000e+00j\n",
      " 2.15614254e-03+0.00000000e+00j 2.13106073e-03+0.00000000e+00j\n",
      " 2.08240971e-03+0.00000000e+00j 1.99194576e-03+0.00000000e+00j\n",
      " 1.92603276e-03+0.00000000e+00j 1.87557110e-03+0.00000000e+00j\n",
      " 1.84826869e-03+0.00000000e+00j 1.82771422e-03+0.00000000e+00j\n",
      " 1.66941096e-03+0.00000000e+00j 1.59920663e-03+0.00000000e+00j\n",
      " 1.57193399e-03+0.00000000e+00j 1.48564490e-03+0.00000000e+00j\n",
      " 1.38916664e-03+0.00000000e+00j 1.26562283e-03+0.00000000e+00j\n",
      " 1.23889346e-03+0.00000000e+00j 1.18233639e-03+0.00000000e+00j\n",
      " 1.13449450e-03+0.00000000e+00j 1.08582879e-03+0.00000000e+00j\n",
      " 1.02136247e-03+0.00000000e+00j 9.22138859e-04+0.00000000e+00j\n",
      " 9.16730650e-04+0.00000000e+00j 7.56792096e-04+0.00000000e+00j\n",
      " 7.54572282e-04+0.00000000e+00j 6.96545569e-04+0.00000000e+00j\n",
      " 6.41705444e-04+0.00000000e+00j 6.17307954e-04+0.00000000e+00j\n",
      " 5.70387930e-04+0.00000000e+00j 5.69786634e-04+0.00000000e+00j\n",
      " 4.63260347e-04+0.00000000e+00j 4.50917140e-04+0.00000000e+00j\n",
      " 4.36110167e-04+0.00000000e+00j 3.85101947e-04+0.00000000e+00j\n",
      " 3.37635190e-04+0.00000000e+00j 3.05822064e-04+0.00000000e+00j\n",
      " 1.77946861e-04+0.00000000e+00j 1.68981637e-04+0.00000000e+00j\n",
      " 1.55443601e-04+0.00000000e+00j 1.37456458e-04+0.00000000e+00j\n",
      " 1.31238026e-04+0.00000000e+00j 1.19397869e-04+0.00000000e+00j\n",
      " 1.17913743e-04+0.00000000e+00j 9.76515793e-05+0.00000000e+00j\n",
      " 8.31143875e-05+0.00000000e+00j 7.83711686e-05+0.00000000e+00j\n",
      " 6.46634764e-05+0.00000000e+00j 5.62799466e-05+0.00000000e+00j\n",
      " 5.50540445e-05+0.00000000e+00j 5.24841076e-05+0.00000000e+00j\n",
      " 4.37308390e-05+0.00000000e+00j 3.76453593e-05+0.00000000e+00j\n",
      " 2.89394398e-05+0.00000000e+00j 2.58841456e-05+0.00000000e+00j\n",
      " 2.48506118e-05+0.00000000e+00j 2.03403289e-05+0.00000000e+00j\n",
      " 1.82755401e-05+0.00000000e+00j 1.17906667e-05+0.00000000e+00j\n",
      " 1.05761239e-05+0.00000000e+00j 8.18641585e-06+0.00000000e+00j\n",
      " 6.18832501e-06+0.00000000e+00j 6.13959286e-06+0.00000000e+00j\n",
      " 4.67419833e-06+0.00000000e+00j 3.31175298e-06+0.00000000e+00j\n",
      " 1.88561964e-06+0.00000000e+00j 4.35724664e-07+0.00000000e+00j\n",
      " 8.07088368e-10+0.00000000e+00j 5.05792459e-18+0.00000000e+00j\n",
      " 1.06210946e-18+3.72021768e-19j 1.06210946e-18-3.72021768e-19j\n",
      " 5.39596439e-19+8.14989618e-20j 5.39596439e-19-8.14989618e-20j\n",
      " 0.00000000e+00+0.00000000e+00j 0.00000000e+00+0.00000000e+00j\n",
      " 0.00000000e+00+0.00000000e+00j 0.00000000e+00+0.00000000e+00j\n",
      " 0.00000000e+00+0.00000000e+00j]\n"
     ]
    }
   ],
   "source": [
    "## PCA feature selection ##\n",
    "x_pca, eig_vec, eig_val,weight = pca(x_train_processed, 50)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivonne/.local/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1340: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return np.asarray(x, float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fee3e907cd0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA62klEQVR4nO3deXhU1f3H8c9MlslCFkhIQkLY9y1AgDQqWCUVUBF3VFqQulK0KrVFrIJLK1Qt0laUVkVttYr6c0dBRVBRZAkEZAsGAwFCEiBkJ9vM+f0RiI1sCUxyk5n363nmGebOvZPvnVxmPjnn3HNtxhgjAAAAi9itLgAAAHg3wggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFK+VhdQHy6XS9nZ2QoJCZHNZrO6HAAAUA/GGBUXFys2NlZ2+8nbP1pEGMnOzlZ8fLzVZQAAgDOwZ88etW/f/qTPt4gwEhISIqlmZ0JDQy2uBgAA1EdRUZHi4+Nrv8dPpkWEkWNdM6GhoYQRAABamNMNsWAAKwAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwVIPDyJdffqmxY8cqNjZWNptN77777mm3WbFihQYPHiyHw6Fu3brppZdeOoNSAQCAJ2pwGCktLVVCQoLmz59fr/UzMzN1ySWX6IILLlBaWpruvvtu3XzzzVq6dGmDiwUAAJ6nwdemGTNmjMaMGVPv9RcsWKDOnTvrr3/9qySpd+/eWrlypZ566imNGjWqoT8eAAB4mEa/UN6qVauUkpJSZ9moUaN09913n3SbiooKVVRU1D4uKipqrPIAAE2o2ulSpdOlapdRtdOo2uVStdPI6TKqcrqO3h997HLVWV6zvlH10e2dLiOXMXIZyWWMdPT+2GNz9N+mzrK660iSy3WCbVSz7jHmJ/tR97mfPHvqhzL/s7E57bo/ff6na7jPr8/trPg2QY32+qfS6GEkJydH0dHRdZZFR0erqKhIR44cUWBg4HHbzJ49Ww8//HBjlwYAOA1jjIorqpVfUqnCI1UqLq9WcXnNfVF5lYr+5/Gx+9KKapVXuVRR7VRFtavmVuVUeXVNqEDzNDYh1nPDyJmYMWOGpk2bVvu4qKhI8fHxFlYEAJ6jyunSgeIK5RSVK6+oXAdLKpVfWvd2qLRS+aUVOlxapUqnq9FqsdkkP7tdvj42+dht8vOx19zbbfLxscnPXvPY18cuX7tNvj62mvujy+12m+w2ySbJbrPJZqt5bLfZZLdLNtlkO/bYVncdm+0E29h+vNy9/SeXvf/JQ9lO9dxPtz1ux//3n/X/OSd63l2iQwMa54XrodHDSExMjHJzc+ssy83NVWho6AlbRSTJ4XDI4XA0dmkA4HEqq13ae7hMew8fUU5RuXILy2vui47dV+hgScVxzf+nE+Tvo/BAP4UE+CkkwPfoza/2PjTw6H2Ar4L9fRXo7yOHr10OXx85/OwKOHrv8LXLz6cmfPjZ7bLbG+mbFS1Ko4eR5ORkffTRR3WWffrpp0pOTm7sHw0AHqmi2qndh8q062Bpzf2hUmXl19zvO3xE9ekJ8bXbFB0aoKhQhyJbORQR7K/Wwf6KCPZXm6O3iGCH2rSqWRbg59P4Owav1eAwUlJSooyMjNrHmZmZSktLU5s2bdShQwfNmDFD+/bt07///W9J0u23366nn35af/jDH/TrX/9an3/+ud544w0tXrzYfXsBAB6ootqpzIOl2pFboozcYu3ILdGOvGLtPlR2yrEXgX4+im8TqJiwQMWEOhQTGqDosICa+6O3iGB/WiXQbDQ4jKxbt04XXHBB7eNjYzsmTZqkl156Sfv371dWVlbt8507d9bixYt1zz336G9/+5vat2+v559/ntN6AeB/5JdWakt2obZkF2lLdpG2Zhdq1ylCRyuHrzpHBqtjRJA6RRy9jwxWxzZBahviOG7MAtCc2YxpaM9h0ysqKlJYWJgKCwsVGhpqdTkAcFYOllRo/e7D2rzvx/CRU1R+wnVDHL7qHt1KPaJD1C2q5r5HdIiiQwkcaP7q+/3dLM+mAQBP4XIZfZ9XotTdh5W6+7DWZx1W5sHSE67bKSJIfWPD1Cc2VH1jQ9UrJpTQAa9AGAEAN6p2uvTdvkJ9s/OQ1mTma33WYRWXVx+3Xo/oVkpoH66+saHqGxem3u1C1crBRzK8E0c+AJwFY4wy8kr0dcZBrcw4pNWZh44LH0H+PhoYH67Ejq01uGNrDY5vrbAgP4sqBpofwggANNCB4gp9ueOAvvr+gL7ZeUh5xRV1ng8N8FVy1wj9rEuEhnZqo14xIfL1afB1SQGvQRgBgNOodrqUtqdAK9IPaMWOPG3eV/d6WQ5fu4Z1bqNzukbq3G4R6hsbJh9OmwXqjTACACdwsKRCn2/P0xfpNS0gRT/peukfF6YRPSJ1Xre2GtwxXA5fJgUDzhRhBACO2pNfpqVbcvTJllyt251fZybT8CA/jejeVuf3aKsRPdqqbQiXrADchTACwGsZY7Q9p1hLt+Ro6ZZcbdtft/ulX1yoLuwVrZ/3bKuE9uF0vQCNhDACwKs4XUbrsw5r6eYcfbI1V1n5ZbXP+dhtGtapjS7qG62L+sYoLvzEF/ME4F6EEQAezxijjXsL9X5atj7clF3n7BeHr10jerTVRX2ildI7Wq2D/S2sFPBOhBEAHis9p1jvb9ynDzbur9MCEhrgq5G9ozWqb7RG9GirIH8+CgEr8T8QgEfZV3BE727Yp/fTspWeW1y7PNDPR7/oE63LEmI1okdb+fsy7wfQXBBGALR45VVOLd2SozfX7dXXOw/q2OU//X3sOr9nW12WEKuRvaNoAQGaKf5nAmiRjDHatLdQb6zbo/c3ZteZgj25S4SuGBSnUX1jmHYdaAEIIwBalAPFFXp3wz69mbpHO3JLapfHhQfq6sT2ujqxveLbBFlYIYCGIowAaPacLqPl2/O0aN0eLd+ep+qjs5E5fO0a3S9G1w6JV3KXCNmZBwRokQgjAJqtvOJyLVqzR6+tyVJ2YXnt8oT4cF07pL0uHRCrsEC6YYCWjjACoFkxxmjVD4f06rdZWrolp7YVJDzIT1cPbq9rh8arR3SIxVUCcCfCCIBmoai8Sm+t26tXV+/WzgOltcsHdwjXL3/WURf3b6cAPy5GB3giwggAS+09XKYXv96l19dkqbTSKUkK8vfR5YPi9MukjuoTG2pxhQAaG2EEgCU27S3Qc19l6qPv9st5tCume1QrTUzuqMsHxSkkgLEggLcgjABoMi6X0efb8/TcVz9odWZ+7fJzu0XoluFddH6PtrLZOCMG8DaEEQCNrtrp0rtp2Xp2RUbteBBfu01jE2J18/DO6hsbZnGFAKxEGAHQaCqrXXpnw17NX76z9kJ1IQ5f3ZDUQTee20ntwgItrhBAc0AYAeB2FdVOvZW6V88s36l9BUckSRHB/rp5eBf98mcdGA8CoA7CCAC3Ka9yatHaPVrwxU7tPzpJWdsQh24b0UU3JHXgQnUATohPBgBnrcrp0qK1e/T3Zd8rr7hCkhQd6tCU87vqumEdmB8EwCkRRgCcMZfL6INN2Zr76Q7tPlQzJiQ2LEBTLuimaxLbE0IA1AthBECDGWO0YscBPb4kXdv2F0mSIlv5684Lu+v6YR3k72u3uEIALQlhBECDrNuVr8eXpGvNrpp5QkIcvrrt/C6afG5nBTv4SAHQcHxyAKiXnQdKNPuj7fpsW64kyeFr16RzOmnK+V3VOtjf4uoAtGSEEQCnlF9aqb99tkOvrs5StcvIx27TtUPa67cjuzNPCAC3IIwAOKGKaqde/maX/vF5horLqyVJKb2jdN+Y3uoW1cri6gB4EsIIgDqMMfp4c45mf7xNe/JrJizr0y5UD1zSW+d0i7S4OgCeiDACoNbGPQV69MOtWrf7sCQpKsShe0f11FWD28vHzgXsADQOwggA5ZdW6vEl27Vo3R4ZIwX42XXbiK66dUQXzpAB0Oj4lAG8mNNl9N/Vu/XkJztUeKRKknTFoDhNH91LMWEBFlcHwFsQRgAvlbo7Xw++u0Vbj05a1rtdqB4Z11dDO7WxuDIA3oYwAniZwrIqzVmyTa+t2SNJCg3w1e9H9dQNSR0ZFwLAEoQRwEsYY/T+xmw9+uFWHSyplCRdO6S9po/upYhWDourA+DNCCOAF9h1sFQPvLtZKzMOSpK6RbXSY1f017DOdMkAsB5hBPBgldUu/evLnfr75xmqrHbJ4WvXb0d21y3Du3AxOwDNBmEE8FDrduXrvre/U0ZeiSRpePdIPTqunzpFBltcGQDURRgBPMyRSqeeWJquF7/JlDFSZCt/PXhpH12WECubjQGqAJofwgjgQdbtytfv39qkzIOlkqRrEtvrgUv6KCzIz+LKAODkCCOAByivcurJpel64eua1pCY0ADNvqq/LugZZXVpAHBahBGghUvdna/fv7lJPxxtDbk6sb0evLSPwgJpDQHQMhBGgBaqvMqpv36SrudX1rSGRIc6NPvK/rqwV7TVpQFAgxBGgBYodfdh/f6tjfrhQE1ryFWD22vmpYwNAdAyEUaAFqSi2qm5n+zQc1/9IJeRokJqWkNG9qY1BEDLRRgBWojtOUW6+/U0bc8pliRdOShOs8b2pTUEQItHGAGaOZfLaOHXmXp8SboqnS5FBPtr9pX9dVHfGKtLAwC3IIwAzVh2wRHd++ZGfbPzkCTpwl5R+stVA9Q2hAvbAfAchBGgmXp/Y7YeeOc7FZVXK9DPRw9c2ls3DOvALKoAPA5hBGhmCo9UaeZ7m/VeWrYkKSE+XE9dm6AubVtZXBkANA7CCNCMrNuVr7teT9O+giPysds09YJuuvPCbvLz4Qq7ADwXYQRoBpwuowVf7NTcT3fI6TLq0CZI864bqMEdWltdGgA0OsIIYLG8onLdvSitdpDquIGx+tPl/RQSwCm7ALwDYQSw0PL0PN37xkYdKq1UoJ+PHhnXV1cntmeQKgCvQhgBLFBZ7dKTn6TrX1/+IEnq3S5U/7h+kLpFMUgVgPchjABNLOtQme58bb027i2UJE1K7qgZF/dWgJ+PxZUBgDUII0AT+mBjtu5/+zsVV1QrLNBPj189QKOYSRWAlyOMAE3gSKVTD3+wRa+v3SNJGtqpteZdN0hx4YEWVwYA1iOMAI0s82CpprySqu05xbLZpDsv6KbfjuwuX+YOAQBJhBGgUX383X79/q1NKqmoVmQrf/39ukE6p1uk1WUBQLNyRn+azZ8/X506dVJAQICSkpK0Zs2aU64/b9489ezZU4GBgYqPj9c999yj8vLyMyoYaAmqnC49+uFWTXl1vUoqqjWsUxst/u1wgggAnECDW0YWLVqkadOmacGCBUpKStK8efM0atQopaenKyoq6rj1//vf/+q+++7TwoULdc4552jHjh268cYbZbPZNHfuXLfsBNCc5BSW647/rte63YclSbeN6KLfj+pJtwwAnITNGGMaskFSUpKGDh2qp59+WpLkcrkUHx+vO++8U/fdd99x699xxx3atm2bli1bVrvsd7/7nVavXq2VK1fW62cWFRUpLCxMhYWFCg0NbUi5QJP6OuOgfvvaBh0qrVRIgK+evCaBs2UAeK36fn836E+1yspKpaamKiUl5ccXsNuVkpKiVatWnXCbc845R6mpqbVdOT/88IM++ugjXXzxxQ350UCz5nIZ/WPZ9/rlC6t1qLRSfdqF6sM7zyOIAEA9NKib5uDBg3I6nYqOjq6zPDo6Wtu3bz/hNjfccIMOHjyo8847T8YYVVdX6/bbb9f9999/0p9TUVGhioqK2sdFRUUNKRNoUodLK3XPG2lakX5AkjR+SLweHteXScwAoJ4avRN7xYoVeuyxx/TMM89o/fr1evvtt7V48WI9+uijJ91m9uzZCgsLq73Fx8c3dpnAGdm0t0CX/mOlVqQfkMPXrsevHqC/XD2AIAIADdCgMSOVlZUKCgrSW2+9pcsvv7x2+aRJk1RQUKD33nvvuG2GDx+un/3sZ3riiSdql73yyiu69dZbVVJSIrv9+Dx0opaR+Ph4xoygWXkvbZ/+8NYmVVS71CkiSM9MSFSfWI5PADimUcaM+Pv7KzExsc5gVJfLpWXLlik5OfmE25SVlR0XOHx8av5qPFkOcjgcCg0NrXMDmguny2jOx9t11+tpqqh2aWSvKL1/53kEEQA4Qw0+tXfatGmaNGmShgwZomHDhmnevHkqLS3V5MmTJUkTJ05UXFycZs+eLUkaO3as5s6dq0GDBikpKUkZGRl68MEHNXbs2NpQArQUReVVuvv1NH2+PU+S9Jufd9XvLuopH7vN4soAoOVqcBgZP368Dhw4oJkzZyonJ0cDBw7UkiVLage1ZmVl1WkJeeCBB2Sz2fTAAw9o3759atu2rcaOHas///nP7tsLoAlkHizVzS+v1c4DpbXjQ8YNjLO6LABo8Ro8z4gVmGcEVvvq+wOa+up6FZVXKyY0QP+amKgB7cOtLgsAmrX6fn9zbRrgFIwxWvj1Lv158Va5jDS4Q7gW/CpRUSEBVpcGAB6DMAKcREW1U398Z7PeSt0rSbo6sb3+fEU/OXwZ6wQA7kQYAU4gr7hct/8nVeuzCmS3SX+8pI9+fW4n2WwMVAUAdyOMAD+xNbtIN728VvsLyxUa4KunbxisET3aWl0WAHgswgjwP5Zty9Wdr21QWaVTXdsG6/lJQ9U5MtjqsgDAoxFGANUMVH3x613609GBqud2i9AzExIVFuhndWkA4PEII/B61U6XHv5gq/7z7W5J0nVD4/Xo5f3k59Pol24CAIgwAi9XUlGtqa+u1xc7Dshmk2aM6aVbhndhoCoANCHCCLxWXlG5bnxxrbbuL1KAn13zxg/S6H4xVpcFAF6HMAKvlJFXrEkL12pfwRFFBPtr4Y1DlRAfbnVZAOCVCCPwOmsy83Xzy2tVVF6tzpHBemnyUHWM4IwZALAKYQReZfGm/brnjTRVVrs0qEO4Xpg0VG2C/a0uCwC8GmEEXuP5r37Qnz/aJmOki/pE62/XDVKgP1O7A4DVCCPweC6X0Z8Wb9PCrzMlSb/6WUc9dFlf+dg5YwYAmgPCCDxaRbVT097YqMWb9kuS7hvTS7eN4NRdAGhOCCPwWCUV1brtP+v0dcYh+fnY9OQ1CRo3MM7qsgAAP0EYgUc6VFKhyS+t1aa9hQry99G/fjVE53WPtLosAMAJEEbgcfYeLtPEhWv0w4FStQn214vMIQIAzRphBB5lR26xJr6wRjlF5YoNC9C/b0pSt6hWVpcFADgFwgg8xvqsw5r84loVHqlSt6hW+s9Nw9QuLNDqsgAAp0EYgUdYkZ6nKa+s15EqpwbGh+vFG4eqNZOZAUCLQBhBi/f+xmxNW5SmapfRiB5t9eyEwQp2cGgDQEvBJzZatEVrs3Tf29/JGOmyhFg9eU2C/H3tVpcFAGgAwgharBe/ztTDH2yVJE1I6qBHx/WTnVlVAaDFIYygRZq/PENPLE2XJN0yvLPuv7g3s6oCQAtFGEGLYozRk5+ka/7ynZKku0Z2190p3QkiANCCEUbQYhhj9MiHW/Xi17skSTPG9NJt53e1tigAwFkjjKBFcLqM/vjOd3p97R5J0qPj+upXyZ2sLQoA4BaEETR7VU6X7n1zo95Ly5bdJj1+dYKuTmxvdVkAADchjKBZq3K6dNfrG/TRdznytdv0t+sG6ZIB7awuCwDgRoQRNFv/G0T8fex69peDNbJ3tNVlAQDcjDCCZqna6dLdr6fVBpEFvxqsC3sRRADAEzFVJZqdaqdLdy1K0+Lv9svPx6Znf0kQAQBPRhhBs1LtdOnuRWlavKkmiCz4ZSJdMwDg4QgjaDaqnS7d88ZGfXg0iDw7gSACAN6AMIJmodrp0rQ3NuqDjdny87HpmQmJSulDEAEAb0AYgeWcLqPfvblR7x8NIvNvGKxfEEQAwGsQRmApl8to+v9t0ntp2fK11wSRi/rGWF0WAKAJEUZgGWOMHv5gi95K3Ssfu01P3zCIIAIAXogwAss8sTRdL6/aLZtNevKaARrdj5lVAcAbEUZgifnLM/TMip2SpD9d3k9XDOJaMwDgrQgjaHIvfZ2pJ5amS5L+eHFvTUjqaHFFAAArEUbQpP4vda8e+mCrJOmukd11y4guFlcEALAaYQRN5vPtufrD/22SJN18XmfdndLd4ooAAM0BYQRNInX3Yf3m1fVyuoyuHByn+y/uLZvNZnVZAIBmgDCCRvd9brF+/dJalVe59POebfWXqwbIbieIAABqEEbQqLILjmjiwjUqPFKlgfHhembCYPn5cNgBAH7EtwIaTUFZpSYtXKP9heXq2jZYL944VEH+vlaXBQBoZggjaBRHKp266eV1+j6vRDGhAfr3TUlqHexvdVkAgGaIMAK3q3a6dMd/1yt192GFBvjq5V8PU1x4oNVlAQCaKcII3MoYoxlvf6dl2/Pk8LXrhRuHqmdMiNVlAQCaMcII3OrvyzL0Zupe2W3S0zcM1tBObawuCQDQzBFG4Dbvb8zWU5/tkCQ9enk//aJPtMUVAQBaAsII3CJ192Hd++ZGSTWzq3K9GQBAfRFGcNb2Fx7Rbf9JVWW1Sym9ozXj4t5WlwQAaEEIIzgr5VVO3f6fVB0sqVCvmBD97bqB8mF2VQBAAxBGcMaMMfrjO5u1cW+hwoP89K9fDVGwg0nNAAANQxjBGXv5m136v/VHz5y5frA6RARZXRIAoAUijOCMrNp5SI8u3iZJuv/i3jqve6TFFQEAWirCCBps7+EyTf3vejldRpcPjNVN53W2uiQAQAtGGEGDlFc5ddt/UpVfWql+caGac9UA2WwMWAUAnDnCCBpk1ntbtCW7SG2C/fXPXw1RgJ+P1SUBAFo4wgjqbdHaLC1at0d2m/SP6wdx8TsAgFsQRlAvm/cV6sH3tkiSfndRT53bjQGrAAD3IIzgtArLqjTl1ZoZVkf2itKU87taXRIAwIMQRnBKLpfRtDfStCf/iOLbBGrutQNlZ4ZVAIAbnVEYmT9/vjp16qSAgAAlJSVpzZo1p1y/oKBAU6dOVbt27eRwONSjRw999NFHZ1QwmtazX+zUsu158ve169kJiQoL8rO6JACAh2nw3N2LFi3StGnTtGDBAiUlJWnevHkaNWqU0tPTFRUVddz6lZWV+sUvfqGoqCi99dZbiouL0+7duxUeHu6O+tGIvv3hkP76Sbok6dFxfdUvLsziigAAnshmjDEN2SApKUlDhw7V008/LUlyuVyKj4/XnXfeqfvuu++49RcsWKAnnnhC27dvl5/fmf1VXVRUpLCwMBUWFio0NPSMXgMNc6ikQhf//SvlFlXo6sT2evKaBKtLAgC0MPX9/m5QN01lZaVSU1OVkpLy4wvY7UpJSdGqVatOuM3777+v5ORkTZ06VdHR0erXr58ee+wxOZ3Ok/6ciooKFRUV1bmh6RhjdO+bG5VbVKGubYP1yLi+VpcEAPBgDQojBw8elNPpVHR0dJ3l0dHRysnJOeE2P/zwg9566y05nU599NFHevDBB/XXv/5Vf/rTn076c2bPnq2wsLDaW3x8fEPKxFl6YWWmlqcfkL+vXU/fMFhB/lyJFwDQeBr9bBqXy6WoqCj961//UmJiosaPH68//vGPWrBgwUm3mTFjhgoLC2tve/bsaewycdSmvQX6y5LtkqQHL+2j3u3oFgMANK4G/ckbGRkpHx8f5ebm1lmem5urmJiYE27Trl07+fn5ycfnx2nDe/furZycHFVWVsrf3/+4bRwOhxwOR0NKgxsUl1fpztc2qMppNKZfjH6Z1MHqkgAAXqBBLSP+/v5KTEzUsmXLape5XC4tW7ZMycnJJ9zm3HPPVUZGhlwuV+2yHTt2qF27dicMIrCGMUZ/fGezdh8qU1x4oOZcyQXwAABNo8HdNNOmTdNzzz2nl19+Wdu2bdOUKVNUWlqqyZMnS5ImTpyoGTNm1K4/ZcoU5efn66677tKOHTu0ePFiPfbYY5o6dar79gJn7e31+/T+xmz52G36+/WDmE8EANBkGjwycfz48Tpw4IBmzpypnJwcDRw4UEuWLKkd1JqVlSW7/ceMEx8fr6VLl+qee+7RgAEDFBcXp7vuukvTp093317grOwrOKKH3q+57sy0X/RQYsfWFlcEAPAmDZ5nxArMM9J4XC6jX76wWt/sPKTEjq31xm3J8mG6dwCAGzTKPCPwPC+v2qVvdh5SoJ+P/npNAkEEANDkCCNeLCOvRHM+rjmN9/5LeqtTZLDFFQEAvBFhxEtVOV2a9kaaKqpdGtGjLafxAgAsQxjxUs8s36lNewsVGuCrx6/iNF4AgHUII17ou72F+sfn30uSHr28n2LCAiyuCADgzQgjXqai2qnfvZmmapfRJf3b6bKEWKtLAgB4OcKIl3n68wztyC1RRLC/Hr28H90zAADLEUa8yOZ9hXpmxU5J0iPj+qlNMNPxAwCsRxjxElVOl/7w1iY5XTUXwbtkQDurSwIAQBJhxGs8u2Kntu4vUniQnx4Z18/qcgAAqEUY8QLpOcW1Z888NLav2oY4LK4IAIAfEUY8XLXTpd+/tVFVTqOU3lEaN5CzZwAAzQthxMM9vzJTm/YWKiTAV3++oj9nzwAAmh3CiAfbk1+meZ/tkCQ9eGkfRYcyuRkAoPkhjHgoY4xmvb9F5VUu/axLG12T2N7qkgAAOCHCiIdauiVXn2/Pk5+PTX+6nO4ZAEDzRRjxQKUV1Xr4gy2SpFtHdFG3qFYWVwQAwMkRRjzQ35Z9r/2F5WrfOlB3XNDd6nIAADglwoiH2Z5TpBdWZkqSHhnXV4H+PhZXBADAqRFGPIjLZfTHdzbL6TIa3TdGF/aKtrokAABOizDiQd5M3aPU3YcV5O+jmWP7WF0OAAD1QhjxEPmllZr98XZJ0j0pPRQbHmhxRQAA1A9hxEPM+XibCsqq1CsmRDee28nqcgAAqDfCiAdYuytfb6zbK0n60+X95OfDrxUA0HLwrdXCVTldeuCdzZKk64bGa0inNhZXBABAwxBGWriXvt6l9NxitQn21/TRvawuBwCABiOMtGB5xeX627LvJUn3je6l1sH+FlcEAEDDEUZasCeWpKukoloJ7cN0NRfCAwC0UISRFmrjngK9mVozaHXWZX1lt3MhPABAy0QYaYFcLqOHjl4I78pBcRrcobXFFQEAcOYIIy3Qu2n7tCGrQEH+Ppo+hkGrAICWjTDSwpRUVGvO0ZlW77iwm6JDAyyuCACAs0MYaWHmL89QXnGFOkYE6abzOltdDgAAZ40w0oLsOliqF77KlCQ9cEkfOXx9LK4IAICzRxhpQWZ/vE2VTpeGd49USu8oq8sBAMAtCCMtROruw1q6JVd2mzTz0j6y2TiVFwDgGQgjLYAxRn9ZUjNo9erE9uoeHWJxRQAAuA9hpAVYkX5AazLz5e9r190pPawuBwAAtyKMNHMu14+tIjee00mx4YEWVwQAgHsRRpq59zbu0/acYoUE+Oo3P+9qdTkAALgdYaQZq6h26q+f7JAkTfl5V4UHcVVeAIDnIYw0Y69+m6W9h48oOtShyecwwRkAwDMRRpqp4vIqPb08Q5J018geCvRngjMAgGcijDRTz32VqfzSSnWJDNa1Q9pbXQ4AAI2GMNIMFZZVaeHKmmnf7x3VU74+/JoAAJ6Lb7lm6N+rdqmkolq9YkI0um+M1eUAANCoCCPNTFlltRZ+XdMqMuXnXWW3M+07AMCzEUaamdfX7NHhsip1aBOkS/q3s7ocAAAaHWGkGamsdum5r36QJN12fhfGigAAvALfds3Iu2n7tL+wXFEhDl01mDNoAADegTDSTLhcRv/6sqZV5KbzOivAj3lFAADegTDSTCzbnqeMvBKFOHx1Q1IHq8sBAKDJEEaaiX9+sVOSNOFnHRUS4GdxNQAANB3CSDOwble+1u0+LH8fuyaf28nqcgAAaFKEkWbgn0fHilwxKE7RoQEWVwMAQNMijFgsI69En27Nlc0m3TKii9XlAADQ5AgjFnvuaKvIL3pHq1tUK4urAQCg6RFGLJRbVK53NuyTJN12fleLqwEAwBqEEQu9+PUuVTpdGtqptRI7tra6HAAALEEYsUhxeZVe/Xa3JOm2EbSKAAC8F2HEIq+tyVJxRbW6RbXShb2irC4HAADLEEYs4HQZvfxNTavILcM7y263WVwRAADWIYxYYPn2PO0rOKLwID+NGxhndTkAAFiKMGKB/xwdK3LtkHguiAcA8HqEkSa262CpvthxQJI0gQviAQBwZmFk/vz56tSpkwICApSUlKQ1a9bUa7vXX39dNptNl19++Zn8WI/w6uqaVpHze7RVx4hgi6sBAMB6DQ4jixYt0rRp0zRr1iytX79eCQkJGjVqlPLy8k653a5du3Tvvfdq+PDhZ1xsS1de5dQb6/ZKkiYmd7S4GgAAmocGh5G5c+fqlltu0eTJk9WnTx8tWLBAQUFBWrhw4Um3cTqdmjBhgh5++GF16eK9119ZvGm/Co9UKS48UD/vyem8AABIDQwjlZWVSk1NVUpKyo8vYLcrJSVFq1atOul2jzzyiKKionTTTTfV6+dUVFSoqKiozs0TLFq3R5I0fmi8fDidFwAASQ0MIwcPHpTT6VR0dHSd5dHR0crJyTnhNitXrtQLL7yg5557rt4/Z/bs2QoLC6u9xcfHN6TMZinzYKnWZObLZpOuTmxvdTkAADQbjXo2TXFxsX71q1/pueeeU2RkZL23mzFjhgoLC2tve/bsacQqm8YbR1tFRnRvq9jwQIurAQCg+fBtyMqRkZHy8fFRbm5uneW5ubmKiYk5bv2dO3dq165dGjt2bO0yl8tV84N9fZWenq6uXY+/LovD4ZDD4WhIac1atdOlt1JrBq5eN7Tlt/IAAOBODWoZ8ff3V2JiopYtW1a7zOVyadmyZUpOTj5u/V69eum7775TWlpa7e2yyy7TBRdcoLS0NI/ofqmP5ekHdKC4QhHB/hrZO/r0GwAA4EUa1DIiSdOmTdOkSZM0ZMgQDRs2TPPmzVNpaakmT54sSZo4caLi4uI0e/ZsBQQEqF+/fnW2Dw8Pl6TjlnuyRWtrumiuGBQnf1/mmQMA4H81OIyMHz9eBw4c0MyZM5WTk6OBAwdqyZIltYNas7KyZLfzhXvMgeIKLU+vmYPlWrpoAAA4js0YY6wu4nSKiooUFhamwsJChYaGWl1Ogzz/1Q/60+JtSogP13tTz7W6HAAAmkx9v79pwmhExhi9eXTGVU7nBQDgxAgjjWjzviKl5xbL39euywbEWl0OAADNEmGkEb2VWjNw9aI+0QoL8rO4GgAAmifCSCOpqHbqvY3ZkqRrhjBwFQCAkyGMNJJl2/JUUFalmNAAndet/rPPAgDgbQgjjeTNo9O/Xzk4joviAQBwCoSRRpBXVK4vdhyQxFk0AACcDmGkEby9YZ9cRkrs2Fpd2rayuhwAAJo1woibGWNqL4pHqwgAAKdHGHGzjXsLlZFXogA/uy4Z0M7qcgAAaPYII252bODq6L4xCg1gbhEAAE6HMOJGFdVOfcDcIgAANAhhxI1WpB9QUXm1YkIDlNwlwupyAABoEQgjbnSsVeTSAe1kZ24RAADqhTDiJmWV1Vq2LU+SdNlALooHAEB9EUbc5NOtuTpS5VTHiCD1jwuzuhwAAFoMwoibfLBxvyRp7IBY2Wx00QAAUF+EETcoLKvSFzvoogEA4EwQRtxg6dYcVTmNekaHqEd0iNXlAADQohBG3ODDTUe7aBKYcRUAgIYijJylw6WV+jrjoCTpkgF00QAA0FCEkbP0ydYcOV1GvduFqnNksNXlAADQ4hBGztLi73Ik1Ux0BgAAGo4wchYOl1bqm6NdNBf3J4wAAHAmCCNn4ZOtOaqmiwYAgLNCGDkLx7poLukfY3ElAAC0XISRM1RQRhcNAADuQBg5Q59sya3tounStpXV5QAA0GIRRs7Qh9/VTHRGFw0AAGeHMHIG6KIBAMB9CCNngC4aAADchzByBhbTRQMAgNsQRhqooOzHa9HQRQMAwNkjjDTQsS6aXjEhdNEAAOAGhJEG+mhzTRcN16IBAMA9CCMNUFHt1Lc/HJIkXdSX8SIAALgDYaQB1u8uUHmVS21DHOoeRRcNAADuQBhpgG921gxcPadrhGw2m8XVAADgGQgjDXDsLJpzu0ZaXAkAAJ6DMFJPxeVV2ri3UJJ0TrcIi6sBAMBzEEbqaU1mvpwuo44RQWrfOsjqcgAA8BiEkXr6OqPmLJpz6KIBAMCtCCP1dGzw6rl00QAA4FaEkXo4WFKh7TnFkqTkLoQRAADciTBSD6t21nTR9IoJUUQrh8XVAADgWQgj9bA6syaMJHelVQQAAHcjjNTD6h/yJUlJnQkjAAC4G2HkNA6VVOj7vBJJ0rDObSyuBgAAz0MYOY21u2paRXpEt1KbYH+LqwEAwPMQRk5jdSZdNAAANCbCyGkcGy9CFw0AAI2DMHIKhUeqtC2nSJKURBgBAKBREEZOYd2ufBkjdY4MVlRogNXlAADgkQgjp7Dm6HiRYZ1oFQEAoLEQRk7h22ODV7sQRgAAaCyEkZMoq6zW5n2FkqShtIwAANBoCCMn8d3eQjldRtGhDrVvHWh1OQAAeCzCyEls2FMgSRoU31o2m83aYgAA8GCEkZPYkHVYkjSoQ7i1hQAA4OEIIydgjNH6rAJJ0uCOra0tBgAAD0cYOYHswnIdKK6Qr92mfrFhVpcDAIBHI4ycwPrdNV00vduFKtDfx+JqAADwbISRE9hwtIuG8SIAADQ+wsgJbNjD4FUAAJoKYeQnKqqd2rKv5uJ4g+IZvAoAQGM7ozAyf/58derUSQEBAUpKStKaNWtOuu5zzz2n4cOHq3Xr1mrdurVSUlJOub7VtmYXqdLpUusgP3WMCLK6HAAAPF6Dw8iiRYs0bdo0zZo1S+vXr1dCQoJGjRqlvLy8E66/YsUKXX/99Vq+fLlWrVql+Ph4XXTRRdq3b99ZF98YfhwvwmRnAAA0hQaHkblz5+qWW27R5MmT1adPHy1YsEBBQUFauHDhCdd/9dVX9Zvf/EYDBw5Ur1699Pzzz8vlcmnZsmVnXXxj2Li3QJI0MD7c0joAAPAWDQojlZWVSk1NVUpKyo8vYLcrJSVFq1atqtdrlJWVqaqqSm3anPzicxUVFSoqKqpzayrHLo7Xvz3ziwAA0BQaFEYOHjwop9Op6OjoOsujo6OVk5NTr9eYPn26YmNj6wSan5o9e7bCwsJqb/Hx8Q0p84yVVVbrh4OlkqS+saFN8jMBAPB2TXo2zZw5c/T666/rnXfeUUBAwEnXmzFjhgoLC2tve/bsaZL6tu0vkjFSVIhDUSEnrw8AALiPb0NWjoyMlI+Pj3Jzc+ssz83NVUxMzCm3ffLJJzVnzhx99tlnGjBgwCnXdTgccjgcDSnNLTYfPaW3XxxdNAAANJUGtYz4+/srMTGxzuDTY4NRk5OTT7rd448/rkcffVRLlizRkCFDzrzaRrYlu2a8CF00AAA0nQa1jEjStGnTNGnSJA0ZMkTDhg3TvHnzVFpaqsmTJ0uSJk6cqLi4OM2ePVuS9Je//EUzZ87Uf//7X3Xq1Kl2bEmrVq3UqlUrN+7K2TvWMtKXi+MBANBkGhxGxo8frwMHDmjmzJnKycnRwIEDtWTJktpBrVlZWbLbf2xwefbZZ1VZWamrr766zuvMmjVLDz300NlV70YV1U59n1csSeoXR8sIAABNxWaMMVYXcTpFRUUKCwtTYWGhQkMbJyhs3leoS/+xUuFBftrw4C+Y8AwAgLNU3+9vrk1z1LH5RfrGhhJEAABoQoSRo7ZkHz2ThvEiAAA0KcLIUZuPnUnDab0AADQpwogkp8to2/5jLSMMXgUAoCkRRiRlHixVeZVLQf4+6hQRbHU5AAB4FcKIpN2Haq5H06VtsOx2Bq8CANCUCCOSsvLLJEnxrYMsrgQAAO9DGJG0J/+IJKlDG8IIAABNjTCiH1tG2hNGAABocoQRSXuOhhFaRgAAaHpeH0aMMdpz+NiYkUCLqwEAwPt4fRg5VFqpskqnbDYpjjACAECT8/owcqyLJiY0QA5fH4urAQDA+3h9GKk9rZfxIgAAWMLrw8jewzWn9TLHCAAA1vD6MJJ1iDNpAACwEmHk2Gm9EQxeBQDACl4fRn48rZeWEQAArODVYaTK6VJ2AVPBAwBgJa8OI/sLyuUyksPXrrYhDqvLAQDAK3l1GPnf03ptNpvF1QAA4J0II2IaeAAArOTVYeTY4FXGiwAAYB2vDiPMvgoAgPW8OozsJYwAAGA5X6sLsNKEpI5K7Fis3jGhVpcCAIDX8uowcu3QeKtLAADA63l1Nw0AALAeYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAAS7WIq/YaYyRJRUVFFlcCAADq69j39rHv8ZNpEWGkuLhYkhQfH29xJQAAoKGKi4sVFhZ20udt5nRxpRlwuVzKzs5WSEiIbDab2163qKhI8fHx2rNnj0JDQ932ui0J7wHvgcR74O37L/EeSLwHkvvfA2OMiouLFRsbK7v95CNDWkTLiN1uV/v27Rvt9UNDQ732wDuG94D3QOI98Pb9l3gPJN4Dyb3vwalaRI5hACsAALAUYQQAAFjKq8OIw+HQrFmz5HA4rC7FMrwHvAcS74G377/EeyDxHkjWvQctYgArAADwXF7dMgIAAKxHGAEAAJYijAAAAEsRRgAAgKW8OozMnz9fnTp1UkBAgJKSkrRmzRqrS2oUs2fP1tChQxUSEqKoqChdfvnlSk9Pr7POz3/+c9lstjq322+/3aKK3e+hhx46bv969epV+3x5ebmmTp2qiIgItWrVSldddZVyc3MtrNj9OnXqdNx7YLPZNHXqVEmeeQx8+eWXGjt2rGJjY2Wz2fTuu+/Wed4Yo5kzZ6pdu3YKDAxUSkqKvv/++zrr5Ofna8KECQoNDVV4eLhuuukmlZSUNOFenJ1TvQdVVVWaPn26+vfvr+DgYMXGxmrixInKzs6u8xonOnbmzJnTxHtyZk53DNx4443H7dvo0aPrrOPJx4CkE34u2Gw2PfHEE7XrNPYx4LVhZNGiRZo2bZpmzZql9evXKyEhQaNGjVJeXp7VpbndF198oalTp+rbb7/Vp59+qqqqKl100UUqLS2ts94tt9yi/fv3194ef/xxiypuHH379q2zfytXrqx97p577tEHH3ygN998U1988YWys7N15ZVXWlit+61du7bO/n/66aeSpGuuuaZ2HU87BkpLS5WQkKD58+ef8PnHH39cf//737VgwQKtXr1awcHBGjVqlMrLy2vXmTBhgrZs2aJPP/1UH374ob788kvdeuutTbULZ+1U70FZWZnWr1+vBx98UOvXr9fbb7+t9PR0XXbZZcet+8gjj9Q5Nu68886mKP+sne4YkKTRo0fX2bfXXnutzvOefAxIqrPv+/fv18KFC2Wz2XTVVVfVWa9RjwHjpYYNG2amTp1a+9jpdJrY2Fgze/ZsC6tqGnl5eUaS+eKLL2qXnX/++eauu+6yrqhGNmvWLJOQkHDC5woKCoyfn5958803a5dt27bNSDKrVq1qogqb3l133WW6du1qXC6XMcbzjwFJ5p133ql97HK5TExMjHniiSdqlxUUFBiHw2Fee+01Y4wxW7duNZLM2rVra9f5+OOPjc1mM/v27Wuy2t3lp+/BiaxZs8ZIMrt3765d1rFjR/PUU081bnFN4ET7P2nSJDNu3LiTbuONx8C4cePMhRdeWGdZYx8DXtkyUllZqdTUVKWkpNQus9vtSklJ0apVqyysrGkUFhZKktq0aVNn+auvvqrIyEj169dPM2bMUFlZmRXlNZrvv/9esbGx6tKliyZMmKCsrCxJUmpqqqqqquocD7169VKHDh089niorKzUK6+8ol//+td1Lj7p6cfA/8rMzFROTk6d33tYWJiSkpJqf++rVq1SeHi4hgwZUrtOSkqK7Ha7Vq9e3eQ1N4XCwkLZbDaFh4fXWT5nzhxFRERo0KBBeuKJJ1RdXW1NgY1gxYoVioqKUs+ePTVlyhQdOnSo9jlvOwZyc3O1ePFi3XTTTcc915jHQIu4UJ67HTx4UE6nU9HR0XWWR0dHa/v27RZV1TRcLpfuvvtunXvuuerXr1/t8htuuEEdO3ZUbGysNm3apOnTpys9PV1vv/22hdW6T1JSkl566SX17NlT+/fv18MPP6zhw4dr8+bNysnJkb+//3EfvtHR0crJybGm4Eb27rvvqqCgQDfeeGPtMk8/Bn7q2O/2RJ8Dx57LyclRVFRUned9fX3Vpk0bjzw2ysvLNX36dF1//fV1LpL229/+VoMHD1abNm30zTffaMaMGdq/f7/mzp1rYbXuMXr0aF155ZXq3Lmzdu7cqfvvv19jxozRqlWr5OPj43XHwMsvv6yQkJDjuqkb+xjwyjDizaZOnarNmzfXGS8hqU7/Z//+/dWuXTuNHDlSO3fuVNeuXZu6TLcbM2ZM7b8HDBigpKQkdezYUW+88YYCAwMtrMwaL7zwgsaMGaPY2NjaZZ5+DODUqqqqdO2118oYo2effbbOc9OmTav994ABA+Tv76/bbrtNs2fPbvFTp1933XW1/+7fv78GDBigrl27asWKFRo5cqSFlVlj4cKFmjBhggICAuosb+xjwCu7aSIjI+Xj43Pc2RK5ubmKiYmxqKrGd8cdd+jDDz/U8uXL1b59+1Oum5SUJEnKyMhoitKaXHh4uHr06KGMjAzFxMSosrJSBQUFddbx1ONh9+7d+uyzz3TzzTefcj1PPwaO/W5P9TkQExNz3KD26upq5efne9SxcSyI7N69W59++ulpLx2flJSk6upq7dq1q2kKbEJdunRRZGRk7XHvLceAJH311VdKT08/7WeD5P5jwCvDiL+/vxITE7Vs2bLaZS6XS8uWLVNycrKFlTUOY4zuuOMOvfPOO/r888/VuXPn026TlpYmSWrXrl0jV2eNkpIS7dy5U+3atVNiYqL8/PzqHA/p6enKysryyOPhxRdfVFRUlC655JJTrufpx0Dnzp0VExNT5/deVFSk1atX1/7ek5OTVVBQoNTU1Np1Pv/8c7lcrtqw1tIdCyLff/+9PvvsM0VERJx2m7S0NNnt9uO6LzzB3r17dejQodrj3huOgWNeeOEFJSYmKiEh4bTruv0YaLShsc3c66+/bhwOh3nppZfM1q1bza233mrCw8NNTk6O1aW53ZQpU0xYWJhZsWKF2b9/f+2trKzMGGNMRkaGeeSRR8y6detMZmamee+990yXLl3MiBEjLK7cfX73u9+ZFStWmMzMTPP111+blJQUExkZafLy8owxxtx+++2mQ4cO5vPPPzfr1q0zycnJJjk52eKq3c/pdJoOHTqY6dOn11nuqcdAcXGx2bBhg9mwYYORZObOnWs2bNhQe6bInDlzTHh4uHnvvffMpk2bzLhx40znzp3NkSNHal9j9OjRZtCgQWb16tVm5cqVpnv37ub666+3apca7FTvQWVlpbnssstM+/btTVpaWp3Ph4qKCmOMMd9884156qmnTFpamtm5c6d55ZVXTNu2bc3EiRMt3rP6OdX+FxcXm3vvvdesWrXKZGZmms8++8wMHjzYdO/e3ZSXl9e+hicfA8cUFhaaoKAg8+yzzx63fVMcA14bRowx5h//+Ifp0KGD8ff3N8OGDTPffvut1SU1CkknvL344ovGGGOysrLMiBEjTJs2bYzD4TDdunUzv//9701hYaG1hbvR+PHjTbt27Yy/v7+Ji4sz48ePNxkZGbXPHzlyxPzmN78xrVu3NkFBQeaKK64w+/fvt7DixrF06VIjyaSnp9dZ7qnHwPLly0947E+aNMkYU3N674MPPmiio6ONw+EwI0eOPO69OXTokLn++utNq1atTGhoqJk8ebIpLi62YG/OzKneg8zMzJN+PixfvtwYY0xqaqpJSkoyYWFhJiAgwPTu3ds89thjdb6sm7NT7X9ZWZm56KKLTNu2bY2fn5/p2LGjueWWW477o9STj4Fj/vnPf5rAwEBTUFBw3PZNcQzYjDHGPW0sAAAADeeVY0YAAEDzQRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKX+H58Q2YdjHImeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cummulation = np.cumsum(weight)\n",
    "cummulation\n",
    "plt.plot(cummulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.03098839+0.j, -1.70724071+0.j, -0.70166208+0.j, ...,\n",
       "         0.62823782+0.j, -1.108276  +0.j, -0.12674551+0.j],\n",
       "       [-1.13949644+0.j,  2.67515781+0.j, -1.21168659+0.j, ...,\n",
       "        -0.15226293+0.j,  0.23719521+0.j, -0.30347162+0.j],\n",
       "       [-1.28108642+0.j, -4.15058859+0.j, -1.9968919 +0.j, ...,\n",
       "         0.32529305+0.j, -0.89379632+0.j,  0.04978111+0.j],\n",
       "       ...,\n",
       "       [12.75633816+0.j,  2.21713921+0.j, -0.37979178+0.j, ...,\n",
       "        -1.34579477+0.j,  0.26304227+0.j,  0.96790177+0.j],\n",
       "       [-1.07178349+0.j, -2.53646503+0.j,  1.39103057+0.j, ...,\n",
       "        -0.100276  +0.j,  0.10325074+0.j, -0.57108643+0.j],\n",
       "       [-2.32969221+0.j,  0.17596515+0.j,  0.52728289+0.j, ...,\n",
       "        -0.018208  +0.j,  0.60802061+0.j,  0.17901644+0.j]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=0.05244203886433046\n",
      "GD iter. 1/99: loss=0.046992050467393895\n",
      "GD iter. 2/99: loss=0.04426576713125997\n",
      "GD iter. 3/99: loss=0.042563377719410544\n",
      "GD iter. 4/99: loss=0.04132919865216976\n",
      "GD iter. 5/99: loss=0.040354035147907696\n",
      "GD iter. 6/99: loss=0.03954754346090279\n",
      "GD iter. 7/99: loss=0.03886356211029585\n",
      "GD iter. 8/99: loss=0.03827448933376379\n",
      "GD iter. 9/99: loss=0.037761789943821185\n",
      "GD iter. 10/99: loss=0.03731205332335607\n",
      "GD iter. 11/99: loss=0.03691511165219017\n",
      "GD iter. 12/99: loss=0.03656301129305868\n",
      "GD iter. 13/99: loss=0.03624938717790965\n",
      "GD iter. 14/99: loss=0.03596905333966319\n",
      "GD iter. 15/99: loss=0.03571772162057294\n",
      "GD iter. 16/99: loss=0.03549180186495732\n",
      "GD iter. 17/99: loss=0.03528825635584409\n",
      "GD iter. 18/99: loss=0.03510449152502497\n",
      "GD iter. 19/99: loss=0.03493827589265606\n",
      "GD iter. 20/99: loss=0.034787676830929246\n",
      "GD iter. 21/99: loss=0.03465101107410429\n",
      "GD iter. 22/99: loss=0.034526805429352464\n",
      "GD iter. 23/99: loss=0.034413765172837035\n",
      "GD iter. 24/99: loss=0.03431074831976222\n",
      "GD iter. 25/99: loss=0.03421674444593442\n",
      "GD iter. 26/99: loss=0.03413085708225855\n",
      "GD iter. 27/99: loss=0.03405228894861146\n",
      "GD iter. 28/99: loss=0.033980329470255886\n",
      "GD iter. 29/99: loss=0.03391434414893317\n",
      "GD iter. 30/99: loss=0.03385376545598878\n",
      "GD iter. 31/99: loss=0.033798084985975176\n",
      "GD iter. 32/99: loss=0.033746846662841244\n",
      "GD iter. 33/99: loss=0.03369964083177053\n",
      "GD iter. 34/99: loss=0.03365609910131316\n",
      "GD iter. 35/99: loss=0.033615889825066206\n",
      "GD iter. 36/99: loss=0.03357871413152608\n",
      "GD iter. 37/99: loss=0.03354430242612777\n",
      "GD iter. 38/99: loss=0.03351241130182997\n",
      "GD iter. 39/99: loss=0.0334828208045926\n",
      "GD iter. 40/99: loss=0.03345533200824179\n",
      "GD iter. 41/99: loss=0.033429764859917076\n",
      "GD iter. 42/99: loss=0.033405956262844726\n",
      "GD iter. 43/99: loss=0.03338375836780868\n",
      "GD iter. 44/99: loss=0.033363037048573706\n",
      "GD iter. 45/99: loss=0.033343670539792716\n",
      "GD iter. 46/99: loss=0.033325548218711594\n",
      "GD iter. 47/99: loss=0.0333085695143559\n",
      "GD iter. 48/99: loss=0.03329264292991561\n",
      "GD iter. 49/99: loss=0.03327768516579062\n",
      "GD iter. 50/99: loss=0.033263620332268366\n",
      "GD iter. 51/99: loss=0.03325037924211136\n",
      "GD iter. 52/99: loss=0.03323789877446797\n",
      "GD iter. 53/99: loss=0.033226121302509314\n",
      "GD iter. 54/99: loss=0.03321499417806043\n",
      "GD iter. 55/99: loss=0.03320446926725071\n",
      "GD iter. 56/99: loss=0.03319450253187451\n",
      "GD iter. 57/99: loss=0.03318505365173748\n",
      "GD iter. 58/99: loss=0.033176085683780304\n",
      "GD iter. 59/99: loss=0.033167564754226726\n",
      "GD iter. 60/99: loss=0.03315945978040574\n",
      "GD iter. 61/99: loss=0.033151742219254564\n",
      "GD iter. 62/99: loss=0.033144385839825354\n",
      "GD iter. 63/99: loss=0.03313736651739988\n",
      "GD iter. 64/99: loss=0.03313066204706598\n",
      "GD iter. 65/99: loss=0.033124251974832514\n",
      "GD iter. 66/99: loss=0.03311811744455716\n",
      "GD iter. 67/99: loss=0.03311224105913881\n",
      "GD iter. 68/99: loss=0.033106606754583705\n",
      "GD iter. 69/99: loss=0.033101199685695665\n",
      "GD iter. 70/99: loss=0.033096006122266944\n",
      "GD iter. 71/99: loss=0.03309101335475897\n",
      "GD iter. 72/99: loss=0.0330862096085632\n",
      "GD iter. 73/99: loss=0.03308158396602326\n",
      "GD iter. 74/99: loss=0.03307712629548023\n",
      "GD iter. 75/99: loss=0.033072827186676215\n",
      "GD iter. 76/99: loss=0.033068677891916476\n",
      "GD iter. 77/99: loss=0.03306467027244925\n",
      "GD iter. 78/99: loss=0.03306079674957513\n",
      "GD iter. 79/99: loss=0.033057050260045365\n",
      "GD iter. 80/99: loss=0.03305342421535108\n",
      "GD iter. 81/99: loss=0.03304991246454384\n",
      "GD iter. 82/99: loss=0.033046509260262646\n",
      "GD iter. 83/99: loss=0.03304320922767333\n",
      "GD iter. 84/99: loss=0.033040007336054895\n",
      "GD iter. 85/99: loss=0.03303689887279195\n",
      "GD iter. 86/99: loss=0.03303387941955579\n",
      "GD iter. 87/99: loss=0.03303094483047692\n",
      "GD iter. 88/99: loss=0.03302809121213058\n",
      "GD iter. 89/99: loss=0.03302531490517325\n",
      "GD iter. 90/99: loss=0.03302261246748371\n",
      "GD iter. 91/99: loss=0.03301998065867559\n",
      "GD iter. 92/99: loss=0.03301741642586067\n",
      "GD iter. 93/99: loss=0.0330149168905535\n",
      "GD iter. 94/99: loss=0.03301247933661807\n",
      "GD iter. 95/99: loss=0.033010101199166024\n",
      "GD iter. 96/99: loss=0.033007780054324594\n",
      "GD iter. 97/99: loss=0.03300551360979969\n",
      "GD iter. 98/99: loss=0.03300329969616629\n",
      "GD iter. 99/99: loss=0.033001136258824657\n",
      "The Accuracy is: 0.6256\n",
      "The F1 score is: 0.2954\n",
      "The precision is: 0.1760\n",
      "The recall is: 0.9180\n"
     ]
    }
   ],
   "source": [
    "## linear regression using all the features except for those having NaN values over 50% ##\n",
    "\n",
    "initial_w = np.random.randn(x_train_processed.shape[1]) * 0.01\n",
    "w, loss = mean_square_error_gd(y_train_processed, x_train_processed, initial_w, max_iters = 100, gamma=0.05)\n",
    "y_pred = y_pred = x_train_processed @ w\n",
    "y_pred_mean = np.mean(y_pred)\n",
    "predict_acc(x_train_processed, y_train_processed, w, logistic=False, threshold=y_pred_mean)\n",
    "predict_f1(x_train_processed, y_train_processed, w, logistic=False, threshold=y_pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/499: loss=0.6927936775859926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 1/499: loss=0.6584336164691538\n",
      "GD iter. 2/499: loss=0.6305150074092856\n",
      "GD iter. 3/499: loss=0.6062358027722053\n",
      "GD iter. 4/499: loss=0.5844614076094538\n",
      "GD iter. 5/499: loss=0.5646146058983955\n",
      "GD iter. 6/499: loss=0.5463629125631273\n",
      "GD iter. 7/499: loss=0.5294911449104092\n",
      "GD iter. 8/499: loss=0.5138442083783207\n",
      "GD iter. 9/499: loss=0.4993004001075236\n",
      "GD iter. 10/499: loss=0.4857584729233072\n",
      "GD iter. 11/499: loss=0.473131054483384\n",
      "GD iter. 12/499: loss=0.46134107779844663\n",
      "GD iter. 13/499: loss=0.45031968371295567\n",
      "GD iter. 14/499: loss=0.44000487190657245\n",
      "GD iter. 15/499: loss=0.4303405524726676\n",
      "GD iter. 16/499: loss=0.4212758260942951\n",
      "GD iter. 17/499: loss=0.41276440493330696\n",
      "GD iter. 18/499: loss=0.40476412742966644\n",
      "GD iter. 19/499: loss=0.3972365407860924\n",
      "GD iter. 20/499: loss=0.39014653552509654\n",
      "GD iter. 21/499: loss=0.38346202216750813\n",
      "GD iter. 22/499: loss=0.37715364322597833\n",
      "GD iter. 23/499: loss=0.37119451553686833\n",
      "GD iter. 24/499: loss=0.3655599990775419\n",
      "GD iter. 25/499: loss=0.36022748914802044\n",
      "GD iter. 26/499: loss=0.3551762293028616\n",
      "GD iter. 27/499: loss=0.3503871427915496\n",
      "GD iter. 28/499: loss=0.34584268055402384\n",
      "GD iter. 29/499: loss=0.34152668405105474\n",
      "GD iter. 30/499: loss=0.3374242614039545\n",
      "GD iter. 31/499: loss=0.3335216754848461\n",
      "GD iter. 32/499: loss=0.32980624274387166\n",
      "GD iter. 33/499: loss=0.32626624168752927\n",
      "GD iter. 34/499: loss=0.322890830035677\n",
      "GD iter. 35/499: loss=0.3196699696857443\n",
      "GD iter. 36/499: loss=0.3165943587029351\n",
      "GD iter. 37/499: loss=0.3136553696359462\n",
      "GD iter. 38/499: loss=0.31084499353005934\n",
      "GD iter. 39/499: loss=0.3081557890742288\n",
      "GD iter. 40/499: loss=0.30558083637682765\n",
      "GD iter. 41/499: loss=0.30311369491669526\n",
      "GD iter. 42/499: loss=0.30074836526266774\n",
      "GD iter. 43/499: loss=0.2984792541964583\n",
      "GD iter. 44/499: loss=0.2963011429110526\n",
      "GD iter. 45/499: loss=0.2942091579901817\n",
      "GD iter. 46/499: loss=0.29219874490431325\n",
      "GD iter. 47/499: loss=0.29026564378535424\n",
      "GD iter. 48/499: loss=0.2884058672661958\n",
      "GD iter. 49/499: loss=0.2866156801926569\n",
      "GD iter. 50/499: loss=0.28489158103457635\n",
      "GD iter. 51/499: loss=0.28323028483998325\n",
      "GD iter. 52/499: loss=0.2816287075916672\n",
      "GD iter. 53/499: loss=0.2800839518392673\n",
      "GD iter. 54/499: loss=0.2785932934923699\n",
      "GD iter. 55/499: loss=0.27715416967118883\n",
      "GD iter. 56/499: loss=0.27576416752136707\n",
      "GD iter. 57/499: loss=0.27442101390837004\n",
      "GD iter. 58/499: loss=0.27312256591497047\n",
      "GD iter. 59/499: loss=0.2718668020725442\n",
      "GD iter. 60/499: loss=0.27065181426338425\n",
      "GD iter. 61/499: loss=0.26947580023708145\n",
      "GD iter. 62/499: loss=0.268337056689284\n",
      "GD iter. 63/499: loss=0.267233972855882\n",
      "GD iter. 64/499: loss=0.2661650245799417\n",
      "GD iter. 65/499: loss=0.2651287688125677\n",
      "GD iter. 66/499: loss=0.2641238385123556\n",
      "GD iter. 67/499: loss=0.2631489379112392\n",
      "GD iter. 68/499: loss=0.26220283811738765\n",
      "GD iter. 69/499: loss=0.26128437302837265\n",
      "GD iter. 70/499: loss=0.2603924355301661\n",
      "GD iter. 71/499: loss=0.25952597395962995\n",
      "GD iter. 72/499: loss=0.2586839888100819\n",
      "GD iter. 73/499: loss=0.25786552966125476\n",
      "GD iter. 74/499: loss=0.257069692316543\n",
      "GD iter. 75/499: loss=0.25629561613186413\n",
      "GD iter. 76/499: loss=0.2555424815217687\n",
      "GD iter. 77/499: loss=0.25480950762961185\n",
      "GD iter. 78/499: loss=0.25409595014968334\n",
      "GD iter. 79/499: loss=0.25340109929017723\n",
      "GD iter. 80/499: loss=0.25272427786677437\n",
      "GD iter. 81/499: loss=0.25206483951743447\n",
      "GD iter. 82/499: loss=0.25142216702973846\n",
      "GD iter. 83/499: loss=0.2507956707728034\n",
      "GD iter. 84/499: loss=0.2501847872264219\n",
      "GD iter. 85/499: loss=0.24958897760063897\n",
      "GD iter. 86/499: loss=0.24900772653951433\n",
      "GD iter. 87/499: loss=0.2484405409032841\n",
      "GD iter. 88/499: loss=0.24788694862358507\n",
      "GD iter. 89/499: loss=0.24734649762680122\n",
      "GD iter. 90/499: loss=0.24681875482096277\n",
      "GD iter. 91/499: loss=0.24630330514196894\n",
      "GD iter. 92/499: loss=0.24579975065521564\n",
      "GD iter. 93/499: loss=0.24530770970899687\n",
      "GD iter. 94/499: loss=0.2448268161363113\n",
      "GD iter. 95/499: loss=0.24435671850195043\n",
      "GD iter. 96/499: loss=0.2438970793919656\n",
      "GD iter. 97/499: loss=0.24344757474281964\n",
      "GD iter. 98/499: loss=0.24300789320771893\n",
      "GD iter. 99/499: loss=0.2425777355577942\n",
      "GD iter. 100/499: loss=0.24215681411596565\n",
      "GD iter. 101/499: loss=0.24174485222147207\n",
      "GD iter. 102/499: loss=0.24134158372318554\n",
      "GD iter. 103/499: loss=0.2409467524999624\n",
      "GD iter. 104/499: loss=0.24056011200639318\n",
      "GD iter. 105/499: loss=0.24018142484243213\n",
      "GD iter. 106/499: loss=0.23981046234548287\n",
      "GD iter. 107/499: loss=0.23944700420361298\n",
      "GD iter. 108/499: loss=0.2390908380886575\n",
      "GD iter. 109/499: loss=0.2387417593080511\n",
      "GD iter. 110/499: loss=0.2383995704743063\n",
      "GD iter. 111/499: loss=0.23806408119112316\n",
      "GD iter. 112/499: loss=0.23773510775518003\n",
      "GD iter. 113/499: loss=0.23741247287271816\n",
      "GD iter. 114/499: loss=0.2370960053900867\n",
      "GD iter. 115/499: loss=0.23678554003746644\n",
      "GD iter. 116/499: loss=0.23648091718504283\n",
      "GD iter. 117/499: loss=0.2361819826109392\n",
      "GD iter. 118/499: loss=0.2358885872802679\n",
      "GD iter. 119/499: loss=0.23560058713469206\n",
      "GD iter. 120/499: loss=0.23531784289193278\n",
      "GD iter. 121/499: loss=0.23504021985468423\n",
      "GD iter. 122/499: loss=0.23476758772843798\n",
      "GD iter. 123/499: loss=0.2344998204477422\n",
      "GD iter. 124/499: loss=0.23423679601045247\n",
      "GD iter. 125/499: loss=0.23397839631955605\n",
      "GD iter. 126/499: loss=0.23372450703217576\n",
      "GD iter. 127/499: loss=0.2334750174153833\n",
      "GD iter. 128/499: loss=0.23322982020847158\n",
      "GD iter. 129/499: loss=0.23298881149135778\n",
      "GD iter. 130/499: loss=0.2327518905588062\n",
      "GD iter. 131/499: loss=0.23251895980017678\n",
      "GD iter. 132/499: loss=0.23228992458442546\n",
      "GD iter. 133/499: loss=0.23206469315009165\n",
      "GD iter. 134/499: loss=0.23184317650003\n",
      "GD iter. 135/499: loss=0.23162528830065068\n",
      "GD iter. 136/499: loss=0.23141094478545016\n",
      "GD iter. 137/499: loss=0.23120006466262247\n",
      "GD iter. 138/499: loss=0.2309925690265555\n",
      "GD iter. 139/499: loss=0.23078838127302606\n",
      "GD iter. 140/499: loss=0.2305874270179173\n",
      "GD iter. 141/499: loss=0.23038963401929058\n",
      "GD iter. 142/499: loss=0.23019493210265626\n",
      "GD iter. 143/499: loss=0.2300032530892912\n",
      "GD iter. 144/499: loss=0.2298145307274632\n",
      "GD iter. 145/499: loss=0.22962870062642665\n",
      "GD iter. 146/499: loss=0.2294457001930627\n",
      "GD iter. 147/499: loss=0.22926546857104269\n",
      "GD iter. 148/499: loss=0.22908794658239995\n",
      "GD iter. 149/499: loss=0.22891307667140234\n",
      "GD iter. 150/499: loss=0.22874080285062046\n",
      "GD iter. 151/499: loss=0.22857107064909443\n",
      "GD iter. 152/499: loss=0.22840382706250623\n",
      "GD iter. 153/499: loss=0.22823902050526798\n",
      "GD iter. 154/499: loss=0.22807660076444325\n",
      "GD iter. 155/499: loss=0.22791651895542075\n",
      "GD iter. 156/499: loss=0.22775872747926326\n",
      "GD iter. 157/499: loss=0.22760317998166174\n",
      "GD iter. 158/499: loss=0.22744983131342353\n",
      "GD iter. 159/499: loss=0.2272986374924301\n",
      "GD iter. 160/499: loss=0.22714955566700187\n",
      "GD iter. 161/499: loss=0.22700254408061007\n",
      "GD iter. 162/499: loss=0.22685756203787996\n",
      "GD iter. 163/499: loss=0.22671456987183022\n",
      "GD iter. 164/499: loss=0.2265735289122983\n",
      "GD iter. 165/499: loss=0.22643440145550198\n",
      "GD iter. 166/499: loss=0.22629715073469017\n",
      "GD iter. 167/499: loss=0.22616174089183924\n",
      "GD iter. 168/499: loss=0.22602813695035143\n",
      "GD iter. 169/499: loss=0.22589630478871509\n",
      "GD iter. 170/499: loss=0.22576621111508824\n",
      "GD iter. 171/499: loss=0.225637823442768\n",
      "GD iter. 172/499: loss=0.2255111100665108\n",
      "GD iter. 173/499: loss=0.22538604003966972\n",
      "GD iter. 174/499: loss=0.2252625831521162\n",
      "GD iter. 175/499: loss=0.22514070990891608\n",
      "GD iter. 176/499: loss=0.2250203915097298\n",
      "GD iter. 177/499: loss=0.22490159982890906\n",
      "GD iter. 178/499: loss=0.2247843073962626\n",
      "GD iter. 179/499: loss=0.22466848737846581\n",
      "GD iter. 180/499: loss=0.22455411356108904\n",
      "GD iter. 181/499: loss=0.22444116033122133\n",
      "GD iter. 182/499: loss=0.22432960266066734\n",
      "GD iter. 183/499: loss=0.2242194160896942\n",
      "GD iter. 184/499: loss=0.2241105767113102\n",
      "GD iter. 185/499: loss=0.22400306115605348\n",
      "GD iter. 186/499: loss=0.223896846577272\n",
      "GD iter. 187/499: loss=0.22379191063687756\n",
      "GD iter. 188/499: loss=0.2236882314915555\n",
      "GD iter. 189/499: loss=0.2235857877794135\n",
      "GD iter. 190/499: loss=0.2234845586070539\n",
      "GD iter. 191/499: loss=0.22338452353705415\n",
      "GD iter. 192/499: loss=0.22328566257583923\n",
      "GD iter. 193/499: loss=0.22318795616193451\n",
      "GD iter. 194/499: loss=0.22309138515458315\n",
      "GD iter. 195/499: loss=0.22299593082271577\n",
      "GD iter. 196/499: loss=0.2229015748342607\n",
      "GD iter. 197/499: loss=0.22280829924578163\n",
      "GD iter. 198/499: loss=0.2227160864924314\n",
      "GD iter. 199/499: loss=0.22262491937821152\n",
      "GD iter. 200/499: loss=0.22253478106652624\n",
      "GD iter. 201/499: loss=0.22244565507102076\n",
      "GD iter. 202/499: loss=0.22235752524669442\n",
      "GD iter. 203/499: loss=0.22227037578127876\n",
      "GD iter. 204/499: loss=0.22218419118687188\n",
      "GD iter. 205/499: loss=0.22209895629182008\n",
      "GD iter. 206/499: loss=0.22201465623283836\n",
      "GD iter. 207/499: loss=0.22193127644736219\n",
      "GD iter. 208/499: loss=0.22184880266612222\n",
      "GD iter. 209/499: loss=0.22176722090593445\n",
      "GD iter. 210/499: loss=0.22168651746269916\n",
      "GD iter. 211/499: loss=0.22160667890460164\n",
      "GD iter. 212/499: loss=0.2215276920655076\n",
      "GD iter. 213/499: loss=0.22144954403854664\n",
      "GD iter. 214/499: loss=0.22137222216987903\n",
      "GD iter. 215/499: loss=0.22129571405263818\n",
      "GD iter. 216/499: loss=0.22122000752104368\n",
      "GD iter. 217/499: loss=0.22114509064468038\n",
      "GD iter. 218/499: loss=0.22107095172293628\n",
      "GD iter. 219/499: loss=0.2209975792795955\n",
      "GD iter. 220/499: loss=0.22092496205758144\n",
      "GD iter. 221/499: loss=0.22085308901384415\n",
      "GD iter. 222/499: loss=0.2207819493143889\n",
      "GD iter. 223/499: loss=0.22071153232944024\n",
      "GD iter. 224/499: loss=0.22064182762873782\n",
      "GD iter. 225/499: loss=0.22057282497696018\n",
      "GD iter. 226/499: loss=0.22050451432927165\n",
      "GD iter. 227/499: loss=0.22043688582698936\n",
      "GD iter. 228/499: loss=0.22036992979336661\n",
      "GD iter. 229/499: loss=0.2203036367294878\n",
      "GD iter. 230/499: loss=0.22023799731027371\n",
      "GD iter. 231/499: loss=0.22017300238059148\n",
      "GD iter. 232/499: loss=0.22010864295146781\n",
      "GD iter. 233/499: loss=0.22004491019640093\n",
      "GD iter. 234/499: loss=0.21998179544776983\n",
      "GD iter. 235/499: loss=0.21991929019333614\n",
      "GD iter. 236/499: loss=0.21985738607283747\n",
      "GD iter. 237/499: loss=0.21979607487466854\n",
      "GD iter. 238/499: loss=0.21973534853264726\n",
      "GD iter. 239/499: loss=0.21967519912286457\n",
      "GD iter. 240/499: loss=0.21961561886061395\n",
      "GD iter. 241/499: loss=0.2195566000973997\n",
      "GD iter. 242/499: loss=0.2194981353180202\n",
      "GD iter. 243/499: loss=0.21944021713772582\n",
      "GD iter. 244/499: loss=0.21938283829944705\n",
      "GD iter. 245/499: loss=0.21932599167109357\n",
      "GD iter. 246/499: loss=0.219269670242919\n",
      "GD iter. 247/499: loss=0.21921386712495255\n",
      "GD iter. 248/499: loss=0.21915857554449364\n",
      "GD iter. 249/499: loss=0.21910378884366805\n",
      "GD iter. 250/499: loss=0.21904950047704497\n",
      "GD iter. 251/499: loss=0.21899570400931143\n",
      "GD iter. 252/499: loss=0.21894239311300412\n",
      "GD iter. 253/499: loss=0.21888956156629605\n",
      "GD iter. 254/499: loss=0.21883720325083678\n",
      "GD iter. 255/499: loss=0.2187853121496446\n",
      "GD iter. 256/499: loss=0.21873388234505012\n",
      "GD iter. 257/499: loss=0.21868290801668763\n",
      "GD iter. 258/499: loss=0.21863238343953595\n",
      "GD iter. 259/499: loss=0.21858230298200487\n",
      "GD iter. 260/499: loss=0.218532661104067\n",
      "GD iter. 261/499: loss=0.218483452355434\n",
      "GD iter. 262/499: loss=0.2184346713737756\n",
      "GD iter. 263/499: loss=0.21838631288297994\n",
      "GD iter. 264/499: loss=0.21833837169145504\n",
      "GD iter. 265/499: loss=0.21829084269046964\n",
      "GD iter. 266/499: loss=0.21824372085253202\n",
      "GD iter. 267/499: loss=0.21819700122980681\n",
      "GD iter. 268/499: loss=0.21815067895256815\n",
      "GD iter. 269/499: loss=0.21810474922768788\n",
      "GD iter. 270/499: loss=0.2180592073371587\n",
      "GD iter. 271/499: loss=0.21801404863665094\n",
      "GD iter. 272/499: loss=0.2179692685541016\n",
      "GD iter. 273/499: loss=0.21792486258833593\n",
      "GD iter. 274/499: loss=0.2178808263077195\n",
      "GD iter. 275/499: loss=0.21783715534884085\n",
      "GD iter. 276/499: loss=0.2177938454152237\n",
      "GD iter. 277/499: loss=0.2177508922760673\n",
      "GD iter. 278/499: loss=0.21770829176501552\n",
      "GD iter. 279/499: loss=0.21766603977895252\n",
      "GD iter. 280/499: loss=0.21762413227682562\n",
      "GD iter. 281/499: loss=0.21758256527849343\n",
      "GD iter. 282/499: loss=0.2175413348635996\n",
      "GD iter. 283/499: loss=0.2175004371704713\n",
      "GD iter. 284/499: loss=0.21745986839504108\n",
      "GD iter. 285/499: loss=0.2174196247897928\n",
      "GD iter. 286/499: loss=0.21737970266272985\n",
      "GD iter. 287/499: loss=0.21734009837636614\n",
      "GD iter. 288/499: loss=0.21730080834673796\n",
      "GD iter. 289/499: loss=0.21726182904243774\n",
      "GD iter. 290/499: loss=0.2172231569836681\n",
      "GD iter. 291/499: loss=0.217184788741316\n",
      "GD iter. 292/499: loss=0.21714672093604662\n",
      "GD iter. 293/499: loss=0.21710895023741633\n",
      "GD iter. 294/499: loss=0.2170714733630042\n",
      "GD iter. 295/499: loss=0.2170342870775622\n",
      "GD iter. 296/499: loss=0.2169973881921824\n",
      "GD iter. 297/499: loss=0.21696077356348228\n",
      "GD iter. 298/499: loss=0.21692444009280673\n",
      "GD iter. 299/499: loss=0.21688838472544628\n",
      "GD iter. 300/499: loss=0.21685260444987195\n",
      "GD iter. 301/499: loss=0.21681709629698548\n",
      "GD iter. 302/499: loss=0.2167818573393853\n",
      "GD iter. 303/499: loss=0.21674688469064696\n",
      "GD iter. 304/499: loss=0.21671217550461885\n",
      "GD iter. 305/499: loss=0.21667772697473198\n",
      "GD iter. 306/499: loss=0.21664353633332348\n",
      "GD iter. 307/499: loss=0.21660960085097428\n",
      "GD iter. 308/499: loss=0.2165759178358598\n",
      "GD iter. 309/499: loss=0.21654248463311354\n",
      "GD iter. 310/499: loss=0.21650929862420373\n",
      "GD iter. 311/499: loss=0.21647635722632244\n",
      "GD iter. 312/499: loss=0.21644365789178632\n",
      "GD iter. 313/499: loss=0.21641119810744985\n",
      "GD iter. 314/499: loss=0.21637897539412992\n",
      "GD iter. 315/499: loss=0.21634698730604152\n",
      "GD iter. 316/499: loss=0.21631523143024484\n",
      "GD iter. 317/499: loss=0.21628370538610328\n",
      "GD iter. 318/499: loss=0.21625240682475153\n",
      "GD iter. 319/499: loss=0.21622133342857455\n",
      "GD iter. 320/499: loss=0.2161904829106963\n",
      "GD iter. 321/499: loss=0.21615985301447876\n",
      "GD iter. 322/499: loss=0.21612944151302985\n",
      "GD iter. 323/499: loss=0.21609924620872192\n",
      "GD iter. 324/499: loss=0.21606926493271833\n",
      "GD iter. 325/499: loss=0.21603949554451005\n",
      "GD iter. 326/499: loss=0.2160099359314602\n",
      "GD iter. 327/499: loss=0.21598058400835815\n",
      "GD iter. 328/499: loss=0.21595143771698158\n",
      "GD iter. 329/499: loss=0.21592249502566632\n",
      "GD iter. 330/499: loss=0.2158937539288855\n",
      "GD iter. 331/499: loss=0.21586521244683549\n",
      "GD iter. 332/499: loss=0.2158368686250304\n",
      "GD iter. 333/499: loss=0.21580872053390326\n",
      "GD iter. 334/499: loss=0.21578076626841575\n",
      "GD iter. 335/499: loss=0.2157530039476742\n",
      "GD iter. 336/499: loss=0.215725431714553\n",
      "GD iter. 337/499: loss=0.21569804773532547\n",
      "GD iter. 338/499: loss=0.2156708501993006\n",
      "GD iter. 339/499: loss=0.21564383731846715\n",
      "GD iter. 340/499: loss=0.21561700732714426\n",
      "GD iter. 341/499: loss=0.21559035848163782\n",
      "GD iter. 342/499: loss=0.21556388905990365\n",
      "GD iter. 343/499: loss=0.21553759736121694\n",
      "GD iter. 344/499: loss=0.21551148170584705\n",
      "GD iter. 345/499: loss=0.21548554043473844\n",
      "GD iter. 346/499: loss=0.21545977190919754\n",
      "GD iter. 347/499: loss=0.2154341745105853\n",
      "GD iter. 348/499: loss=0.21540874664001455\n",
      "GD iter. 349/499: loss=0.21538348671805368\n",
      "GD iter. 350/499: loss=0.21535839318443506\n",
      "GD iter. 351/499: loss=0.21533346449776844\n",
      "GD iter. 352/499: loss=0.21530869913526007\n",
      "GD iter. 353/499: loss=0.21528409559243628\n",
      "GD iter. 354/499: loss=0.21525965238287204\n",
      "GD iter. 355/499: loss=0.21523536803792415\n",
      "GD iter. 356/499: loss=0.21521124110646964\n",
      "GD iter. 357/499: loss=0.21518727015464797\n",
      "GD iter. 358/499: loss=0.21516345376560828\n",
      "GD iter. 359/499: loss=0.21513979053926074\n",
      "GD iter. 360/499: loss=0.21511627909203235\n",
      "GD iter. 361/499: loss=0.2150929180566269\n",
      "GD iter. 362/499: loss=0.21506970608178905\n",
      "GD iter. 363/499: loss=0.2150466418320725\n",
      "GD iter. 364/499: loss=0.2150237239876121\n",
      "GD iter. 365/499: loss=0.21500095124389976\n",
      "GD iter. 366/499: loss=0.21497832231156447\n",
      "GD iter. 367/499: loss=0.21495583591615577\n",
      "GD iter. 368/499: loss=0.21493349079793087\n",
      "GD iter. 369/499: loss=0.21491128571164556\n",
      "GD iter. 370/499: loss=0.21488921942634864\n",
      "GD iter. 371/499: loss=0.2148672907251796\n",
      "GD iter. 372/499: loss=0.21484549840516987\n",
      "GD iter. 373/499: loss=0.21482384127704734\n",
      "GD iter. 374/499: loss=0.2148023181650441\n",
      "GD iter. 375/499: loss=0.21478092790670752\n",
      "GD iter. 376/499: loss=0.21475966935271415\n",
      "GD iter. 377/499: loss=0.21473854136668719\n",
      "GD iter. 378/499: loss=0.21471754282501637\n",
      "GD iter. 379/499: loss=0.21469667261668152\n",
      "GD iter. 380/499: loss=0.21467592964307813\n",
      "GD iter. 381/499: loss=0.21465531281784664\n",
      "GD iter. 382/499: loss=0.21463482106670412\n",
      "GD iter. 383/499: loss=0.21461445332727855\n",
      "GD iter. 384/499: loss=0.2145942085489461\n",
      "GD iter. 385/499: loss=0.21457408569267078\n",
      "GD iter. 386/499: loss=0.21455408373084692\n",
      "GD iter. 387/499: loss=0.21453420164714412\n",
      "GD iter. 388/499: loss=0.21451443843635457\n",
      "GD iter. 389/499: loss=0.21449479310424271\n",
      "GD iter. 390/499: loss=0.21447526466739805\n",
      "GD iter. 391/499: loss=0.2144558521530892\n",
      "GD iter. 392/499: loss=0.21443655459912117\n",
      "GD iter. 393/499: loss=0.21441737105369418\n",
      "GD iter. 394/499: loss=0.2143983005752658\n",
      "GD iter. 395/499: loss=0.2143793422324137\n",
      "GD iter. 396/499: loss=0.21436049510370217\n",
      "GD iter. 397/499: loss=0.2143417582775496\n",
      "GD iter. 398/499: loss=0.21432313085209842\n",
      "GD iter. 399/499: loss=0.21430461193508737\n",
      "GD iter. 400/499: loss=0.21428620064372536\n",
      "GD iter. 401/499: loss=0.2142678961045673\n",
      "GD iter. 402/499: loss=0.21424969745339212\n",
      "GD iter. 403/499: loss=0.21423160383508258\n",
      "GD iter. 404/499: loss=0.21421361440350667\n",
      "GD iter. 405/499: loss=0.21419572832140155\n",
      "GD iter. 406/499: loss=0.2141779447602582\n",
      "GD iter. 407/499: loss=0.2141602629002089\n",
      "GD iter. 408/499: loss=0.21414268192991573\n",
      "GD iter. 409/499: loss=0.21412520104646118\n",
      "GD iter. 410/499: loss=0.21410781945524007\n",
      "GD iter. 411/499: loss=0.21409053636985342\n",
      "GD iter. 412/499: loss=0.21407335101200403\n",
      "GD iter. 413/499: loss=0.2140562626113928\n",
      "GD iter. 414/499: loss=0.214039270405618\n",
      "GD iter. 415/499: loss=0.2140223736400748\n",
      "GD iter. 416/499: loss=0.2140055715678569\n",
      "GD iter. 417/499: loss=0.21398886344965976\n",
      "GD iter. 418/499: loss=0.21397224855368463\n",
      "GD iter. 419/499: loss=0.2139557261555449\n",
      "GD iter. 420/499: loss=0.21393929553817306\n",
      "GD iter. 421/499: loss=0.21392295599172967\n",
      "GD iter. 422/499: loss=0.21390670681351287\n",
      "GD iter. 423/499: loss=0.21389054730787033\n",
      "GD iter. 424/499: loss=0.2138744767861115\n",
      "GD iter. 425/499: loss=0.21385849456642161\n",
      "GD iter. 426/499: loss=0.21384259997377697\n",
      "GD iter. 427/499: loss=0.21382679233986146\n",
      "GD iter. 428/499: loss=0.2138110710029839\n",
      "GD iter. 429/499: loss=0.21379543530799727\n",
      "GD iter. 430/499: loss=0.21377988460621847\n",
      "GD iter. 431/499: loss=0.21376441825534975\n",
      "GD iter. 432/499: loss=0.21374903561940092\n",
      "GD iter. 433/499: loss=0.21373373606861276\n",
      "GD iter. 434/499: loss=0.21371851897938168\n",
      "GD iter. 435/499: loss=0.21370338373418546\n",
      "GD iter. 436/499: loss=0.2136883297215098\n",
      "GD iter. 437/499: loss=0.2136733563357761\n",
      "GD iter. 438/499: loss=0.21365846297727045\n",
      "GD iter. 439/499: loss=0.21364364905207314\n",
      "GD iter. 440/499: loss=0.21362891397198974\n",
      "GD iter. 441/499: loss=0.21361425715448268\n",
      "GD iter. 442/499: loss=0.21359967802260432\n",
      "GD iter. 443/499: loss=0.21358517600493018\n",
      "GD iter. 444/499: loss=0.21357075053549407\n",
      "GD iter. 445/499: loss=0.21355640105372325\n",
      "GD iter. 446/499: loss=0.21354212700437525\n",
      "GD iter. 447/499: loss=0.21352792783747485\n",
      "GD iter. 448/499: loss=0.21351380300825262\n",
      "GD iter. 449/499: loss=0.21349975197708393\n",
      "GD iter. 450/499: loss=0.2134857742094287\n",
      "GD iter. 451/499: loss=0.21347186917577243\n",
      "GD iter. 452/499: loss=0.2134580363515676\n",
      "GD iter. 453/499: loss=0.2134442752171761\n",
      "GD iter. 454/499: loss=0.21343058525781256\n",
      "GD iter. 455/499: loss=0.21341696596348814\n",
      "GD iter. 456/499: loss=0.21340341682895533\n",
      "GD iter. 457/499: loss=0.21338993735365352\n",
      "GD iter. 458/499: loss=0.21337652704165508\n",
      "GD iter. 459/499: loss=0.2133631854016125\n",
      "GD iter. 460/499: loss=0.21334991194670602\n",
      "GD iter. 461/499: loss=0.21333670619459186\n",
      "GD iter. 462/499: loss=0.21332356766735175\n",
      "GD iter. 463/499: loss=0.21331049589144224\n",
      "GD iter. 464/499: loss=0.2132974903976456\n",
      "GD iter. 465/499: loss=0.21328455072102054\n",
      "GD iter. 466/499: loss=0.21327167640085426\n",
      "GD iter. 467/499: loss=0.21325886698061486\n",
      "GD iter. 468/499: loss=0.21324612200790447\n",
      "GD iter. 469/499: loss=0.21323344103441272\n",
      "GD iter. 470/499: loss=0.2132208236158713\n",
      "GD iter. 471/499: loss=0.21320826931200873\n",
      "GD iter. 472/499: loss=0.21319577768650594\n",
      "GD iter. 473/499: loss=0.21318334830695246\n",
      "GD iter. 474/499: loss=0.21317098074480292\n",
      "GD iter. 475/499: loss=0.21315867457533452\n",
      "GD iter. 476/499: loss=0.21314642937760464\n",
      "GD iter. 477/499: loss=0.2131342447344094\n",
      "GD iter. 478/499: loss=0.2131221202322425\n",
      "GD iter. 479/499: loss=0.21311005546125467\n",
      "GD iter. 480/499: loss=0.21309805001521373\n",
      "GD iter. 481/499: loss=0.21308610349146495\n",
      "GD iter. 482/499: loss=0.21307421549089223\n",
      "GD iter. 483/499: loss=0.21306238561787946\n",
      "GD iter. 484/499: loss=0.21305061348027282\n",
      "GD iter. 485/499: loss=0.2130388986893429\n",
      "GD iter. 486/499: loss=0.21302724085974814\n",
      "GD iter. 487/499: loss=0.21301563960949788\n",
      "GD iter. 488/499: loss=0.2130040945599166\n",
      "GD iter. 489/499: loss=0.21299260533560815\n",
      "GD iter. 490/499: loss=0.21298117156442076\n",
      "GD iter. 491/499: loss=0.2129697928774122\n",
      "GD iter. 492/499: loss=0.21295846890881556\n",
      "GD iter. 493/499: loss=0.21294719929600553\n",
      "GD iter. 494/499: loss=0.21293598367946498\n",
      "GD iter. 495/499: loss=0.21292482170275184\n",
      "GD iter. 496/499: loss=0.2129137130124667\n",
      "GD iter. 497/499: loss=0.21290265725822075\n",
      "GD iter. 498/499: loss=0.21289165409260366\n",
      "GD iter. 499/499: loss=0.2128807031711528\n",
      "The Accuracy is: 0.9171\n",
      "The F1 score is: 0.2023\n",
      "The precision is: 0.5702\n",
      "The recall is: 0.1230\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using all the features except for those having NaN values over 50% ##\n",
    "initial_w = np.random.randn(x_train_processed.shape[1]) * 0.01\n",
    "w, loss = logistic_regression(y_train_processed, x_train_processed, initial_w, max_iters=500, gamma=0.15)\n",
    "predict_acc(x_train_processed, y_train_processed, w, logistic=True, threshold=0.5)\n",
    "predict_f1(x_train_processed, y_train_processed, w, logistic=True, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is: 0.6300\n",
      "The F1 score is: 0.2991\n",
      "The precision is: 0.1784\n",
      "The recall is: 0.9234\n"
     ]
    }
   ],
   "source": [
    "# ridge regression using all the features except for those having NaN values over 50% ##\n",
    "w, loss = ridge_regression(y_train_processed, x_train_processed, lambda_=0.001)\n",
    "y_pred = y_pred = x_train_processed @ w\n",
    "y_pred_mean = np.mean(y_pred)\n",
    "predict_acc(x_train_processed, y_train_processed, w, logistic=False, threshold=y_pred_mean)\n",
    "predict_f1(x_train_processed, y_train_processed, w, logistic=False, threshold=y_pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=(0.052871086847053966+0j)\n",
      "GD iter. 1/99: loss=(0.04238291130638302+0j)\n",
      "GD iter. 2/99: loss=(0.040326566175738675+0j)\n",
      "GD iter. 3/99: loss=(0.03936491064666693+0j)\n",
      "GD iter. 4/99: loss=(0.038795757929153456+0j)\n",
      "GD iter. 5/99: loss=(0.03842162757285372+0j)\n",
      "GD iter. 6/99: loss=(0.03816273621251975+0j)\n",
      "GD iter. 7/99: loss=(0.037978512631980904+0j)\n",
      "GD iter. 8/99: loss=(0.0378450945799688+0j)\n",
      "GD iter. 9/99: loss=(0.03774722452434276+0j)\n",
      "GD iter. 10/99: loss=(0.037674670273025034+0j)\n",
      "GD iter. 11/99: loss=(0.0376203698136672+0j)\n",
      "GD iter. 12/99: loss=(0.0375793569256343+0j)\n",
      "GD iter. 13/99: loss=(0.037548092460087897+0j)\n",
      "GD iter. 14/99: loss=(0.037524028463705464+0j)\n",
      "GD iter. 15/99: loss=(0.037505315343687416+0j)\n",
      "GD iter. 16/99: loss=(0.03749060099013997+0j)\n",
      "GD iter. 17/99: loss=(0.03747889086520389+0j)\n",
      "GD iter. 18/99: loss=(0.03746944939939819+0j)\n",
      "GD iter. 19/99: loss=(0.03746172982844771+0j)\n",
      "GD iter. 20/99: loss=(0.03745532385844996+0j)\n",
      "GD iter. 21/99: loss=(0.03744992529718011+0j)\n",
      "GD iter. 22/99: loss=(0.03744530360822779+0j)\n",
      "GD iter. 23/99: loss=(0.037441284568963375+0j)\n",
      "GD iter. 24/99: loss=(0.037437736048856474+0j)\n",
      "GD iter. 25/99: loss=(0.03743455750134515+0j)\n",
      "GD iter. 26/99: loss=(0.037431672164301384+0j)\n",
      "GD iter. 27/99: loss=(0.03742902124650558+0j)\n",
      "GD iter. 28/99: loss=(0.03742655957744439+0j)\n",
      "GD iter. 29/99: loss=(0.037424252340230654+0j)\n",
      "GD iter. 30/99: loss=(0.03742207260964372+0j)\n",
      "GD iter. 31/99: loss=(0.03741999949102307+0j)\n",
      "GD iter. 32/99: loss=(0.0374180167092378+0j)\n",
      "GD iter. 33/99: loss=(0.0374161115359595+0j)\n",
      "GD iter. 34/99: loss=(0.03741427397204619+0j)\n",
      "GD iter. 35/99: loss=(0.03741249612288359+0j)\n",
      "GD iter. 36/99: loss=(0.03741077172008356+0j)\n",
      "GD iter. 37/99: loss=(0.0374090957544855+0j)\n",
      "GD iter. 38/99: loss=(0.03740746419400939+0j)\n",
      "GD iter. 39/99: loss=(0.03740587376634381+0j)\n",
      "GD iter. 40/99: loss=(0.03740432179127968+0j)\n",
      "GD iter. 41/99: loss=(0.03740280605113515+0j)\n",
      "GD iter. 42/99: loss=(0.03740132469046068+0j)\n",
      "GD iter. 43/99: loss=(0.037399876138291065+0j)\n",
      "GD iter. 44/99: loss=(0.037398459047787894+0j)\n",
      "GD iter. 45/99: loss=(0.037397072249315884+0j)\n",
      "GD iter. 46/99: loss=(0.03739571471391212+0j)\n",
      "GD iter. 47/99: loss=(0.03739438552480619+0j)\n",
      "GD iter. 48/99: loss=(0.037393083855185374+0j)\n",
      "GD iter. 49/99: loss=(0.037391808950809985+0j)\n",
      "GD iter. 50/99: loss=(0.037390560116399844+0j)\n",
      "GD iter. 51/99: loss=(0.037389336704956366+0j)\n",
      "GD iter. 52/99: loss=(0.037388138109372256+0j)\n",
      "GD iter. 53/99: loss=(0.037386963755825606+0j)\n",
      "GD iter. 54/99: loss=(0.03738581309856749+0j)\n",
      "GD iter. 55/99: loss=(0.037384685615798656+0j)\n",
      "GD iter. 56/99: loss=(0.0373835808063984+0j)\n",
      "GD iter. 57/99: loss=(0.037382498187320964+0j)\n",
      "GD iter. 58/99: loss=(0.037381437291515+0j)\n",
      "GD iter. 59/99: loss=(0.037380397666253826+0j)\n",
      "GD iter. 60/99: loss=(0.03737937887178825+0j)\n",
      "GD iter. 61/99: loss=(0.03737838048025318+0j)\n",
      "GD iter. 62/99: loss=(0.03737740207477407+0j)\n",
      "GD iter. 63/99: loss=(0.03737644324873105+0j)\n",
      "GD iter. 64/99: loss=(0.03737550360514752+0j)\n",
      "GD iter. 65/99: loss=(0.03737458275617731+0j)\n",
      "GD iter. 66/99: loss=(0.03737368032266996+0j)\n",
      "GD iter. 67/99: loss=(0.03737279593379802+0j)\n",
      "GD iter. 68/99: loss=(0.037371929226734+0j)\n",
      "GD iter. 69/99: loss=(0.0373710798463667+0j)\n",
      "GD iter. 70/99: loss=(0.0373702474450494+0j)\n",
      "GD iter. 71/99: loss=(0.03736943168237362+0j)\n",
      "GD iter. 72/99: loss=(0.03736863222496359+0j)\n",
      "GD iter. 73/99: loss=(0.03736784874628763+0j)\n",
      "GD iter. 74/99: loss=(0.03736708092648337+0j)\n",
      "GD iter. 75/99: loss=(0.037366328452194446+0j)\n",
      "GD iter. 76/99: loss=(0.037365591016416726+0j)\n",
      "GD iter. 77/99: loss=(0.037364868318352604+0j)\n",
      "GD iter. 78/99: loss=(0.03736416006327209+0j)\n",
      "GD iter. 79/99: loss=(0.03736346596237981+0j)\n",
      "GD iter. 80/99: loss=(0.037362785732687115+0j)\n",
      "GD iter. 81/99: loss=(0.037362119096888614+0j)\n",
      "GD iter. 82/99: loss=(0.03736146578324287+0j)\n",
      "GD iter. 83/99: loss=(0.037360825525456484+0j)\n",
      "GD iter. 84/99: loss=(0.03736019806257161+0j)\n",
      "GD iter. 85/99: loss=(0.03735958313885646+0j)\n",
      "GD iter. 86/99: loss=(0.03735898050369848+0j)\n",
      "GD iter. 87/99: loss=(0.037358389911500325+0j)\n",
      "GD iter. 88/99: loss=(0.03735781112157813+0j)\n",
      "GD iter. 89/99: loss=(0.03735724389806217+0j)\n",
      "GD iter. 90/99: loss=(0.0373566880097998+0j)\n",
      "GD iter. 91/99: loss=(0.037356143230260395+0j)\n",
      "GD iter. 92/99: loss=(0.03735560933744248+0j)\n",
      "GD iter. 93/99: loss=(0.03735508611378272+0j)\n",
      "GD iter. 94/99: loss=(0.037354573346066906+0j)\n",
      "GD iter. 95/99: loss=(0.03735407082534272+0j)\n",
      "GD iter. 96/99: loss=(0.03735357834683435+0j)\n",
      "GD iter. 97/99: loss=(0.03735309570985887+0j)\n",
      "GD iter. 98/99: loss=(0.03735262271774429+0j)\n",
      "GD iter. 99/99: loss=(0.03735215917774927+0j)\n",
      "The Accuracy is: 0.6221\n",
      "The F1 score is: 0.2886\n",
      "The precision is: 0.1720\n",
      "The recall is: 0.8966\n"
     ]
    }
   ],
   "source": [
    "## linear regression using PCA feature selection ##\n",
    "\n",
    "initial_w = np.random.randn(x_pca.shape[1]) * 0.01\n",
    "w, loss = mean_square_error_gd(y_train_processed, x_pca, initial_w, max_iters = 100, gamma=0.1)\n",
    "y_pred = x_pca @ w\n",
    "y_pred_mean = np.mean(y_pred)\n",
    "predict_acc(x_pca, y_train_processed, w, logistic=False, threshold=y_pred_mean)\n",
    "predict_f1(x_pca, y_train_processed, w, logistic=False, threshold=y_pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=0.6953251025133772\n",
      "GD iter. 1/99: loss=0.6597077787630218\n",
      "GD iter. 2/99: loss=0.6309233895484205\n",
      "GD iter. 3/99: loss=0.6061935263709747\n",
      "GD iter. 4/99: loss=0.5841908431516583\n",
      "GD iter. 5/99: loss=0.564231274482722\n",
      "GD iter. 6/99: loss=0.5459285501914203\n",
      "GD iter. 7/99: loss=0.5290397934882721\n",
      "GD iter. 8/99: loss=0.5133950886416532\n",
      "GD iter. 9/99: loss=0.498864480579501\n",
      "GD iter. 10/99: loss=0.48534195479049397\n",
      "GD iter. 11/99: loss=0.47273729457207225\n",
      "GD iter. 12/99: loss=0.4609716910211406\n",
      "GD iter. 13/99: loss=0.44997519819811815\n",
      "GD iter. 14/99: loss=0.4396851312026764\n",
      "GD iter. 15/99: loss=0.4300449696135705\n",
      "GD iter. 16/99: loss=0.42100354796753364\n",
      "GD iter. 17/99: loss=0.41251442064614746\n",
      "GD iter. 18/99: loss=0.4045353407299958\n",
      "GD iter. 19/99: loss=0.3970278188435656\n",
      "GD iter. 20/99: loss=0.38995674183797513\n",
      "GD iter. 21/99: loss=0.38329003862418126\n",
      "GD iter. 22/99: loss=0.3769983846551128\n",
      "GD iter. 23/99: loss=0.3710549390028115\n",
      "GD iter. 24/99: loss=0.36543510947627594\n",
      "GD iter. 25/99: loss=0.3601163421922073\n",
      "GD iter. 26/99: loss=0.3550779326675853\n",
      "GD iter. 27/99: loss=0.350300855973266\n",
      "GD iter. 28/99: loss=0.3457676138413471\n",
      "GD iter. 29/99: loss=0.34146209689646323\n",
      "GD iter. 30/99: loss=0.3373694604065893\n",
      "GD iter. 31/99: loss=0.3334760121372104\n",
      "GD iter. 32/99: loss=0.3297691110531929\n",
      "GD iter. 33/99: loss=0.32623707575156785\n",
      "GD iter. 34/99: loss=0.3228691016298306\n",
      "GD iter. 35/99: loss=0.3196551859012928\n",
      "GD iter. 36/99: loss=0.3165860596636313\n",
      "GD iter. 37/99: loss=0.3136531263108058\n",
      "GD iter. 38/99: loss=0.31084840565326427\n",
      "GD iter. 39/99: loss=0.3081644831779763\n",
      "GD iter. 40/99: loss=0.30559446393924383\n",
      "GD iter. 41/99: loss=0.3031319306242536\n",
      "GD iter. 42/99: loss=0.30077090538466583\n",
      "GD iter. 43/99: loss=0.2985058150678036\n",
      "GD iter. 44/99: loss=0.2963314595187432\n",
      "GD iter. 45/99: loss=0.2942429826583397\n",
      "GD iter. 46/99: loss=0.2922358460723391\n",
      "GD iter. 47/99: loss=0.29030580487366225\n",
      "GD iter. 48/99: loss=0.28844888562401416\n",
      "GD iter. 49/99: loss=0.28666136612249077\n",
      "GD iter. 50/99: loss=0.2849397568881124\n",
      "GD iter. 51/99: loss=0.2832807841804349\n",
      "GD iter. 52/99: loss=0.28168137441781005\n",
      "GD iter. 53/99: loss=0.28013863986668075\n",
      "GD iter. 54/99: loss=0.27864986548766374\n",
      "GD iter. 55/99: loss=0.27721249683527144\n",
      "GD iter. 56/99: loss=0.27582412891806773\n",
      "GD iter. 57/99: loss=0.2744824959349878\n",
      "GD iter. 58/99: loss=0.27318546181156445\n",
      "GD iter. 59/99: loss=0.2719310114670119\n",
      "GD iter. 60/99: loss=0.27071724274959436\n",
      "GD iter. 61/99: loss=0.26954235898353457\n",
      "GD iter. 62/99: loss=0.268404662075964\n",
      "GD iter. 63/99: loss=0.2673025461371445\n",
      "GD iter. 64/99: loss=0.2662344915714505\n",
      "GD iter. 65/99: loss=0.2651990596004453\n",
      "GD iter. 66/99: loss=0.2641948871828596\n",
      "GD iter. 67/99: loss=0.26322068229940615\n",
      "GD iter. 68/99: loss=0.26227521957321037\n",
      "GD iter. 69/99: loss=0.2613573361991899\n",
      "GD iter. 70/99: loss=0.2604659281580468\n",
      "GD iter. 71/99: loss=0.2595999466926319\n",
      "GD iter. 72/99: loss=0.2587583950263509\n",
      "GD iter. 73/99: loss=0.2579403253050128\n",
      "GD iter. 74/99: loss=0.25714483574508856\n",
      "GD iter. 75/99: loss=0.25637106797277753\n",
      "GD iter. 76/99: loss=0.25561820453957507\n",
      "GD iter. 77/99: loss=0.25488546660121675\n",
      "GD iter. 78/99: loss=0.2541721117479467\n",
      "GD iter. 79/99: loss=0.25347743197504047\n",
      "GD iter. 80/99: loss=0.2528007517834017\n",
      "GD iter. 81/99: loss=0.25214142640087056\n",
      "GD iter. 82/99: loss=0.2514988401156203\n",
      "GD iter. 83/99: loss=0.25087240471370537\n",
      "GD iter. 84/99: loss=0.2502615580134363\n",
      "GD iter. 85/99: loss=0.24966576248983305\n",
      "GD iter. 86/99: loss=0.24908450398292534\n",
      "GD iter. 87/99: loss=0.24851729048414253\n",
      "GD iter. 88/99: loss=0.24796365099547865\n",
      "GD iter. 89/99: loss=0.2474231344565116\n",
      "GD iter. 90/99: loss=0.24689530873473073\n",
      "GD iter. 91/99: loss=0.24637975967495684\n",
      "GD iter. 92/99: loss=0.24587609020395612\n",
      "GD iter. 93/99: loss=0.24538391948663138\n",
      "GD iter. 94/99: loss=0.24490288213043507\n",
      "GD iter. 95/99: loss=0.2444326274348946\n",
      "GD iter. 96/99: loss=0.24397281868335938\n",
      "GD iter. 97/99: loss=0.24352313247428609\n",
      "GD iter. 98/99: loss=0.24308325808956757\n",
      "GD iter. 99/99: loss=0.24265289689758554\n",
      "The Accuracy is: 0.9162\n",
      "The F1 score is: 0.1158\n",
      "The precision is: 0.5902\n",
      "The recall is: 0.0642\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using pca feature selection #\n",
    "initial_w = np.random.randn(x_train_processed.shape[1]) * 0.01\n",
    "w, loss = logistic_regression(y_train_processed, x_train_processed, initial_w, max_iters=100, gamma=0.15)\n",
    "predict_acc(x_train_processed, y_train_processed, w, logistic=True, threshold=0.5)\n",
    "predict_f1(x_train_processed, y_train_processed, w, logistic=True, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
