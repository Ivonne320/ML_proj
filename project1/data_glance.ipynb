{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "import SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"/home/zewzhang/Course/ML/ML_course/projects/project1/data/dataset_to_release\", sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6563, 321)\n",
      "(6563,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# threshold to keep the features\n",
    "threshold_nan = 0.3\n",
    "x_train_processed = x_train.copy()\n",
    "x_train_processed = fillna_with_mean(x_train_processed, threshold=threshold_nan)\n",
    "x_train_processed = standardize(x_train_processed)\n",
    "x_train_processed_hinge = x_train_processed.copy()\n",
    "x_train_processed = add_bias(x_train_processed)\n",
    "# add a column of ones\n",
    "x_train_processed, y_train_processed = data_augmentation(x_train_processed, y_train)\n",
    "y_train_processed_hinge = y_train.copy()    \n",
    "y_train_processed = process_y(y_train_processed)\n",
    "print(np.isnan(x_train_processed).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12004, 143)\n",
      "(12004,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_train_processed.shape)\n",
    "print(y_train_processed.shape)\n",
    "print(np.sum(x_train_processed.std(axis=0) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x_train_processed_orig = x_train.copy()\n",
    "x_train_processed_orig = fillna_with_mean(x_train_processed_orig, threshold=threshold_nan)\n",
    "x_train_processed_orig = standardize(x_train_processed_orig)\n",
    "x_train_processed_orig = add_bias(x_train_processed_orig)\n",
    "# add a column of ones\n",
    "y_train_processed_orig = y_train.copy()\n",
    "y_train_processed_orig = process_y(y_train_processed_orig)\n",
    "print(np.isnan(x_train_processed_orig).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6563, 143)\n",
      "(6563,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_processed_orig.shape)\n",
    "print(y_train_processed_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.17056338e-01+0.00000000e+00j  5.19817810e-02+0.00000000e+00j\n",
      "  3.51028326e-02+0.00000000e+00j  3.10519268e-02+0.00000000e+00j\n",
      "  2.79084523e-02+0.00000000e+00j  2.60579658e-02+0.00000000e+00j\n",
      "  2.07247229e-02+0.00000000e+00j  2.01854996e-02+0.00000000e+00j\n",
      "  1.97504477e-02+0.00000000e+00j  1.82827562e-02+0.00000000e+00j\n",
      "  1.78592729e-02+0.00000000e+00j  1.75873812e-02+0.00000000e+00j\n",
      "  1.63961497e-02+0.00000000e+00j  1.50021104e-02+0.00000000e+00j\n",
      "  1.45288842e-02+0.00000000e+00j  1.43520448e-02+0.00000000e+00j\n",
      "  1.28929996e-02+0.00000000e+00j  1.25382113e-02+0.00000000e+00j\n",
      "  1.19441474e-02+0.00000000e+00j  1.14548177e-02+0.00000000e+00j\n",
      "  1.12093145e-02+0.00000000e+00j  1.11791640e-02+0.00000000e+00j\n",
      "  1.08731483e-02+0.00000000e+00j  1.05566411e-02+0.00000000e+00j\n",
      "  1.01249886e-02+0.00000000e+00j  1.00281106e-02+0.00000000e+00j\n",
      "  9.78736703e-03+0.00000000e+00j  9.37595682e-03+0.00000000e+00j\n",
      "  9.15056134e-03+0.00000000e+00j  8.90260043e-03+0.00000000e+00j\n",
      "  8.71822997e-03+0.00000000e+00j  8.54952070e-03+0.00000000e+00j\n",
      "  8.37849378e-03+0.00000000e+00j  8.03693573e-03+0.00000000e+00j\n",
      "  7.57756311e-03+0.00000000e+00j  7.53348568e-03+0.00000000e+00j\n",
      "  7.42791998e-03+0.00000000e+00j  7.25574511e-03+0.00000000e+00j\n",
      "  7.07468322e-03+0.00000000e+00j  6.94425333e-03+0.00000000e+00j\n",
      "  6.83299449e-03+0.00000000e+00j  6.74113705e-03+0.00000000e+00j\n",
      "  6.59082339e-03+0.00000000e+00j  6.52552193e-03+0.00000000e+00j\n",
      "  6.37261159e-03+0.00000000e+00j  6.33119565e-03+0.00000000e+00j\n",
      "  6.20812026e-03+0.00000000e+00j  6.19444367e-03+0.00000000e+00j\n",
      "  6.10632437e-03+0.00000000e+00j  6.01107057e-03+0.00000000e+00j\n",
      "  5.93813447e-03+0.00000000e+00j  5.88812186e-03+0.00000000e+00j\n",
      "  5.70968396e-03+0.00000000e+00j  5.65146284e-03+0.00000000e+00j\n",
      "  5.58359368e-03+0.00000000e+00j  5.55589571e-03+0.00000000e+00j\n",
      "  5.50608793e-03+0.00000000e+00j  5.31720341e-03+0.00000000e+00j\n",
      "  5.25622515e-03+0.00000000e+00j  5.22727388e-03+0.00000000e+00j\n",
      "  5.10224578e-03+0.00000000e+00j  5.07034609e-03+0.00000000e+00j\n",
      "  4.99704716e-03+0.00000000e+00j  4.98837839e-03+0.00000000e+00j\n",
      "  4.92333657e-03+0.00000000e+00j  4.83721630e-03+0.00000000e+00j\n",
      "  4.74167368e-03+0.00000000e+00j  4.72378205e-03+0.00000000e+00j\n",
      "  4.61664613e-03+0.00000000e+00j  4.59973266e-03+0.00000000e+00j\n",
      "  4.52272236e-03+0.00000000e+00j  4.47022553e-03+0.00000000e+00j\n",
      "  4.33610133e-03+0.00000000e+00j  4.26220080e-03+0.00000000e+00j\n",
      "  4.15228191e-03+0.00000000e+00j  4.12890849e-03+0.00000000e+00j\n",
      "  3.97075614e-03+0.00000000e+00j  3.89410968e-03+0.00000000e+00j\n",
      "  3.85447762e-03+0.00000000e+00j  3.71743972e-03+0.00000000e+00j\n",
      "  3.69801861e-03+0.00000000e+00j  3.58231399e-03+0.00000000e+00j\n",
      "  3.55635023e-03+0.00000000e+00j  3.41447376e-03+0.00000000e+00j\n",
      "  3.36044025e-03+0.00000000e+00j  3.26316918e-03+0.00000000e+00j\n",
      "  3.16803132e-03+0.00000000e+00j  3.07496469e-03+0.00000000e+00j\n",
      "  2.98594071e-03+0.00000000e+00j  2.86565074e-03+0.00000000e+00j\n",
      "  2.78752926e-03+0.00000000e+00j  2.71270049e-03+0.00000000e+00j\n",
      "  2.65837081e-03+0.00000000e+00j  2.64132533e-03+0.00000000e+00j\n",
      "  2.55905552e-03+0.00000000e+00j  2.50133714e-03+0.00000000e+00j\n",
      "  2.42849069e-03+0.00000000e+00j  2.37384090e-03+0.00000000e+00j\n",
      "  2.29616912e-03+0.00000000e+00j  2.21413375e-03+0.00000000e+00j\n",
      "  2.15614254e-03+0.00000000e+00j  2.13106073e-03+0.00000000e+00j\n",
      "  2.08240971e-03+0.00000000e+00j  1.99194576e-03+0.00000000e+00j\n",
      "  1.92603276e-03+0.00000000e+00j  1.87557110e-03+0.00000000e+00j\n",
      "  1.84826869e-03+0.00000000e+00j  1.82771422e-03+0.00000000e+00j\n",
      "  1.66941096e-03+0.00000000e+00j  1.59920663e-03+0.00000000e+00j\n",
      "  1.57193399e-03+0.00000000e+00j  1.48564490e-03+0.00000000e+00j\n",
      "  1.38916664e-03+0.00000000e+00j  1.26562283e-03+0.00000000e+00j\n",
      "  1.23889346e-03+0.00000000e+00j  1.18233639e-03+0.00000000e+00j\n",
      "  1.13449450e-03+0.00000000e+00j  1.08582879e-03+0.00000000e+00j\n",
      "  1.02136247e-03+0.00000000e+00j  9.22138859e-04+0.00000000e+00j\n",
      "  9.16730650e-04+0.00000000e+00j  7.56792096e-04+0.00000000e+00j\n",
      "  7.54572282e-04+0.00000000e+00j  6.96545569e-04+0.00000000e+00j\n",
      "  6.41705444e-04+0.00000000e+00j  6.17307954e-04+0.00000000e+00j\n",
      "  5.70387930e-04+0.00000000e+00j  5.69786634e-04+0.00000000e+00j\n",
      "  4.63260347e-04+0.00000000e+00j  4.50917140e-04+0.00000000e+00j\n",
      "  4.36110167e-04+0.00000000e+00j  3.85101947e-04+0.00000000e+00j\n",
      "  3.37635190e-04+0.00000000e+00j  3.05822064e-04+0.00000000e+00j\n",
      "  1.77946861e-04+0.00000000e+00j  1.68981637e-04+0.00000000e+00j\n",
      "  1.55443601e-04+0.00000000e+00j  1.37456458e-04+0.00000000e+00j\n",
      "  1.31238026e-04+0.00000000e+00j  1.19397869e-04+0.00000000e+00j\n",
      "  1.17913743e-04+0.00000000e+00j  9.76515793e-05+0.00000000e+00j\n",
      "  8.31143875e-05+0.00000000e+00j  7.83711686e-05+0.00000000e+00j\n",
      "  6.46634764e-05+0.00000000e+00j  5.62799466e-05+0.00000000e+00j\n",
      "  5.50540445e-05+0.00000000e+00j  5.24841076e-05+0.00000000e+00j\n",
      "  4.37308390e-05+0.00000000e+00j  3.76453593e-05+0.00000000e+00j\n",
      "  2.89394398e-05+0.00000000e+00j  2.58841456e-05+0.00000000e+00j\n",
      "  2.48506118e-05+0.00000000e+00j  2.03403289e-05+0.00000000e+00j\n",
      "  1.82755401e-05+0.00000000e+00j  1.17906667e-05+0.00000000e+00j\n",
      "  1.05761239e-05+0.00000000e+00j  8.18641585e-06+0.00000000e+00j\n",
      "  6.18832501e-06+0.00000000e+00j  6.13959286e-06+0.00000000e+00j\n",
      "  4.67419833e-06+0.00000000e+00j  3.31175298e-06+0.00000000e+00j\n",
      "  1.88561964e-06+0.00000000e+00j  4.35724664e-07+0.00000000e+00j\n",
      "  8.07088374e-10+0.00000000e+00j  2.65840491e-19+0.00000000e+00j\n",
      "  3.62944757e-20+0.00000000e+00j -1.58789572e-20+6.06158131e-19j\n",
      " -1.58789572e-20-6.06158131e-19j -2.54652860e-18+0.00000000e+00j]\n"
     ]
    }
   ],
   "source": [
    "## PCA feature selection \n",
    "threshold_nan_pca = 0.5\n",
    "pre_train_data = x_train.copy()\n",
    "pre_train_data = fillna_with_mean(pre_train_data, threshold=threshold_nan_pca)\n",
    "\n",
    "x_pca, eig_vec, eig_val,weight = pca(pre_train_data, 100)\n",
    "x_pca = standardize(x_pca)\n",
    "x_pca = add_bias(x_pca)\n",
    "x_train_processed_orig_pca = x_pca.copy()\n",
    "x_pca, _ = data_augmentation(x_pca, y_train)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6563, 174)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zewzhang/miniconda3/envs/ML/lib/python3.12/site-packages/matplotlib/cbook.py:1699: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return math.isfinite(val)\n",
      "/home/zewzhang/miniconda3/envs/ML/lib/python3.12/site-packages/matplotlib/cbook.py:1345: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return np.asarray(x, float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff09c0eaff0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGdCAYAAAA8F1jjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7mUlEQVR4nO3deXwU9eHG8Wc3x4aQixBykXDfAuGOUfGoUaCKd8XjJ4hXVbQq2iIeULUVj4q0SqW1Um3ViveFooCgouEKRO5AuBIgBwnkJtfu9/dHSDByJiSZTfbzfr32RXZ2ZvPsOLCPM9+ZsRljjAAAANyQ3eoAAAAAx0NRAQAAbouiAgAA3BZFBQAAuC2KCgAAcFsUFQAA4LYoKgAAwG1RVAAAgNvytjrAqXC5XNq3b58CAwNls9msjgMAAE6BMUZFRUWKjo6W3d6wfSMtoqjs27dPsbGxVscAAAANkJGRoZiYmAYt2yKKSmBgoKTqDxoUFGRxGgAAcCoKCwsVGxtb+z3eEC2iqNQc7gkKCqKoAADQwpzOsA0G0wIAALdFUQEAAG6LogIAANwWRQUAALgtigoAAHBbFBUAAOC2KCoAAMBtUVQAAIDboqgAAAC3Ve+i8t1332ns2LGKjo6WzWbTxx9/fNJlli5dqiFDhsjhcKhHjx56/fXXGxAVAAB4mnoXlZKSEsXFxWn27NmnNP/OnTt1ySWX6IILLlBKSoruv/9+3Xbbbfrqq6/qHRYAAHiWet/rZ8yYMRozZswpzz9nzhx17dpVL7zwgiSpb9++WrZsmV588UWNGjWqvr8eAAB4kCa/KWFSUpISExPrTBs1apTuv//+4y5TXl6u8vLy2ueFhYVNFQ8A4IZcLqMKp0tVLqOq2j+NqlwuOV1GlU4jp6v6efX06vmqp/1iusvI5TJyGSOXkVzGSIf/rHlujJE5/HuPTJOMfvH8Z8u4Dj83Ne/5Mz9/9ouXZHTUhBM9lfnle590/hP8rhO45eyuig31P+X5m0uTF5WsrCxFRETUmRYREaHCwkIdOnRIbdq0OWqZGTNm6IknnmjqaACARuJyGRUcqtSB0goVHqpUUVmVisqqVFxe/XNhWZWKy6pUVFZ5eHqVisqrVF7pVEWVS+VVLpVXOVVeWf1zhdNl9UfyOGPjoj2zqDTE1KlTNXny5NrnhYWFio2NtTARAHgWY4wKD1Upu6hM2YVlyi0uV15xhQ6WVuhASaUOlJTrYEl1MTlYUj3dder/894g3nabvOw2+XjZD/9Z/dzbbpd37c9Hntf87HV4ObvdJrtNskmy22yy2aqfV/9c90+7TbL98rlssttVdzkdma/62RG2nz2t+0rd13T4Peo8/+WH/+X8J/hdv5z9l68dT0SQ36nN2MyavKhERkYqOzu7zrTs7GwFBQUdc2+KJDkcDjkcjqaOBgAeqcrpUmZBmTIOliqnsFzZhWXKLixXdlGZcmp+LixTeVX992oEOrwV7O+jAIe3gvx8FOjnrQA/bwX6eSvQr2Z69bQAh4/8fOxyeHvV/unwtstx+Gdfb/vhslFdNH75ZQ7P0ORFJSEhQV988UWdaQsXLlRCQkJT/2oA8FhllU7tzivV7rwSpR8orf75QKnS80q05+AhVZ3i7o8Qfx9FBPqpQ6BDoW19FdrWV+38fRXa1kehbR1q19andnpIG1/5enN5LjSueheV4uJipaWl1T7fuXOnUlJSFBoaqk6dOmnq1Knau3ev/vOf/0iS7rzzTr388sv6wx/+oFtuuUXffPON3n33Xc2fP7/xPgUAeKji8iptzylWWk6xth3+My2nSOkHSk94KMbXy66Ydm0UGeyniCA/hQc5FBFY/XNEkEMRQdXlxM/Hq/k+DHAM9S4qq1ev1gUXXFD7vGYsyYQJE/T6668rMzNT6enpta937dpV8+fP1wMPPKC//vWviomJ0b/+9S9OTQaAenC6jHbnlWhTZqE2ZxZqc2aRtmQWal9B2XGXCfTzVpf2bdWpvb86h/qrU6h/9c/t2yoyyE9edg6lwP3ZzC/Pe3JDhYWFCg4OVkFBgYKCgqyOAwBNqtLp0pbMIqXsydfmzEJt2leo1KwiHap0HnP+sACHeoYHqMfhR83PHQIdjOuApRrj+9stz/oBAE+yv6hca9IPak36Qa3dna91e/NVVnn0QFY/H7t6RwapX1Sg+kUFqU9UkHqFByrY38eC1EDzoKgAQDMyxmhHbomStudp5c4DWpN+UHsOHjpqviA/bw3q1E5nRAepX1SQ+kYFqWtYWw7XwONQVACgCRljlH6gVEnb85S0I0/Ld+Qpu7C8zjw2m9QrPFBDOodocKd2GtIpRN3CAmSnlAAUFQBobPuLyrUsbb+WbctT0vbcowa8+nrZNbhTiBK6t9ewzqEaGBusID8O3wDHQlEBgNNUXuVU8q6D+m5brr7bul+bMuven8zbbtOg2OpiktCtvYZ0bsdpv8ApoqgAQAPszivRN1ty9N3W/Vq+48BRZ+ScER2kc3qG6ezuYRrWpZ38ffnnFmgI/uYAwClwuYxS9uRr0aZsLdyUrW05xXVeDwtw6NyeYTq3Vwed3SNMHQK5DQjQGCgqAHAcZZVOLduWq0Wbs7Voc45yi48MgvWy2zS8Szud3ztc5/bsoL5RgVyzBGgCFBUA+Jnc4nJ9syVHCzdl6/tt++tczyTA4a3zenfQxf0idH6vcK5fAjQDigoAj5dXXK4FG7M0f12mlu/Iq3OPnOhgPyX2i9BF/SIU37U9N90DmhlFBYBHOlhSoa83ZenzdZn6cXuenD9rJ2dEB+miw+WkX1QQh3QAC1FUAHiM4vIqfbk+U5+vy9QPabmq+lk56d8xSJcOjNYlA6IUG+pvYUoAP0dRAdCqOV1GP27P1QfJe7RgY1adMSd9o4J06cAoXTIgSl3C2lqYEsDxUFQAtErbsov0wZq9+njtXmUVHrkybLcObXV5XEddGhel7h0CLEwI4FRQVAC0GgdKKvRpyl59uHav1u0pqJ0e3MZHl8VF6+qhMYqLCWbMCdCCUFQAtGhVTpe+2ZKj95P3aElqjiqd1eNOvO02nd87XNcM7agL+oTL4c0l64GWiKICoEXKOFCqeasy9O7qDOUUHbkQ24COwbpqSEddFhet9gFcHRZo6SgqAFqMSqdLizZl63+rMvT9tv0yh0/aad/WV1cPjdHVQ2LUOzLQ2pAAGhVFBYDbS88r1f9Wpeu91XvqXMZ+ZM8wXTe8ky7qF8GF2IBWiqICwC0ZY7Ri5wG9tmynFm3Ort17Ehbg0LXDYjRueKw6t+eUYqC1o6gAcCvlVU599lOm5i7bqU2ZhbXTR/YM043xnXRh3wj5eLH3BPAUFBUAbiG3uFxvLU/Xf5fvrj284+dj11VDYnTL2V3UI5yxJ4AnoqgAsNTO3BL949vt+nDtXlVUVV81NjLIT+PP6qzrh3dSu7a+FicEYCWKCgBLbM4s1N+Xbtf8dftq71YcFxOsW87pql8PiOLwDgBJFBUAzSx590H9fUmaFm/JqZ12YZ9w3XV+dw3t3I6rxgKog6ICoMkZY/RDWp5mL0lT0o48SZLdJl0yMFp3nddd/aKDLE4IwF1RVAA0GWOMlqXl6oWvtyolI1+S5ONl01WDY3Tn+d3VlTsWAzgJigqAJrFiR55eWLhVK3cekFR9Bs/1Izrp9pHdFB3SxuJ0AFoKigqARpWSka8Xvk7V99tyJUm+3nbddGZn3Xled3UI5N47AOqHogKgUWzcV6AXF27Vos3Vg2S97TZdNyJW91zQU5HBfhanA9BSUVQAnJZduSV6/utUzV+XKal6kOzVQ2L0uwt7KjbU3+J0AFo6igqABsktLtdLi7fprRXpqnIZ2WzS2IHRui+xp7p3CLA6HoBWgqICoF5KK6o0d9lOzfl2h4rLqyRJ5/fuoCmj+6hvFKcZA2hcFBUAp6TK6dL7yXs0c+FW5RRV34tnQMdgTR3TR2f1CLM4HYDWiqIC4ISMMfpmS45mfLlFaTnFkqTY0DZ66OLeGjswWnY7V5IF0HQoKgCOa1t2kZ78fFPtqcYh/j6691c99X9ndpLD28vidAA8AUUFwFHySys0a9E2/Xf5bjldRr5edk08p4vuPr+Hgtv4WB0PgAehqACo5XQZvb1it15YuFX5pZWSpIv7RejRS/qqc3sudw+g+VFUAEiS1qQf1GMfbdCmzEJJUq+IAE279Ayd05OBsgCsQ1EBPNzBkgo9u2CL3lmVIUkK8vPWgxf31o3xneTtZbc4HQBPR1EBPJTLZfTu6gw9s2BL7WGea4bG6OExfRQWwD15ALgHigrggTbsLdDjn2zQ2vR8SVKfyEA9dUV/De8Sam0wAPgFigrgQYrKKvXC11v1n6Rdchmpra+XHriol24+qwuHeQC4JYoK4CGWbMnRIx+tV2ZBmSRpbFy0HrukryKCuLMxAPdFUQFauQMlFXrys436OGWfJKlze389feUAnc1l7wG0ABQVoJUyxuizdZn646cbdaCkQnabdNvIbnogsZfa+HJVWQAtA0UFaIWyCsr02McbtGhztiSpd0Sgnr1moAbFhlgbDADqiaICtCLGGL2zKkNPz9+sovIq+XjZdM8FPXXX+d3l681gWQAtD0UFaCV255Xo4Q/WK2lHniQpLjZEz109UL0jAy1OBgANR1EBWjiny+jfP+zUX75OVVmlS34+dj10cW9NPLurvOw2q+MBwGmhqAAt2O68Ej303k9ateugJOms7u31zFUD1am9v8XJAKBxUFSAFsgYozdXpOvp+Zt1qNKptr5eevSSfrp+RKxsNvaiAGg9KCpAC7Mv/5CmfLBO32/LlSTFdw3VX34Tp9hQ9qIAaH0oKkALYYzRB2v26olPN6qovEoOb7umjO6jm8/qIjtjUQC0UhQVoAXYX1SuRz5ar4Wbqq+LMig2RC9cG6fuHQIsTgYATYuiAri5RZuy9YcP1ulASYV8vGy6P7GXfntuN24iCMAjUFQAN3Wowqk/f7FJby5PlyT1iQzUi+MGqW9UkMXJAKD5UFQAN7RxX4HueydFaTnFkqTbzumq34/uLYc39+gB4FkoKoAbcbmMXlu2U899tUWVTqPwQIdeuDZOI3t2sDoaAFiCogK4iezCMj347k9allZ92vFF/SL07NUDFdrW1+JkAGAdigrgBr7amKUpH6xTfmml/HzsmnbpGVy8DQBEUQEsVVpRpac+36z/raweMHtGdJD+et1g9QjntGMAkCgqgGU27C3Q795Zqx37S2SzSXec200PXtRbvt6cdgwANSgqQDMzxug/Sbv15/mbVeF0KSLIoRevHaSzeoRZHQ0A3A5FBWhGBYcqNeX9dVqwMUuSlNg3Qs9fM1DtGDALAMfUoH3Ms2fPVpcuXeTn56f4+HitXLnyhPPPmjVLvXv3Vps2bRQbG6sHHnhAZWVlDQoMtFQpGfm65G/fa8HGLPl42TTt0n56dfxQSgoAnEC996jMmzdPkydP1pw5cxQfH69Zs2Zp1KhRSk1NVXh4+FHzv/3223r44Yc1d+5cnXXWWdq6datuvvlm2Ww2zZw5s1E+BODOjKm+NsqzC6qvjRIb2kYvXz9EcbEhVkcDALdnM8aY+iwQHx+v4cOH6+WXX5YkuVwuxcbG6t5779XDDz981Pz33HOPNm/erMWLF9dOe/DBB7VixQotW7bslH5nYWGhgoODVVBQoKAgLh+OliO/tEIPvbdOizZX30xwTP9IPXP1QAW38bE4GQA0vcb4/q7XoZ+KigolJycrMTHxyBvY7UpMTFRSUtIxlznrrLOUnJxce3hox44d+uKLL/TrX/+6QYGBliJ590H9+q/fa9HmbPl62fXk5Wfo7zcOoaQAQD3U69BPbm6unE6nIiIi6kyPiIjQli1bjrnMDTfcoNzcXJ1zzjkyxqiqqkp33nmnHnnkkeP+nvLycpWXl9c+LywsrE9MwFIul9Gr3+/Q81+lqspl1KW9v16+YYj6dwy2OhoAtDhNfsGGpUuX6umnn9bf//53rVmzRh9++KHmz5+vp5566rjLzJgxQ8HBwbWP2NjYpo4JNIqDJRW69Y1VmvHlFlW5jMbGReuze8+hpABAA9VrjEpFRYX8/f31/vvv64orrqidPmHCBOXn5+uTTz45apmRI0fqzDPP1PPPP1877c0339Qdd9yh4uJi2e1Hd6Vj7VGJjY1ljArc2sZ9Bfrtf5O15+AhObztmj6Wy+AD8GzNPkbF19dXQ4cOrTMw1uVyafHixUpISDjmMqWlpUeVES+v6lvVH68jORwOBQUF1XkA7uyTlL26+pUftefgIXUK9ddHd5+tG+I7UVIA4DTV+/TkyZMna8KECRo2bJhGjBihWbNmqaSkRBMnTpQkjR8/Xh07dtSMGTMkSWPHjtXMmTM1ePBgxcfHKy0tTY8//rjGjh1bW1iAlqrK6dKML7fotWU7JUnn9eqgv143SCH+XBsFABpDvYvKuHHjtH//fk2bNk1ZWVkaNGiQFixYUDvANj09vc4elMcee0w2m02PPfaY9u7dqw4dOmjs2LH685//3HifArBAbnG57nl7jZbvOCBJmnRBd02+qLe87OxFAYDGUu/rqFiB66jA3azbk687/5usfQVlauvrpReujdPo/lFWxwIAt9IY39/c6weop/dWZ+jRjzeoosqlbmFt9Y+bhqpnRKDVsQCgVaKoAKeoosqlpz7fpP8u3y1JSuwbrpnjBinIjwu4AUBToagApyCnqEx3v7lGq3cflCQ9kNhL9/6qh+yMRwGAJkVRAU5i3Z583f6f1couLFegw1uzrhukC/tGnHxBAMBpo6gAJ/DpT/v0+/d+UnmVSz3CA/TPm4aqW4cAq2MBgMegqADH4HIZvbhoq176Jk2SdGGfcM26bpACGY8CAM2KogL8Qkl5lSa/m6KvNmZLkn57Xjf9YVQfro8CABagqAA/szf/kG57Y7U2ZxbK18uuGVcN0NVDY6yOBQAei6ICHLZ+T4FueWOV9heVKyzAoX/cNFRDO7ezOhYAeDSKCiDp641Zuu+dFB2qdKpPZKBeu3m4Ooa0sToWAHg8igo8mjFGc3/YpT/N3yRjqm8q+PINgxk0CwBugqICj1XldOmJz45cafbG+E564rIz5O1lP8mSAIDmQlGBRyour9I9b6/R0tT9stmkR3/dV7ee01U2G2f2AIA7oajA42QWHNLEf6/Slqwi+fnYNWvcYI3uH2l1LADAMVBU4FE27ivQLa+vUnZh9Zk9r00YprjYEKtjAQCOg6ICj7FsW67ufDNZxeVV6hURoLk3D1dMO3+rYwEAToCiAo/w8dq9eui9n1TlMkro1l7/GD9UQZzZAwBuj6KCVs0Yo398t0PPfLlFkjQ2Llp/+c1AOby9LE4GADgVFBW0Wk6X0VOfb9LrP+6SJN0+squmjukrO/fsAYAWg6KCVqms0qkH5qXoyw1ZkqTHLumr20Z2szgVAKC+KCpodQpKK3X7f1Zr5a4D8vWy64Vr4zQ2LtrqWACABqCooFXZl39IE+au1LacYgX6eeufNw1TQvf2VscCADQQRQWtxvb9xbrpXyu0r6BMkUF+ev2W4eoTGWR1LADAaaCooFXYsLdAE+auVF5Jhbp1aKv/3hrP3Y8BoBWgqKDFW74jT7e9sVrF5VUa0DFYr08crvYBDqtjAQAaAUUFLdrCTdma9PYaVVS5dGa3UL06fpgCuZAbALQaFBW0WB8k79EfPlgnp8sosW+EXr5hsPx8uJAbALQmFBW0SHOX7dSTn2+SJF09JEbPXj1A3l52i1MBABobRQUtijFGLy7cqr99kyZJuuXsrnrsEq42CwCtFUUFLYYxRk99vllzf9gpSXro4l6adEEP2WyUFABorSgqaBFcLqPpn27Uf5fvliQ9efkZGp/QxdpQAIAmR1GB23O5jB75aL3eWZUhm0169qqBunZ4rNWxAADNgKICt+Z0Gf3h/XX6YM0e2W3SX34Tp6uGxFgdCwDQTCgqcFtVTpcefO8nfZKyT152m14cN0iXcXNBAPAoFBW4pUqnS/e9s1ZfrM+St92ml64frDEDoqyOBQBoZhQVuJ3yKqfueXutFm7Klq+XXbNvHKKL+kVYHQsAYAGKCtxKWaVTd72ZrCWp++Xrbdc/bhqqC3qHWx0LAGARigrcRnnVkZLi52PXv8YP1zk9w6yOBQCwEEUFbqGiyqVJb62tLSlzbx6us7pTUgDA03FzFFiuZuDsos3Z8vWu3pNCSQEASBQVWKzK6dLkd3/Slxuy5Otl1z9vGsrhHgBALYoKLFNzMbfPftonHy+bXvm/ITqfgbMAgJ+hqMASxhg9+tF6fbh2r7zsNr10/RBd2JdTkAEAdVFU0OyMMXr6i816Z1WG7Dbpr9cN0uj+kVbHAgC4IYoKmt3sJWl69fudkqRnrh6oSwdyWXwAwLFRVNCs/pu0S3/5eqsk6bFL+uraYdwFGQBwfBQVNJtPUvZq2qcbJUm/+1UP3Taym8WJAADujqKCZrF4c7Ymv/uTjJFuPquLHriol9WRAAAtAEUFTW75jjzd/dYaOV1GVw3uqGmX9pPNZrM6FgCgBaCooEmt25Ov295YrfIqlxL7RujZawbKbqekAABODUUFTSYtp0gT5q5UcXmVErq118s3DJaPF5scAODU8a2BJpFTWKYJc1fpYGml4mKC9eqEYfLz8bI6FgCghaGooNGVlFdp4uurtDf/kLqFtdW/J45QgIMbdQMA6o+igkZV5XTpnrfXaOO+QrVv66vXJ45QaFtfq2MBAFooigoa1Z/mb9aS1P3y87HrXxOGqVN7f6sjAQBaMIoKGs1bK3br9R93SZJmjRukwZ3aWRsIANDiUVTQKJK252n6J9VXnX3o4l4a3T/K4kQAgNaAooLTlp5XqrveSlaVy2hsXLQmXdDD6kgAgFaCooLTUlRWqVvfWKX80koNjAnW89cM5KqzAIBGQ1FBgzldRve9k6JtOcWKCHLo1fFcKwUA0LgoKmiw579K1TdbcuTwtuufNw1TRJCf1ZEAAK0MRQUN8vm6fZrz7XZJ0nPXDFRcbIi1gQAArRJFBfW2JatQv39vnSTpt+d10+WDOlqcCADQWlFUUC8FpZW64z/JOlTp1MieYfrDqD5WRwIAtGIUFZwyp8vovnlrlX6gVDHt2uhv1w2Wl50zfAAATYeiglP24sKtWnr48vj/uGmo2nEPHwBAE2tQUZk9e7a6dOkiPz8/xcfHa+XKlSecPz8/X5MmTVJUVJQcDod69eqlL774okGBYY0FGzL18pI0SdKzVw/UGdHBFicCAHgC7/ouMG/ePE2ePFlz5sxRfHy8Zs2apVGjRik1NVXh4eFHzV9RUaGLLrpI4eHhev/999WxY0ft3r1bISEhjZEfzWDH/mI9+O5PkqRbz+nK4FkAQLOxGWNMfRaIj4/X8OHD9fLLL0uSXC6XYmNjde+99+rhhx8+av45c+bo+eef15YtW+Tj49OgkIWFhQoODlZBQYGCgoIa9B5omLJKp66Y/YO2ZBUpvmuo3rotXt5eHDEEAJxcY3x/1+sbp6KiQsnJyUpMTDzyBna7EhMTlZSUdMxlPv30UyUkJGjSpEmKiIhQ//799fTTT8vpdB7395SXl6uwsLDOA9aY/slGbckqUliAr166fjAlBQDQrOr1rZObmyun06mIiIg60yMiIpSVlXXMZXbs2KH3339fTqdTX3zxhR5//HG98MIL+tOf/nTc3zNjxgwFBwfXPmJjY+sTE43k/eQ9mrc6Qzab9LfrBiucK88CAJpZk//vscvlUnh4uP75z39q6NChGjdunB599FHNmTPnuMtMnTpVBQUFtY+MjIymjolf2JpdpMc+Xi9Juv/CXjqrR5jFiQAAnqheg2nDwsLk5eWl7OzsOtOzs7MVGRl5zGWioqLk4+MjL68jN6vr27evsrKyVFFRIV/fo09xdTgccjgc9YmGRlRSXqW731qjskqXRvYM0z2/6mF1JACAh6rXHhVfX18NHTpUixcvrp3mcrm0ePFiJSQkHHOZs88+W2lpaXK5XLXTtm7dqqioqGOWFFjLGKNHP1qvtMN3RH5x3CAu6gYAsEy9D/1MnjxZr776qt544w1t3rxZd911l0pKSjRx4kRJ0vjx4zV16tTa+e+66y4dOHBA9913n7Zu3ar58+fr6aef1qRJkxrvU6DRvJe8Rx+n7JOX3aaXrh+isAD2bAEArFPv66iMGzdO+/fv17Rp05SVlaVBgwZpwYIFtQNs09PTZbcf6T+xsbH66quv9MADD2jgwIHq2LGj7rvvPk2ZMqXxPgUaxa7cEv3x042SpMkX9dKIrqEWJwIAeLp6X0fFClxHpelVOl26Zk6SfsrIV3zXUL19+5kc8gEAnJZmv44KWq+XFm/TTxn5CvTz1kzGpQAA3ARFBVq960DtfXz+fOUAdQxpY3EiAACqUVQ8XFFZpe6flyKXka4c3FGXxUVbHQkAgFoUFQ83/dON2nPwkDqGtNETl59hdRwAAOqgqHiwz9ft04dr9spuk2ZdN0hBfg27aSQAAE2FouKhcgrL9OhHGyRJd5/fQ8O7cCoyAMD9UFQ8kDFGD3+4XgWHKnVGdJDuS+xpdSQAAI6JouKB3kveo2+25MjXy66Z1w6SjxebAQDAPfEN5WH2HCzVk59tkiRNvriXekcGWpwIAIDjo6h4EJfLaMoH61RcXqUhnUJ0+8huVkcCAOCEKCoe5K0Vu/VDWp78fOz6y2/iuPosAMDtUVQ8xK7cEj39xRZJ0sOj+6hbhwCLEwEAcHIUFQ/gchn9/v2fdKjSqYRu7TU+oYvVkQAAOCUUFQ/w1ordWrXroNr6eum5awbKziEfAEALQVFp5bIKyvTcglRJ0h9G91FsqL/FiQAAOHUUlVbuj59uVFF5lQbFhuj/zuxsdRwAAOqFotKKfbUxSws2ZsnbbtOMqwZwlg8AoMWhqLRSRWWVmv7JRknSHed2U9+oIIsTAQBQfxSVVuovX6Uqq7BMndv763cXci8fAEDLRFFphdakH9R/lu+WJD195QD5+XhZnAgAgIahqLQylU6Xpn6wXsZIVw3pqLN7hFkdCQCABqOotDKvfr9DqdlFaufvo8cu6Wd1HAAATgtFpRXZlVuivy7aJkl6/NJ+Cm3ra3EiAABOD0WllTDG6NGP16u8yqVzeoTpysEdrY4EAMBpo6i0Eh+u2asf0vLk8Lbrz1f2l83GNVMAAC0fRaUVOFhSoT/N3yRJuj+xlzq3b2txIgAAGgdFpRV47qtUHSytVO+IQN02sqvVcQAAaDQUlRZu3Z58vbMqXZL01BX95ePFf1IAQOvBt1oL5nIZPf7xBhkjXTm4o0Z0DbU6EgAAjYqi0oLNW52hn/YUKNDhram/7mN1HAAAGh1FpYU6WFKh5xZskSTdf1EvhQf6WZwIAIDGR1FpoZ7/unoAbZ/IQE1I6Gx1HAAAmgRFpQVatydf/1tZPYD2icvOkDcDaAEArRTfcC2My2X0+CcbZYx0xaBoxXdrb3UkAACaDEWlhXl/zR79lJGvAIe3Hvl1X6vjAADQpCgqLUhpRZVe+DpVkvS7C3soPIgBtACA1o2i0oK89v1OZReWK6ZdG004q4vVcQAAaHIUlRZif1G55ny7XZL0h9F95PD2sjgRAABNj6LSQsxatFUlFU7FxQRr7MAoq+MAANAsKCotQFpOsd5ZlSFJeuTXfWWz2SxOBABA86CotADPfLlFTpdRYt8ITkcGAHgUioqbW7EjT4s2Z8vLbtPDY7ifDwDAs1BU3JjLZfT0F5slSdcNj1WP8ACLEwEA0LwoKm7s8/WZ+mlPgdr6eun+xF5WxwEAoNlRVNxUldOlmYcv7vbb87qrQ6DD4kQAADQ/ioqb+nxdpnbllaqdv49uPaer1XEAALAERcUNuVxGLy9JkyTdek5XtXV4W5wIAABrUFTc0JcbspSWU6wgP2+N51L5AAAPRlFxMy6X0UvfbJMkTTy7q4L8fCxOBACAdSgqbmbxlhxtySpSgMNbt5zN2BQAgGejqLgRY46MTbkpobOC/dmbAgDwbBQVN5K0PU8/ZeTL4W3nTB8AAERRcSuzl1bvTblueKzCArhuCgAAFBU3kZKRrx/S8uRtt+n2c7tZHQcAALdAUXETfz88NuXyQR0V087f4jQAALgHioob2JZdpK83Zctmk+46n70pAADUoKi4gVeWbpckjeoXqR7hgRanAQDAfVBULJZxoFSf/LRPknT3Bd0tTgMAgHuhqFjsn9/tkNNlNLJnmAbGhFgdBwAAt0JRsVBOUZnmrc6QJN19fg+L0wAA4H4oKhaau2yXKqpcGtwpRGd2C7U6DgAAboeiYpHSiiq9vWK3JOmu87rLZrNZnAgAAPdDUbHIR2v3qrCsSp3b+yuxb4TVcQAAcEsUFQsYY/TGj7skSeMTushuZ28KAADHQlGxQNL2PG3NLpa/r5d+MyzG6jgAALitBhWV2bNnq0uXLvLz81N8fLxWrlx5Ssu98847stlsuuKKKxrya1uN1w/vTbl6SIyC/HysDQMAgBurd1GZN2+eJk+erOnTp2vNmjWKi4vTqFGjlJOTc8Lldu3apYceekgjR45scNjWIONAqRZtzpYkTTirs8VpAABwb/UuKjNnztTtt9+uiRMnql+/fpozZ478/f01d+7c4y7jdDp144036oknnlC3bp59L5u3V6bLZaRzeoRxuXwAAE6iXkWloqJCycnJSkxMPPIGdrsSExOVlJR03OWefPJJhYeH69Zbbz2l31NeXq7CwsI6j9agosql9w5f4O3/zmRvCgAAJ1OvopKbmyun06mIiLqn00ZERCgrK+uYyyxbtkyvvfaaXn311VP+PTNmzFBwcHDtIzY2tj4x3dbizdnKLa5Qh0CHLuwbbnUcAADcXpOe9VNUVKSbbrpJr776qsLCwk55ualTp6qgoKD2kZGR0YQpm8/bK9MlSdcOi5GPFydcAQBwMt71mTksLExeXl7Kzs6uMz07O1uRkZFHzb99+3bt2rVLY8eOrZ3mcrmqf7G3t1JTU9W9+9F3DHY4HHI4HPWJ5vYyDpTq+225kqTrhneyOA0AAC1Dvf633tfXV0OHDtXixYtrp7lcLi1evFgJCQlHzd+nTx+tX79eKSkptY/LLrtMF1xwgVJSUlrNIZ1T8c6q6r0pI3uGKTbU3+I0AAC0DPXaoyJJkydP1oQJEzRs2DCNGDFCs2bNUklJiSZOnChJGj9+vDp27KgZM2bIz89P/fv3r7N8SEiIJB01vTWrdLr03uo9kqQbRrA3BQCAU1XvojJu3Djt379f06ZNU1ZWlgYNGqQFCxbUDrBNT0+X3c74i59bsiVHOUXlCgvw1YXc1wcAgFNmM8YYq0OcTGFhoYKDg1VQUKCgoCCr49TbbW+s0qLNOfrtud009dd9rY4DAECzaIzvb3Z9NLHswjItSd0vSfrNMM8ZkwMAQGOgqDSx95P3yOkyGta5nXqEB1gdBwCAFoWi0oSMMbVXoh03nL0pAADUF0WlCa3YeUC78koV4PDWJQOjrI4DAECLQ1FpQvNWVe9NGRsXJX/fep9gBQCAx6OoNJGCQ5X6Yn2mJGkcV6IFAKBBKCpN5NOf9qm8yqXeEYGKiwm2Og4AAC0SRaWJzDt8yfxrh8fKZrNZnAYAgJaJotIENu4r0Ia9hfL1suvKwR2tjgMAQItFUWkC7x4eRHvRGREKbetrcRoAAFouikojK6t06qO1eyVJ47gSLQAAp4Wi0si+2pilwrIqdQxpo3N6hFkdBwCAFo2i0sg+Prw35eohHWW3M4gWAIDTQVFpRHnF5fpuW64k6XIG0QIAcNooKo1o/vpMOV1GAzoGq3sHbkAIAMDpoqg0oprDPpcPirY4CQAArQNFpZGk55VqTXq+7DbpsjiKCgAAjYGi0kg+Sanem3JW9zCFB/lZnAYAgNaBotIIjDH6OIXDPgAANDaKSiPYlFmo7ftL5Ott1+j+kVbHAQCg1aCoNILPfsqUJF3YJ1yBfj4WpwEAoPWgqJwmY4w+X7dPknTpQA77AADQmCgqpyklI197Dh6Sv6+XftUn3Oo4AAC0KhSV01Rz2OeifhFq4+tlcRoAAFoXisppcLmM5q+vPuwzlsM+AAA0OorKaVi164CyC8sV6Oetkb24UzIAAI2NonIaPjs8iHb0GZFyeHPYBwCAxkZRaaAqp0tfrM+SJI3lkvkAADQJikoD/bg9TwdKKhTa1ldndW9vdRwAAFolikoD1Vw7ZUz/SHl7sRoBAGgKfMM2QHmVUws2cNgHAICmRlFpgO+35qqwrEoRQQ4N7xJqdRwAAFotikoD1Bz2uWRAtLzsNovTAADQelFU6ulQhVMLN2VLki6Ni7I4DQAArRtFpZ6WpuaopMKpjiFtNDg2xOo4AAC0ahSVelqaul+SNLp/pGw2DvsAANCUKCr1YIzRsrRcSdLInlwyHwCApkZRqYeduSXam39Ivl52xXflIm8AADQ1iko91OxNGdq5ndr4cm8fAACaGkWlHr7fVl1UzuGwDwAAzYKicoqqnC4t354nifEpAAA0F4rKKfppT76KyqsU4u+jM6KDrY4DAIBHoKicoprDPmd3D+NqtAAANBOKyin64fBA2rN7cNgHAIDmQlE5BcXlVVqbni+J8SkAADQnisopWLXrgKpcRrGhbRQb6m91HAAAPAZF5RQs31F9tk9CNy7yBgBAc6KonIKa05ITulNUAABoThSVkygqq9T6vQWSxGXzAQBoZhSVk1i164BcRurc3l/RIW2sjgMAgEehqJzE8h0HJDE+BQAAK1BUTqJmIO2ZFBUAAJodReUECssqteHw+BSKCgAAzY+icgKrdlaPT+ka1laRwX5WxwEAwONQVE7gyGGfUIuTAADgmSgqJ7BiZ/VAWg77AABgDYrKcZRWVGnjvkJJ0rAu7FEBAMAKFJXjWLenQE6XUWSQn6IZnwIAgCUoKseRvPugJGlo53ay2WwWpwEAwDNRVI5jbXp1URnSuZ3FSQAA8FwUlWMwxtTuURnSKcTaMAAAeDCKyjHszC3RwdJK+XrbdUZ0sNVxAADwWBSVY6jZmxIXEyxfb1YRAABW4Vv4GNak50tifAoAAFajqBzDmpozfjpRVAAAsFKDisrs2bPVpUsX+fn5KT4+XitXrjzuvK+++qpGjhypdu3aqV27dkpMTDzh/FYrOFSprTlFktijAgCA1epdVObNm6fJkydr+vTpWrNmjeLi4jRq1Cjl5OQcc/6lS5fq+uuv15IlS5SUlKTY2FhdfPHF2rt372mHbwopGfkyRurc3l9hAQ6r4wAA4NHqXVRmzpyp22+/XRMnTlS/fv00Z84c+fv7a+7cucec/6233tLdd9+tQYMGqU+fPvrXv/4ll8ulxYsXn3b4ppBSMz6Fwz4AAFiuXkWloqJCycnJSkxMPPIGdrsSExOVlJR0Su9RWlqqyspKhYYe//455eXlKiwsrPNoLuv3FkiSBnTktGQAAKxWr6KSm5srp9OpiIiIOtMjIiKUlZV1Su8xZcoURUdH1yk7vzRjxgwFBwfXPmJjY+sT87Ss35svSRoQQ1EBAMBqzXrWzzPPPKN33nlHH330kfz8jn+jv6lTp6qgoKD2kZGR0Sz5corKlF1YLptN6hcV1Cy/EwAAHJ93fWYOCwuTl5eXsrOz60zPzs5WZGTkCZf9y1/+omeeeUaLFi3SwIEDTzivw+GQw9H8A1k3HD7s06NDgNo66rVqAABAE6jXHhVfX18NHTq0zkDYmoGxCQkJx13uueee01NPPaUFCxZo2LBhDU/bxNbvqR4Lw/gUAADcQ713G0yePFkTJkzQsGHDNGLECM2aNUslJSWaOHGiJGn8+PHq2LGjZsyYIUl69tlnNW3aNL399tvq0qVL7ViWgIAABQQENOJHOX01A2n7U1QAAHAL9S4q48aN0/79+zVt2jRlZWVp0KBBWrBgQe0A2/T0dNntR3bUvPLKK6qoqNA111xT532mT5+uP/7xj6eXvpExkBYAAPdiM8YYq0OcTGFhoYKDg1VQUKCgoKYZ5JpTVKYRf14sm03a8MdRjFEBAOA0Ncb3N/f6OaxmIG13BtICAOA2KCqHMZAWAAD3Q1E5jIG0AAC4H4rKYbUDaSkqAAC4DYqKpNzi8iNXpI3mirQAALgLioqk3XklkqTo4DYKYCAtAABug6IiaXdeqSSpU6i/xUkAAMDPUVQkpR+gqAAA4I4oKpLSa/aotKeoAADgTigqYo8KAADuiqIiaffhotKZPSoAALgVjy8qhyqc2l9ULok9KgAAuBuPLyo1h32C/LwV4u9rcRoAAPBzFJUDDKQFAMBdeXxRqbnYW+fQthYnAQAAv+TxRSWDPSoAALgtjy8quzk1GQAAt+XxRaVmjEpnigoAAG7Ho4uK02W058AhSVIsRQUAALfj0UUlu7BMFU6XvO02RYe0sToOAAD4BY8uKjV3TY5p10ZedpvFaQAAwC95dFE5csYPpyYDAOCOPLqo7D5QfQ2VTqEc9gEAwB15dFFJPzyQlou9AQDgnjy7qBy+Ki0XewMAwD15Wx3ASjfGd9bQzkXqGxlkdRQAAHAMHl1Urh0ea3UEAABwAh596AcAALg3igoAAHBbFBUAAOC2KCoAAMBtUVQAAIDboqgAAAC3RVEBAABui6ICAADcFkUFAAC4LYoKAABwWxQVAADgtigqAADAbVFUAACA22oRd082xkiSCgsLLU4CAABOVc33ds33eEO0iKJSVFQkSYqNjbU4CQAAqK+ioiIFBwc3aFmbOZ2a00xcLpf27dunwMBA2Wy2RnvfwsJCxcbGKiMjQ0FBQY32vi0R6+II1kU11sMRrIsjWBfVWA9HnGhdGGNUVFSk6Oho2e0NG23SIvao2O12xcTENNn7BwUFefyGVoN1cQTrohrr4QjWxRGsi2qshyOOty4auielBoNpAQCA26KoAAAAt+XRRcXhcGj69OlyOBxWR7Ec6+II1kU11sMRrIsjWBfVWA9HNPW6aBGDaQEAgGfy6D0qAADAvVFUAACA26KoAAAAt0VRAQAAbsuji8rs2bPVpUsX+fn5KT4+XitXrrQ6UpOaMWOGhg8frsDAQIWHh+uKK65QampqnXnOP/982Wy2Oo8777zTosRN549//ONRn7NPnz61r5eVlWnSpElq3769AgICdPXVVys7O9vCxE2nS5cuR60Lm82mSZMmSWq928R3332nsWPHKjo6WjabTR9//HGd140xmjZtmqKiotSmTRslJiZq27ZtdeY5cOCAbrzxRgUFBSkkJES33nqriouLm/FTNI4TrYvKykpNmTJFAwYMUNu2bRUdHa3x48dr3759dd7jWNvRM88808yf5PSdbLu4+eabj/qco0ePrjOPJ2wXko7574bNZtPzzz9fO09jbBceW1TmzZunyZMna/r06VqzZo3i4uI0atQo5eTkWB2tyXz77beaNGmSli9froULF6qyslIXX3yxSkpK6sx3++23KzMzs/bx3HPPWZS4aZ1xxhl1PueyZctqX3vggQf02Wef6b333tO3336rffv26aqrrrIwbdNZtWpVnfWwcOFCSdJvfvOb2nla4zZRUlKiuLg4zZ49+5ivP/fcc/rb3/6mOXPmaMWKFWrbtq1GjRqlsrKy2nluvPFGbdy4UQsXLtTnn3+u7777TnfccUdzfYRGc6J1UVpaqjVr1ujxxx/XmjVr9OGHHyo1NVWXXXbZUfM++eSTdbaTe++9tzniN6qTbReSNHr06Dqf83//+1+d1z1hu5BUZx1kZmZq7ty5stlsuvrqq+vMd9rbhfFQI0aMMJMmTap97nQ6TXR0tJkxY4aFqZpXTk6OkWS+/fbb2mnnnXeeue+++6wL1UymT59u4uLijvlafn6+8fHxMe+9917ttM2bNxtJJikpqZkSWue+++4z3bt3Ny6XyxjjGduEJPPRRx/VPne5XCYyMtI8//zztdPy8/ONw+Ew//vf/4wxxmzatMlIMqtWraqd58svvzQ2m83s3bu32bI3tl+ui2NZuXKlkWR2795dO61z587mxRdfbNpwzexY62LChAnm8ssvP+4ynrxdXH755eZXv/pVnWmNsV145B6ViooKJScnKzExsXaa3W5XYmKikpKSLEzWvAoKCiRJoaGhdaa/9dZbCgsLU//+/TV16lSVlpZaEa/Jbdu2TdHR0erWrZtuvPFGpaenS5KSk5NVWVlZZ/vo06ePOnXq1Oq3j4qKCr355pu65ZZb6twA1FO2iRo7d+5UVlZWnW0gODhY8fHxtdtAUlKSQkJCNGzYsNp5EhMTZbfbtWLFimbP3JwKCgpks9kUEhJSZ/ozzzyj9u3ba/DgwXr++edVVVVlTcAmtnTpUoWHh6t379666667lJeXV/uap24X2dnZmj9/vm699dajXjvd7aJF3JSwseXm5srpdCoiIqLO9IiICG3ZssWiVM3L5XLp/vvv19lnn63+/fvXTr/hhhvUuXNnRUdHa926dZoyZYpSU1P14YcfWpi28cXHx+v1119X7969lZmZqSeeeEIjR47Uhg0blJWVJV9f36P+EY6IiFBWVpY1gZvJxx9/rPz8fN1888210zxlm/i5mv/Ox/o3oua1rKwshYeH13nd29tboaGhrXo7KSsr05QpU3T99dfXuQHd7373Ow0ZMkShoaH68ccfNXXqVGVmZmrmzJkWpm18o0eP1lVXXaWuXbtq+/bteuSRRzRmzBglJSXJy8vLY7eLN954Q4GBgUcdIm+M7cIjiwqkSZMmacOGDXXGZUiqcxx1wIABioqK0oUXXqjt27ere/fuzR2zyYwZM6b254EDByo+Pl6dO3fWu+++qzZt2liYzFqvvfaaxowZo+jo6NppnrJN4OQqKyt17bXXyhijV155pc5rkydPrv154MCB8vX11W9/+1vNmDGjVV1m/rrrrqv9ecCAARo4cKC6d++upUuX6sILL7QwmbXmzp2rG2+8UX5+fnWmN8Z24ZGHfsLCwuTl5XXUWRzZ2dmKjIy0KFXzueeee/T5559ryZIliomJOeG88fHxkqS0tLTmiGaZkJAQ9erVS2lpaYqMjFRFRYXy8/PrzNPat4/du3dr0aJFuu222044nydsEzX/nU/0b0RkZORRg++rqqp04MCBVrmd1JSU3bt3a+HChXX2phxLfHy8qqqqtGvXruYJaJFu3bopLCys9u+Dp20XkvT9998rNTX1pP92SA3bLjyyqPj6+mro0KFavHhx7TSXy6XFixcrISHBwmRNyxije+65Rx999JG++eYbde3a9aTLpKSkSJKioqKaOJ21iouLtX37dkVFRWno0KHy8fGps32kpqYqPT29VW8f//73vxUeHq5LLrnkhPN5wjbRtWtXRUZG1tkGCgsLtWLFitptICEhQfn5+UpOTq6d55tvvpHL5aotc61FTUnZtm2bFi1apPbt2590mZSUFNnt9qMOg7Q2e/bsUV5eXu3fB0/aLmq89tprGjp0qOLi4k46b4O2i9MaituCvfPOO8bhcJjXX3/dbNq0ydxxxx0mJCTEZGVlWR2tydx1110mODjYLF261GRmZtY+SktLjTHGpKWlmSeffNKsXr3a7Ny503zyySemW7du5txzz7U4eeN78MEHzdKlS83OnTvNDz/8YBITE01YWJjJyckxxhhz5513mk6dOplvvvnGrF692iQkJJiEhASLUzcdp9NpOnXqZKZMmVJnemveJoqKiszatWvN2rVrjSQzc+ZMs3bt2tozWZ555hkTEhJiPvnkE7Nu3Tpz+eWXm65du5pDhw7Vvsfo0aPN4MGDzYoVK8yyZctMz549zfXXX2/VR2qwE62LiooKc9lll5mYmBiTkpJS59+O8vJyY4wxP/74o3nxxRdNSkqK2b59u3nzzTdNhw4dzPjx4y3+ZPV3onVRVFRkHnroIZOUlGR27txpFi1aZIYMGWJ69uxpysrKat/DE7aLGgUFBcbf39+88sorRy3fWNuFxxYVY4x56aWXTKdOnYyvr68ZMWKEWb58udWRmpSkYz7+/e9/G2OMSU9PN+eee64JDQ01DofD9OjRw/z+9783BQUF1gZvAuPGjTNRUVHG19fXdOzY0YwbN86kpaXVvn7o0CFz9913m3bt2hl/f39z5ZVXmszMTAsTN62vvvrKSDKpqal1prfmbWLJkiXH/PswYcIEY0z1KcqPP/64iYiIMA6Hw1x44YVHrZ+8vDxz/fXXm4CAABMUFGQmTpxoioqKLPg0p+dE62Lnzp3H/bdjyZIlxhhjkpOTTXx8vAkODjZ+fn6mb9++5umnn67z5d1SnGhdlJaWmosvvth06NDB+Pj4mM6dO5vbb7/9qP/B9YTtosY//vEP06ZNG5Ofn3/U8o21XdiMMebU978AAAA0H48cowIAAFoGigoAAHBbFBUAAOC2KCoAAMBtUVQAAIDboqgAAAC3RVEBAABui6ICAADcFkUFAAC4LYoKAABwWxQVAADgtigqAADAbf0/HJlGZrO7gFYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cummulation = np.cumsum(weight)\n",
    "cummulation\n",
    "plt.plot(cummulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12004, 101)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=0.28378493642397645\n",
      "GD iter. 10/99: loss=0.09787248294806823\n",
      "GD iter. 20/99: loss=0.08422354520420035\n",
      "GD iter. 30/99: loss=0.0789744153186847\n",
      "GD iter. 40/99: loss=0.0767252868519264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 50/99: loss=0.0756989520329858\n",
      "GD iter. 60/99: loss=0.07520085063526796\n",
      "GD iter. 70/99: loss=0.07494046523119388\n",
      "GD iter. 80/99: loss=0.07479141651027899\n",
      "GD iter. 90/99: loss=0.07469700899118516\n",
      "The Accuracy is: 0.7539\n",
      "The F1 score is: 0.3644\n",
      "The precision is: 0.2338\n",
      "The recall is: 0.8253\n"
     ]
    }
   ],
   "source": [
    "## linear regression using all the features except for those having NaN values over 50% ##\n",
    "\n",
    "initial_w = np.random.randn(x_train_processed.shape[1]) * 0.01\n",
    "w, loss = mean_square_error_gd(y_train_processed, x_train_processed, initial_w, max_iters = 100, gamma=0.05)\n",
    "y_pred = x_train_processed @ w\n",
    "y_pred_mean = np.mean(y_pred)\n",
    "predict_acc(x_train_processed_orig, y_train_processed_orig, w, logistic=False, threshold=y_pred_mean)\n",
    "predict_f1(x_train_processed_orig, y_train_processed_orig, w, logistic=False, threshold=y_pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12004, 143)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/799: loss=0.6909355818630069\n",
      "GD iter. 1/799: loss=0.6108760742804626\n",
      "GD iter. 2/799: loss=0.5779684725864976\n",
      "GD iter. 3/799: loss=0.5611013979976893\n",
      "GD iter. 4/799: loss=0.5508843542612606\n",
      "GD iter. 5/799: loss=0.5438531878013491\n",
      "GD iter. 6/799: loss=0.5385209017349654\n",
      "GD iter. 7/799: loss=0.5341792298435389\n",
      "GD iter. 8/799: loss=0.5304633165411469\n",
      "GD iter. 9/799: loss=0.5271727750984119\n",
      "GD iter. 10/799: loss=0.5241911684807654\n",
      "GD iter. 11/799: loss=0.521447179598998\n",
      "GD iter. 12/799: loss=0.5188948338474354\n",
      "GD iter. 13/799: loss=0.5165029682604019\n",
      "GD iter. 14/799: loss=0.5142494108799176\n",
      "GD iter. 15/799: loss=0.5121176564809675\n",
      "GD iter. 16/799: loss=0.5100949099230249\n",
      "GD iter. 17/799: loss=0.5081709004499305\n",
      "GD iter. 18/799: loss=0.5063371418242676\n",
      "GD iter. 19/799: loss=0.5045864565175804\n",
      "GD iter. 20/799: loss=0.5029126599938083\n",
      "GD iter. 21/799: loss=0.5013103443923926\n",
      "GD iter. 22/799: loss=0.49977472548019425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 23/799: loss=0.49830153094373686\n",
      "GD iter. 24/799: loss=0.49688691644554805\n",
      "GD iter. 25/799: loss=0.49552740086080416\n",
      "GD iter. 26/799: loss=0.49421981514370217\n",
      "GD iter. 27/799: loss=0.4929612611473991\n",
      "GD iter. 28/799: loss=0.4917490779008385\n",
      "GD iter. 29/799: loss=0.4905808136026821\n",
      "GD iter. 30/799: loss=0.48945420208863893\n",
      "GD iter. 31/799: loss=0.4883671428610581\n",
      "GD iter. 32/799: loss=0.48731768399790826\n",
      "GD iter. 33/799: loss=0.48630400741871105\n",
      "GD iter. 34/799: loss=0.4853244161004554\n",
      "GD iter. 35/799: loss=0.4843773229214756\n",
      "GD iter. 36/799: loss=0.4834612408750799\n",
      "GD iter. 37/799: loss=0.48257477444354\n",
      "GD iter. 38/799: loss=0.48171661196100696\n",
      "GD iter. 39/799: loss=0.48088551882384506\n",
      "GD iter. 40/799: loss=0.4800803314307571\n",
      "GD iter. 41/799: loss=0.4792999517543161\n",
      "GD iter. 42/799: loss=0.47854334246116625\n",
      "GD iter. 43/799: loss=0.4778095225109692\n",
      "GD iter. 44/799: loss=0.47709756317472385\n",
      "GD iter. 45/799: loss=0.476406584421847\n",
      "GD iter. 46/799: loss=0.4757357516326808\n",
      "GD iter. 47/799: loss=0.47508427259919334\n",
      "GD iter. 48/799: loss=0.4744513947817598\n",
      "GD iter. 49/799: loss=0.47383640279422456\n",
      "GD iter. 50/799: loss=0.47323861609310264\n",
      "GD iter. 51/799: loss=0.47265738684988023\n",
      "GD iter. 52/799: loss=0.4720920979880154\n",
      "GD iter. 53/799: loss=0.4715421613685021\n",
      "GD iter. 54/799: loss=0.47100701610979745\n",
      "GD iter. 55/799: loss=0.47048612702957576\n",
      "GD iter. 56/799: loss=0.46997898319721587\n",
      "GD iter. 57/799: loss=0.4694850965871663\n",
      "GD iter. 58/799: loss=0.46900400082441684\n",
      "GD iter. 59/799: loss=0.46853525001424395\n",
      "GD iter. 60/799: loss=0.4680784176492148\n",
      "GD iter. 61/799: loss=0.4676330955871581\n",
      "GD iter. 62/799: loss=0.4671988930944359\n",
      "GD iter. 63/799: loss=0.4667754359494076\n",
      "GD iter. 64/799: loss=0.4663623656014676\n",
      "GD iter. 65/799: loss=0.46595933838146936\n",
      "GD iter. 66/799: loss=0.4655660247597367\n",
      "GD iter. 67/799: loss=0.4651821086482024\n",
      "GD iter. 68/799: loss=0.4648072867435215\n",
      "GD iter. 69/799: loss=0.4644412679082768\n",
      "GD iter. 70/799: loss=0.46408377258764266\n",
      "GD iter. 71/799: loss=0.4637345322590896\n",
      "GD iter. 72/799: loss=0.46339328891290915\n",
      "GD iter. 73/799: loss=0.4630597945615203\n",
      "GD iter. 74/799: loss=0.4627338107756772\n",
      "GD iter. 75/799: loss=0.46241510824584364\n",
      "GD iter. 76/799: loss=0.46210346636713334\n",
      "GD iter. 77/799: loss=0.4617986728463363\n",
      "GD iter. 78/799: loss=0.46150052332965774\n",
      "GD iter. 79/799: loss=0.4612088210499002\n",
      "GD iter. 80/799: loss=0.46092337649190784\n",
      "GD iter. 81/799: loss=0.46064400707517744\n",
      "GD iter. 82/799: loss=0.46037053685261514\n",
      "GD iter. 83/799: loss=0.4601027962244921\n",
      "GD iter. 84/799: loss=0.4598406216667121\n",
      "GD iter. 85/799: loss=0.4595838554725673\n",
      "GD iter. 86/799: loss=0.4593323455072123\n",
      "GD iter. 87/799: loss=0.45908594497413635\n",
      "GD iter. 88/799: loss=0.4588445121929619\n",
      "GD iter. 89/799: loss=0.45860791038793836\n",
      "GD iter. 90/799: loss=0.45837600748654495\n",
      "GD iter. 91/799: loss=0.4581486759276459\n",
      "GD iter. 92/799: loss=0.4579257924786849\n",
      "GD iter. 93/799: loss=0.4577072380614292\n",
      "GD iter. 94/799: loss=0.4574928975858094\n",
      "GD iter. 95/799: loss=0.4572826597914262\n",
      "GD iter. 96/799: loss=0.45707641709632024\n",
      "GD iter. 97/799: loss=0.45687406545262743\n",
      "GD iter. 98/799: loss=0.45667550420876385\n",
      "GD iter. 99/799: loss=0.45648063597780175\n",
      "GD iter. 100/799: loss=0.4562893665117253\n",
      "GD iter. 101/799: loss=0.45610160458126336\n",
      "GD iter. 102/799: loss=0.45591726186102227\n",
      "GD iter. 103/799: loss=0.4557362528196515\n",
      "GD iter. 104/799: loss=0.45555849461479264\n",
      "GD iter. 105/799: loss=0.45538390699257664\n",
      "GD iter. 106/799: loss=0.4552124121914447\n",
      "GD iter. 107/799: loss=0.45504393485008415\n",
      "GD iter. 108/799: loss=0.4548784019192777\n",
      "GD iter. 109/799: loss=0.4547157425774808\n",
      "GD iter. 110/799: loss=0.45455588814994535\n",
      "GD iter. 111/799: loss=0.45439877203122503\n",
      "GD iter. 112/799: loss=0.45424432961089867\n",
      "GD iter. 113/799: loss=0.45409249820236397\n",
      "GD iter. 114/799: loss=0.45394321697455525\n",
      "GD iter. 115/799: loss=0.4537964268864529\n",
      "GD iter. 116/799: loss=0.453652070624251\n",
      "GD iter. 117/799: loss=0.45351009254106683\n",
      "GD iter. 118/799: loss=0.45337043859907117\n",
      "GD iter. 119/799: loss=0.4532330563139327\n",
      "GD iter. 120/799: loss=0.4530978947014711\n",
      "GD iter. 121/799: loss=0.45296490422641933\n",
      "GD iter. 122/799: loss=0.4528340367532024\n",
      "GD iter. 123/799: loss=0.4527052454986403\n",
      "GD iter. 124/799: loss=0.4525784849864945\n",
      "GD iter. 125/799: loss=0.4524537110037721\n",
      "GD iter. 126/799: loss=0.45233088055871484\n",
      "GD iter. 127/799: loss=0.4522099518403977\n",
      "GD iter. 128/799: loss=0.452090884179868\n",
      "GD iter. 129/799: loss=0.45197363801275886\n",
      "GD iter. 130/799: loss=0.45185817484331375\n",
      "GD iter. 131/799: loss=0.4517444572097625\n",
      "GD iter. 132/799: loss=0.45163244865099045\n",
      "GD iter. 133/799: loss=0.4515221136744482\n",
      "GD iter. 134/799: loss=0.45141341772524735\n",
      "GD iter. 135/799: loss=0.45130632715639557\n",
      "GD iter. 136/799: loss=0.4512008092001206\n",
      "GD iter. 137/799: loss=0.45109683194024186\n",
      "GD iter. 138/799: loss=0.4509943642855426\n",
      "GD iter. 139/799: loss=0.45089337594410533\n",
      "GD iter. 140/799: loss=0.4507938373985685\n",
      "GD iter. 141/799: loss=0.4506957198822686\n",
      "GD iter. 142/799: loss=0.4505989953562319\n",
      "GD iter. 143/799: loss=0.45050363648698105\n",
      "GD iter. 144/799: loss=0.4504096166251243\n",
      "GD iter. 145/799: loss=0.45031690978469646\n",
      "GD iter. 146/799: loss=0.45022549062322215\n",
      "GD iter. 147/799: loss=0.4501353344224707\n",
      "GD iter. 148/799: loss=0.4500464170698796\n",
      "GD iter. 149/799: loss=0.44995871504061635\n",
      "GD iter. 150/799: loss=0.4498722053802558\n",
      "GD iter. 151/799: loss=0.4497868656880499\n",
      "GD iter. 152/799: loss=0.44970267410076475\n",
      "GD iter. 153/799: loss=0.4496196092770665\n",
      "GD iter. 154/799: loss=0.44953765038243026\n",
      "GD iter. 155/799: loss=0.4494567770745582\n",
      "GD iter. 156/799: loss=0.44937696948928285\n",
      "GD iter. 157/799: loss=0.44929820822693833\n",
      "GD iter. 158/799: loss=0.44922047433918433\n",
      "GD iter. 159/799: loss=0.44914374931626255\n",
      "GD iter. 160/799: loss=0.44906801507467226\n",
      "GD iter. 161/799: loss=0.4489932539452471\n",
      "GD iter. 162/799: loss=0.44891944866162103\n",
      "GD iter. 163/799: loss=0.4488465823490664\n",
      "GD iter. 164/799: loss=0.44877463851369226\n",
      "GD iter. 165/799: loss=0.44870360103198964\n",
      "GD iter. 166/799: loss=0.4486334541407103\n",
      "GD iter. 167/799: loss=0.448564182427068\n",
      "GD iter. 168/799: loss=0.4484957708192501\n",
      "GD iter. 169/799: loss=0.44842820457722765\n",
      "GD iter. 170/799: loss=0.44836146928385634\n",
      "GD iter. 171/799: loss=0.44829555083625333\n",
      "GD iter. 172/799: loss=0.44823043543744373\n",
      "GD iter. 173/799: loss=0.448166109588267\n",
      "GD iter. 174/799: loss=0.4481025600795318\n",
      "GD iter. 175/799: loss=0.448039773984413\n",
      "GD iter. 176/799: loss=0.4479777386510818\n",
      "GD iter. 177/799: loss=0.44791644169555983\n",
      "GD iter. 178/799: loss=0.44785587099479107\n",
      "GD iter. 179/799: loss=0.44779601467992297\n",
      "GD iter. 180/799: loss=0.4477368611297899\n",
      "GD iter. 181/799: loss=0.4476783989645923\n",
      "GD iter. 182/799: loss=0.4476206170397648\n",
      "GD iter. 183/799: loss=0.4475635044400266\n",
      "GD iter. 184/799: loss=0.4475070504736079\n",
      "GD iter. 185/799: loss=0.4474512446666471\n",
      "GD iter. 186/799: loss=0.4473960767577528\n",
      "GD iter. 187/799: loss=0.44734153669272464\n",
      "GD iter. 188/799: loss=0.4472876146194281\n",
      "GD iter. 189/799: loss=0.4472343008828186\n",
      "GD iter. 190/799: loss=0.44718158602010843\n",
      "GD iter. 191/799: loss=0.4471294607560742\n",
      "GD iter. 192/799: loss=0.4470779159984981\n",
      "GD iter. 193/799: loss=0.4470269428337395\n",
      "GD iter. 194/799: loss=0.4469765325224327\n",
      "GD iter. 195/799: loss=0.4469266764953068\n",
      "GD iter. 196/799: loss=0.4468773663491235\n",
      "GD iter. 197/799: loss=0.4468285938427284\n",
      "GD iter. 198/799: loss=0.4467803508932142\n",
      "GD iter. 199/799: loss=0.4467326295721902\n",
      "GD iter. 200/799: loss=0.4466854221021551\n",
      "GD iter. 201/799: loss=0.44663872085297107\n",
      "GD iter. 202/799: loss=0.44659251833843444\n",
      "GD iter. 203/799: loss=0.4465468072129409\n",
      "GD iter. 204/799: loss=0.4465015802682412\n",
      "GD iter. 205/799: loss=0.4464568304302862\n",
      "GD iter. 206/799: loss=0.4464125507561572\n",
      "GD iter. 207/799: loss=0.446368734431079\n",
      "GD iter. 208/799: loss=0.4463253747655135\n",
      "GD iter. 209/799: loss=0.4462824651923312\n",
      "GD iter. 210/799: loss=0.44623999926405883\n",
      "GD iter. 211/799: loss=0.44619797065019956\n",
      "GD iter. 212/799: loss=0.44615637313462425\n",
      "GD iter. 213/799: loss=0.4461152006130322\n",
      "GD iter. 214/799: loss=0.4460744470904782\n",
      "GD iter. 215/799: loss=0.4460341066789645\n",
      "GD iter. 216/799: loss=0.4459941735950956\n",
      "GD iter. 217/799: loss=0.4459546421577942\n",
      "GD iter. 218/799: loss=0.4459155067860761\n",
      "GD iter. 219/799: loss=0.445876761996883\n",
      "GD iter. 220/799: loss=0.4458384024029696\n",
      "GD iter. 221/799: loss=0.4458004227108474\n",
      "GD iter. 222/799: loss=0.44576281771877785\n",
      "GD iter. 223/799: loss=0.44572558231481896\n",
      "GD iter. 224/799: loss=0.44568871147492006\n",
      "GD iter. 225/799: loss=0.4456522002610653\n",
      "GD iter. 226/799: loss=0.44561604381946296\n",
      "GD iter. 227/799: loss=0.44558023737878105\n",
      "GD iter. 228/799: loss=0.44554477624842626\n",
      "GD iter. 229/799: loss=0.44550965581686564\n",
      "GD iter. 230/799: loss=0.4454748715499905\n",
      "GD iter. 231/799: loss=0.4454404189895194\n",
      "GD iter. 232/799: loss=0.44540629375144203\n",
      "GD iter. 233/799: loss=0.445372491524499\n",
      "GD iter. 234/799: loss=0.4453390080687013\n",
      "GD iter. 235/799: loss=0.44530583921388317\n",
      "GD iter. 236/799: loss=0.4452729808582917\n",
      "GD iter. 237/799: loss=0.4452404289672097\n",
      "GD iter. 238/799: loss=0.44520817957161174\n",
      "GD iter. 239/799: loss=0.44517622876685226\n",
      "GD iter. 240/799: loss=0.44514457271138536\n",
      "GD iter. 241/799: loss=0.44511320762551415\n",
      "GD iter. 242/799: loss=0.4450821297901707\n",
      "GD iter. 243/799: loss=0.44505133554572396\n",
      "GD iter. 244/799: loss=0.44502082129081616\n",
      "GD iter. 245/799: loss=0.44499058348122555\n",
      "GD iter. 246/799: loss=0.44496061862875735\n",
      "GD iter. 247/799: loss=0.44493092330015827\n",
      "GD iter. 248/799: loss=0.44490149411605784\n",
      "GD iter. 249/799: loss=0.44487232774993374\n",
      "GD iter. 250/799: loss=0.44484342092710016\n",
      "GD iter. 251/799: loss=0.44481477042372025\n",
      "GD iter. 252/799: loss=0.4447863730658401\n",
      "GD iter. 253/799: loss=0.444758225728446\n",
      "GD iter. 254/799: loss=0.44473032533454143\n",
      "GD iter. 255/799: loss=0.4447026688542462\n",
      "GD iter. 256/799: loss=0.444675253303915\n",
      "GD iter. 257/799: loss=0.44464807574527615\n",
      "GD iter. 258/799: loss=0.4446211332845891\n",
      "GD iter. 259/799: loss=0.4445944230718209\n",
      "GD iter. 260/799: loss=0.4445679422998413\n",
      "GD iter. 261/799: loss=0.4445416882036346\n",
      "GD iter. 262/799: loss=0.44451565805952964\n",
      "GD iter. 263/799: loss=0.4444898491844465\n",
      "GD iter. 264/799: loss=0.44446425893515956\n",
      "GD iter. 265/799: loss=0.44443888470757603\n",
      "GD iter. 266/799: loss=0.4444137239360314\n",
      "GD iter. 267/799: loss=0.4443887740925987\n",
      "GD iter. 268/799: loss=0.44436403268641345\n",
      "GD iter. 269/799: loss=0.4443394972630128\n",
      "GD iter. 270/799: loss=0.44431516540368887\n",
      "GD iter. 271/799: loss=0.4442910347248556\n",
      "GD iter. 272/799: loss=0.4442671028774295\n",
      "GD iter. 273/799: loss=0.44424336754622273\n",
      "GD iter. 274/799: loss=0.44421982644934993\n",
      "GD iter. 275/799: loss=0.4441964773376464\n",
      "GD iter. 276/799: loss=0.4441733179940996\n",
      "GD iter. 277/799: loss=0.4441503462332912\n",
      "GD iter. 278/799: loss=0.44412755990085184\n",
      "GD iter. 279/799: loss=0.4441049568729267\n",
      "GD iter. 280/799: loss=0.44408253505565154\n",
      "GD iter. 281/799: loss=0.4440602923846408\n",
      "GD iter. 282/799: loss=0.4440382268244846\n",
      "GD iter. 283/799: loss=0.44401633636825777\n",
      "GD iter. 284/799: loss=0.4439946190370365\n",
      "GD iter. 285/799: loss=0.4439730728794276\n",
      "GD iter. 286/799: loss=0.4439516959711048\n",
      "GD iter. 287/799: loss=0.44393048641435556\n",
      "GD iter. 288/799: loss=0.44390944233763724\n",
      "GD iter. 289/799: loss=0.4438885618951408\n",
      "GD iter. 290/799: loss=0.4438678432663651\n",
      "GD iter. 291/799: loss=0.44384728465569745\n",
      "GD iter. 292/799: loss=0.44382688429200423\n",
      "GD iter. 293/799: loss=0.4438066404282288\n",
      "GD iter. 294/799: loss=0.4437865513409976\n",
      "GD iter. 295/799: loss=0.4437666153302329\n",
      "GD iter. 296/799: loss=0.4437468307187752\n",
      "GD iter. 297/799: loss=0.4437271958520106\n",
      "GD iter. 298/799: loss=0.44370770909750695\n",
      "GD iter. 299/799: loss=0.44368836884465684\n",
      "GD iter. 300/799: loss=0.4436691735043268\n",
      "GD iter. 301/799: loss=0.4436501215085135\n",
      "GD iter. 302/799: loss=0.4436312113100069\n",
      "GD iter. 303/799: loss=0.44361244138205935\n",
      "GD iter. 304/799: loss=0.44359381021806105\n",
      "GD iter. 305/799: loss=0.44357531633122216\n",
      "GD iter. 306/799: loss=0.44355695825425995\n",
      "GD iter. 307/799: loss=0.4435387345390926\n",
      "GD iter. 308/799: loss=0.44352064375653827\n",
      "GD iter. 309/799: loss=0.4435026844960203\n",
      "GD iter. 310/799: loss=0.4434848553652776\n",
      "GD iter. 311/799: loss=0.44346715499007955\n",
      "GD iter. 312/799: loss=0.4434495820139483\n",
      "GD iter. 313/799: loss=0.44343213509788393\n",
      "GD iter. 314/799: loss=0.4434148129200959\n",
      "GD iter. 315/799: loss=0.44339761417573925\n",
      "GD iter. 316/799: loss=0.4433805375766551\n",
      "GD iter. 317/799: loss=0.44336358185111674\n",
      "GD iter. 318/799: loss=0.4433467457435795\n",
      "GD iter. 319/799: loss=0.44333002801443555\n",
      "GD iter. 320/799: loss=0.4433134274397729\n",
      "GD iter. 321/799: loss=0.4432969428111393\n",
      "GD iter. 322/799: loss=0.44328057293530915\n",
      "GD iter. 323/799: loss=0.443264316634056\n",
      "GD iter. 324/799: loss=0.44324817274392825\n",
      "GD iter. 325/799: loss=0.4432321401160287\n",
      "GD iter. 326/799: loss=0.44321621761579877\n",
      "GD iter. 327/799: loss=0.44320040412280576\n",
      "GD iter. 328/799: loss=0.44318469853053366\n",
      "GD iter. 329/799: loss=0.4431690997461796\n",
      "GD iter. 330/799: loss=0.4431536066904504\n",
      "GD iter. 331/799: loss=0.4431382182973659\n",
      "GD iter. 332/799: loss=0.44312293351406395\n",
      "GD iter. 333/799: loss=0.4431077513006093\n",
      "GD iter. 334/799: loss=0.44309267062980573\n",
      "GD iter. 335/799: loss=0.4430776904870111\n",
      "GD iter. 336/799: loss=0.4430628098699564\n",
      "GD iter. 337/799: loss=0.44304802778856667\n",
      "GD iter. 338/799: loss=0.44303334326478616\n",
      "GD iter. 339/799: loss=0.44301875533240564\n",
      "GD iter. 340/799: loss=0.44300426303689305\n",
      "GD iter. 341/799: loss=0.4429898654352269\n",
      "GD iter. 342/799: loss=0.44297556159573215\n",
      "GD iter. 343/799: loss=0.44296135059791975\n",
      "GD iter. 344/799: loss=0.44294723153232757\n",
      "GD iter. 345/799: loss=0.44293320350036525\n",
      "GD iter. 346/799: loss=0.4429192656141606\n",
      "GD iter. 347/799: loss=0.44290541699640945\n",
      "GD iter. 348/799: loss=0.442891656780227\n",
      "GD iter. 349/799: loss=0.4428779841090025\n",
      "GD iter. 350/799: loss=0.44286439813625594\n",
      "GD iter. 351/799: loss=0.4428508980254971\n",
      "GD iter. 352/799: loss=0.44283748295008696\n",
      "GD iter. 353/799: loss=0.44282415209310105\n",
      "GD iter. 354/799: loss=0.4428109046471956\n",
      "GD iter. 355/799: loss=0.4427977398144755\n",
      "GD iter. 356/799: loss=0.4427846568063644\n",
      "GD iter. 357/799: loss=0.442771654843477\n",
      "GD iter. 358/799: loss=0.4427587331554933\n",
      "GD iter. 359/799: loss=0.4427458909810348\n",
      "GD iter. 360/799: loss=0.442733127567543\n",
      "GD iter. 361/799: loss=0.44272044217115974\n",
      "GD iter. 362/799: loss=0.4427078340566089\n",
      "GD iter. 363/799: loss=0.4426953024970805\n",
      "GD iter. 364/799: loss=0.442682846774117\n",
      "GD iter. 365/799: loss=0.44267046617750044\n",
      "GD iter. 366/799: loss=0.44265816000514174\n",
      "GD iter. 367/799: loss=0.4426459275629727\n",
      "GD iter. 368/799: loss=0.44263376816483757\n",
      "GD iter. 369/799: loss=0.44262168113238837\n",
      "GD iter. 370/799: loss=0.44260966579498107\n",
      "GD iter. 371/799: loss=0.44259772148957305\n",
      "GD iter. 372/799: loss=0.44258584756062247\n",
      "GD iter. 373/799: loss=0.4425740433599891\n",
      "GD iter. 374/799: loss=0.4425623082468371\n",
      "GD iter. 375/799: loss=0.44255064158753854\n",
      "GD iter. 376/799: loss=0.44253904275557904\n",
      "GD iter. 377/799: loss=0.44252751113146427\n",
      "GD iter. 378/799: loss=0.4425160461026287\n",
      "GD iter. 379/799: loss=0.44250464706334497\n",
      "GD iter. 380/799: loss=0.44249331341463455\n",
      "GD iter. 381/799: loss=0.44248204456418083\n",
      "GD iter. 382/799: loss=0.4424708399262423\n",
      "GD iter. 383/799: loss=0.44245969892156767\n",
      "GD iter. 384/799: loss=0.44244862097731197\n",
      "GD iter. 385/799: loss=0.4424376055269544\n",
      "GD iter. 386/799: loss=0.44242665201021686\n",
      "GD iter. 387/799: loss=0.44241575987298354\n",
      "GD iter. 388/799: loss=0.44240492856722297\n",
      "GD iter. 389/799: loss=0.44239415755090933\n",
      "GD iter. 390/799: loss=0.4423834462879466\n",
      "GD iter. 391/799: loss=0.4423727942480929\n",
      "GD iter. 392/799: loss=0.4423622009068861\n",
      "GD iter. 393/799: loss=0.44235166574557067\n",
      "GD iter. 394/799: loss=0.4423411882510256\n",
      "GD iter. 395/799: loss=0.44233076791569337\n",
      "GD iter. 396/799: loss=0.4423204042375096\n",
      "GD iter. 397/799: loss=0.44231009671983446\n",
      "GD iter. 398/799: loss=0.44229984487138424\n",
      "GD iter. 399/799: loss=0.4422896482061643\n",
      "GD iter. 400/799: loss=0.4422795062434036\n",
      "GD iter. 401/799: loss=0.4422694185074886\n",
      "GD iter. 402/799: loss=0.4422593845278999\n",
      "GD iter. 403/799: loss=0.4422494038391487\n",
      "GD iter. 404/799: loss=0.44223947598071417\n",
      "GD iter. 405/799: loss=0.44222960049698223\n",
      "GD iter. 406/799: loss=0.44221977693718495\n",
      "GD iter. 407/799: loss=0.44221000485534107\n",
      "GD iter. 408/799: loss=0.44220028381019605\n",
      "GD iter. 409/799: loss=0.4421906133651657\n",
      "GD iter. 410/799: loss=0.4421809930882773\n",
      "GD iter. 411/799: loss=0.44217142255211417\n",
      "GD iter. 412/799: loss=0.44216190133375977\n",
      "GD iter. 413/799: loss=0.4421524290147428\n",
      "GD iter. 414/799: loss=0.4421430051809828\n",
      "GD iter. 415/799: loss=0.44213362942273765\n",
      "GD iter. 416/799: loss=0.44212430133455005\n",
      "GD iter. 417/799: loss=0.44211502051519624\n",
      "GD iter. 418/799: loss=0.4421057865676345\n",
      "GD iter. 419/799: loss=0.44209659909895505\n",
      "GD iter. 420/799: loss=0.44208745772033037\n",
      "GD iter. 421/799: loss=0.4420783620469653\n",
      "GD iter. 422/799: loss=0.4420693116980498\n",
      "GD iter. 423/799: loss=0.4420603062967108\n",
      "GD iter. 424/799: loss=0.4420513454699645\n",
      "GD iter. 425/799: loss=0.44204242884867107\n",
      "GD iter. 426/799: loss=0.4420335560674881\n",
      "GD iter. 427/799: loss=0.44202472676482574\n",
      "GD iter. 428/799: loss=0.44201594058280175\n",
      "GD iter. 429/799: loss=0.4420071971671984\n",
      "GD iter. 430/799: loss=0.44199849616741804\n",
      "GD iter. 431/799: loss=0.4419898372364413\n",
      "GD iter. 432/799: loss=0.4419812200307844\n",
      "GD iter. 433/799: loss=0.4419726442104575\n",
      "GD iter. 434/799: loss=0.4419641094389239\n",
      "GD iter. 435/799: loss=0.4419556153830593\n",
      "GD iter. 436/799: loss=0.4419471617131119\n",
      "GD iter. 437/799: loss=0.4419387481026632\n",
      "GD iter. 438/799: loss=0.44193037422858855\n",
      "GD iter. 439/799: loss=0.44192203977101957\n",
      "GD iter. 440/799: loss=0.4419137444133052\n",
      "GD iter. 441/799: loss=0.4419054878419753\n",
      "GD iter. 442/799: loss=0.44189726974670324\n",
      "GD iter. 443/799: loss=0.4418890898202698\n",
      "GD iter. 444/799: loss=0.44188094775852665\n",
      "GD iter. 445/799: loss=0.4418728432603619\n",
      "GD iter. 446/799: loss=0.441864776027664\n",
      "GD iter. 447/799: loss=0.4418567457652881\n",
      "GD iter. 448/799: loss=0.44184875218102143\n",
      "GD iter. 449/799: loss=0.4418407949855501\n",
      "GD iter. 450/799: loss=0.4418328738924255\n",
      "GD iter. 451/799: loss=0.44182498861803166\n",
      "GD iter. 452/799: loss=0.4418171388815533\n",
      "GD iter. 453/799: loss=0.4418093244049431\n",
      "GD iter. 454/799: loss=0.44180154491289086\n",
      "GD iter. 455/799: loss=0.44179380013279274\n",
      "GD iter. 456/799: loss=0.44178608979471906\n",
      "GD iter. 457/799: loss=0.44177841363138587\n",
      "GD iter. 458/799: loss=0.44177077137812365\n",
      "GD iter. 459/799: loss=0.4417631627728485\n",
      "GD iter. 460/799: loss=0.4417555875560326\n",
      "GD iter. 461/799: loss=0.44174804547067575\n",
      "GD iter. 462/799: loss=0.44174053626227655\n",
      "GD iter. 463/799: loss=0.44173305967880494\n",
      "GD iter. 464/799: loss=0.441725615470674\n",
      "GD iter. 465/799: loss=0.44171820339071266\n",
      "GD iter. 466/799: loss=0.44171082319413896\n",
      "GD iter. 467/799: loss=0.44170347463853304\n",
      "GD iter. 468/799: loss=0.4416961574838114\n",
      "GD iter. 469/799: loss=0.4416888714922002\n",
      "GD iter. 470/799: loss=0.4416816164282099\n",
      "GD iter. 471/799: loss=0.4416743920586102\n",
      "GD iter. 472/799: loss=0.4416671981524046\n",
      "GD iter. 473/799: loss=0.4416600344808059\n",
      "GD iter. 474/799: loss=0.4416529008172115\n",
      "GD iter. 475/799: loss=0.4416457969371801\n",
      "GD iter. 476/799: loss=0.4416387226184068\n",
      "GD iter. 477/799: loss=0.4416316776407006\n",
      "GD iter. 478/799: loss=0.4416246617859603\n",
      "GD iter. 479/799: loss=0.4416176748381525\n",
      "GD iter. 480/799: loss=0.44161071658328804\n",
      "GD iter. 481/799: loss=0.4416037868094004\n",
      "GD iter. 482/799: loss=0.4415968853065229\n",
      "GD iter. 483/799: loss=0.441590011866668\n",
      "GD iter. 484/799: loss=0.4415831662838044\n",
      "GD iter. 485/799: loss=0.4415763483538366\n",
      "GD iter. 486/799: loss=0.4415695578745836\n",
      "GD iter. 487/799: loss=0.4415627946457584\n",
      "GD iter. 488/799: loss=0.4415560584689473\n",
      "GD iter. 489/799: loss=0.44154934914758953\n",
      "GD iter. 490/799: loss=0.44154266648695734\n",
      "GD iter. 491/799: loss=0.44153601029413647\n",
      "GD iter. 492/799: loss=0.4415293803780063\n",
      "GD iter. 493/799: loss=0.44152277654922045\n",
      "GD iter. 494/799: loss=0.4415161986201882\n",
      "GD iter. 495/799: loss=0.44150964640505547\n",
      "GD iter. 496/799: loss=0.4415031197196858\n",
      "GD iter. 497/799: loss=0.4414966183816426\n",
      "GD iter. 498/799: loss=0.44149014221017074\n",
      "GD iter. 499/799: loss=0.4414836910261784\n",
      "GD iter. 500/799: loss=0.4414772646522196\n",
      "GD iter. 501/799: loss=0.44147086291247667\n",
      "GD iter. 502/799: loss=0.44146448563274293\n",
      "GD iter. 503/799: loss=0.44145813264040545\n",
      "GD iter. 504/799: loss=0.44145180376442833\n",
      "GD iter. 505/799: loss=0.4414454988353361\n",
      "GD iter. 506/799: loss=0.44143921768519695\n",
      "GD iter. 507/799: loss=0.4414329601476064\n",
      "GD iter. 508/799: loss=0.44142672605767186\n",
      "GD iter. 509/799: loss=0.44142051525199577\n",
      "GD iter. 510/799: loss=0.4414143275686604\n",
      "GD iter. 511/799: loss=0.44140816284721246\n",
      "GD iter. 512/799: loss=0.4414020209286475\n",
      "GD iter. 513/799: loss=0.4413959016553944\n",
      "GD iter. 514/799: loss=0.4413898048713012\n",
      "GD iter. 515/799: loss=0.4413837304216197\n",
      "GD iter. 516/799: loss=0.4413776781529908\n",
      "GD iter. 517/799: loss=0.4413716479134302\n",
      "GD iter. 518/799: loss=0.44136563955231406\n",
      "GD iter. 519/799: loss=0.44135965292036505\n",
      "GD iter. 520/799: loss=0.44135368786963786\n",
      "GD iter. 521/799: loss=0.4413477442535059\n",
      "GD iter. 522/799: loss=0.44134182192664745\n",
      "GD iter. 523/799: loss=0.44133592074503214\n",
      "GD iter. 524/799: loss=0.4413300405659078\n",
      "GD iter. 525/799: loss=0.4413241812477867\n",
      "GD iter. 526/799: loss=0.4413183426504337\n",
      "GD iter. 527/799: loss=0.44131252463485166\n",
      "GD iter. 528/799: loss=0.44130672706327057\n",
      "GD iter. 529/799: loss=0.44130094979913337\n",
      "GD iter. 530/799: loss=0.44129519270708484\n",
      "GD iter. 531/799: loss=0.4412894556529582\n",
      "GD iter. 532/799: loss=0.44128373850376357\n",
      "GD iter. 533/799: loss=0.4412780411276759\n",
      "GD iter. 534/799: loss=0.44127236339402287\n",
      "GD iter. 535/799: loss=0.4412667051732733\n",
      "GD iter. 536/799: loss=0.44126106633702566\n",
      "GD iter. 537/799: loss=0.44125544675799644\n",
      "GD iter. 538/799: loss=0.4412498463100086\n",
      "GD iter. 539/799: loss=0.44124426486798074\n",
      "GD iter. 540/799: loss=0.4412387023079159\n",
      "GD iter. 541/799: loss=0.4412331585068904\n",
      "GD iter. 542/799: loss=0.44122763334304327\n",
      "GD iter. 543/799: loss=0.44122212669556526\n",
      "GD iter. 544/799: loss=0.44121663844468845\n",
      "GD iter. 545/799: loss=0.4412111684716761\n",
      "GD iter. 546/799: loss=0.441205716658811\n",
      "GD iter. 547/799: loss=0.44120028288938695\n",
      "GD iter. 548/799: loss=0.44119486704769756\n",
      "GD iter. 549/799: loss=0.44118946901902634\n",
      "GD iter. 550/799: loss=0.44118408868963704\n",
      "GD iter. 551/799: loss=0.441178725946764\n",
      "GD iter. 552/799: loss=0.4411733806786019\n",
      "GD iter. 553/799: loss=0.44116805277429666\n",
      "GD iter. 554/799: loss=0.44116274212393597\n",
      "GD iter. 555/799: loss=0.4411574486185395\n",
      "GD iter. 556/799: loss=0.4411521721500502\n",
      "GD iter. 557/799: loss=0.44114691261132477\n",
      "GD iter. 558/799: loss=0.4411416698961249\n",
      "GD iter. 559/799: loss=0.4411364438991079\n",
      "GD iter. 560/799: loss=0.44113123451581826\n",
      "GD iter. 561/799: loss=0.44112604164267855\n",
      "GD iter. 562/799: loss=0.44112086517698124\n",
      "GD iter. 563/799: loss=0.4411157050168796\n",
      "GD iter. 564/799: loss=0.4411105610613795\n",
      "GD iter. 565/799: loss=0.44110543321033113\n",
      "GD iter. 566/799: loss=0.4411003213644204\n",
      "GD iter. 567/799: loss=0.44109522542516133\n",
      "GD iter. 568/799: loss=0.44109014529488694\n",
      "GD iter. 569/799: loss=0.4410850808767424\n",
      "GD iter. 570/799: loss=0.44108003207467644\n",
      "GD iter. 571/799: loss=0.4410749987934333\n",
      "GD iter. 572/799: loss=0.4410699809385459\n",
      "GD iter. 573/799: loss=0.44106497841632714\n",
      "GD iter. 574/799: loss=0.4410599911338627\n",
      "GD iter. 575/799: loss=0.4410550189990041\n",
      "GD iter. 576/799: loss=0.4410500619203603\n",
      "GD iter. 577/799: loss=0.4410451198072907\n",
      "GD iter. 578/799: loss=0.4410401925698988\n",
      "GD iter. 579/799: loss=0.4410352801190233\n",
      "GD iter. 580/799: loss=0.4410303823662327\n",
      "GD iter. 581/799: loss=0.44102549922381684\n",
      "GD iter. 582/799: loss=0.4410206306047812\n",
      "GD iter. 583/799: loss=0.44101577642283896\n",
      "GD iter. 584/799: loss=0.44101093659240487\n",
      "GD iter. 585/799: loss=0.44100611102858833\n",
      "GD iter. 586/799: loss=0.4410012996471866\n",
      "GD iter. 587/799: loss=0.4409965023646785\n",
      "GD iter. 588/799: loss=0.44099171909821777\n",
      "GD iter. 589/799: loss=0.4409869497656262\n",
      "GD iter. 590/799: loss=0.4409821942853881\n",
      "GD iter. 591/799: loss=0.4409774525766432\n",
      "GD iter. 592/799: loss=0.44097272455918085\n",
      "GD iter. 593/799: loss=0.4409680101534338\n",
      "GD iter. 594/799: loss=0.440963309280472\n",
      "GD iter. 595/799: loss=0.44095862186199636\n",
      "GD iter. 596/799: loss=0.44095394782033337\n",
      "GD iter. 597/799: loss=0.44094928707842845\n",
      "GD iter. 598/799: loss=0.44094463955984065\n",
      "GD iter. 599/799: loss=0.44094000518873644\n",
      "GD iter. 600/799: loss=0.44093538388988457\n",
      "GD iter. 601/799: loss=0.44093077558864974\n",
      "GD iter. 602/799: loss=0.4409261802109873\n",
      "GD iter. 603/799: loss=0.4409215976834377\n",
      "GD iter. 604/799: loss=0.44091702793312104\n",
      "GD iter. 605/799: loss=0.44091247088773156\n",
      "GD iter. 606/799: loss=0.4409079264755321\n",
      "GD iter. 607/799: loss=0.4409033946253492\n",
      "GD iter. 608/799: loss=0.44089887526656746\n",
      "GD iter. 609/799: loss=0.4408943683291245\n",
      "GD iter. 610/799: loss=0.4408898737435056\n",
      "GD iter. 611/799: loss=0.4408853914407389\n",
      "GD iter. 612/799: loss=0.4408809213523903\n",
      "GD iter. 613/799: loss=0.4408764634105582\n",
      "GD iter. 614/799: loss=0.44087201754786887\n",
      "GD iter. 615/799: loss=0.4408675836974711\n",
      "GD iter. 616/799: loss=0.4408631617930322\n",
      "GD iter. 617/799: loss=0.4408587517687324\n",
      "GD iter. 618/799: loss=0.4408543535592603\n",
      "GD iter. 619/799: loss=0.4408499670998084\n",
      "GD iter. 620/799: loss=0.4408455923260683\n",
      "GD iter. 621/799: loss=0.4408412291742262\n",
      "GD iter. 622/799: loss=0.44083687758095835\n",
      "GD iter. 623/799: loss=0.4408325374834262\n",
      "GD iter. 624/799: loss=0.44082820881927254\n",
      "GD iter. 625/799: loss=0.44082389152661655\n",
      "GD iter. 626/799: loss=0.4408195855440498\n",
      "GD iter. 627/799: loss=0.4408152908106318\n",
      "GD iter. 628/799: loss=0.4408110072658858\n",
      "GD iter. 629/799: loss=0.44080673484979427\n",
      "GD iter. 630/799: loss=0.4408024735027951\n",
      "GD iter. 631/799: loss=0.4407982231657772\n",
      "GD iter. 632/799: loss=0.44079398378007667\n",
      "GD iter. 633/799: loss=0.44078975528747244\n",
      "GD iter. 634/799: loss=0.44078553763018224\n",
      "GD iter. 635/799: loss=0.44078133075085874\n",
      "GD iter. 636/799: loss=0.4407771345925861\n",
      "GD iter. 637/799: loss=0.440772949098875\n",
      "GD iter. 638/799: loss=0.4407687742136597\n",
      "GD iter. 639/799: loss=0.4407646098812938\n",
      "GD iter. 640/799: loss=0.4407604560465467\n",
      "GD iter. 641/799: loss=0.4407563126545995\n",
      "GD iter. 642/799: loss=0.4407521796510418\n",
      "GD iter. 643/799: loss=0.4407480569818675\n",
      "GD iter. 644/799: loss=0.44074394459347166\n",
      "GD iter. 645/799: loss=0.44073984243264647\n",
      "GD iter. 646/799: loss=0.4407357504465779\n",
      "GD iter. 647/799: loss=0.4407316685828425\n",
      "GD iter. 648/799: loss=0.4407275967894032\n",
      "GD iter. 649/799: loss=0.4407235350146065\n",
      "GD iter. 650/799: loss=0.4407194832071787\n",
      "GD iter. 651/799: loss=0.4407154413162227\n",
      "GD iter. 652/799: loss=0.44071140929121455\n",
      "GD iter. 653/799: loss=0.44070738708200013\n",
      "GD iter. 654/799: loss=0.4407033746387921\n",
      "GD iter. 655/799: loss=0.4406993719121663\n",
      "GD iter. 656/799: loss=0.44069537885305876\n",
      "GD iter. 657/799: loss=0.4406913954127624\n",
      "GD iter. 658/799: loss=0.440687421542924\n",
      "GD iter. 659/799: loss=0.44068345719554114\n",
      "GD iter. 660/799: loss=0.44067950232295866\n",
      "GD iter. 661/799: loss=0.4406755568778663\n",
      "GD iter. 662/799: loss=0.44067162081329514\n",
      "GD iter. 663/799: loss=0.44066769408261475\n",
      "GD iter. 664/799: loss=0.4406637766395302\n",
      "GD iter. 665/799: loss=0.4406598684380794\n",
      "GD iter. 666/799: loss=0.4406559694326297\n",
      "GD iter. 667/799: loss=0.44065207957787544\n",
      "GD iter. 668/799: loss=0.4406481988288349\n",
      "GD iter. 669/799: loss=0.44064432714084734\n",
      "GD iter. 670/799: loss=0.44064046446957056\n",
      "GD iter. 671/799: loss=0.44063661077097777\n",
      "GD iter. 672/799: loss=0.4406327660013552\n",
      "GD iter. 673/799: loss=0.4406289301172991\n",
      "GD iter. 674/799: loss=0.4406251030757132\n",
      "GD iter. 675/799: loss=0.4406212848338058\n",
      "GD iter. 676/799: loss=0.44061747534908774\n",
      "GD iter. 677/799: loss=0.4406136745793691\n",
      "GD iter. 678/799: loss=0.44060988248275673\n",
      "GD iter. 679/799: loss=0.44060609901765246\n",
      "GD iter. 680/799: loss=0.4406023241427493\n",
      "GD iter. 681/799: loss=0.44059855781703\n",
      "GD iter. 682/799: loss=0.4405947999997642\n",
      "GD iter. 683/799: loss=0.4405910506505057\n",
      "GD iter. 684/799: loss=0.4405873097290905\n",
      "GD iter. 685/799: loss=0.4405835771956338\n",
      "GD iter. 686/799: loss=0.4405798530105284\n",
      "GD iter. 687/799: loss=0.44057613713444155\n",
      "GD iter. 688/799: loss=0.4405724295283131\n",
      "GD iter. 689/799: loss=0.4405687301533532\n",
      "GD iter. 690/799: loss=0.44056503897103944\n",
      "GD iter. 691/799: loss=0.44056135594311513\n",
      "GD iter. 692/799: loss=0.44055768103158716\n",
      "GD iter. 693/799: loss=0.44055401419872303\n",
      "GD iter. 694/799: loss=0.44055035540704934\n",
      "GD iter. 695/799: loss=0.4405467046193494\n",
      "GD iter. 696/799: loss=0.440543061798661\n",
      "GD iter. 697/799: loss=0.44053942690827413\n",
      "GD iter. 698/799: loss=0.4405357999117291\n",
      "GD iter. 699/799: loss=0.4405321807728144\n",
      "GD iter. 700/799: loss=0.44052856945556457\n",
      "GD iter. 701/799: loss=0.4405249659242578\n",
      "GD iter. 702/799: loss=0.4405213701434147\n",
      "GD iter. 703/799: loss=0.4405177820777953\n",
      "GD iter. 704/799: loss=0.44051420169239797\n",
      "GD iter. 705/799: loss=0.4405106289524565\n",
      "GD iter. 706/799: loss=0.4405070638234389\n",
      "GD iter. 707/799: loss=0.44050350627104506\n",
      "GD iter. 708/799: loss=0.44049995626120503\n",
      "GD iter. 709/799: loss=0.44049641376007675\n",
      "GD iter. 710/799: loss=0.4404928787340445\n",
      "GD iter. 711/799: loss=0.4404893511497172\n",
      "GD iter. 712/799: loss=0.44048583097392563\n",
      "GD iter. 713/799: loss=0.44048231817372174\n",
      "GD iter. 714/799: loss=0.44047881271637634\n",
      "GD iter. 715/799: loss=0.44047531456937666\n",
      "GD iter. 716/799: loss=0.44047182370042587\n",
      "GD iter. 717/799: loss=0.44046834007744007\n",
      "GD iter. 718/799: loss=0.44046486366854726\n",
      "GD iter. 719/799: loss=0.44046139444208554\n",
      "GD iter. 720/799: loss=0.4404579323666009\n",
      "GD iter. 721/799: loss=0.44045447741084603\n",
      "GD iter. 722/799: loss=0.44045102954377846\n",
      "GD iter. 723/799: loss=0.4404475887345589\n",
      "GD iter. 724/799: loss=0.44044415495254935\n",
      "GD iter. 725/799: loss=0.4404407281673119\n",
      "GD iter. 726/799: loss=0.44043730834860656\n",
      "GD iter. 727/799: loss=0.4404338954663903\n",
      "GD iter. 728/799: loss=0.44043048949081476\n",
      "GD iter. 729/799: loss=0.4404270903922251\n",
      "GD iter. 730/799: loss=0.4404236981411584\n",
      "GD iter. 731/799: loss=0.44042031270834203\n",
      "GD iter. 732/799: loss=0.4404169340646919\n",
      "GD iter. 733/799: loss=0.44041356218131134\n",
      "GD iter. 734/799: loss=0.4404101970294893\n",
      "GD iter. 735/799: loss=0.44040683858069896\n",
      "GD iter. 736/799: loss=0.4404034868065963\n",
      "GD iter. 737/799: loss=0.44040014167901825\n",
      "GD iter. 738/799: loss=0.440396803169982\n",
      "GD iter. 739/799: loss=0.4403934712516827\n",
      "GD iter. 740/799: loss=0.44039014589649245\n",
      "GD iter. 741/799: loss=0.44038682707695914\n",
      "GD iter. 742/799: loss=0.44038351476580456\n",
      "GD iter. 743/799: loss=0.440380208935923\n",
      "GD iter. 744/799: loss=0.44037690956038045\n",
      "GD iter. 745/799: loss=0.44037361661241275\n",
      "GD iter. 746/799: loss=0.44037033006542414\n",
      "GD iter. 747/799: loss=0.4403670498929864\n",
      "GD iter. 748/799: loss=0.44036377606883703\n",
      "GD iter. 749/799: loss=0.44036050856687836\n",
      "GD iter. 750/799: loss=0.4403572473611757\n",
      "GD iter. 751/799: loss=0.44035399242595685\n",
      "GD iter. 752/799: loss=0.44035074373560984\n",
      "GD iter. 753/799: loss=0.4403475012646826\n",
      "GD iter. 754/799: loss=0.44034426498788104\n",
      "GD iter. 755/799: loss=0.4403410348800681\n",
      "GD iter. 756/799: loss=0.44033781091626245\n",
      "GD iter. 757/799: loss=0.4403345930716372\n",
      "GD iter. 758/799: loss=0.4403313813215188\n",
      "GD iter. 759/799: loss=0.440328175641386\n",
      "GD iter. 760/799: loss=0.440324976006868\n",
      "GD iter. 761/799: loss=0.44032178239374414\n",
      "GD iter. 762/799: loss=0.44031859477794216\n",
      "GD iter. 763/799: loss=0.4403154131355371\n",
      "GD iter. 764/799: loss=0.44031223744275044\n",
      "GD iter. 765/799: loss=0.44030906767594863\n",
      "GD iter. 766/799: loss=0.4403059038116419\n",
      "GD iter. 767/799: loss=0.4403027458264839\n",
      "GD iter. 768/799: loss=0.44029959369726945\n",
      "GD iter. 769/799: loss=0.44029644740093443\n",
      "GD iter. 770/799: loss=0.4402933069145541\n",
      "GD iter. 771/799: loss=0.4402901722153423\n",
      "GD iter. 772/799: loss=0.44028704328064994\n",
      "GD iter. 773/799: loss=0.4402839200879649\n",
      "GD iter. 774/799: loss=0.4402808026149099\n",
      "GD iter. 775/799: loss=0.4402776908392422\n",
      "GD iter. 776/799: loss=0.4402745847388521\n",
      "GD iter. 777/799: loss=0.4402714842917622\n",
      "GD iter. 778/799: loss=0.4402683894761264\n",
      "GD iter. 779/799: loss=0.4402653002702286\n",
      "GD iter. 780/799: loss=0.4402622166524821\n",
      "GD iter. 781/799: loss=0.4402591386014283\n",
      "GD iter. 782/799: loss=0.44025606609573587\n",
      "GD iter. 783/799: loss=0.4402529991141999\n",
      "GD iter. 784/799: loss=0.4402499376357406\n",
      "GD iter. 785/799: loss=0.44024688163940257\n",
      "GD iter. 786/799: loss=0.44024383110435394\n",
      "GD iter. 787/799: loss=0.44024078600988525\n",
      "GD iter. 788/799: loss=0.44023774633540863\n",
      "GD iter. 789/799: loss=0.4402347120604569\n",
      "GD iter. 790/799: loss=0.4402316831646824\n",
      "GD iter. 791/799: loss=0.4402286596278566\n",
      "GD iter. 792/799: loss=0.4402256414298688\n",
      "GD iter. 793/799: loss=0.4402226285507251\n",
      "GD iter. 794/799: loss=0.4402196209705481\n",
      "GD iter. 795/799: loss=0.44021661866957557\n",
      "GD iter. 796/799: loss=0.4402136216281598\n",
      "GD iter. 797/799: loss=0.4402106298267664\n",
      "GD iter. 798/799: loss=0.44020764324597417\n",
      "GD iter. 799/799: loss=0.4402046618664733\n",
      "The Accuracy is: 0.8627\n",
      "The F1 score is: 0.4344\n",
      "The precision is: 0.3353\n",
      "The recall is: 0.6168\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using all the features except for those having NaN values over 50% ##\n",
    "initial_w = np.random.randn(x_train_processed.shape[1]) * 0.01\n",
    "w, loss = logistic_regression(y_train_processed, x_train_processed, initial_w, max_iters=800, gamma=0.15)\n",
    "predict_acc(x_train_processed_orig, y_train_processed_orig, w, logistic=False, threshold=0.85)\n",
    "predict_f1(x_train_processed_orig, y_train_processed_orig, w, logistic=False, threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/499: loss=0.696055437077474\n",
      "GD iter. 1/499: loss=0.6140070182275016\n",
      "GD iter. 2/499: loss=0.5793007369331515\n",
      "GD iter. 3/499: loss=0.5617015637942935\n",
      "GD iter. 4/499: loss=0.5511577807553913\n",
      "GD iter. 5/499: loss=0.543946785314281\n",
      "GD iter. 6/499: loss=0.5384952331598506\n",
      "GD iter. 7/499: loss=0.5340644995280746\n",
      "GD iter. 8/499: loss=0.5302780440720292\n",
      "GD iter. 9/499: loss=0.5269303554203718\n",
      "GD iter. 10/499: loss=0.523902283587119\n",
      "GD iter. 11/499: loss=0.5211207330047702\n",
      "GD iter. 12/499: loss=0.5185383528744661\n",
      "GD iter. 13/499: loss=0.5161228180092293\n",
      "GD iter. 14/499: loss=0.5138509440002619\n",
      "GD iter. 15/499: loss=0.5117053407662869\n",
      "GD iter. 16/499: loss=0.5096724455944445\n",
      "GD iter. 17/499: loss=0.5077413287851396\n",
      "GD iter. 18/499: loss=0.5059029441120507\n",
      "GD iter. 19/499: loss=0.504149642270499\n",
      "GD iter. 20/499: loss=0.5024748440162808\n",
      "GD iter. 21/499: loss=0.5008728129818297\n",
      "GD iter. 22/499: loss=0.4993384925280153\n",
      "GD iter. 23/499: loss=0.49786738498153016\n",
      "GD iter. 24/499: loss=0.49645545979117944\n",
      "GD iter. 25/499: loss=0.49509908201271197\n",
      "GD iter. 26/499: loss=0.49379495549464375\n",
      "GD iter. 27/499: loss=0.4925400769755365\n",
      "GD iter. 28/499: loss=0.49133169846936636\n",
      "GD iter. 29/499: loss=0.4901672960735511\n",
      "GD iter. 30/499: loss=0.489044543839468\n",
      "GD iter. 31/499: loss=0.4879612916909394\n",
      "GD iter. 32/499: loss=0.4869155466187503\n",
      "GD iter. 33/499: loss=0.4859054565536969\n",
      "GD iter. 34/499: loss=0.4849292964489401\n",
      "GD iter. 35/499: loss=0.4839854561986649\n",
      "GD iter. 36/499: loss=0.4830724300934814\n",
      "GD iter. 37/499: loss=0.48218880756988974\n",
      "GD iter. 38/499: loss=0.481333265055734\n",
      "GD iter. 39/499: loss=0.4805045587489158\n",
      "GD iter. 40/499: loss=0.4797015181948878\n",
      "GD iter. 41/499: loss=0.4789230405511964\n",
      "GD iter. 42/499: loss=0.47816808544577644\n",
      "GD iter. 43/499: loss=0.4774356703507281\n",
      "GD iter. 44/499: loss=0.4767248664056034\n",
      "GD iter. 45/499: loss=0.4760347946343622\n",
      "GD iter. 46/499: loss=0.47536462250851763\n",
      "GD iter. 47/499: loss=0.47471356081593347\n",
      "GD iter. 48/499: loss=0.47408086080052386\n",
      "GD iter. 49/499: loss=0.4734658115429303\n",
      "GD iter. 50/499: loss=0.472867737556321\n",
      "GD iter. 51/499: loss=0.4722859965748682\n",
      "GD iter. 52/499: loss=0.4717199775153565\n",
      "GD iter. 53/499: loss=0.47116909859482664\n",
      "GD iter. 54/499: loss=0.470632805589251\n",
      "GD iter. 55/499: loss=0.470110570220027\n",
      "GD iter. 56/499: loss=0.4696018886566048\n",
      "GD iter. 57/499: loss=0.4691062801248922\n",
      "GD iter. 58/499: loss=0.46862328561221905\n",
      "GD iter. 59/499: loss=0.4681524666606344\n",
      "GD iter. 60/499: loss=0.4676934042411739\n",
      "GD iter. 61/499: loss=0.46724569770248797\n",
      "GD iter. 62/499: loss=0.4668089637878799\n",
      "GD iter. 63/499: loss=0.4663828357153853\n",
      "GD iter. 64/499: loss=0.4659669623160375\n",
      "GD iter. 65/499: loss=0.4655610072259124\n",
      "GD iter. 66/499: loss=0.46516464812795355\n",
      "GD iter. 67/499: loss=0.46477757603993125\n",
      "GD iter. 68/499: loss=0.4643994946452135\n",
      "GD iter. 69/499: loss=0.46403011966330804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 70/499: loss=0.463669178257393\n",
      "GD iter. 71/499: loss=0.46331640847628486\n",
      "GD iter. 72/499: loss=0.4629715587284988\n",
      "GD iter. 73/499: loss=0.4626343872862444\n",
      "GD iter. 74/499: loss=0.46230466181737045\n",
      "GD iter. 75/499: loss=0.46198215894342504\n",
      "GD iter. 76/499: loss=0.46166666382213967\n",
      "GD iter. 77/499: loss=0.46135796975276966\n",
      "GD iter. 78/499: loss=0.46105587780284407\n",
      "GD iter. 79/499: loss=0.4607601964549809\n",
      "GD iter. 80/499: loss=0.46047074127252136\n",
      "GD iter. 81/499: loss=0.46018733458282834\n",
      "GD iter. 82/499: loss=0.4599098051771709\n",
      "GD iter. 83/499: loss=0.459637988026198\n",
      "GD iter. 84/499: loss=0.45937172401006726\n",
      "GD iter. 85/499: loss=0.4591108596623632\n",
      "GD iter. 86/499: loss=0.45885524692699486\n",
      "GD iter. 87/499: loss=0.45860474292731856\n",
      "GD iter. 88/499: loss=0.4583592097467799\n",
      "GD iter. 89/499: loss=0.4581185142204165\n",
      "GD iter. 90/499: loss=0.4578825277366064\n",
      "GD iter. 91/499: loss=0.4576511260484841\n",
      "GD iter. 92/499: loss=0.45742418909448535\n",
      "GD iter. 93/499: loss=0.4572016008275153\n",
      "GD iter. 94/499: loss=0.4569832490522657\n",
      "GD iter. 95/499: loss=0.4567690252702364\n",
      "GD iter. 96/499: loss=0.4565588245320446\n",
      "GD iter. 97/499: loss=0.45635254529663116\n",
      "GD iter. 98/499: loss=0.45615008929699197\n",
      "GD iter. 99/499: loss=0.45595136141209497\n",
      "GD iter. 100/499: loss=0.45575626954465176\n",
      "GD iter. 101/499: loss=0.45556472450444224\n",
      "GD iter. 102/499: loss=0.45537663989690136\n",
      "GD iter. 103/499: loss=0.4551919320166989\n",
      "GD iter. 104/499: loss=0.4550105197460554\n",
      "GD iter. 105/499: loss=0.45483232445755645\n",
      "GD iter. 106/499: loss=0.4546572699212333\n",
      "GD iter. 107/499: loss=0.4544852822157006\n",
      "GD iter. 108/499: loss=0.4543162896431483\n",
      "GD iter. 109/499: loss=0.45415022264799376\n",
      "GD iter. 110/499: loss=0.4539870137390185\n",
      "GD iter. 111/499: loss=0.4538265974148165\n",
      "GD iter. 112/499: loss=0.4536689100923946\n",
      "GD iter. 113/499: loss=0.45351389003877124\n",
      "GD iter. 114/499: loss=0.4533614773054333\n",
      "GD iter. 115/499: loss=0.4532116136655106\n",
      "GD iter. 116/499: loss=0.4530642425535423\n",
      "GD iter. 117/499: loss=0.4529193090077118\n",
      "GD iter. 118/499: loss=0.45277675961443675\n",
      "GD iter. 119/499: loss=0.4526365424552006\n",
      "GD iter. 120/499: loss=0.4524986070555266\n",
      "GD iter. 121/499: loss=0.4523629043359929\n",
      "GD iter. 122/499: loss=0.4522293865651972\n",
      "GD iter. 123/499: loss=0.45209800731458016\n",
      "GD iter. 124/499: loss=0.4519687214150266\n",
      "GD iter. 125/499: loss=0.4518414849151628\n",
      "GD iter. 126/499: loss=0.4517162550412737\n",
      "GD iter. 127/499: loss=0.45159299015877086\n",
      "GD iter. 128/499: loss=0.4514716497351386\n",
      "GD iter. 129/499: loss=0.45135219430429785\n",
      "GD iter. 130/499: loss=0.4512345854323227\n",
      "GD iter. 131/499: loss=0.45111878568445224\n",
      "GD iter. 132/499: loss=0.4510047585933411\n",
      "GD iter. 133/499: loss=0.450892468628497\n",
      "GD iter. 134/499: loss=0.45078188116685314\n",
      "GD iter. 135/499: loss=0.45067296246442695\n",
      "GD iter. 136/499: loss=0.45056567962902183\n",
      "GD iter. 137/499: loss=0.4504600005939245\n",
      "GD iter. 138/499: loss=0.45035589409256\n",
      "GD iter. 139/499: loss=0.45025332963406145\n",
      "GD iter. 140/499: loss=0.450152277479719\n",
      "GD iter. 141/499: loss=0.4500527086202702\n",
      "GD iter. 142/499: loss=0.4499545947539987\n",
      "GD iter. 143/499: loss=0.4498579082656074\n",
      "GD iter. 144/499: loss=0.44976262220583546\n",
      "GD iter. 145/499: loss=0.4496687102717885\n",
      "GD iter. 146/499: loss=0.4495761467879537\n",
      "GD iter. 147/499: loss=0.4494849066878731\n",
      "GD iter. 148/499: loss=0.4493949654964479\n",
      "GD iter. 149/499: loss=0.4493062993128492\n",
      "GD iter. 150/499: loss=0.4492188847940128\n",
      "GD iter. 151/499: loss=0.4491326991386925\n",
      "GD iter. 152/499: loss=0.4490477200720526\n",
      "GD iter. 153/499: loss=0.44896392583077777\n",
      "GD iter. 154/499: loss=0.4488812951486802\n",
      "GD iter. 155/499: loss=0.44879980724278495\n",
      "GD iter. 156/499: loss=0.448719441799877\n",
      "GD iter. 157/499: loss=0.4486401789634888\n",
      "GD iter. 158/499: loss=0.4485619993213162\n",
      "GD iter. 159/499: loss=0.4484848838930425\n",
      "GD iter. 160/499: loss=0.4484088141185589\n",
      "GD iter. 161/499: loss=0.44833377184656303\n",
      "GD iter. 162/499: loss=0.44825973932352614\n",
      "GD iter. 163/499: loss=0.4481866991830101\n",
      "GD iter. 164/499: loss=0.4481146344353252\n",
      "GD iter. 165/499: loss=0.44804352845751566\n",
      "GD iter. 166/499: loss=0.4479733649836593\n",
      "GD iter. 167/499: loss=0.44790412809547125\n",
      "GD iter. 168/499: loss=0.44783580221320124\n",
      "GD iter. 169/499: loss=0.4477683720868128\n",
      "GD iter. 170/499: loss=0.44770182278743514\n",
      "GD iter. 171/499: loss=0.44763613969907706\n",
      "GD iter. 172/499: loss=0.4475713085105949\n",
      "GD iter. 173/499: loss=0.4475073152079052\n",
      "GD iter. 174/499: loss=0.4474441460664318\n",
      "GD iter. 175/499: loss=0.4473817876437826\n",
      "GD iter. 176/499: loss=0.44732022677264494\n",
      "GD iter. 177/499: loss=0.4472594505538931\n",
      "GD iter. 178/499: loss=0.4471994463499014\n",
      "GD iter. 179/499: loss=0.44714020177805436\n",
      "GD iter. 180/499: loss=0.4470817047044487\n",
      "GD iter. 181/499: loss=0.4470239432377798\n",
      "GD iter. 182/499: loss=0.4469669057234057\n",
      "GD iter. 183/499: loss=0.4469105807375854\n",
      "GD iter. 184/499: loss=0.44685495708188155\n",
      "GD iter. 185/499: loss=0.446800023777726\n",
      "GD iter. 186/499: loss=0.4467457700611402\n",
      "GD iter. 187/499: loss=0.4466921853776061\n",
      "GD iter. 188/499: loss=0.4466392593770827\n",
      "GD iter. 189/499: loss=0.44658698190916485\n",
      "GD iter. 190/499: loss=0.44653534301837555\n",
      "GD iter. 191/499: loss=0.44648433293959267\n",
      "GD iter. 192/499: loss=0.4464339420936016\n",
      "GD iter. 193/499: loss=0.4463841610827723\n",
      "GD iter. 194/499: loss=0.4463349806868554\n",
      "GD iter. 195/499: loss=0.44628639185889374\n",
      "GD iter. 196/499: loss=0.44623838572124647\n",
      "GD iter. 197/499: loss=0.4461909535617215\n",
      "GD iter. 198/499: loss=0.44614408682981216\n",
      "GD iter. 199/499: loss=0.4460977771330367\n",
      "GD iter. 200/499: loss=0.4460520162333762\n",
      "GD iter. 201/499: loss=0.4460067960438068\n",
      "GD iter. 202/499: loss=0.44596210862492563\n",
      "GD iter. 203/499: loss=0.4459179461816663\n",
      "GD iter. 204/499: loss=0.44587430106010073\n",
      "GD iter. 205/499: loss=0.44583116574432535\n",
      "GD iter. 206/499: loss=0.44578853285342984\n",
      "GD iter. 207/499: loss=0.4457463951385434\n",
      "GD iter. 208/499: loss=0.44570474547995953\n",
      "GD iter. 209/499: loss=0.4456635768843341\n",
      "GD iter. 210/499: loss=0.44562288248195625\n",
      "GD iter. 211/499: loss=0.4455826555240894\n",
      "GD iter. 212/499: loss=0.4455428893803795\n",
      "GD iter. 213/499: loss=0.4455035775363306\n",
      "GD iter. 214/499: loss=0.4454647135908432\n",
      "GD iter. 215/499: loss=0.4454262912538154\n",
      "GD iter. 216/499: loss=0.44538830434380416\n",
      "GD iter. 217/499: loss=0.44535074678574543\n",
      "GD iter. 218/499: loss=0.4453136126087305\n",
      "GD iter. 219/499: loss=0.44527689594383874\n",
      "GD iter. 220/499: loss=0.4452405910220225\n",
      "GD iter. 221/499: loss=0.4452046921720453\n",
      "GD iter. 222/499: loss=0.44516919381847\n",
      "GD iter. 223/499: loss=0.4451340904796963\n",
      "GD iter. 224/499: loss=0.44509937676604644\n",
      "GD iter. 225/499: loss=0.4450650473778963\n",
      "GD iter. 226/499: loss=0.44503109710385236\n",
      "GD iter. 227/499: loss=0.44499752081897287\n",
      "GD iter. 228/499: loss=0.4449643134830307\n",
      "GD iter. 229/499: loss=0.4449314701388179\n",
      "GD iter. 230/499: loss=0.4448989859104912\n",
      "GD iter. 231/499: loss=0.4448668560019563\n",
      "GD iter. 232/499: loss=0.4448350756952894\n",
      "GD iter. 233/499: loss=0.4448036403491975\n",
      "GD iter. 234/499: loss=0.44477254539751315\n",
      "GD iter. 235/499: loss=0.4447417863477253\n",
      "GD iter. 236/499: loss=0.4447113587795437\n",
      "GD iter. 237/499: loss=0.44468125834349703\n",
      "GD iter. 238/499: loss=0.44465148075956235\n",
      "GD iter. 239/499: loss=0.44462202181582755\n",
      "GD iter. 240/499: loss=0.4445928773671828\n",
      "GD iter. 241/499: loss=0.4445640433340426\n",
      "GD iter. 242/499: loss=0.44453551570109723\n",
      "GD iter. 243/499: loss=0.4445072905160911\n",
      "GD iter. 244/499: loss=0.4444793638886304\n",
      "GD iter. 245/499: loss=0.4444517319890156\n",
      "GD iter. 246/499: loss=0.4444243910471015\n",
      "GD iter. 247/499: loss=0.4443973373511823\n",
      "GD iter. 248/499: loss=0.444370567246901\n",
      "GD iter. 249/499: loss=0.44434407713618285\n",
      "GD iter. 250/499: loss=0.4443178634761936\n",
      "GD iter. 251/499: loss=0.4442919227783187\n",
      "GD iter. 252/499: loss=0.4442662516071662\n",
      "GD iter. 253/499: loss=0.44424084657959073\n",
      "GD iter. 254/499: loss=0.4442157043637391\n",
      "GD iter. 255/499: loss=0.444190821678116\n",
      "GD iter. 256/499: loss=0.44416619529067075\n",
      "GD iter. 257/499: loss=0.44414182201790214\n",
      "GD iter. 258/499: loss=0.4441176987239844\n",
      "GD iter. 259/499: loss=0.4440938223199098\n",
      "GD iter. 260/499: loss=0.444070189762651\n",
      "GD iter. 261/499: loss=0.44404679805434055\n",
      "GD iter. 262/499: loss=0.44402364424146745\n",
      "GD iter. 263/499: loss=0.44400072541409125\n",
      "GD iter. 264/499: loss=0.4439780387050717\n",
      "GD iter. 265/499: loss=0.4439555812893156\n",
      "GD iter. 266/499: loss=0.4439333503830384\n",
      "GD iter. 267/499: loss=0.44391134324304177\n",
      "GD iter. 268/499: loss=0.44388955716600526\n",
      "GD iter. 269/499: loss=0.44386798948779355\n",
      "GD iter. 270/499: loss=0.4438466375827776\n",
      "GD iter. 271/499: loss=0.44382549886316874\n",
      "GD iter. 272/499: loss=0.44380457077836827\n",
      "GD iter. 273/499: loss=0.443783850814328\n",
      "GD iter. 274/499: loss=0.44376333649292593\n",
      "GD iter. 275/499: loss=0.4437430253713528\n",
      "GD iter. 276/499: loss=0.4437229150415117\n",
      "GD iter. 277/499: loss=0.4437030031294302\n",
      "GD iter. 278/499: loss=0.4436832872946831\n",
      "GD iter. 279/499: loss=0.44366376522982787\n",
      "GD iter. 280/499: loss=0.4436444346598503\n",
      "GD iter. 281/499: loss=0.4436252933416219\n",
      "GD iter. 282/499: loss=0.44360633906336716\n",
      "GD iter. 283/499: loss=0.44358756964414253\n",
      "GD iter. 284/499: loss=0.4435689829333239\n",
      "GD iter. 285/499: loss=0.44355057681010635\n",
      "GD iter. 286/499: loss=0.443532349183011\n",
      "GD iter. 287/499: loss=0.4435142979894035\n",
      "GD iter. 288/499: loss=0.4434964211950212\n",
      "GD iter. 289/499: loss=0.4434787167935093\n",
      "GD iter. 290/499: loss=0.4434611828059656\n",
      "GD iter. 291/499: loss=0.4434438172804947\n",
      "GD iter. 292/499: loss=0.4434266182917707\n",
      "GD iter. 293/499: loss=0.44340958394060737\n",
      "GD iter. 294/499: loss=0.443392712353537\n",
      "GD iter. 295/499: loss=0.44337600168239805\n",
      "GD iter. 296/499: loss=0.44335945010392863\n",
      "GD iter. 297/499: loss=0.44334305581937006\n",
      "GD iter. 298/499: loss=0.4433268170540754\n",
      "GD iter. 299/499: loss=0.44331073205712757\n",
      "GD iter. 300/499: loss=0.4432947991009628\n",
      "GD iter. 301/499: loss=0.4432790164810022\n",
      "GD iter. 302/499: loss=0.4432633825152893\n",
      "GD iter. 303/499: loss=0.4432478955441356\n",
      "GD iter. 304/499: loss=0.44323255392977096\n",
      "GD iter. 305/499: loss=0.44321735605600177\n",
      "GD iter. 306/499: loss=0.4432023003278748\n",
      "GD iter. 307/499: loss=0.4431873851713471\n",
      "GD iter. 308/499: loss=0.44317260903296224\n",
      "GD iter. 309/499: loss=0.44315797037953236\n",
      "GD iter. 310/499: loss=0.4431434676978255\n",
      "GD iter. 311/499: loss=0.4431290994942597\n",
      "GD iter. 312/499: loss=0.443114864294601\n",
      "GD iter. 313/499: loss=0.4431007606436685\n",
      "GD iter. 314/499: loss=0.4430867871050433\n",
      "GD iter. 315/499: loss=0.44307294226078375\n",
      "GD iter. 316/499: loss=0.44305922471114506\n",
      "GD iter. 317/499: loss=0.4430456330743043\n",
      "GD iter. 318/499: loss=0.4430321659860899\n",
      "GD iter. 319/499: loss=0.4430188220997161\n",
      "GD iter. 320/499: loss=0.44300560008552226\n",
      "GD iter. 321/499: loss=0.44299249863071644\n",
      "GD iter. 322/499: loss=0.4429795164391237\n",
      "GD iter. 323/499: loss=0.44296665223093856\n",
      "GD iter. 324/499: loss=0.44295390474248186\n",
      "GD iter. 325/499: loss=0.44294127272596245\n",
      "GD iter. 326/499: loss=0.4429287549492413\n",
      "GD iter. 327/499: loss=0.44291635019560194\n",
      "GD iter. 328/499: loss=0.44290405726352344\n",
      "GD iter. 329/499: loss=0.4428918749664569\n",
      "GD iter. 330/499: loss=0.44287980213260747\n",
      "GD iter. 331/499: loss=0.44286783760471854\n",
      "GD iter. 332/499: loss=0.4428559802398597\n",
      "GD iter. 333/499: loss=0.44284422890922\n",
      "GD iter. 334/499: loss=0.4428325824979019\n",
      "GD iter. 335/499: loss=0.4428210399047218\n",
      "GD iter. 336/499: loss=0.442809600042011\n",
      "GD iter. 337/499: loss=0.4427982618354224\n",
      "GD iter. 338/499: loss=0.44278702422373895\n",
      "GD iter. 339/499: loss=0.4427758861586857\n",
      "GD iter. 340/499: loss=0.44276484660474574\n",
      "GD iter. 341/499: loss=0.44275390453897756\n",
      "GD iter. 342/499: loss=0.44274305895083715\n",
      "GD iter. 343/499: loss=0.4427323088420022\n",
      "GD iter. 344/499: loss=0.4427216532261991\n",
      "GD iter. 345/499: loss=0.4427110911290329\n",
      "GD iter. 346/499: loss=0.4427006215878208\n",
      "GD iter. 347/499: loss=0.44269024365142684\n",
      "GD iter. 348/499: loss=0.4426799563801008\n",
      "GD iter. 349/499: loss=0.4426697588453192\n",
      "GD iter. 350/499: loss=0.44265965012962816\n",
      "GD iter. 351/499: loss=0.4426496293264902\n",
      "GD iter. 352/499: loss=0.44263969554013194\n",
      "GD iter. 353/499: loss=0.44262984788539594\n",
      "GD iter. 354/499: loss=0.44262008548759335\n",
      "GD iter. 355/499: loss=0.4426104074823603\n",
      "GD iter. 356/499: loss=0.4426008130155154\n",
      "GD iter. 357/499: loss=0.44259130124292007\n",
      "GD iter. 358/499: loss=0.4425818713303415\n",
      "GD iter. 359/499: loss=0.44257252245331685\n",
      "GD iter. 360/499: loss=0.44256325379702055\n",
      "GD iter. 361/499: loss=0.44255406455613316\n",
      "GD iter. 362/499: loss=0.4425449539347123\n",
      "GD iter. 363/499: loss=0.4425359211460655\n",
      "GD iter. 364/499: loss=0.442526965412626\n",
      "GD iter. 365/499: loss=0.4425180859658288\n",
      "GD iter. 366/499: loss=0.44250928204599055\n",
      "GD iter. 367/499: loss=0.44250055290218965\n",
      "GD iter. 368/499: loss=0.44249189779214954\n",
      "GD iter. 369/499: loss=0.4424833159821226\n",
      "GD iter. 370/499: loss=0.4424748067467769\n",
      "GD iter. 371/499: loss=0.44246636936908423\n",
      "GD iter. 372/499: loss=0.4424580031402096\n",
      "GD iter. 373/499: loss=0.4424497073594032\n",
      "GD iter. 374/499: loss=0.44244148133389277\n",
      "GD iter. 375/499: loss=0.4424333243787792\n",
      "GD iter. 376/499: loss=0.44242523581693255\n",
      "GD iter. 377/499: loss=0.44241721497888997\n",
      "GD iter. 378/499: loss=0.4424092612027552\n",
      "GD iter. 379/499: loss=0.44240137383409983\n",
      "GD iter. 380/499: loss=0.4423935522258657\n",
      "GD iter. 381/499: loss=0.44238579573826886\n",
      "GD iter. 382/499: loss=0.4423781037387051\n",
      "GD iter. 383/499: loss=0.442370475601657\n",
      "GD iter. 384/499: loss=0.4423629107086017\n",
      "GD iter. 385/499: loss=0.44235540844792104\n",
      "GD iter. 386/499: loss=0.44234796821481254\n",
      "GD iter. 387/499: loss=0.4423405894112013\n",
      "GD iter. 388/499: loss=0.4423332714456541\n",
      "GD iter. 389/499: loss=0.44232601373329417\n",
      "GD iter. 390/499: loss=0.442318815695717\n",
      "GD iter. 391/499: loss=0.4423116767609083\n",
      "GD iter. 392/499: loss=0.44230459636316244\n",
      "GD iter. 393/499: loss=0.44229757394300206\n",
      "GD iter. 394/499: loss=0.4422906089470989\n",
      "GD iter. 395/499: loss=0.442283700828197\n",
      "GD iter. 396/499: loss=0.4422768490450346\n",
      "GD iter. 397/499: loss=0.4422700530622696\n",
      "GD iter. 398/499: loss=0.4422633123504044\n",
      "GD iter. 399/499: loss=0.44225662638571267\n",
      "GD iter. 400/499: loss=0.44224999465016757\n",
      "GD iter. 401/499: loss=0.44224341663136946\n",
      "GD iter. 402/499: loss=0.44223689182247644\n",
      "GD iter. 403/499: loss=0.44223041972213445\n",
      "GD iter. 404/499: loss=0.4422239998344097\n",
      "GD iter. 405/499: loss=0.44221763166872063\n",
      "GD iter. 406/499: loss=0.4422113147397722\n",
      "GD iter. 407/499: loss=0.4422050485674904\n",
      "GD iter. 408/499: loss=0.442198832676957\n",
      "GD iter. 409/499: loss=0.44219266659834733\n",
      "GD iter. 410/499: loss=0.44218654986686673\n",
      "GD iter. 411/499: loss=0.44218048202268906\n",
      "GD iter. 412/499: loss=0.4421744626108958\n",
      "GD iter. 413/499: loss=0.4421684911814161\n",
      "GD iter. 414/499: loss=0.4421625672889673\n",
      "GD iter. 415/499: loss=0.44215669049299716\n",
      "GD iter. 416/499: loss=0.442150860357626\n",
      "GD iter. 417/499: loss=0.4421450764515895\n",
      "GD iter. 418/499: loss=0.44213933834818386\n",
      "GD iter. 419/499: loss=0.4421336456252095\n",
      "GD iter. 420/499: loss=0.44212799786491763\n",
      "GD iter. 421/499: loss=0.44212239465395586\n",
      "GD iter. 422/499: loss=0.4421168355833156\n",
      "GD iter. 423/499: loss=0.4421113202482804\n",
      "GD iter. 424/499: loss=0.44210584824837335\n",
      "GD iter. 425/499: loss=0.4421004191873074\n",
      "GD iter. 426/499: loss=0.44209503267293504\n",
      "GD iter. 427/499: loss=0.44208968831719847\n",
      "GD iter. 428/499: loss=0.4420843857360815\n",
      "GD iter. 429/499: loss=0.44207912454956144\n",
      "GD iter. 430/499: loss=0.4420739043815614\n",
      "GD iter. 431/499: loss=0.4420687248599039\n",
      "GD iter. 432/499: loss=0.44206358561626447\n",
      "GD iter. 433/499: loss=0.4420584862861267\n",
      "GD iter. 434/499: loss=0.4420534265087368\n",
      "GD iter. 435/499: loss=0.44204840592705985\n",
      "GD iter. 436/499: loss=0.4420434241877356\n",
      "GD iter. 437/499: loss=0.4420384809410362\n",
      "GD iter. 438/499: loss=0.4420335758408228\n",
      "GD iter. 439/499: loss=0.44202870854450443\n",
      "GD iter. 440/499: loss=0.4420238787129961\n",
      "GD iter. 441/499: loss=0.442019086010678\n",
      "GD iter. 442/499: loss=0.4420143301053559\n",
      "GD iter. 443/499: loss=0.4420096106682205\n",
      "GD iter. 444/499: loss=0.442004927373809\n",
      "GD iter. 445/499: loss=0.44200027989996576\n",
      "GD iter. 446/499: loss=0.4419956679278048\n",
      "GD iter. 447/499: loss=0.4419910911416714\n",
      "GD iter. 448/499: loss=0.4419865492291055\n",
      "GD iter. 449/499: loss=0.4419820418808049\n",
      "GD iter. 450/499: loss=0.44197756879058886\n",
      "GD iter. 451/499: loss=0.44197312965536223\n",
      "GD iter. 452/499: loss=0.4419687241750808\n",
      "GD iter. 453/499: loss=0.44196435205271595\n",
      "GD iter. 454/499: loss=0.4419600129942204\n",
      "GD iter. 455/499: loss=0.44195570670849443\n",
      "GD iter. 456/499: loss=0.4419514329073523\n",
      "GD iter. 457/499: loss=0.44194719130548915\n",
      "GD iter. 458/499: loss=0.4419429816204488\n",
      "GD iter. 459/499: loss=0.44193880357259085\n",
      "GD iter. 460/499: loss=0.4419346568850594\n",
      "GD iter. 461/499: loss=0.44193054128375175\n",
      "GD iter. 462/499: loss=0.4419264564972871\n",
      "GD iter. 463/499: loss=0.44192240225697615\n",
      "GD iter. 464/499: loss=0.44191837829679076\n",
      "GD iter. 465/499: loss=0.4419143843533349\n",
      "GD iter. 466/499: loss=0.4419104201658137\n",
      "GD iter. 467/499: loss=0.44190648547600625\n",
      "GD iter. 468/499: loss=0.4419025800282352\n",
      "GD iter. 469/499: loss=0.4418987035693397\n",
      "GD iter. 470/499: loss=0.4418948558486471\n",
      "GD iter. 471/499: loss=0.4418910366179449\n",
      "GD iter. 472/499: loss=0.44188724563145415\n",
      "GD iter. 473/499: loss=0.441883482645802\n",
      "GD iter. 474/499: loss=0.4418797474199956\n",
      "GD iter. 475/499: loss=0.44187603971539524\n",
      "GD iter. 476/499: loss=0.4418723592956892\n",
      "GD iter. 477/499: loss=0.4418687059268675\n",
      "GD iter. 478/499: loss=0.44186507937719705\n",
      "GD iter. 479/499: loss=0.44186147941719656\n",
      "GD iter. 480/499: loss=0.44185790581961165\n",
      "GD iter. 481/499: loss=0.4418543583593909\n",
      "GD iter. 482/499: loss=0.44185083681366155\n",
      "GD iter. 483/499: loss=0.441847340961706\n",
      "GD iter. 484/499: loss=0.441843870584938\n",
      "GD iter. 485/499: loss=0.44184042546687985\n",
      "GD iter. 486/499: loss=0.44183700539313914\n",
      "GD iter. 487/499: loss=0.44183361015138667\n",
      "GD iter. 488/499: loss=0.4418302395313335\n",
      "GD iter. 489/499: loss=0.4418268933247096\n",
      "GD iter. 490/499: loss=0.44182357132524114\n",
      "GD iter. 491/499: loss=0.4418202733286302\n",
      "GD iter. 492/499: loss=0.4418169991325325\n",
      "GD iter. 493/499: loss=0.4418137485365367\n",
      "GD iter. 494/499: loss=0.44181052134214377\n",
      "GD iter. 495/499: loss=0.4418073173527467\n",
      "GD iter. 496/499: loss=0.44180413637360966\n",
      "GD iter. 497/499: loss=0.4418009782118485\n",
      "GD iter. 498/499: loss=0.4417978426764109\n",
      "GD iter. 499/499: loss=0.44179472957805693\n",
      "The Accuracy is: 0.8675\n",
      "The F1 score is: 0.4790\n",
      "The precision is: 0.3883\n",
      "The recall is: 0.6250\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "initial_w = np.random.randn(x_train_processed.shape[1]) * 0.01\n",
    "x_t, y_t, x_v, y_v = split_data(x_train_processed_orig, y_train_processed_orig, 0.8)\n",
    "x_t, y_t = data_augmentation(x_t, y_t)\n",
    "w, loss = reg_logistic_regression(y_t, x_t, lambda_=0.01, initial_w=initial_w, max_iters=500, gamma=0.15)\n",
    "y_pred = (x_v @ w >= 0.85).astype(int)\n",
    "# predict_acc(x_v, y_v, w, logistic=False, threshold=0.85)\n",
    "# predict_f1(x_v, y_v, w, logistic=False, threshold=0.85)\n",
    "predict_acc_pure(y_pred, y_v)\n",
    "predict_f1_pure(y_pred, y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is: 0.7597\n",
      "The F1 score is: 0.3715\n",
      "The precision is: 0.2392\n",
      "The recall is: 0.8307\n"
     ]
    }
   ],
   "source": [
    "# ridge regression using all the features except for those having NaN values over 50% ##\n",
    "w, loss = ridge_regression(y_train_processed, x_train_processed, lambda_=0.001)\n",
    "y_pred = x_train_processed @ w\n",
    "y_pred_mean = np.mean(y_pred)\n",
    "predict_acc(x_train_processed_orig, y_train_processed_orig, w, logistic=False, threshold=y_pred_mean)\n",
    "predict_f1(x_train_processed_orig, y_train_processed_orig, w, logistic=False, threshold=y_pred_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinge loss gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6563, 142)\n",
      "(6563,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_processed_hinge.shape)\n",
    "print(y_train_processed_hinge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/499: loss=1.0101429697922446\n",
      "GD iter. 1/499: loss=1.0075180631970206\n",
      "GD iter. 2/499: loss=1.0049010372901752\n",
      "GD iter. 3/499: loss=1.0022918657973539\n",
      "GD iter. 4/499: loss=0.9996905225387615\n",
      "GD iter. 5/499: loss=0.9970969814288042\n",
      "GD iter. 6/499: loss=0.9945112164757338\n",
      "GD iter. 7/499: loss=0.9919332017812944\n",
      "GD iter. 8/499: loss=0.9893642809754302\n",
      "GD iter. 9/499: loss=0.986808324976748\n",
      "GD iter. 10/499: loss=0.9842600223627922\n",
      "GD iter. 11/499: loss=0.9817193476710686\n",
      "GD iter. 12/499: loss=0.9791862755305898\n",
      "GD iter. 13/499: loss=0.9766611451977779\n",
      "GD iter. 14/499: loss=0.9741644481202621\n",
      "GD iter. 15/499: loss=0.9716787433658162\n",
      "GD iter. 16/499: loss=0.9692204368368255\n",
      "GD iter. 17/499: loss=0.9668001909304184\n",
      "GD iter. 18/499: loss=0.96438717271559\n",
      "GD iter. 19/499: loss=0.9619884852028004\n",
      "GD iter. 20/499: loss=0.9596091979325074\n",
      "GD iter. 21/499: loss=0.9572445969733999\n",
      "GD iter. 22/499: loss=0.9548905688310565\n",
      "GD iter. 23/499: loss=0.9525499749055623\n",
      "GD iter. 24/499: loss=0.9502261099839456\n",
      "GD iter. 25/499: loss=0.9479134577309417\n",
      "GD iter. 26/499: loss=0.945607693484164\n",
      "GD iter. 27/499: loss=0.943308794425844\n",
      "GD iter. 28/499: loss=0.9410192480703036\n",
      "GD iter. 29/499: loss=0.938750537491033\n",
      "GD iter. 30/499: loss=0.9365039531989772\n",
      "GD iter. 31/499: loss=0.9342854058824009\n",
      "GD iter. 32/499: loss=0.9321124702125929\n",
      "GD iter. 33/499: loss=0.9299719429305343\n",
      "GD iter. 34/499: loss=0.9278584407869701\n",
      "GD iter. 35/499: loss=0.9257993983027455\n",
      "GD iter. 36/499: loss=0.9238074254210865\n",
      "GD iter. 37/499: loss=0.9218841852286639\n",
      "GD iter. 38/499: loss=0.9200346539606887\n",
      "GD iter. 39/499: loss=0.9182535437620432\n",
      "GD iter. 40/499: loss=0.9165751064424241\n",
      "GD iter. 41/499: loss=0.9149888543430386\n",
      "GD iter. 42/499: loss=0.9134650336227249\n",
      "GD iter. 43/499: loss=0.9120068765674301\n",
      "GD iter. 44/499: loss=0.910616385510028\n",
      "GD iter. 45/499: loss=0.9092714630128094\n",
      "GD iter. 46/499: loss=0.9079822806493053\n",
      "GD iter. 47/499: loss=0.90677604593239\n",
      "GD iter. 48/499: loss=0.9056381840532068\n",
      "GD iter. 49/499: loss=0.9046129948748601\n",
      "GD iter. 50/499: loss=0.903635782736948\n",
      "GD iter. 51/499: loss=0.9027003605580303\n",
      "GD iter. 52/499: loss=0.9018218935641967\n",
      "GD iter. 53/499: loss=0.9009957731643213\n",
      "GD iter. 54/499: loss=0.9002121983558019\n",
      "GD iter. 55/499: loss=0.8994650160858442\n",
      "GD iter. 56/499: loss=0.8987535462141658\n",
      "GD iter. 57/499: loss=0.8980757733869315\n",
      "GD iter. 58/499: loss=0.8974419422295083\n",
      "GD iter. 59/499: loss=0.8968477287670403\n",
      "GD iter. 60/499: loss=0.8962738906069135\n",
      "GD iter. 61/499: loss=0.8957210626870581\n",
      "GD iter. 62/499: loss=0.895195721549315\n",
      "GD iter. 63/499: loss=0.8946954652772269\n",
      "GD iter. 64/499: loss=0.8942190281777344\n",
      "GD iter. 65/499: loss=0.893775362424473\n",
      "GD iter. 66/499: loss=0.8933445075787639\n",
      "GD iter. 67/499: loss=0.8929278745577185\n",
      "GD iter. 68/499: loss=0.8925230726443156\n",
      "GD iter. 69/499: loss=0.8921275830484064\n",
      "GD iter. 70/499: loss=0.8917466814869368\n",
      "GD iter. 71/499: loss=0.8913744902054748\n",
      "GD iter. 72/499: loss=0.8910184645968318\n",
      "GD iter. 73/499: loss=0.8906748391773653\n",
      "GD iter. 74/499: loss=0.8903419510560225\n",
      "GD iter. 75/499: loss=0.8900205475789877\n",
      "GD iter. 76/499: loss=0.8897196513022032\n",
      "GD iter. 77/499: loss=0.8894330393569134\n",
      "GD iter. 78/499: loss=0.8891527465493967\n",
      "GD iter. 79/499: loss=0.8888812705862396\n",
      "GD iter. 80/499: loss=0.8886194024298752\n",
      "GD iter. 81/499: loss=0.8883676371410083\n",
      "GD iter. 82/499: loss=0.88812248102167\n",
      "GD iter. 83/499: loss=0.887887064406218\n",
      "GD iter. 84/499: loss=0.8876551135470814\n",
      "GD iter. 85/499: loss=0.887428172079643\n",
      "GD iter. 86/499: loss=0.8872055877308759\n",
      "GD iter. 87/499: loss=0.8869927963187387\n",
      "GD iter. 88/499: loss=0.8867867031002277\n",
      "GD iter. 89/499: loss=0.8865841950523504\n",
      "GD iter. 90/499: loss=0.8863841652526058\n",
      "GD iter. 91/499: loss=0.8861879609359858\n",
      "GD iter. 92/499: loss=0.8859947618508963\n",
      "GD iter. 93/499: loss=0.8858029516046078\n",
      "GD iter. 94/499: loss=0.8856147941308675\n",
      "GD iter. 95/499: loss=0.8854299579903653\n",
      "GD iter. 96/499: loss=0.8852493050409513\n",
      "GD iter. 97/499: loss=0.8850712211053862\n",
      "GD iter. 98/499: loss=0.8848950340764968\n",
      "GD iter. 99/499: loss=0.8847230300534457\n",
      "GD iter. 100/499: loss=0.8845555322585741\n",
      "GD iter. 101/499: loss=0.884390218976675\n",
      "GD iter. 102/499: loss=0.8842285021548993\n",
      "GD iter. 103/499: loss=0.8840688750648201\n",
      "GD iter. 104/499: loss=0.8839118575712842\n",
      "GD iter. 105/499: loss=0.8837579693977458\n",
      "GD iter. 106/499: loss=0.8836097065557504\n",
      "GD iter. 107/499: loss=0.8834648288927589\n",
      "GD iter. 108/499: loss=0.883322198123877\n",
      "GD iter. 109/499: loss=0.8831806087177339\n",
      "GD iter. 110/499: loss=0.8830401117093635\n",
      "GD iter. 111/499: loss=0.882902763933167\n",
      "GD iter. 112/499: loss=0.882766633638796\n",
      "GD iter. 113/499: loss=0.8826314862131178\n",
      "GD iter. 114/499: loss=0.8824979073528093\n",
      "GD iter. 115/499: loss=0.8823653696440035\n",
      "GD iter. 116/499: loss=0.8822355024958741\n",
      "GD iter. 117/499: loss=0.8821066969158522\n",
      "GD iter. 118/499: loss=0.8819787252895676\n",
      "GD iter. 119/499: loss=0.8818543398380694\n",
      "GD iter. 120/499: loss=0.8817317890170978\n",
      "GD iter. 121/499: loss=0.8816104942538493\n",
      "GD iter. 122/499: loss=0.8814924088641395\n",
      "GD iter. 123/499: loss=0.8813774328306321\n",
      "GD iter. 124/499: loss=0.8812649454524065\n",
      "GD iter. 125/499: loss=0.881153004331639\n",
      "GD iter. 126/499: loss=0.8810424169517096\n",
      "GD iter. 127/499: loss=0.8809336003310121\n",
      "GD iter. 128/499: loss=0.8808269624303988\n",
      "GD iter. 129/499: loss=0.8807207208226155\n",
      "GD iter. 130/499: loss=0.8806154688096877\n",
      "GD iter. 131/499: loss=0.880513241754846\n",
      "GD iter. 132/499: loss=0.8804144864056005\n",
      "GD iter. 133/499: loss=0.8803166894377898\n",
      "GD iter. 134/499: loss=0.8802195171520709\n",
      "GD iter. 135/499: loss=0.8801248564431675\n",
      "GD iter. 136/499: loss=0.8800306434724358\n",
      "GD iter. 137/499: loss=0.8799371537123661\n",
      "GD iter. 138/499: loss=0.8798446933418759\n",
      "GD iter. 139/499: loss=0.8797543431947683\n",
      "GD iter. 140/499: loss=0.8796667309378519\n",
      "GD iter. 141/499: loss=0.8795796140279056\n",
      "GD iter. 142/499: loss=0.8794931946919053\n",
      "GD iter. 143/499: loss=0.8794076441904622\n",
      "GD iter. 144/499: loss=0.8793223874734508\n",
      "GD iter. 145/499: loss=0.8792381640036705\n",
      "GD iter. 146/499: loss=0.8791562858654718\n",
      "GD iter. 147/499: loss=0.8790758797550103\n",
      "GD iter. 148/499: loss=0.8789978051973861\n",
      "GD iter. 149/499: loss=0.878922069726643\n",
      "GD iter. 150/499: loss=0.8788478732157244\n",
      "GD iter. 151/499: loss=0.8787743268973324\n",
      "GD iter. 152/499: loss=0.878701015903838\n",
      "GD iter. 153/499: loss=0.8786282482607127\n",
      "GD iter. 154/499: loss=0.8785564276540172\n",
      "GD iter. 155/499: loss=0.8784849102973467\n",
      "GD iter. 156/499: loss=0.8784142285059067\n",
      "GD iter. 157/499: loss=0.8783438826677047\n",
      "GD iter. 158/499: loss=0.8782738543211132\n",
      "GD iter. 159/499: loss=0.8782045040600858\n",
      "GD iter. 160/499: loss=0.8781362090124744\n",
      "GD iter. 161/499: loss=0.8780683038793091\n",
      "GD iter. 162/499: loss=0.8780016553711664\n",
      "GD iter. 163/499: loss=0.87793559813167\n",
      "GD iter. 164/499: loss=0.8778703494774804\n",
      "GD iter. 165/499: loss=0.8778058269281583\n",
      "GD iter. 166/499: loss=0.8777413203011238\n",
      "GD iter. 167/499: loss=0.8776770621117493\n",
      "GD iter. 168/499: loss=0.8776135062591611\n",
      "GD iter. 169/499: loss=0.8775503859682444\n",
      "GD iter. 170/499: loss=0.8774878963203265\n",
      "GD iter. 171/499: loss=0.8774258987919336\n",
      "GD iter. 172/499: loss=0.8773646562039039\n",
      "GD iter. 173/499: loss=0.87730367836619\n",
      "GD iter. 174/499: loss=0.8772434731240722\n",
      "GD iter. 175/499: loss=0.8771840990510611\n",
      "GD iter. 176/499: loss=0.8771251889923154\n",
      "GD iter. 177/499: loss=0.877066592443173\n",
      "GD iter. 178/499: loss=0.8770079885869222\n",
      "GD iter. 179/499: loss=0.8769504628878063\n",
      "GD iter. 180/499: loss=0.8768932380844323\n",
      "GD iter. 181/499: loss=0.8768362884610527\n",
      "GD iter. 182/499: loss=0.8767792701917321\n",
      "GD iter. 183/499: loss=0.8767234629332891\n",
      "GD iter. 184/499: loss=0.8766689538644266\n",
      "GD iter. 185/499: loss=0.8766150071560425\n",
      "GD iter. 186/499: loss=0.8765612801362255\n",
      "GD iter. 187/499: loss=0.8765076802982954\n",
      "GD iter. 188/499: loss=0.8764544942871414\n",
      "GD iter. 189/499: loss=0.8764022080486077\n",
      "GD iter. 190/499: loss=0.8763506740754069\n",
      "GD iter. 191/499: loss=0.8762991201870518\n",
      "GD iter. 192/499: loss=0.8762485925740008\n",
      "GD iter. 193/499: loss=0.8761983630098742\n",
      "GD iter. 194/499: loss=0.8761485666587007\n",
      "GD iter. 195/499: loss=0.8760989990879284\n",
      "GD iter. 196/499: loss=0.8760502727103996\n",
      "GD iter. 197/499: loss=0.8760024299301143\n",
      "GD iter. 198/499: loss=0.8759544942559514\n",
      "GD iter. 199/499: loss=0.8759072397319484\n",
      "GD iter. 200/499: loss=0.8758595583740238\n",
      "GD iter. 201/499: loss=0.8758128447085933\n",
      "GD iter. 202/499: loss=0.8757665292439841\n",
      "GD iter. 203/499: loss=0.8757211380545686\n",
      "GD iter. 204/499: loss=0.8756756381460234\n",
      "GD iter. 205/499: loss=0.8756304277630027\n",
      "GD iter. 206/499: loss=0.8755854106494795\n",
      "GD iter. 207/499: loss=0.8755404464333243\n",
      "GD iter. 208/499: loss=0.8754962396785113\n",
      "GD iter. 209/499: loss=0.8754526319699982\n",
      "GD iter. 210/499: loss=0.8754089438832009\n",
      "GD iter. 211/499: loss=0.8753656879993477\n",
      "GD iter. 212/499: loss=0.8753227053968233\n",
      "GD iter. 213/499: loss=0.8752795913957702\n",
      "GD iter. 214/499: loss=0.8752372081857057\n",
      "GD iter. 215/499: loss=0.8751954107330697\n",
      "GD iter. 216/499: loss=0.8751535960090492\n",
      "GD iter. 217/499: loss=0.8751125494273669\n",
      "GD iter. 218/499: loss=0.8750715073260256\n",
      "GD iter. 219/499: loss=0.875030788714894\n",
      "GD iter. 220/499: loss=0.8749902656118651\n",
      "GD iter. 221/499: loss=0.8749506652313715\n",
      "GD iter. 222/499: loss=0.8749111584207664\n",
      "GD iter. 223/499: loss=0.8748722620883883\n",
      "GD iter. 224/499: loss=0.8748335890234039\n",
      "GD iter. 225/499: loss=0.8747946045684384\n",
      "GD iter. 226/499: loss=0.8747562190686721\n",
      "GD iter. 227/499: loss=0.8747177033656259\n",
      "GD iter. 228/499: loss=0.8746795855701996\n",
      "GD iter. 229/499: loss=0.874641790447594\n",
      "GD iter. 230/499: loss=0.874603982232792\n",
      "GD iter. 231/499: loss=0.8745666881719166\n",
      "GD iter. 232/499: loss=0.8745294347780803\n",
      "GD iter. 233/499: loss=0.8744925775824103\n",
      "GD iter. 234/499: loss=0.8744559890255152\n",
      "GD iter. 235/499: loss=0.8744198934765239\n",
      "GD iter. 236/499: loss=0.8743844422360232\n",
      "GD iter. 237/499: loss=0.8743487482110542\n",
      "GD iter. 238/499: loss=0.8743140926550508\n",
      "GD iter. 239/499: loss=0.8742791508918651\n",
      "GD iter. 240/499: loss=0.8742446309954846\n",
      "GD iter. 241/499: loss=0.8742105477958185\n",
      "GD iter. 242/499: loss=0.8741764214626657\n",
      "GD iter. 243/499: loss=0.8741429670887987\n",
      "GD iter. 244/499: loss=0.8741092702773814\n",
      "GD iter. 245/499: loss=0.8740758078967348\n",
      "GD iter. 246/499: loss=0.8740427395488969\n",
      "GD iter. 247/499: loss=0.8740096489070501\n",
      "GD iter. 248/499: loss=0.873976587509182\n",
      "GD iter. 249/499: loss=0.8739436792285841\n",
      "GD iter. 250/499: loss=0.8739111591145297\n",
      "GD iter. 251/499: loss=0.873879614664167\n",
      "GD iter. 252/499: loss=0.8738466939040268\n",
      "GD iter. 253/499: loss=0.8738154239620619\n",
      "GD iter. 254/499: loss=0.8737834059529566\n",
      "GD iter. 255/499: loss=0.8737522518711798\n",
      "GD iter. 256/499: loss=0.8737209693774854\n",
      "GD iter. 257/499: loss=0.8736896402272243\n",
      "GD iter. 258/499: loss=0.8736589227158763\n",
      "GD iter. 259/499: loss=0.8736286533062045\n",
      "GD iter. 260/499: loss=0.8735991245920004\n",
      "GD iter. 261/499: loss=0.8735697713607102\n",
      "GD iter. 262/499: loss=0.8735404033223938\n",
      "GD iter. 263/499: loss=0.8735111388215897\n",
      "GD iter. 264/499: loss=0.8734820168693725\n",
      "GD iter. 265/499: loss=0.8734531820389277\n",
      "GD iter. 266/499: loss=0.8734240868082701\n",
      "GD iter. 267/499: loss=0.873395790027726\n",
      "GD iter. 268/499: loss=0.8733671620726002\n",
      "GD iter. 269/499: loss=0.8733390639729174\n",
      "GD iter. 270/499: loss=0.8733105074269808\n",
      "GD iter. 271/499: loss=0.8732821769327346\n",
      "GD iter. 272/499: loss=0.873254349323577\n",
      "GD iter. 273/499: loss=0.8732268043376082\n",
      "GD iter. 274/499: loss=0.8731994371230772\n",
      "GD iter. 275/499: loss=0.8731732424146796\n",
      "GD iter. 276/499: loss=0.873146143095664\n",
      "GD iter. 277/499: loss=0.8731194911280233\n",
      "GD iter. 278/499: loss=0.8730931547949207\n",
      "GD iter. 279/499: loss=0.8730669923239305\n",
      "GD iter. 280/499: loss=0.8730406395586107\n",
      "GD iter. 281/499: loss=0.8730152568933183\n",
      "GD iter. 282/499: loss=0.872989121122014\n",
      "GD iter. 283/499: loss=0.8729638758691699\n",
      "GD iter. 284/499: loss=0.8729378656927503\n",
      "GD iter. 285/499: loss=0.8729130341358261\n",
      "GD iter. 286/499: loss=0.8728895321699042\n",
      "GD iter. 287/499: loss=0.8728658589907811\n",
      "GD iter. 288/499: loss=0.8728424545830511\n",
      "GD iter. 289/499: loss=0.8728193986410473\n",
      "GD iter. 290/499: loss=0.8727971273015287\n",
      "GD iter. 291/499: loss=0.8727746678996458\n",
      "GD iter. 292/499: loss=0.8727524643618324\n",
      "GD iter. 293/499: loss=0.8727307609348124\n",
      "GD iter. 294/499: loss=0.8727080647512941\n",
      "GD iter. 295/499: loss=0.8726863777649659\n",
      "GD iter. 296/499: loss=0.8726643899085774\n",
      "GD iter. 297/499: loss=0.8726425267713495\n",
      "GD iter. 298/499: loss=0.8726210817835374\n",
      "GD iter. 299/499: loss=0.8725991707145624\n",
      "GD iter. 300/499: loss=0.8725782788175392\n",
      "GD iter. 301/499: loss=0.8725569820876576\n",
      "GD iter. 302/499: loss=0.8725367096455211\n",
      "GD iter. 303/499: loss=0.8725169086509756\n",
      "GD iter. 304/499: loss=0.872497121491537\n",
      "GD iter. 305/499: loss=0.8724771465700657\n",
      "GD iter. 306/499: loss=0.8724580655605666\n",
      "GD iter. 307/499: loss=0.8724388348597348\n",
      "GD iter. 308/499: loss=0.8724197824394018\n",
      "GD iter. 309/499: loss=0.8724004336799873\n",
      "GD iter. 310/499: loss=0.8723819071604291\n",
      "GD iter. 311/499: loss=0.8723629061497533\n",
      "GD iter. 312/499: loss=0.8723439319454296\n",
      "GD iter. 313/499: loss=0.8723252246309487\n",
      "GD iter. 314/499: loss=0.8723071210797613\n",
      "GD iter. 315/499: loss=0.8722882952195369\n",
      "GD iter. 316/499: loss=0.8722701748047711\n",
      "GD iter. 317/499: loss=0.8722519682673017\n",
      "GD iter. 318/499: loss=0.8722339040306109\n",
      "GD iter. 319/499: loss=0.8722161568752955\n",
      "GD iter. 320/499: loss=0.8721983039387741\n",
      "GD iter. 321/499: loss=0.872181182026628\n",
      "GD iter. 322/499: loss=0.8721633823959729\n",
      "GD iter. 323/499: loss=0.8721460518966044\n",
      "GD iter. 324/499: loss=0.8721286064428257\n",
      "GD iter. 325/499: loss=0.8721120569620041\n",
      "GD iter. 326/499: loss=0.8720946724349992\n",
      "GD iter. 327/499: loss=0.8720780052296444\n",
      "GD iter. 328/499: loss=0.8720614922857005\n",
      "GD iter. 329/499: loss=0.8720448438930124\n",
      "GD iter. 330/499: loss=0.8720287993546367\n",
      "GD iter. 331/499: loss=0.8720119907478632\n",
      "GD iter. 332/499: loss=0.8719961678456701\n",
      "GD iter. 333/499: loss=0.8719798033620196\n",
      "GD iter. 334/499: loss=0.8719637515078912\n",
      "GD iter. 335/499: loss=0.8719475537271271\n",
      "GD iter. 336/499: loss=0.8719320699262332\n",
      "GD iter. 337/499: loss=0.871916031220661\n",
      "GD iter. 338/499: loss=0.8719002859885647\n",
      "GD iter. 339/499: loss=0.8718839544135111\n",
      "GD iter. 340/499: loss=0.87186819817888\n",
      "GD iter. 341/499: loss=0.8718526276946487\n",
      "GD iter. 342/499: loss=0.87183645314591\n",
      "GD iter. 343/499: loss=0.8718207256828893\n",
      "GD iter. 344/499: loss=0.8718051492313837\n",
      "GD iter. 345/499: loss=0.8717901460493356\n",
      "GD iter. 346/499: loss=0.8717756523329964\n",
      "GD iter. 347/499: loss=0.871759869420595\n",
      "GD iter. 348/499: loss=0.8717448199722521\n",
      "GD iter. 349/499: loss=0.8717299368087276\n",
      "GD iter. 350/499: loss=0.8717151020117487\n",
      "GD iter. 351/499: loss=0.8717000123953554\n",
      "GD iter. 352/499: loss=0.871685031151326\n",
      "GD iter. 353/499: loss=0.8716709025253124\n",
      "GD iter. 354/499: loss=0.8716560523416267\n",
      "GD iter. 355/499: loss=0.8716411736840609\n",
      "GD iter. 356/499: loss=0.8716273226185137\n",
      "GD iter. 357/499: loss=0.8716122849110058\n",
      "GD iter. 358/499: loss=0.8715978868617209\n",
      "GD iter. 359/499: loss=0.8715835661550538\n",
      "GD iter. 360/499: loss=0.8715700284401586\n",
      "GD iter. 361/499: loss=0.8715556307677955\n",
      "GD iter. 362/499: loss=0.8715416392385787\n",
      "GD iter. 363/499: loss=0.8715278914614638\n",
      "GD iter. 364/499: loss=0.8715135518048402\n",
      "GD iter. 365/499: loss=0.8714999391413791\n",
      "GD iter. 366/499: loss=0.871486341547528\n",
      "GD iter. 367/499: loss=0.8714727851167654\n",
      "GD iter. 368/499: loss=0.8714587651769994\n",
      "GD iter. 369/499: loss=0.8714449666428083\n",
      "GD iter. 370/499: loss=0.871431535242985\n",
      "GD iter. 371/499: loss=0.8714185739466037\n",
      "GD iter. 372/499: loss=0.8714055434719183\n",
      "GD iter. 373/499: loss=0.8713923452768009\n",
      "GD iter. 374/499: loss=0.8713801750489168\n",
      "GD iter. 375/499: loss=0.8713667539413771\n",
      "GD iter. 376/499: loss=0.871354262216657\n",
      "GD iter. 377/499: loss=0.8713417664000443\n",
      "GD iter. 378/499: loss=0.8713292273544826\n",
      "GD iter. 379/499: loss=0.8713166565273001\n",
      "GD iter. 380/499: loss=0.8713048558393742\n",
      "GD iter. 381/499: loss=0.8712918265422437\n",
      "GD iter. 382/499: loss=0.8712796606474672\n",
      "GD iter. 383/499: loss=0.8712686423546341\n",
      "GD iter. 384/499: loss=0.8712567979535666\n",
      "GD iter. 385/499: loss=0.8712453437285016\n",
      "GD iter. 386/499: loss=0.8712334809930167\n",
      "GD iter. 387/499: loss=0.8712224517058148\n",
      "GD iter. 388/499: loss=0.8712111297214528\n",
      "GD iter. 389/499: loss=0.8711991725354638\n",
      "GD iter. 390/499: loss=0.8711881540695675\n",
      "GD iter. 391/499: loss=0.8711769972076662\n",
      "GD iter. 392/499: loss=0.8711652690526428\n",
      "GD iter. 393/499: loss=0.8711540623150784\n",
      "GD iter. 394/499: loss=0.8711429708382042\n",
      "GD iter. 395/499: loss=0.8711321561798122\n",
      "GD iter. 396/499: loss=0.8711203859212627\n",
      "GD iter. 397/499: loss=0.8711093300401509\n",
      "GD iter. 398/499: loss=0.8710986929419607\n",
      "GD iter. 399/499: loss=0.8710871703212015\n",
      "GD iter. 400/499: loss=0.8710763808505402\n",
      "GD iter. 401/499: loss=0.8710655549337831\n",
      "GD iter. 402/499: loss=0.8710546448261777\n",
      "GD iter. 403/499: loss=0.871043643415753\n",
      "GD iter. 404/499: loss=0.8710332628442986\n",
      "GD iter. 405/499: loss=0.871022268943554\n",
      "GD iter. 406/499: loss=0.8710115012098772\n",
      "GD iter. 407/499: loss=0.871000856886338\n",
      "GD iter. 408/499: loss=0.8709899756776839\n",
      "GD iter. 409/499: loss=0.870979129097897\n",
      "GD iter. 410/499: loss=0.8709694065581404\n",
      "GD iter. 411/499: loss=0.8709584252422781\n",
      "GD iter. 412/499: loss=0.8709481177393462\n",
      "GD iter. 413/499: loss=0.8709381017708248\n",
      "GD iter. 414/499: loss=0.8709273240405413\n",
      "GD iter. 415/499: loss=0.8709174574498157\n",
      "GD iter. 416/499: loss=0.8709076142734555\n",
      "GD iter. 417/499: loss=0.8708972897901637\n",
      "GD iter. 418/499: loss=0.8708877076320414\n",
      "GD iter. 419/499: loss=0.8708778023328906\n",
      "GD iter. 420/499: loss=0.8708678837198237\n",
      "GD iter. 421/499: loss=0.8708574191737513\n",
      "GD iter. 422/499: loss=0.8708479855310565\n",
      "GD iter. 423/499: loss=0.8708379657962232\n",
      "GD iter. 424/499: loss=0.8708285603664747\n",
      "GD iter. 425/499: loss=0.8708186365592809\n",
      "GD iter. 426/499: loss=0.8708085865050889\n",
      "GD iter. 427/499: loss=0.8707996092184502\n",
      "GD iter. 428/499: loss=0.8707897864513863\n",
      "GD iter. 429/499: loss=0.8707800484201186\n",
      "GD iter. 430/499: loss=0.8707703290050319\n",
      "GD iter. 431/499: loss=0.8707608371929892\n",
      "GD iter. 432/499: loss=0.8707509958686528\n",
      "GD iter. 433/499: loss=0.8707415767185587\n",
      "GD iter. 434/499: loss=0.8707318916333452\n",
      "GD iter. 435/499: loss=0.870722836873812\n",
      "GD iter. 436/499: loss=0.8707126469855238\n",
      "GD iter. 437/499: loss=0.8707031246950744\n",
      "GD iter. 438/499: loss=0.8706948034790888\n",
      "GD iter. 439/499: loss=0.8706850511663948\n",
      "GD iter. 440/499: loss=0.8706757161963835\n",
      "GD iter. 441/499: loss=0.870666869410602\n",
      "GD iter. 442/499: loss=0.8706574693240027\n",
      "GD iter. 443/499: loss=0.8706481805422932\n",
      "GD iter. 444/499: loss=0.8706395894819289\n",
      "GD iter. 445/499: loss=0.8706303422184073\n",
      "GD iter. 446/499: loss=0.8706219512539948\n",
      "GD iter. 447/499: loss=0.8706120716254694\n",
      "GD iter. 448/499: loss=0.8706032740054754\n",
      "GD iter. 449/499: loss=0.870594863245047\n",
      "GD iter. 450/499: loss=0.8705864271606962\n",
      "GD iter. 451/499: loss=0.8705773295343598\n",
      "GD iter. 452/499: loss=0.8705686265376545\n",
      "GD iter. 453/499: loss=0.8705605297224666\n",
      "GD iter. 454/499: loss=0.8705525277523212\n",
      "GD iter. 455/499: loss=0.870543921325041\n",
      "GD iter. 456/499: loss=0.8705360171829978\n",
      "GD iter. 457/499: loss=0.870527725509332\n",
      "GD iter. 458/499: loss=0.870519332564606\n",
      "GD iter. 459/499: loss=0.8705113459249221\n",
      "GD iter. 460/499: loss=0.870503258216702\n",
      "GD iter. 461/499: loss=0.8704953945968718\n",
      "GD iter. 462/499: loss=0.8704872738124635\n",
      "GD iter. 463/499: loss=0.8704800878996674\n",
      "GD iter. 464/499: loss=0.8704717493950318\n",
      "GD iter. 465/499: loss=0.8704633723455757\n",
      "GD iter. 466/499: loss=0.8704557873043397\n",
      "GD iter. 467/499: loss=0.8704481259195056\n",
      "GD iter. 468/499: loss=0.8704403361787748\n",
      "GD iter. 469/499: loss=0.8704321850394412\n",
      "GD iter. 470/499: loss=0.8704244409448731\n",
      "GD iter. 471/499: loss=0.8704168369885752\n",
      "GD iter. 472/499: loss=0.8704092211517714\n",
      "GD iter. 473/499: loss=0.8704028234855232\n",
      "GD iter. 474/499: loss=0.8703943467918717\n",
      "GD iter. 475/499: loss=0.8703862545703434\n",
      "GD iter. 476/499: loss=0.8703792009518804\n",
      "GD iter. 477/499: loss=0.8703721251035528\n",
      "GD iter. 478/499: loss=0.8703639497703831\n",
      "GD iter. 479/499: loss=0.8703566849671672\n",
      "GD iter. 480/499: loss=0.8703491816722783\n",
      "GD iter. 481/499: loss=0.8703419318100247\n",
      "GD iter. 482/499: loss=0.8703341294837745\n",
      "GD iter. 483/499: loss=0.870326932361783\n",
      "GD iter. 484/499: loss=0.8703200361236315\n",
      "GD iter. 485/499: loss=0.8703122612174539\n",
      "GD iter. 486/499: loss=0.8703053901544351\n",
      "GD iter. 487/499: loss=0.8702981732968214\n",
      "GD iter. 488/499: loss=0.8702909083463022\n",
      "GD iter. 489/499: loss=0.8702841507512413\n",
      "GD iter. 490/499: loss=0.8702770818526587\n",
      "GD iter. 491/499: loss=0.8702702470527971\n",
      "GD iter. 492/499: loss=0.8702626803062511\n",
      "GD iter. 493/499: loss=0.8702558983467306\n",
      "GD iter. 494/499: loss=0.8702489770538839\n",
      "GD iter. 495/499: loss=0.870241821699622\n",
      "GD iter. 496/499: loss=0.8702347835956714\n",
      "GD iter. 497/499: loss=0.8702278492406476\n",
      "GD iter. 498/499: loss=0.8702211520951868\n",
      "GD iter. 499/499: loss=0.8702137732443149\n",
      "The Accuracy is: 0.6177\n",
      "The F1 score is: 0.2854\n",
      "The precision is: 0.1698\n",
      "The recall is: 0.8930\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.random.randn(x_train_processed_hinge.shape[1]) * 0.01\n",
    "w, loss = hinge_regression(y_train_processed_hinge, x_train_processed_hinge, initial_w, lambda_=0.1, max_iters=500, gamma=0.01)\n",
    "y_pred = hinge_predict(x_train_processed_hinge, w)\n",
    "predict_acc_pure(y_pred, y_train_processed_hinge)\n",
    "predict_f1_pure(y_pred, y_train_processed_hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is: 0.8965\n",
      "The F1 score is: 0.2766\n",
      "The precision is: 0.3611\n",
      "The recall is: 0.2241\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "cls = svm.SVC(C=50, kernel='rbf')\n",
    "x_t, y_t, x_v, y_v = split_data(x_train_processed_hinge, y_train_processed_hinge, 0.9)\n",
    "cls.fit(x_t, y_t)\n",
    "y_pred = cls.predict(x_v)\n",
    "predict_acc_pure(y_pred, y_v)\n",
    "predict_f1_pure(y_pred, y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=(0.26011268218427586+0j)\n",
      "GD iter. 10/99: loss=(0.08147707064840479+0j)\n",
      "GD iter. 20/99: loss=(0.0766382660743874+0j)\n",
      "GD iter. 30/99: loss=(0.07613497378527605+0j)\n",
      "GD iter. 40/99: loss=(0.07604349722691961+0j)\n",
      "GD iter. 50/99: loss=(0.0760236271905045+0j)\n",
      "GD iter. 60/99: loss=(0.07601893469399162+0j)\n",
      "GD iter. 70/99: loss=(0.0760177697151582+0j)\n",
      "GD iter. 80/99: loss=(0.0760174706076141+0j)\n",
      "GD iter. 90/99: loss=(0.07601739193304777+0j)\n",
      "The Accuracy is: 0.7510\n",
      "The F1 score is: 0.3597\n",
      "The precision is: 0.2305\n",
      "The recall is: 0.8182\n"
     ]
    }
   ],
   "source": [
    "## linear regression using PCA feature selection ##\n",
    "\n",
    "initial_w = np.random.randn(x_pca.shape[1]) * 0.01\n",
    "w, loss = mean_square_error_gd(y_train_processed, x_pca, initial_w, max_iters = 100, gamma=0.1)\n",
    "y_pred = x_pca @ w\n",
    "y_pred_mean = np.mean(y_pred)\n",
    "predict_acc(x_train_processed_orig_pca, y_train_processed_orig, w, logistic=False, threshold=y_pred_mean)\n",
    "predict_f1(x_train_processed_orig_pca, y_train_processed_orig, w, logistic=False, threshold=y_pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=(0.6897111576101024-0j)\n",
      "GD iter. 1/99: loss=(0.6726574569842683-0j)\n",
      "GD iter. 2/99: loss=(0.6576164404171536-0j)\n",
      "GD iter. 3/99: loss=(0.64429229886218-0j)\n",
      "GD iter. 4/99: loss=(0.6324330027251944-0j)\n",
      "GD iter. 5/99: loss=(0.6218259718780282-0j)\n",
      "GD iter. 6/99: loss=(0.6122928526325127-0j)\n",
      "GD iter. 7/99: loss=(0.6036843072799801-0j)\n",
      "GD iter. 8/99: loss=(0.5958752597785073-0j)\n",
      "GD iter. 9/99: loss=(0.58876076383266-0j)\n",
      "GD iter. 10/99: loss=(0.5822525120019079-0j)\n",
      "GD iter. 11/99: loss=(0.5762759352355062-0j)\n",
      "GD iter. 12/99: loss=(0.5707678161261212-0j)\n",
      "GD iter. 13/99: loss=(0.5656743349923622-0j)\n",
      "GD iter. 14/99: loss=(0.5609494737928054-0j)\n",
      "GD iter. 15/99: loss=(0.5565537125831983-0j)\n",
      "GD iter. 16/99: loss=(0.5524529636405321-0j)\n",
      "GD iter. 17/99: loss=(0.5486176980740622-0j)\n",
      "GD iter. 18/99: loss=(0.5450222281845131-0j)\n",
      "GD iter. 19/99: loss=(0.5416441159120801-0j)\n",
      "GD iter. 20/99: loss=(0.5384636835215202-0j)\n",
      "GD iter. 21/99: loss=(0.5354636073737228-0j)\n",
      "GD iter. 22/99: loss=(0.5326285794081244-0j)\n",
      "GD iter. 23/99: loss=(0.5299450239779399-0j)\n",
      "GD iter. 24/99: loss=(0.5274008600871671-0j)\n",
      "GD iter. 25/99: loss=(0.5249853009972365-0j)\n",
      "GD iter. 26/99: loss=(0.5226886847019598-0j)\n",
      "GD iter. 27/99: loss=(0.5205023299923769-0j)\n",
      "GD iter. 28/99: loss=(0.5184184138121192-0j)\n",
      "GD iter. 29/99: loss=(0.5164298663896073-0j)\n",
      "GD iter. 30/99: loss=(0.5145302812657063-0j)\n",
      "GD iter. 31/99: loss=(0.5127138378458512-0j)\n",
      "GD iter. 32/99: loss=(0.510975234518925-0j)\n",
      "GD iter. 33/99: loss=(0.509309630720887-0j)\n",
      "GD iter. 34/99: loss=(0.5077125965947579-0j)\n",
      "GD iter. 35/99: loss=(0.5061800691222916-0j)\n",
      "GD iter. 36/99: loss=(0.5047083137862076-0j)\n",
      "GD iter. 37/99: loss=(0.503293890972926-0j)\n",
      "GD iter. 38/99: loss=(0.501933626450488-0j)\n",
      "GD iter. 39/99: loss=(0.5006245853596805-0j)\n",
      "GD iter. 40/99: loss=(0.4993640492422527-0j)\n",
      "GD iter. 41/99: loss=(0.49814949570168493-0j)\n",
      "GD iter. 42/99: loss=(0.49697858035181347-0j)\n",
      "GD iter. 43/99: loss=(0.4958491207587937-0j)\n",
      "GD iter. 44/99: loss=(0.4947590821240812-0j)\n",
      "GD iter. 45/99: loss=(0.4937065644916966-0j)\n",
      "GD iter. 46/99: loss=(0.4926897912931371-0j)\n",
      "GD iter. 47/99: loss=(0.4917070990688115-0j)\n",
      "GD iter. 48/99: loss=(0.4907569282265705-0j)\n",
      "GD iter. 49/99: loss=(0.4898378147163958-0j)\n",
      "GD iter. 50/99: loss=(0.4889483825161047-0j)\n",
      "GD iter. 51/99: loss=(0.48808733683646366-0j)\n",
      "GD iter. 52/99: loss=(0.4872534579657197-0j)\n",
      "GD iter. 53/99: loss=(0.48644559568355056-0j)\n",
      "GD iter. 54/99: loss=(0.4856626641830629-0j)\n",
      "GD iter. 55/99: loss=(0.48490363744691656-0j)\n",
      "GD iter. 56/99: loss=(0.48416754503010306-0j)\n",
      "GD iter. 57/99: loss=(0.4834534682075172-0j)\n",
      "GD iter. 58/99: loss=(0.48276053644932476-0j)\n",
      "GD iter. 59/99: loss=(0.48208792419137936-0j)\n",
      "GD iter. 60/99: loss=(0.48143484787164975-0j)\n",
      "GD iter. 61/99: loss=(0.48080056320685605-0j)\n",
      "GD iter. 62/99: loss=(0.48018436268636366-0j)\n",
      "GD iter. 63/99: loss=(0.47958557326287016-0j)\n",
      "GD iter. 64/99: loss=(0.4790035542216203-0j)\n",
      "GD iter. 65/99: loss=(0.4784376952118168-0j)\n",
      "GD iter. 66/99: loss=(0.47788741442559735-0j)\n",
      "GD iter. 67/99: loss=(0.47735215691145944-0j)\n",
      "GD iter. 68/99: loss=(0.4768313930103477-0j)\n",
      "GD iter. 69/99: loss=(0.4763246169038028-0j)\n",
      "GD iter. 70/99: loss=(0.4758313452646208-0j)\n",
      "GD iter. 71/99: loss=(0.4753511160014096-0j)\n",
      "GD iter. 72/99: loss=(0.4748834870892565-0j)\n",
      "GD iter. 73/99: loss=(0.47442803547947066-0j)\n",
      "GD iter. 74/99: loss=(0.4739843560820203-0j)\n",
      "GD iter. 75/99: loss=(0.47355206081488316-0j)\n",
      "GD iter. 76/99: loss=(0.4731307777150573-0j)\n",
      "GD iter. 77/99: loss=(0.47272015010645835-0j)\n",
      "GD iter. 78/99: loss=(0.4723198358203535-0j)\n",
      "GD iter. 79/99: loss=(0.4719295064643722-0j)\n",
      "GD iter. 80/99: loss=(0.4715488467364768-0j)\n",
      "GD iter. 81/99: loss=(0.47117755378058906-0j)\n",
      "GD iter. 82/99: loss=(0.47081533658085123-0j)\n",
      "GD iter. 83/99: loss=(0.47046191539175636-0j)\n",
      "GD iter. 84/99: loss=(0.4701170212016089-0j)\n",
      "GD iter. 85/99: loss=(0.4697803952269905-0j)\n",
      "GD iter. 86/99: loss=(0.4694517884360908-0j)\n",
      "GD iter. 87/99: loss=(0.4691309610989391-0j)\n",
      "GD iter. 88/99: loss=(0.4688176823627236-0j)\n",
      "GD iter. 89/99: loss=(0.46851172985053313-0j)\n",
      "GD iter. 90/99: loss=(0.4682128892819807-0j)\n",
      "GD iter. 91/99: loss=(0.467920954114289-0j)\n",
      "GD iter. 92/99: loss=(0.4676357252025245-0j)\n",
      "GD iter. 93/99: loss=(0.46735701047776596-0j)\n",
      "GD iter. 94/99: loss=(0.4670846246420811-0j)\n",
      "GD iter. 95/99: loss=(0.46681838887927235-0j)\n",
      "GD iter. 96/99: loss=(0.4665581305804212-0j)\n",
      "GD iter. 97/99: loss=(0.46630368308333575-0j)\n",
      "GD iter. 98/99: loss=(0.46605488542506746-0j)\n",
      "GD iter. 99/99: loss=(0.46581158210672047-0j)\n",
      "The Accuracy is: 0.8629\n",
      "The F1 score is: 0.4186\n",
      "The precision is: 0.3283\n",
      "The recall is: 0.5775\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using pca feature selection #\n",
    "initial_w = np.random.randn(x_pca.shape[1]) * 0.01\n",
    "w, loss = logistic_regression(y_train_processed, x_pca, initial_w, max_iters=100, gamma=0.15)\n",
    "predict_acc(x_train_processed_orig_pca, y_train_processed_orig, w, logistic=False, threshold=0.85)\n",
    "predict_f1(x_train_processed_orig_pca, y_train_processed_orig, w, logistic=False, threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=(0.6909626798061412-0j)\n",
      "GD iter. 1/99: loss=(0.6735794558601624-0j)\n",
      "GD iter. 2/99: loss=(0.6583223113486754-0j)\n",
      "GD iter. 3/99: loss=(0.6448689264505854-0j)\n",
      "GD iter. 4/99: loss=(0.6329464838761912-0j)\n",
      "GD iter. 5/99: loss=(0.6223263407459684-0j)\n",
      "GD iter. 6/99: loss=(0.6128178374165825-0j)\n",
      "GD iter. 7/99: loss=(0.6042622492063763-0j)\n",
      "GD iter. 8/99: loss=(0.5965273504120941-0j)\n",
      "GD iter. 9/99: loss=(0.5895027471942651-0j)\n",
      "GD iter. 10/99: loss=(0.5830959752063374-0j)\n",
      "GD iter. 11/99: loss=(0.5772292865389332-0j)\n",
      "GD iter. 12/99: loss=(0.5718370269939669-0j)\n",
      "GD iter. 13/99: loss=(0.566863504476215-0j)\n",
      "GD iter. 14/99: loss=(0.5622612591162924-0j)\n",
      "GD iter. 15/99: loss=(0.5579896588306951-0j)\n",
      "GD iter. 16/99: loss=(0.554013757150464-0j)\n",
      "GD iter. 17/99: loss=(0.5503033619432851-0j)\n",
      "GD iter. 18/99: loss=(0.5468322736803585-0j)\n",
      "GD iter. 19/99: loss=(0.5435776601625875-0j)\n",
      "GD iter. 20/99: loss=(0.5405195413063457-0j)\n",
      "GD iter. 21/99: loss=(0.5376403629393663-0j)\n",
      "GD iter. 22/99: loss=(0.534924642811943-0j)\n",
      "GD iter. 23/99: loss=(0.5323586754010304-0j)\n",
      "GD iter. 24/99: loss=(0.5299302847547922-0j)\n",
      "GD iter. 25/99: loss=(0.5276286167395428-0j)\n",
      "GD iter. 26/99: loss=(0.5254439637276672-0j)\n",
      "GD iter. 27/99: loss=(0.5233676160972459-0j)\n",
      "GD iter. 28/99: loss=(0.5213917359752566-0j)\n",
      "GD iter. 29/99: loss=(0.5195092495039314-0j)\n",
      "GD iter. 30/99: loss=(0.5177137545891721-0j)\n",
      "GD iter. 31/99: loss=(0.515999441636123-0j)\n",
      "GD iter. 32/99: loss=(0.5143610252176699-0j)\n",
      "GD iter. 33/99: loss=(0.5127936849783787-0j)\n",
      "GD iter. 34/99: loss=(0.5112930143662293-0j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 35/99: loss=(0.5098549760207924-0j)\n",
      "GD iter. 36/99: loss=(0.5084758628398119-0j)\n",
      "GD iter. 37/99: loss=(0.5071522639048595-0j)\n",
      "GD iter. 38/99: loss=(0.5058810345774382-0j)\n",
      "GD iter. 39/99: loss=(0.5046592701849643-0j)\n",
      "GD iter. 40/99: loss=(0.5034842828056246-0j)\n",
      "GD iter. 41/99: loss=(0.5023535807356293-0j)\n",
      "GD iter. 42/99: loss=(0.5012648502845554-0j)\n",
      "GD iter. 43/99: loss=(0.5002159395965198-0j)\n",
      "GD iter. 44/99: loss=(0.49920484423860506-0j)\n",
      "GD iter. 45/99: loss=(0.49822969433474773-0j)\n",
      "GD iter. 46/99: loss=(0.4972887430543463-0j)\n",
      "GD iter. 47/99: loss=(0.49638035629114724-0j)\n",
      "GD iter. 48/99: loss=(0.49550300339027636-0j)\n",
      "GD iter. 49/99: loss=(0.49465524880028805-0j)\n",
      "GD iter. 50/99: loss=(0.4938357445433074-0j)\n",
      "GD iter. 51/99: loss=(0.4930432234102107-0j)\n",
      "GD iter. 52/99: loss=(0.4922764927996747-0j)\n",
      "GD iter. 53/99: loss=(0.49153442913014783-0j)\n",
      "GD iter. 54/99: loss=(0.4908159727625925-0j)\n",
      "GD iter. 55/99: loss=(0.49012012337945404-0j)\n",
      "GD iter. 56/99: loss=(0.48944593577188256-0j)\n",
      "GD iter. 57/99: loss=(0.4887925159929348-0j)\n",
      "GD iter. 58/99: loss=(0.4881590178394382-0j)\n",
      "GD iter. 59/99: loss=(0.4875446396295103-0j)\n",
      "GD iter. 60/99: loss=(0.4869486212464849-0j)\n",
      "GD iter. 61/99: loss=(0.48637024142328966-0j)\n",
      "GD iter. 62/99: loss=(0.4858088152441888-0j)\n",
      "GD iter. 63/99: loss=(0.4852636918433361-0j)\n",
      "GD iter. 64/99: loss=(0.4847342522817974-0j)\n",
      "GD iter. 65/99: loss=(0.4842199075866536-0j)\n",
      "GD iter. 66/99: loss=(0.48372009693751894-0j)\n",
      "GD iter. 67/99: loss=(0.483234285987327-0j)\n",
      "GD iter. 68/99: loss=(0.48276196530558124-0j)\n",
      "GD iter. 69/99: loss=(0.4823026489334607-0j)\n",
      "GD iter. 70/99: loss=(0.48185587304122457-0j)\n",
      "GD iter. 71/99: loss=(0.4814211946793034-0j)\n",
      "GD iter. 72/99: loss=(0.48099819061529525-0j)\n",
      "GD iter. 73/99: loss=(0.480586456249836-0j)\n",
      "GD iter. 74/99: loss=(0.48018560460497345-0j)\n",
      "GD iter. 75/99: loss=(0.47979526537927275-0j)\n",
      "GD iter. 76/99: loss=(0.47941508406440964-0j)\n",
      "GD iter. 77/99: loss=(0.4790447211184899-0j)\n",
      "GD iter. 78/99: loss=(0.4786838511917539-0j)\n",
      "GD iter. 79/99: loss=(0.4783321624007202-0j)\n",
      "GD iter. 80/99: loss=(0.4779893556471564-0j)\n",
      "GD iter. 81/99: loss=(0.4776551439785894-0j)\n",
      "GD iter. 82/99: loss=(0.47732925198733767-0j)\n",
      "GD iter. 83/99: loss=(0.47701141524531215-0j)\n",
      "GD iter. 84/99: loss=(0.47670137977205324-0j)\n",
      "GD iter. 85/99: loss=(0.4763989015336878-0j)\n",
      "GD iter. 86/99: loss=(0.4761037459706711-0j)\n",
      "GD iter. 87/99: loss=(0.47581568755235654-0j)\n",
      "GD iter. 88/99: loss=(0.4755345093565869-0j)\n",
      "GD iter. 89/99: loss=(0.4752600026726446-0j)\n",
      "GD iter. 90/99: loss=(0.4749919666260259-0j)\n",
      "GD iter. 91/99: loss=(0.474730207823623-0j)\n",
      "GD iter. 92/99: loss=(0.47447454001800576-0j)\n",
      "GD iter. 93/99: loss=(0.4742247837895879-0j)\n",
      "GD iter. 94/99: loss=(0.4739807662455588-0j)\n",
      "GD iter. 95/99: loss=(0.47374232073453953-0j)\n",
      "GD iter. 96/99: loss=(0.4735092865759984-0j)\n",
      "GD iter. 97/99: loss=(0.47328150880352926-0j)\n",
      "GD iter. 98/99: loss=(0.4730588379211607-0j)\n",
      "GD iter. 99/99: loss=(0.47284112967192193-0j)\n",
      "The Accuracy is: 0.8703\n",
      "The F1 score is: 0.4238\n",
      "The precision is: 0.3417\n",
      "The recall is: 0.5579\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using pca feature selection #\n",
    "initial_w = np.random.randn(x_pca.shape[1]) * 0.01\n",
    "w, loss = reg_logistic_regression(y_train_processed, x_pca, lambda_=0.01, initial_w=initial_w, max_iters=100, gamma=0.15)\n",
    "predict_acc(x_train_processed_orig_pca, y_train_processed_orig, w, logistic=False, threshold=0.85)\n",
    "predict_f1(x_train_processed_orig_pca, y_train_processed_orig, w, logistic=False, threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.random.randn(x_train_processed_hinge.shape[1]) * 0.01\n",
    "w, loss = hinge_regression(y_train_processed_hinge, x_train_processed_hinge, initial_w, lambda_=0.1, max_iters=500, gamma=0.01)\n",
    "y_pred = hinge_predict(x_train_processed_hinge, w)\n",
    "predict_acc_pure(y_pred, y_train_processed_hinge)\n",
    "predict_f1_pure(y_pred, y_train_processed_hinge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SVM to classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_svm, b_svm = SVM.gradient_descent(x_train_processed_orig_pca[:, 1:], y_train_processed_orig, epochs=500, lr=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
