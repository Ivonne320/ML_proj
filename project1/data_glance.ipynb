{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "# import SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure: 1. normalize coding for missing values: check each feature, assign NaN to values > 95% percentile. --> 2. check each row, drop the rows where over 50% of the features are NaN. Also remove the corresponding y  --> 3. Check each columns, drop the feature where over 50% of the rows are NaN or std == 0. Record the index for test set. --> 4. Handling NaN values: check for categorical feature (record the index), assign NaN to mean for non-categorical and majority label for categorial. --> 5. One-hot encoding for categorical data. --> 6. Standardize (skip categorical 0-1 columns)  --> 7. Regard a datapoint having more than 30% of the features with Z-score>2.5 as outliers, remove from x and corresponding y. --> 8. Data Augmentation --> 9. PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"./data/dataset_to_release\", sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6563, 321)\n",
      "(6563,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32813, 321)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_sub, y_sub = split_cross_validation(x_train, y_train, 10)\n",
    "# np.shape(x_sub[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6368, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6098, 521)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thresholds for nans\n",
    "row_nan = 0.6\n",
    "feature_nan = 0.8\n",
    "# threshold for categorical features\n",
    "threshold_cat = 10\n",
    "# threshold for outliers\n",
    "z_threshold=2\n",
    "feature_threshold=0.2\n",
    "\n",
    "x_train_processed = x_train.copy()\n",
    "y_train_processed = y_train.copy()\n",
    "\n",
    "# transform y to 0-1 encoding\n",
    "y_train_processed = process_y(y_train_processed)\n",
    "\n",
    "# Uniform missing value encoding\n",
    "# x_train_processed = normalize_nan(x_train_processed)\n",
    "# Remove rows with too many nans\n",
    "x_train_processed, y_train_processed = drop_rows(x_train_processed, y_train_processed, row_nan) # 0.55 remains 6101 rows\n",
    "# x_train_processed.shape\n",
    "# Remove features with too many nans\n",
    "x_train_processed, nan_indices = drop_features(x_train_processed, feature_nan) # 0.5 remains 174 features\n",
    "print(x_train_processed.shape)\n",
    "\n",
    "# get categorical feature indices\n",
    "cat_indices = check_categorical(x_train_processed, threshold_cat)\n",
    "# handling remaining nans\n",
    "x_train_processed = fillna(x_train_processed, cat_indices)\n",
    "# One hot encoding for categorical features\n",
    "x_train_processed = one_hot_encoding(x_train_processed, cat_indices)\n",
    "x_train_processed = standardize(x_train_processed)\n",
    "x_train_processed, y_train_processed = z_outlier_removal(x_train_processed, y_train_processed, z_threshold, feature_threshold)\n",
    "x_train_processed.shape\n",
    "# x_train_processed = add_bias(x_train_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27108"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_train_processed==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(302903, 524)\n",
      "(302903,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(x_train_processed.shape)\n",
    "print(y_train_processed.shape)\n",
    "print(np.sum(x_train_processed.std(axis=0) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_processed_orig = x_train.copy()\n",
    "# x_train_processed_orig = fillna_with_mean(x_train_processed_orig, threshold=threshold_nan)\n",
    "# x_train_processed_orig = standardize(x_train_processed_orig)\n",
    "# x_train_processed_orig = polynomial_expansion_single(x_train_processed_orig, degree=2)\n",
    "# x_train_processed_orig = standardize(x_train_processed_orig)\n",
    "# x_train_processed_orig = add_bias(x_train_processed_orig)\n",
    "# # add a column of ones\n",
    "# y_train_processed_orig = y_train.copy()\n",
    "# y_train_processed_orig = process_y(y_train_processed_orig)\n",
    "# print(np.isnan(x_train_processed_orig).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_processed_orig.shape)\n",
    "# print(y_train_processed_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PCA feature selection \n",
    "# pre_train_data = x_train_processed.copy()\n",
    "\n",
    "# x_pca, eig_vec, eig_val,weight = pca(pre_train_data, 200)\n",
    "# print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302903, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zewzhang/miniconda3/envs/ML/lib/python3.12/site-packages/matplotlib/cbook.py:1699: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return math.isfinite(val)\n",
      "/home/zewzhang/miniconda3/envs/ML/lib/python3.12/site-packages/matplotlib/cbook.py:1345: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return np.asarray(x, float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbd7989a900>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cummulation = np.cumsum(weight)\n",
    "## cummulation\n",
    "# plt.plot(cummulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302903, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/199: loss=0.2865729328029284\n",
      "GD iter. 10/199: loss=0.13946601263445396\n",
      "GD iter. 20/199: loss=0.12366667198056777\n",
      "GD iter. 30/199: loss=0.1144078560481228\n",
      "GD iter. 40/199: loss=0.10775494697222973\n",
      "GD iter. 50/199: loss=0.10258642241592701\n",
      "GD iter. 60/199: loss=0.09843715485050078\n",
      "GD iter. 70/199: loss=0.09505082171348743\n",
      "GD iter. 80/199: loss=0.09225969624479984\n",
      "GD iter. 90/199: loss=0.08994367253009612\n",
      "GD iter. 100/199: loss=0.08801239776067839\n",
      "GD iter. 110/199: loss=0.08639582359121509\n",
      "GD iter. 120/199: loss=0.08503853588740733\n",
      "GD iter. 130/199: loss=0.0838960635333123\n",
      "GD iter. 140/199: loss=0.08293233693501748\n",
      "GD iter. 150/199: loss=0.08211786404834029\n",
      "GD iter. 160/199: loss=0.08142837904183471\n",
      "GD iter. 170/199: loss=0.08084381603199367\n",
      "GD iter. 180/199: loss=0.08034751445680566\n",
      "GD iter. 190/199: loss=0.0799255943708258\n",
      "The Accuracy is: 0.6084\n",
      "The F1 score is: 0.3031\n",
      "The precision is: 0.1810\n",
      "The recall is: 0.9314\n",
      "GD iter. 0/199: loss=0.29142764490592826\n",
      "GD iter. 10/199: loss=0.132664013426769\n",
      "GD iter. 20/199: loss=0.1187580880118554\n",
      "GD iter. 30/199: loss=0.1107658118251581\n",
      "GD iter. 40/199: loss=0.10498848570513296\n",
      "GD iter. 50/199: loss=0.10045439504456595\n",
      "GD iter. 60/199: loss=0.09678110233160316\n",
      "GD iter. 70/199: loss=0.09376048298438475\n",
      "GD iter. 80/199: loss=0.09125494882112921\n",
      "GD iter. 90/199: loss=0.08916445274298992\n",
      "GD iter. 100/199: loss=0.08741266977342442\n",
      "GD iter. 110/199: loss=0.08593973824623131\n",
      "GD iter. 120/199: loss=0.08469785247295433\n",
      "GD iter. 130/199: loss=0.08364834393178537\n",
      "GD iter. 140/199: loss=0.08275963742961913\n",
      "GD iter. 150/199: loss=0.08200576262797596\n",
      "GD iter. 160/199: loss=0.08136523766770747\n",
      "GD iter. 170/199: loss=0.08082021263587341\n",
      "GD iter. 180/199: loss=0.0803558005867121\n",
      "GD iter. 190/199: loss=0.07995954762701192\n",
      "The Accuracy is: 0.6029\n",
      "The F1 score is: 0.2906\n",
      "The precision is: 0.1720\n",
      "The recall is: 0.9369\n",
      "GD iter. 0/199: loss=0.2917071019509754\n",
      "GD iter. 10/199: loss=0.1402384591417274\n",
      "GD iter. 20/199: loss=0.12613096468455412\n",
      "GD iter. 30/199: loss=0.1169832900663504\n",
      "GD iter. 40/199: loss=0.11010896559621332\n",
      "GD iter. 50/199: loss=0.10465815507408945\n",
      "GD iter. 60/199: loss=0.10023254717422318\n",
      "GD iter. 70/199: loss=0.09659305902425779\n",
      "GD iter. 80/199: loss=0.09357551798390477\n",
      "GD iter. 90/199: loss=0.09105916939457467\n",
      "GD iter. 100/199: loss=0.08895168423941972\n",
      "GD iter. 110/199: loss=0.08718068582967975\n",
      "GD iter. 120/199: loss=0.08568843368225412\n",
      "GD iter. 130/199: loss=0.08442826288748218\n",
      "GD iter. 140/199: loss=0.08336208748959678\n",
      "GD iter. 150/199: loss=0.08245858714886559\n",
      "GD iter. 160/199: loss=0.08169185228043148\n",
      "GD iter. 170/199: loss=0.08104034822958445\n",
      "GD iter. 180/199: loss=0.08048610849036449\n",
      "GD iter. 190/199: loss=0.08001409682512625\n",
      "The Accuracy is: 0.6033\n",
      "The F1 score is: 0.2984\n",
      "The precision is: 0.1779\n",
      "The recall is: 0.9247\n",
      "GD iter. 0/199: loss=0.3058717297096762\n",
      "GD iter. 10/199: loss=0.14406167290504798\n",
      "GD iter. 20/199: loss=0.12767762259762125\n",
      "GD iter. 30/199: loss=0.11782283049468158\n",
      "GD iter. 40/199: loss=0.11065280068543086\n",
      "GD iter. 50/199: loss=0.10504177015093646\n",
      "GD iter. 60/199: loss=0.10051125905468646\n",
      "GD iter. 70/199: loss=0.09679473832331172\n",
      "GD iter. 80/199: loss=0.09371707777563587\n",
      "GD iter. 90/199: loss=0.09115233369025512\n",
      "GD iter. 100/199: loss=0.08900524628906961\n",
      "GD iter. 110/199: loss=0.08720152420120442\n",
      "GD iter. 120/199: loss=0.08568204984860534\n",
      "GD iter. 130/199: loss=0.08439911214418637\n",
      "GD iter. 140/199: loss=0.083313806527916\n",
      "GD iter. 150/199: loss=0.08239416149538717\n",
      "GD iter. 160/199: loss=0.08161374421538245\n",
      "GD iter. 170/199: loss=0.08095059658818178\n",
      "GD iter. 180/199: loss=0.08038640752779193\n",
      "GD iter. 190/199: loss=0.07990585908847134\n",
      "The Accuracy is: 0.6040\n",
      "The F1 score is: 0.3017\n",
      "The precision is: 0.1801\n",
      "The recall is: 0.9280\n",
      "GD iter. 0/199: loss=0.27127607594471825\n",
      "GD iter. 10/199: loss=0.13916914771589878\n",
      "GD iter. 20/199: loss=0.12377870583040032\n",
      "GD iter. 30/199: loss=0.11442081913873824\n",
      "GD iter. 40/199: loss=0.10764235333009567\n",
      "GD iter. 50/199: loss=0.10238673728669609\n",
      "GD iter. 60/199: loss=0.09818445889346548\n",
      "GD iter. 70/199: loss=0.09476815797598871\n",
      "GD iter. 80/199: loss=0.09196185689491637\n",
      "GD iter. 90/199: loss=0.0896400293552918\n",
      "GD iter. 100/199: loss=0.08770879071656643\n",
      "GD iter. 110/199: loss=0.08609576522530142\n",
      "GD iter. 120/199: loss=0.08474399849484426\n",
      "GD iter. 130/199: loss=0.08360800837103628\n",
      "GD iter. 140/199: loss=0.08265107527917133\n",
      "GD iter. 150/199: loss=0.08184330373895364\n",
      "GD iter. 160/199: loss=0.08116019128380299\n",
      "GD iter. 170/199: loss=0.08058154637540807\n",
      "GD iter. 180/199: loss=0.08009065490850392\n",
      "GD iter. 190/199: loss=0.07967362874136517\n",
      "The Accuracy is: 0.6056\n",
      "The F1 score is: 0.2931\n",
      "The precision is: 0.1739\n",
      "The recall is: 0.9312\n",
      "GD iter. 0/199: loss=0.2896278542618197\n",
      "GD iter. 10/199: loss=0.1401869666548427\n",
      "GD iter. 20/199: loss=0.12416276498438421\n",
      "GD iter. 30/199: loss=0.11475997923998237\n",
      "GD iter. 40/199: loss=0.1079978613510187\n",
      "GD iter. 50/199: loss=0.10274368396864607\n",
      "GD iter. 60/199: loss=0.09852472873425297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#X65sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m x_t, y_t \u001b[39m=\u001b[39m data_augmentation(x_t, y_t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#X65sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m initial_w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(x_t\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39m*\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#X65sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m w, loss \u001b[39m=\u001b[39m mean_square_error_gd(y_t, x_t, initial_w, max_iters\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#X65sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y_pred \u001b[39m=\u001b[39m x_v \u001b[39m@\u001b[39m w\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#X65sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m y_pred_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(y_pred)\n",
      "File \u001b[0;32m~/Course/ML/Project/ML_proj/project1/implementations.py:61\u001b[0m, in \u001b[0;36mmean_square_error_gd\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m n_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[1;32m     60\u001b[0m     gradient \u001b[39m=\u001b[39m compute_gradient(y, tx, w)\n\u001b[0;32m---> 61\u001b[0m     loss \u001b[39m=\u001b[39m compute_mse(y, tx, w)\n\u001b[1;32m     62\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39m gamma \u001b[39m*\u001b[39m gradient\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m n_iter \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Course/ML/Project/ML_proj/project1/implementations.py:5\u001b[0m, in \u001b[0;36mcompute_mse\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"a function used to compute the mean squared error.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_mse\u001b[39m(y, tx, w):\n\u001b[1;32m      6\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculate the loss using MSE.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m        the value of the loss (a scalar), corresponding to the input parameters w.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     MSE \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((y \u001b[39m-\u001b[39m tx\u001b[39m.\u001b[39mdot(w))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(y))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# linear regression using all the features except for those having NaN values over 50% ##\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_train_processed), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(10):\n",
    "    # x_t, y_t, x_v, y_v = split_data(add_bias(x_train_processed), y_train_processed, 0.9)\n",
    "    # pick one of the 10 folds as validation set\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = mean_square_error_gd(y_t, x_t, initial_w, max_iters=200, gamma=0.01)\n",
    "    y_pred = x_v @ w\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    y_pred = (y_pred > y_pred_mean).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/149: loss=0.6810767909783251\n",
      "GD iter. 1/149: loss=0.5486970120893967\n",
      "GD iter. 2/149: loss=0.5308640416663686\n",
      "GD iter. 3/149: loss=0.5211693036161503\n",
      "GD iter. 4/149: loss=0.5141655030862539\n",
      "GD iter. 5/149: loss=0.5085142799203712\n",
      "GD iter. 6/149: loss=0.5037133769331086\n",
      "GD iter. 7/149: loss=0.49951253656925987\n",
      "GD iter. 8/149: loss=0.49576189430851453\n",
      "GD iter. 9/149: loss=0.49236170193341666\n",
      "GD iter. 10/149: loss=0.48924158500706244\n",
      "GD iter. 11/149: loss=0.48635015562652273\n",
      "GD iter. 12/149: loss=0.48364899964595187\n",
      "GD iter. 13/149: loss=0.4811088622324665\n",
      "GD iter. 14/149: loss=0.4787070956302764\n",
      "GD iter. 15/149: loss=0.47642589678381747\n",
      "GD iter. 16/149: loss=0.4742510651005098\n",
      "GD iter. 17/149: loss=0.4721711133604357\n",
      "GD iter. 18/149: loss=0.4701766232246444\n",
      "GD iter. 19/149: loss=0.46825977266741586\n",
      "GD iter. 20/149: loss=0.4664139857199197\n",
      "GD iter. 21/149: loss=0.46463367016720425\n",
      "GD iter. 22/149: loss=0.4629140191238966\n",
      "GD iter. 23/149: loss=0.4612508594445647\n",
      "GD iter. 24/149: loss=0.4596405347870179\n",
      "GD iter. 25/149: loss=0.45807981454374\n",
      "GD iter. 26/149: loss=0.4565658222518709\n",
      "GD iter. 27/149: loss=0.4550959787957751\n",
      "GD iter. 28/149: loss=0.4536679569379936\n",
      "GD iter. 29/149: loss=0.4522796445974993\n",
      "GD iter. 30/149: loss=0.4509291149374206\n",
      "GD iter. 31/149: loss=0.44961460179636925\n",
      "GD iter. 32/149: loss=0.44833447934630555\n",
      "GD iter. 33/149: loss=0.4470872451194493\n",
      "GD iter. 34/149: loss=0.44587150574123907\n",
      "GD iter. 35/149: loss=0.4446859648530619\n",
      "GD iter. 36/149: loss=0.44352941281988045\n",
      "GD iter. 37/149: loss=0.44240071790303986\n",
      "GD iter. 38/149: loss=0.4412988186440356\n",
      "GD iter. 39/149: loss=0.4402227172557437\n",
      "GD iter. 40/149: loss=0.4391714738571219\n",
      "GD iter. 41/149: loss=0.43814420141837634\n",
      "GD iter. 42/149: loss=0.43714006130802885\n",
      "GD iter. 43/149: loss=0.4361582593527267\n",
      "GD iter. 44/149: loss=0.43519804233613274\n",
      "GD iter. 45/149: loss=0.43425869487569085\n",
      "GD iter. 46/149: loss=0.4333395366261261\n",
      "GD iter. 47/149: loss=0.4324399197667202\n",
      "GD iter. 48/149: loss=0.4315592267360976\n",
      "GD iter. 49/149: loss=0.43069686818375374\n",
      "GD iter. 50/149: loss=0.4298522811121024\n",
      "GD iter. 51/149: loss=0.4290249271865945\n",
      "GD iter. 52/149: loss=0.42821429119461085\n",
      "GD iter. 53/149: loss=0.42741987963647693\n",
      "GD iter. 54/149: loss=0.42664121943416866\n",
      "GD iter. 55/149: loss=0.4258778567451681\n",
      "GD iter. 56/149: loss=0.42512935587052575\n",
      "GD iter. 57/149: loss=0.42439529824754835\n",
      "GD iter. 58/149: loss=0.4236752815187083\n",
      "GD iter. 59/149: loss=0.4229689186693644\n",
      "GD iter. 60/149: loss=0.42227583722776063\n",
      "GD iter. 61/149: loss=0.42159567852150936\n",
      "GD iter. 62/149: loss=0.42092809698542194\n",
      "GD iter. 63/149: loss=0.42027275951611204\n",
      "GD iter. 64/149: loss=0.4196293448692918\n",
      "GD iter. 65/149: loss=0.4189975430961119\n",
      "GD iter. 66/149: loss=0.4183770550152793\n",
      "GD iter. 67/149: loss=0.41776759171801264\n",
      "GD iter. 68/149: loss=0.41716887410319786\n",
      "GD iter. 69/149: loss=0.4165806324403606\n",
      "GD iter. 70/149: loss=0.4160026059583017\n",
      "GD iter. 71/149: loss=0.4154345424574528\n",
      "GD iter. 72/149: loss=0.4148761979441806\n",
      "GD iter. 73/149: loss=0.41432733628544116\n",
      "GD iter. 74/149: loss=0.4137877288823192\n",
      "GD iter. 75/149: loss=0.4132571543611267\n",
      "GD iter. 76/149: loss=0.41273539828084405\n",
      "GD iter. 77/149: loss=0.4122222528557929\n",
      "GD iter. 78/149: loss=0.41171751669252526\n",
      "GD iter. 79/149: loss=0.4112209945399944\n",
      "GD iter. 80/149: loss=0.4107324970521523\n",
      "GD iter. 81/149: loss=0.41025184056218494\n",
      "GD iter. 82/149: loss=0.4097788468676583\n",
      "GD iter. 83/149: loss=0.40931334302590805\n",
      "GD iter. 84/149: loss=0.40885516115905224\n",
      "GD iter. 85/149: loss=0.4084041382680556\n",
      "GD iter. 86/149: loss=0.40796011605531596\n",
      "GD iter. 87/149: loss=0.4075229407552819\n",
      "GD iter. 88/149: loss=0.4070924629726451\n",
      "GD iter. 89/149: loss=0.4066685375276836\n",
      "GD iter. 90/149: loss=0.406251023308363\n",
      "GD iter. 91/149: loss=0.40583978312882646\n",
      "GD iter. 92/149: loss=0.4054346835939319\n",
      "GD iter. 93/149: loss=0.40503559496951747\n",
      "GD iter. 94/149: loss=0.4046423910580946\n",
      "GD iter. 95/149: loss=0.40425494907969\n",
      "GD iter. 96/149: loss=0.40387314955757625\n",
      "GD iter. 97/149: loss=0.4034968762086422\n",
      "GD iter. 98/149: loss=0.403126015838177\n",
      "GD iter. 99/149: loss=0.4027604582388477\n",
      "GD iter. 100/149: loss=0.4024000960936704\n",
      "GD iter. 101/149: loss=0.402044824882782\n",
      "GD iter. 102/149: loss=0.4016945427938333\n",
      "GD iter. 103/149: loss=0.4013491506358327\n",
      "GD iter. 104/149: loss=0.4010085517562831\n",
      "GD iter. 105/149: loss=0.4006726519614583\n",
      "GD iter. 106/149: loss=0.4003413594396779\n",
      "GD iter. 107/149: loss=0.4000145846874461\n",
      "GD iter. 108/149: loss=0.3996922404383259\n",
      "GD iter. 109/149: loss=0.39937424159442947\n",
      "GD iter. 110/149: loss=0.3990605051604096\n",
      "GD iter. 111/149: loss=0.39875095017984424\n",
      "GD iter. 112/149: loss=0.39844549767391163\n",
      "GD iter. 113/149: loss=0.39814407058225826\n",
      "GD iter. 114/149: loss=0.39784659370596875\n",
      "GD iter. 115/149: loss=0.39755299365254676\n",
      "GD iter. 116/149: loss=0.3972631987828275\n",
      "GD iter. 117/149: loss=0.3969771391597387\n",
      "GD iter. 118/149: loss=0.3966947464988372\n",
      "GD iter. 119/149: loss=0.3964159541205481\n",
      "GD iter. 120/149: loss=0.39614069690403836\n",
      "GD iter. 121/149: loss=0.3958689112426594\n",
      "GD iter. 122/149: loss=0.39560053500089676\n",
      "GD iter. 123/149: loss=0.39533550747276786\n",
      "GD iter. 124/149: loss=0.39507376934161004\n",
      "GD iter. 125/149: loss=0.39481526264120687\n",
      "GD iter. 126/149: loss=0.39455993071819906\n",
      "GD iter. 127/149: loss=0.3943077181957328\n",
      "GD iter. 128/149: loss=0.3940585709382963\n",
      "GD iter. 129/149: loss=0.3938124360177018\n",
      "GD iter. 130/149: loss=0.3935692616801679\n",
      "GD iter. 131/149: loss=0.3933289973144625\n",
      "GD iter. 132/149: loss=0.39309159342106703\n",
      "GD iter. 133/149: loss=0.39285700158232245\n",
      "GD iter. 134/149: loss=0.3926251744335233\n",
      "GD iter. 135/149: loss=0.3923960656349236\n",
      "GD iter. 136/149: loss=0.39216962984462234\n",
      "GD iter. 137/149: loss=0.3919458226922955\n",
      "GD iter. 138/149: loss=0.39172460075374643\n",
      "GD iter. 139/149: loss=0.3915059215262425\n",
      "GD iter. 140/149: loss=0.39128974340461203\n",
      "GD iter. 141/149: loss=0.3910760256580747\n",
      "GD iter. 142/149: loss=0.3908647284077783\n",
      "GD iter. 143/149: loss=0.3906558126050167\n",
      "GD iter. 144/149: loss=0.39044924001010844\n",
      "GD iter. 145/149: loss=0.39024497317190876\n",
      "GD iter. 146/149: loss=0.39004297540793637\n",
      "GD iter. 147/149: loss=0.3898432107850924\n",
      "GD iter. 148/149: loss=0.3896456441009508\n",
      "GD iter. 149/149: loss=0.38945024086560265\n",
      "The Accuracy is: 0.8489\n",
      "The F1 score is: 0.4103\n",
      "The precision is: 0.3265\n",
      "The recall is: 0.5517\n",
      "GD iter. 0/149: loss=0.6878412904929624\n",
      "GD iter. 1/149: loss=0.5579346812627674\n",
      "GD iter. 2/149: loss=0.5379792434325696\n",
      "GD iter. 3/149: loss=0.5275619921016633\n",
      "GD iter. 4/149: loss=0.520214639110671\n",
      "GD iter. 5/149: loss=0.5143577814348937\n",
      "GD iter. 6/149: loss=0.5094105133619041\n",
      "GD iter. 7/149: loss=0.5050924716691934\n",
      "GD iter. 8/149: loss=0.5012411697175642\n",
      "GD iter. 9/149: loss=0.4977512407965729\n",
      "GD iter. 10/149: loss=0.4945496717336209\n",
      "GD iter. 11/149: loss=0.49158374267467714\n",
      "GD iter. 12/149: loss=0.4888142852451792\n",
      "GD iter. 13/149: loss=0.48621154300445085\n",
      "GD iter. 14/149: loss=0.4837524741889673\n",
      "GD iter. 15/149: loss=0.4814189264638152\n",
      "GD iter. 16/149: loss=0.4791963694868408\n",
      "GD iter. 17/149: loss=0.4770729975825925\n",
      "GD iter. 18/149: loss=0.47503908421737245\n",
      "GD iter. 19/149: loss=0.4730865110235561\n",
      "GD iter. 20/149: loss=0.47120841968089205\n",
      "GD iter. 21/149: loss=0.46939895142714205\n",
      "GD iter. 22/149: loss=0.46765304983329026\n",
      "GD iter. 23/149: loss=0.465966309775464\n",
      "GD iter. 24/149: loss=0.46433486050880185\n",
      "GD iter. 25/149: loss=0.4627552741801845\n",
      "GD iter. 26/149: loss=0.4612244935112452\n",
      "GD iter. 27/149: loss=0.4597397740711304\n",
      "GD iter. 28/149: loss=0.4582986377600675\n",
      "GD iter. 29/149: loss=0.4568988349880733\n",
      "GD iter. 30/149: loss=0.4555383136588998\n",
      "GD iter. 31/149: loss=0.45421519352688144\n",
      "GD iter. 32/149: loss=0.45292774483175086\n",
      "GD iter. 33/149: loss=0.4516743703673799\n",
      "GD iter. 34/149: loss=0.45045359032846294\n",
      "GD iter. 35/149: loss=0.44926402942126537\n",
      "GD iter. 36/149: loss=0.4481044058327832\n",
      "GD iter. 37/149: loss=0.4469735217356956\n",
      "GD iter. 38/149: loss=0.44587025507070405\n",
      "GD iter. 39/149: loss=0.4447935523978452\n",
      "GD iter. 40/149: loss=0.44374242264758157\n",
      "GD iter. 41/149: loss=0.44271593163343764\n",
      "GD iter. 42/149: loss=0.44171319721256336\n",
      "GD iter. 43/149: loss=0.4407333850003014\n",
      "GD iter. 44/149: loss=0.43977570456068094\n",
      "GD iter. 45/149: loss=0.438839406007599\n",
      "GD iter. 46/149: loss=0.43792377696189616\n",
      "GD iter. 47/149: loss=0.4370281398180957\n",
      "GD iter. 48/149: loss=0.43615184928161294\n",
      "GD iter. 49/149: loss=0.4352942901430693\n",
      "GD iter. 50/149: loss=0.4344548752611861\n",
      "GD iter. 51/149: loss=0.4336330437297761\n",
      "GD iter. 52/149: loss=0.4328282592077397\n",
      "GD iter. 53/149: loss=0.4320400083938304\n",
      "GD iter. 54/149: loss=0.4312677996303655\n",
      "GD iter. 55/149: loss=0.43051116162210723\n",
      "GD iter. 56/149: loss=0.4297696422582905\n",
      "GD iter. 57/149: loss=0.42904280752725754\n",
      "GD iter. 58/149: loss=0.4283302405144502\n",
      "GD iter. 59/149: loss=0.42763154047560537\n",
      "GD iter. 60/149: loss=0.4269463219779563\n",
      "GD iter. 61/149: loss=0.42627421410306354\n",
      "GD iter. 62/149: loss=0.42561485970561647\n",
      "GD iter. 63/149: loss=0.4249679147231736\n",
      "GD iter. 64/149: loss=0.42433304753234674\n",
      "GD iter. 65/149: loss=0.4237099383474184\n",
      "GD iter. 66/149: loss=0.4230982786577979\n",
      "GD iter. 67/149: loss=0.42249777070108907\n",
      "GD iter. 68/149: loss=0.4219081269688661\n",
      "GD iter. 69/149: loss=0.421329069742547\n",
      "GD iter. 70/149: loss=0.42076033065700025\n",
      "GD iter. 71/149: loss=0.4202016502897516\n",
      "GD iter. 72/149: loss=0.4196527777738563\n",
      "GD iter. 73/149: loss=0.41911347043268243\n",
      "GD iter. 74/149: loss=0.4185834934350068\n",
      "GD iter. 75/149: loss=0.4180626194689727\n",
      "GD iter. 76/149: loss=0.4175506284335832\n",
      "GD iter. 77/149: loss=0.41704730714651733\n",
      "GD iter. 78/149: loss=0.41655244906716543\n",
      "GD iter. 79/149: loss=0.416065854033864\n",
      "GD iter. 80/149: loss=0.4155873280144027\n",
      "GD iter. 81/149: loss=0.4151166828689454\n",
      "GD iter. 82/149: loss=0.4146537361245797\n",
      "GD iter. 83/149: loss=0.41419831076076785\n",
      "GD iter. 84/149: loss=0.4137502350050304\n",
      "GD iter. 85/149: loss=0.41330934213824555\n",
      "GD iter. 86/149: loss=0.4128754703089904\n",
      "GD iter. 87/149: loss=0.4124484623563957\n",
      "GD iter. 88/149: loss=0.41202816564102274\n",
      "GD iter. 89/149: loss=0.4116144318833069\n",
      "GD iter. 90/149: loss=0.4112071170091427\n",
      "GD iter. 91/149: loss=0.41080608100221894\n",
      "GD iter. 92/149: loss=0.41041118776273355\n",
      "GD iter. 93/149: loss=0.41002230497214787\n",
      "GD iter. 94/149: loss=0.4096393039636604\n",
      "GD iter. 95/149: loss=0.4092620595981001\n",
      "GD iter. 96/149: loss=0.40889045014496145\n",
      "GD iter. 97/149: loss=0.408524357168319\n",
      "GD iter. 98/149: loss=0.4081636654173757\n",
      "GD iter. 99/149: loss=0.40780826272141596\n",
      "GD iter. 100/149: loss=0.4074580398889466\n",
      "GD iter. 101/149: loss=0.4071128906108239\n",
      "GD iter. 102/149: loss=0.4067727113671737\n",
      "GD iter. 103/149: loss=0.4064374013379285\n",
      "GD iter. 104/149: loss=0.40610686231680715\n",
      "GD iter. 105/149: loss=0.40578099862858286\n",
      "GD iter. 106/149: loss=0.40545971704948475\n",
      "GD iter. 107/149: loss=0.40514292673059255\n",
      "GD iter. 108/149: loss=0.40483053912408995\n",
      "GD iter. 109/149: loss=0.40452246791224905\n",
      "GD iter. 110/149: loss=0.40421862893902505\n",
      "GD iter. 111/149: loss=0.4039189401441485\n",
      "GD iter. 112/149: loss=0.4036233214996061\n",
      "GD iter. 113/149: loss=0.4033316949484078\n",
      "GD iter. 114/149: loss=0.4030439843455434\n",
      "GD iter. 115/149: loss=0.4027601154010358\n",
      "GD iter. 116/149: loss=0.4024800156250037\n",
      "GD iter. 117/149: loss=0.40220361427465057\n",
      "GD iter. 118/149: loss=0.4019308423031009\n",
      "GD iter. 119/149: loss=0.4016616323100057\n",
      "GD iter. 120/149: loss=0.4013959184938515\n",
      "GD iter. 121/149: loss=0.4011336366058968\n",
      "GD iter. 122/149: loss=0.40087472390567785\n",
      "GD iter. 123/149: loss=0.40061911911801695\n",
      "GD iter. 124/149: loss=0.4003667623914766\n",
      "GD iter. 125/149: loss=0.4001175952582017\n",
      "GD iter. 126/149: loss=0.3998715605950966\n",
      "GD iter. 127/149: loss=0.3996286025862862\n",
      "GD iter. 128/149: loss=0.39938866668680895\n",
      "GD iter. 129/149: loss=0.3991516995874995\n",
      "GD iter. 130/149: loss=0.39891764918101136\n",
      "GD iter. 131/149: loss=0.3986864645289394\n",
      "GD iter. 132/149: loss=0.3984580958300013\n",
      "GD iter. 133/149: loss=0.3982324943892357\n",
      "GD iter. 134/149: loss=0.39800961258818335\n",
      "GD iter. 135/149: loss=0.3977894038560123\n",
      "GD iter. 136/149: loss=0.39757182264155344\n",
      "GD iter. 137/149: loss=0.397356824386214\n",
      "GD iter. 138/149: loss=0.39714436549773613\n",
      "GD iter. 139/149: loss=0.3969344033247708\n",
      "GD iter. 140/149: loss=0.3967268961322384\n",
      "GD iter. 141/149: loss=0.39652180307744656\n",
      "GD iter. 142/149: loss=0.39631908418693973\n",
      "GD iter. 143/149: loss=0.39611870033405433\n",
      "GD iter. 144/149: loss=0.3959206132171544\n",
      "GD iter. 145/149: loss=0.39572478533852457\n",
      "GD iter. 146/149: loss=0.39553117998389703\n",
      "GD iter. 147/149: loss=0.39533976120259134\n",
      "GD iter. 148/149: loss=0.3951504937882447\n",
      "GD iter. 149/149: loss=0.3949633432601141\n",
      "The Accuracy is: 0.8456\n",
      "The F1 score is: 0.3733\n",
      "The precision is: 0.2857\n",
      "The recall is: 0.5385\n",
      "GD iter. 0/149: loss=0.6981167613389104\n",
      "GD iter. 1/149: loss=0.5578238146918838\n",
      "GD iter. 2/149: loss=0.5394538838977802\n",
      "GD iter. 3/149: loss=0.5296476318343969\n",
      "GD iter. 4/149: loss=0.522560715469557\n",
      "GD iter. 5/149: loss=0.5168167402930622\n",
      "GD iter. 6/149: loss=0.5119169559479645\n",
      "GD iter. 7/149: loss=0.5076183318220809\n",
      "GD iter. 8/149: loss=0.5037762309597392\n",
      "GD iter. 9/149: loss=0.5002938459212702\n",
      "GD iter. 10/149: loss=0.4971020679892245\n",
      "GD iter. 11/149: loss=0.49414970634641087\n",
      "GD iter. 12/149: loss=0.4913979212535078\n",
      "GD iter. 13/149: loss=0.48881670552479506\n",
      "GD iter. 14/149: loss=0.48638251765124746\n",
      "GD iter. 15/149: loss=0.48407663014209873\n",
      "GD iter. 16/149: loss=0.48188394987489386\n",
      "GD iter. 17/149: loss=0.47979216150756804\n",
      "GD iter. 18/149: loss=0.4777910971971561\n",
      "GD iter. 19/149: loss=0.4758722674703156\n",
      "GD iter. 20/149: loss=0.47402850834437193\n",
      "GD iter. 21/149: loss=0.4722537132453475\n",
      "GD iter. 22/149: loss=0.4705426274056565\n",
      "GD iter. 23/149: loss=0.468890688733032\n",
      "GD iter. 24/149: loss=0.4672939035545128\n",
      "GD iter. 25/149: loss=0.4657487487585225\n",
      "GD iter. 26/149: loss=0.46425209408453577\n",
      "GD iter. 27/149: loss=0.46280113991329025\n",
      "GD iter. 28/149: loss=0.4613933670750812\n",
      "GD iter. 29/149: loss=0.46002649604632767\n",
      "GD iter. 30/149: loss=0.45869845353371086\n",
      "GD iter. 31/149: loss=0.4574073449128584\n",
      "GD iter. 32/149: loss=0.45615143133868663\n",
      "GD iter. 33/149: loss=0.4549291106085301\n",
      "GD iter. 34/149: loss=0.453738901059575\n",
      "GD iter. 35/149: loss=0.45257942793522493\n",
      "GD iter. 36/149: loss=0.4514494117727595\n",
      "GD iter. 37/149: loss=0.450347658455713\n",
      "GD iter. 38/149: loss=0.4492730506452909\n",
      "GD iter. 39/149: loss=0.4482245403606185\n",
      "GD iter. 40/149: loss=0.4472011425212953\n",
      "GD iter. 41/149: loss=0.4462019293002971\n",
      "GD iter. 42/149: loss=0.4452260251627767\n",
      "GD iter. 43/149: loss=0.44427260248831485\n",
      "GD iter. 44/149: loss=0.4433408776918659\n",
      "GD iter. 45/149: loss=0.4424301077729283\n",
      "GD iter. 46/149: loss=0.4415395872340745\n",
      "GD iter. 47/149: loss=0.4406686453194382\n",
      "GD iter. 48/149: loss=0.4398166435315089\n",
      "GD iter. 49/149: loss=0.4389829733909652\n",
      "GD iter. 50/149: loss=0.4381670544095603\n",
      "GD iter. 51/149: loss=0.4373683322504482\n",
      "GD iter. 52/149: loss=0.43658627705399766\n",
      "GD iter. 53/149: loss=0.43582038191019934\n",
      "GD iter. 54/149: loss=0.43507016146134175\n",
      "GD iter. 55/149: loss=0.43433515062080613\n",
      "GD iter. 56/149: loss=0.43361490339566616\n",
      "GD iter. 57/149: loss=0.4329089918023478\n",
      "GD iter. 58/149: loss=0.4322170048659346\n",
      "GD iter. 59/149: loss=0.43153854769485295\n",
      "GD iter. 60/149: loss=0.43087324062365145\n",
      "GD iter. 61/149: loss=0.4302207184174418\n",
      "GD iter. 62/149: loss=0.42958062953229437\n",
      "GD iter. 63/149: loss=0.42895263542653034\n",
      "GD iter. 64/149: loss=0.4283364099183946\n",
      "GD iter. 65/149: loss=0.42773163858608815\n",
      "GD iter. 66/149: loss=0.42713801820655717\n",
      "GD iter. 67/149: loss=0.4265552562298074\n",
      "GD iter. 68/149: loss=0.4259830702858469\n",
      "GD iter. 69/149: loss=0.4254211877216401\n",
      "GD iter. 70/149: loss=0.4248693451657187\n",
      "GD iter. 71/149: loss=0.4243272881183184\n",
      "GD iter. 72/149: loss=0.42379477056511206\n",
      "GD iter. 73/149: loss=0.4232715546127897\n",
      "GD iter. 74/149: loss=0.4227574101448949\n",
      "GD iter. 75/149: loss=0.4222521144964694\n",
      "GD iter. 76/149: loss=0.42175545214618654\n",
      "GD iter. 77/149: loss=0.421267214424769\n",
      "GD iter. 78/149: loss=0.42078719923858604\n",
      "GD iter. 79/149: loss=0.4203152108074232\n",
      "GD iter. 80/149: loss=0.419851059415496\n",
      "GD iter. 81/149: loss=0.4193945611748573\n",
      "GD iter. 82/149: loss=0.41894553780041477\n",
      "GD iter. 83/149: loss=0.4185038163958374\n",
      "GD iter. 84/149: loss=0.41806922924968554\n",
      "GD iter. 85/149: loss=0.4176416136411478\n",
      "GD iter. 86/149: loss=0.4172208116548188\n",
      "GD iter. 87/149: loss=0.4168066700039878\n",
      "GD iter. 88/149: loss=0.4163990398619518\n",
      "GD iter. 89/149: loss=0.4159977767008978\n",
      "GD iter. 90/149: loss=0.4156027401379337\n",
      "GD iter. 91/149: loss=0.41521379378787304\n",
      "GD iter. 92/149: loss=0.4148308051224105\n",
      "GD iter. 93/149: loss=0.41445364533534473\n",
      "GD iter. 94/149: loss=0.4140821892135292\n",
      "GD iter. 95/149: loss=0.41371631501325623\n",
      "GD iter. 96/149: loss=0.41335590434179087\n",
      "GD iter. 97/149: loss=0.41300084204379756\n",
      "GD iter. 98/149: loss=0.41265101609241067\n",
      "GD iter. 99/149: loss=0.412306317484722\n",
      "GD iter. 100/149: loss=0.4119666401414681\n",
      "GD iter. 101/149: loss=0.41163188081071245\n",
      "GD iter. 102/149: loss=0.4113019389753352\n",
      "GD iter. 103/149: loss=0.41097671676414593\n",
      "GD iter. 104/149: loss=0.41065611886645337\n",
      "GD iter. 105/149: loss=0.41034005244992916\n",
      "GD iter. 106/149: loss=0.41002842708161596\n",
      "GD iter. 107/149: loss=0.4097211546519375\n",
      "GD iter. 108/149: loss=0.40941814930157233\n",
      "GD iter. 109/149: loss=0.40911932735106854\n",
      "GD iter. 110/149: loss=0.4088246072330735\n",
      "GD iter. 111/149: loss=0.4085339094270668\n",
      "GD iter. 112/149: loss=0.40824715639648684\n",
      "GD iter. 113/149: loss=0.4079642725281487\n",
      "GD iter. 114/149: loss=0.4076851840738541\n",
      "GD iter. 115/149: loss=0.4074098190941017\n",
      "GD iter. 116/149: loss=0.4071381074038089\n",
      "GD iter. 117/149: loss=0.4068699805199617\n",
      "GD iter. 118/149: loss=0.40660537161111265\n",
      "GD iter. 119/149: loss=0.40634421544864974\n",
      "GD iter. 120/149: loss=0.40608644835976687\n",
      "GD iter. 121/149: loss=0.40583200818206316\n",
      "GD iter. 122/149: loss=0.40558083421970914\n",
      "GD iter. 123/149: loss=0.4053328672011148\n",
      "GD iter. 124/149: loss=0.40508804923804065\n",
      "GD iter. 125/149: loss=0.4048463237860956\n",
      "GD iter. 126/149: loss=0.40460763560656554\n",
      "GD iter. 127/149: loss=0.4043719307295232\n",
      "GD iter. 128/149: loss=0.4041391564181669\n",
      "GD iter. 129/149: loss=0.403909261134344\n",
      "GD iter. 130/149: loss=0.40368219450521037\n",
      "GD iter. 131/149: loss=0.4034579072909864\n",
      "GD iter. 132/149: loss=0.4032363513537641\n",
      "GD iter. 133/149: loss=0.4030174796273298\n",
      "GD iter. 134/149: loss=0.40280124608796075\n",
      "GD iter. 135/149: loss=0.4025876057261623\n",
      "GD iter. 136/149: loss=0.40237651451930945\n",
      "GD iter. 137/149: loss=0.40216792940515955\n",
      "GD iter. 138/149: loss=0.40196180825620453\n",
      "GD iter. 139/149: loss=0.40175810985483246\n",
      "GD iter. 140/149: loss=0.40155679386926835\n",
      "GD iter. 141/149: loss=0.4013578208302665\n",
      "GD iter. 142/149: loss=0.40116115210852765\n",
      "GD iter. 143/149: loss=0.40096674989281444\n",
      "GD iter. 144/149: loss=0.4007745771687415\n",
      "GD iter. 145/149: loss=0.40058459769821464\n",
      "GD iter. 146/149: loss=0.400396775999498\n",
      "GD iter. 147/149: loss=0.4002110773278861\n",
      "GD iter. 148/149: loss=0.40002746765696\n",
      "GD iter. 149/149: loss=0.39984591366040784\n",
      "The Accuracy is: 0.8456\n",
      "The F1 score is: 0.4659\n",
      "The precision is: 0.3694\n",
      "The recall is: 0.6308\n",
      "GD iter. 0/149: loss=0.7162742041849673\n",
      "GD iter. 1/149: loss=0.5584163999550853\n",
      "GD iter. 2/149: loss=0.5402475933213133\n",
      "GD iter. 3/149: loss=0.5304899315421658\n",
      "GD iter. 4/149: loss=0.5234082539562883\n",
      "GD iter. 5/149: loss=0.517659695074955\n",
      "GD iter. 6/149: loss=0.512754353893341\n",
      "GD iter. 7/149: loss=0.508450998280584\n",
      "GD iter. 8/149: loss=0.5046049852494313\n",
      "GD iter. 9/149: loss=0.5011192090507763\n",
      "GD iter. 10/149: loss=0.4979243360450803\n",
      "GD iter. 11/149: loss=0.4949690595515188\n",
      "GD iter. 12/149: loss=0.49221449513177495\n",
      "GD iter. 13/149: loss=0.4896306234053284\n",
      "GD iter. 14/149: loss=0.48719389840365646\n",
      "GD iter. 15/149: loss=0.4848855835540037\n",
      "GD iter. 16/149: loss=0.48269056772615715\n",
      "GD iter. 17/149: loss=0.4805965087411079\n",
      "GD iter. 18/149: loss=0.47859320518368403\n",
      "GD iter. 19/149: loss=0.47667212997364783\n",
      "GD iter. 20/149: loss=0.4748260800738761\n",
      "GD iter. 21/149: loss=0.4730489105640401\n",
      "GD iter. 22/149: loss=0.4713353306677709\n",
      "GD iter. 23/149: loss=0.4696807457441036\n",
      "GD iter. 24/149: loss=0.46808113371686744\n",
      "GD iter. 25/149: loss=0.46653294755080305\n",
      "GD iter. 26/149: loss=0.4650330376078335\n",
      "GD iter. 27/149: loss=0.4635785893104939\n",
      "GD iter. 28/149: loss=0.4621670726914842\n",
      "GD iter. 29/149: loss=0.4607962012483028\n",
      "GD iter. 30/149: loss=0.45946389813966027\n",
      "GD iter. 31/149: loss=0.45816826821838275\n",
      "GD iter. 32/149: loss=0.456907574737813\n",
      "GD iter. 33/149: loss=0.4556802198265108\n",
      "GD iter. 34/149: loss=0.45448472802166195\n",
      "GD iter. 35/149: loss=0.4533197323011017\n",
      "GD iter. 36/149: loss=0.4521839621689227\n",
      "GD iter. 37/149: loss=0.4510762334387966\n",
      "GD iter. 38/149: loss=0.44999543942868536\n",
      "GD iter. 39/149: loss=0.448940543335207\n",
      "GD iter. 40/149: loss=0.44791057159903763\n",
      "GD iter. 41/149: loss=0.44690460810699006\n",
      "GD iter. 42/149: loss=0.4459217891037739\n",
      "GD iter. 43/149: loss=0.4449612987084429\n",
      "GD iter. 44/149: loss=0.4440223649482988\n",
      "GD iter. 45/149: loss=0.44310425623744276\n",
      "GD iter. 46/149: loss=0.44220627823893965\n",
      "GD iter. 47/149: loss=0.441327771059207\n",
      "GD iter. 48/149: loss=0.44046810673118914\n",
      "GD iter. 49/149: loss=0.4396266869494517\n",
      "GD iter. 50/149: loss=0.4388029410257919\n",
      "GD iter. 51/149: loss=0.43799632403851696\n",
      "GD iter. 52/149: loss=0.43720631515235575\n",
      "GD iter. 53/149: loss=0.4364324160891766\n",
      "GD iter. 54/149: loss=0.43567414973238866\n",
      "GD iter. 55/149: loss=0.43493105885019034\n",
      "GD iter. 56/149: loss=0.4342027049247774\n",
      "GD iter. 57/149: loss=0.4334886670762744\n",
      "GD iter. 58/149: loss=0.4327885410715736\n",
      "GD iter. 59/149: loss=0.432101938409473\n",
      "GD iter. 60/149: loss=0.4314284854745565\n",
      "GD iter. 61/149: loss=0.4307678227531485\n",
      "GD iter. 62/149: loss=0.4301196041054611\n",
      "GD iter. 63/149: loss=0.42948349608872005\n",
      "GD iter. 64/149: loss=0.42885917732664475\n",
      "GD iter. 65/149: loss=0.4282463379211635\n",
      "GD iter. 66/149: loss=0.4276446789026954\n",
      "GD iter. 67/149: loss=0.4270539117157139\n",
      "GD iter. 68/149: loss=0.42647375773665397\n",
      "GD iter. 69/149: loss=0.425903947821524\n",
      "GD iter. 70/149: loss=0.42534422188084525\n",
      "GD iter. 71/149: loss=0.4247943284797802\n",
      "GD iter. 72/149: loss=0.42425402446151544\n",
      "GD iter. 73/149: loss=0.42372307459214803\n",
      "GD iter. 74/149: loss=0.42320125122548935\n",
      "GD iter. 75/149: loss=0.4226883339863425\n",
      "GD iter. 76/149: loss=0.4221841094709434\n",
      "GD iter. 77/149: loss=0.42168837096336603\n",
      "GD iter. 78/149: loss=0.4212009181668015\n",
      "GD iter. 79/149: loss=0.42072155694870866\n",
      "GD iter. 80/149: loss=0.42025009909892125\n",
      "GD iter. 81/149: loss=0.41978636209987114\n",
      "GD iter. 82/149: loss=0.41933016890815206\n",
      "GD iter. 83/149: loss=0.41888134774671537\n",
      "GD iter. 84/149: loss=0.4184397319070389\n",
      "GD iter. 85/149: loss=0.41800515956066503\n",
      "GD iter. 86/149: loss=0.41757747357954617\n",
      "GD iter. 87/149: loss=0.4171565213646807\n",
      "GD iter. 88/149: loss=0.4167421546825581\n",
      "GD iter. 89/149: loss=0.4163342295089666\n",
      "GD iter. 90/149: loss=0.4159326058797492\n",
      "GD iter. 91/149: loss=0.4155371477481243\n",
      "GD iter. 92/149: loss=0.4151477228482078\n",
      "GD iter. 93/149: loss=0.4147642025644054\n",
      "GD iter. 94/149: loss=0.4143864618063611\n",
      "GD iter. 95/149: loss=0.4140143788891694\n",
      "GD iter. 96/149: loss=0.4136478354185792\n",
      "GD iter. 97/149: loss=0.41328671618093227\n",
      "GD iter. 98/149: loss=0.4129309090375978\n",
      "GD iter. 99/149: loss=0.4125803048236776\n",
      "GD iter. 100/149: loss=0.4122347972507703\n",
      "GD iter. 101/149: loss=0.411894282813599\n",
      "GD iter. 102/149: loss=0.4115586607003117\n",
      "GD iter. 103/149: loss=0.41122783270628144\n",
      "GD iter. 104/149: loss=0.41090170315124236\n",
      "GD iter. 105/149: loss=0.41058017879960196\n",
      "GD iter. 106/149: loss=0.41026316878378605\n",
      "GD iter. 107/149: loss=0.409950584530475\n",
      "GD iter. 108/149: loss=0.40964233968960256\n",
      "GD iter. 109/149: loss=0.4093383500659918\n",
      "GD iter. 110/149: loss=0.40903853355351105\n",
      "GD iter. 111/149: loss=0.4087428100716393\n",
      "GD iter. 112/149: loss=0.40845110150433633\n",
      "GD iter. 113/149: loss=0.40816333164111673\n",
      "GD iter. 114/149: loss=0.40787942612023437\n",
      "GD iter. 115/149: loss=0.4075993123738879\n",
      "GD iter. 116/149: loss=0.4073229195753609\n",
      "GD iter. 117/149: loss=0.4070501785880174\n",
      "GD iter. 118/149: loss=0.40678102191607474\n",
      "GD iter. 119/149: loss=0.40651538365708145\n",
      "GD iter. 120/149: loss=0.4062531994560306\n",
      "GD iter. 121/149: loss=0.4059944064610421\n",
      "GD iter. 122/149: loss=0.4057389432805508\n",
      "GD iter. 123/149: loss=0.4054867499419417\n",
      "GD iter. 124/149: loss=0.40523776785157306\n",
      "GD iter. 125/149: loss=0.4049919397561347\n",
      "GD iter. 126/149: loss=0.4047492097052881\n",
      "GD iter. 127/149: loss=0.4045095230155398\n",
      "GD iter. 128/149: loss=0.4042728262352993\n",
      "GD iter. 129/149: loss=0.4040390671110776\n",
      "GD iter. 130/149: loss=0.4038081945547819\n",
      "GD iter. 131/149: loss=0.40358015861206603\n",
      "GD iter. 132/149: loss=0.40335491043169636\n",
      "GD iter. 133/149: loss=0.4031324022358958\n",
      "GD iter. 134/149: loss=0.40291258729162904\n",
      "GD iter. 135/149: loss=0.4026954198827955\n",
      "GD iter. 136/149: loss=0.4024808552832953\n",
      "GD iter. 137/149: loss=0.40226884973093807\n",
      "GD iter. 138/149: loss=0.40205936040216256\n",
      "GD iter. 139/149: loss=0.40185234538753906\n",
      "GD iter. 140/149: loss=0.401647763668026\n",
      "GD iter. 141/149: loss=0.40144557509195405\n",
      "GD iter. 142/149: loss=0.4012457403527125\n",
      "GD iter. 143/149: loss=0.40104822096711135\n",
      "GD iter. 144/149: loss=0.4008529792543984\n",
      "GD iter. 145/149: loss=0.40065997831590594\n",
      "GD iter. 146/149: loss=0.40046918201530607\n",
      "GD iter. 147/149: loss=0.4002805549594542\n",
      "GD iter. 148/149: loss=0.40009406247979995\n",
      "GD iter. 149/149: loss=0.39990967061434596\n",
      "The Accuracy is: 0.8604\n",
      "The F1 score is: 0.4444\n",
      "The precision is: 0.3579\n",
      "The recall is: 0.5862\n",
      "GD iter. 0/149: loss=0.7112534074885991\n",
      "GD iter. 1/149: loss=0.5528886461082506\n",
      "GD iter. 2/149: loss=0.534914507471748\n",
      "GD iter. 3/149: loss=0.5251722545143114\n",
      "GD iter. 4/149: loss=0.518080434377904\n",
      "GD iter. 5/149: loss=0.512323837227024\n",
      "GD iter. 6/149: loss=0.5074146631250659\n",
      "GD iter. 7/149: loss=0.503109363303471\n",
      "GD iter. 8/149: loss=0.4992610757835871\n",
      "GD iter. 9/149: loss=0.49577131859211754\n",
      "GD iter. 10/149: loss=0.4925701252417295\n",
      "GD iter. 11/149: loss=0.4896060657046131\n",
      "GD iter. 12/149: loss=0.48684043907437474\n",
      "GD iter. 13/149: loss=0.484243571290843\n",
      "GD iter. 14/149: loss=0.481792328213491\n",
      "GD iter. 15/149: loss=0.4794683928442845\n",
      "GD iter. 16/149: loss=0.47725704754903164\n",
      "GD iter. 17/149: loss=0.4751463000614561\n",
      "GD iter. 18/149: loss=0.4731262481143699\n",
      "GD iter. 19/149: loss=0.47118861211776586\n",
      "GD iter. 20/149: loss=0.469326387607012\n",
      "GD iter. 21/149: loss=0.46753358397329575\n",
      "GD iter. 22/149: loss=0.4658050259772698\n",
      "GD iter. 23/149: loss=0.46413620138716083\n",
      "GD iter. 24/149: loss=0.4625231428202446\n",
      "GD iter. 25/149: loss=0.46096233518081003\n",
      "GD iter. 26/149: loss=0.45945064242737976\n",
      "GD iter. 27/149: loss=0.4579852490677419\n",
      "GD iter. 28/149: loss=0.4565636129759653\n",
      "GD iter. 29/149: loss=0.4551834269903504\n",
      "GD iter. 30/149: loss=0.4538425873814673\n",
      "GD iter. 31/149: loss=0.45253916774199343\n",
      "GD iter. 32/149: loss=0.45127139719202547\n",
      "GD iter. 33/149: loss=0.4500376420481049\n",
      "GD iter. 34/149: loss=0.4488363902950477\n",
      "GD iter. 35/149: loss=0.44766623834373254\n",
      "GD iter. 36/149: loss=0.4465258796675228\n",
      "GD iter. 37/149: loss=0.4454140949938371\n",
      "GD iter. 38/149: loss=0.44432974379202\n",
      "GD iter. 39/149: loss=0.4432717568488408\n",
      "GD iter. 40/149: loss=0.4422391297621977\n",
      "GD iter. 41/149: loss=0.44123091721449265\n",
      "GD iter. 42/149: loss=0.44024622791164936\n",
      "GD iter. 43/149: loss=0.4392842200933149\n",
      "GD iter. 44/149: loss=0.4383440975355138\n",
      "GD iter. 45/149: loss=0.43742510597975576\n",
      "GD iter. 46/149: loss=0.4365265299329738\n",
      "GD iter. 47/149: loss=0.43564768979117\n",
      "GD iter. 48/149: loss=0.4347879392466602\n",
      "GD iter. 49/149: loss=0.43394666294462314\n",
      "GD iter. 50/149: loss=0.4331232743595135\n",
      "GD iter. 51/149: loss=0.4323172138659606\n",
      "GD iter. 52/149: loss=0.43152794698220137\n",
      "GD iter. 53/149: loss=0.4307549627669931\n",
      "GD iter. 54/149: loss=0.42999777235341186\n",
      "GD iter. 55/149: loss=0.42925590760504545\n",
      "GD iter. 56/149: loss=0.42852891988188524\n",
      "GD iter. 57/149: loss=0.42781637890476587\n",
      "GD iter. 58/149: loss=0.42711787170853577\n",
      "GD iter. 59/149: loss=0.42643300167528797\n",
      "GD iter. 60/149: loss=0.4257613876399842\n",
      "GD iter. 61/149: loss=0.42510266306166516\n",
      "GD iter. 62/149: loss=0.4244564752542032\n",
      "GD iter. 63/149: loss=0.42382248467120953\n",
      "GD iter. 64/149: loss=0.4232003642402866\n",
      "GD iter. 65/149: loss=0.42258979874232516\n",
      "GD iter. 66/149: loss=0.4219904842319921\n",
      "GD iter. 67/149: loss=0.4214021274959456\n",
      "GD iter. 68/149: loss=0.4208244455456654\n",
      "GD iter. 69/149: loss=0.4202571651420913\n",
      "GD iter. 70/149: loss=0.41970002234953496\n",
      "GD iter. 71/149: loss=0.4191527621165748\n",
      "GD iter. 72/149: loss=0.41861513788185656\n",
      "GD iter. 73/149: loss=0.41808691120291636\n",
      "GD iter. 74/149: loss=0.41756785140631425\n",
      "GD iter. 75/149: loss=0.41705773525751716\n",
      "GD iter. 76/149: loss=0.41655634664911495\n",
      "GD iter. 77/149: loss=0.41606347630606716\n",
      "GD iter. 78/149: loss=0.41557892150679787\n",
      "GD iter. 79/149: loss=0.41510248581905085\n",
      "GD iter. 80/149: loss=0.4146339788495118\n",
      "GD iter. 81/149: loss=0.41417321600627927\n",
      "GD iter. 82/149: loss=0.4137200182733495\n",
      "GD iter. 83/149: loss=0.413274211996339\n",
      "GD iter. 84/149: loss=0.41283562867873197\n",
      "GD iter. 85/149: loss=0.41240410478799855\n",
      "GD iter. 86/149: loss=0.41197948157097325\n",
      "GD iter. 87/149: loss=0.4115616048779354\n",
      "GD iter. 88/149: loss=0.41115032499487036\n",
      "GD iter. 89/149: loss=0.4107454964834308\n",
      "GD iter. 90/149: loss=0.4103469780281515\n",
      "GD iter. 91/149: loss=0.409954632290504\n",
      "GD iter. 92/149: loss=0.40956832576940244\n",
      "GD iter. 93/149: loss=0.40918792866780684\n",
      "GD iter. 94/149: loss=0.4088133147650859\n",
      "GD iter. 95/149: loss=0.4084443612948291\n",
      "GD iter. 96/149: loss=0.40808094882781865\n",
      "GD iter. 97/149: loss=0.40772296115988743\n",
      "GD iter. 98/149: loss=0.40737028520441226\n",
      "GD iter. 99/149: loss=0.40702281088920167\n",
      "GD iter. 100/149: loss=0.4066804310575581\n",
      "GD iter. 101/149: loss=0.4063430413733054\n",
      "GD iter. 102/149: loss=0.4060105402295852\n",
      "GD iter. 103/149: loss=0.4056828286612391\n",
      "GD iter. 104/149: loss=0.4053598102606041\n",
      "GD iter. 105/149: loss=0.40504139109655773\n",
      "GD iter. 106/149: loss=0.4047274796366617\n",
      "GD iter. 107/149: loss=0.40441798667225787\n",
      "GD iter. 108/149: loss=0.40411282524638215\n",
      "GD iter. 109/149: loss=0.40381191058436794\n",
      "GD iter. 110/149: loss=0.403515160027017\n",
      "GD iter. 111/149: loss=0.4032224929662242\n",
      "GD iter. 112/149: loss=0.4029338307829477\n",
      "GD iter. 113/149: loss=0.40264909678742256\n",
      "GD iter. 114/149: loss=0.40236821616151924\n",
      "GD iter. 115/149: loss=0.4020911159031574\n",
      "GD iter. 116/149: loss=0.401817724772685\n",
      "GD iter. 117/149: loss=0.4015479732411433\n",
      "GD iter. 118/149: loss=0.4012817934403355\n",
      "GD iter. 119/149: loss=0.4010191191146281\n",
      "GD iter. 120/149: loss=0.4007598855744109\n",
      "GD iter. 121/149: loss=0.40050402965115034\n",
      "GD iter. 122/149: loss=0.4002514896539705\n",
      "GD iter. 123/149: loss=0.40000220532770253\n",
      "GD iter. 124/149: loss=0.39975611781234144\n",
      "GD iter. 125/149: loss=0.3995131696038573\n",
      "GD iter. 126/149: loss=0.39927330451630755\n",
      "GD iter. 127/149: loss=0.3990364676451982\n",
      "GD iter. 128/149: loss=0.3988026053320475\n",
      "GD iter. 129/149: loss=0.3985716651301057\n",
      "GD iter. 130/149: loss=0.3983435957711869\n",
      "GD iter. 131/149: loss=0.39811834713356997\n",
      "GD iter. 132/149: loss=0.39789587021093004\n",
      "GD iter. 133/149: loss=0.39767611708226197\n",
      "GD iter. 134/149: loss=0.3974590408827568\n",
      "GD iter. 135/149: loss=0.39724459577560006\n",
      "GD iter. 136/149: loss=0.39703273692465324\n",
      "GD iter. 137/149: loss=0.39682342046799174\n",
      "GD iter. 138/149: loss=0.3966166034922637\n",
      "GD iter. 139/149: loss=0.39641224400784275\n",
      "GD iter. 140/149: loss=0.39621030092474563\n",
      "GD iter. 141/149: loss=0.3960107340292869\n",
      "GD iter. 142/149: loss=0.39581350396144627\n",
      "GD iter. 143/149: loss=0.3956185721929213\n",
      "GD iter. 144/149: loss=0.3954259010058432\n",
      "GD iter. 145/149: loss=0.3952354534721324\n",
      "GD iter. 146/149: loss=0.39504719343347044\n",
      "GD iter. 147/149: loss=0.39486108548186893\n",
      "GD iter. 148/149: loss=0.39467709494081327\n",
      "GD iter. 149/149: loss=0.39449518784696314\n",
      "The Accuracy is: 0.8391\n",
      "The F1 score is: 0.3000\n",
      "The precision is: 0.2143\n",
      "The recall is: 0.5000\n",
      "GD iter. 0/149: loss=0.7264401870458332\n",
      "GD iter. 1/149: loss=0.5572456879128505\n",
      "GD iter. 2/149: loss=0.538330566350257\n",
      "GD iter. 3/149: loss=0.5280779107552366\n",
      "GD iter. 4/149: loss=0.5207405222982125\n",
      "GD iter. 5/149: loss=0.5148942238538308\n",
      "GD iter. 6/149: loss=0.5099866780395402\n",
      "GD iter. 7/149: loss=0.5057353052331467\n",
      "GD iter. 8/149: loss=0.5019695331483853\n",
      "GD iter. 9/149: loss=0.498576477286538\n",
      "GD iter. 10/149: loss=0.4954774190752904\n",
      "GD iter. 11/149: loss=0.4926156174539496\n",
      "GD iter. 12/149: loss=0.48994917722306564\n",
      "GD iter. 13/149: loss=0.4874465510152343\n",
      "GD iter. 14/149: loss=0.4850835681114376\n",
      "GD iter. 15/149: loss=0.48284141121830604\n",
      "GD iter. 16/149: loss=0.48070520814242373\n",
      "GD iter. 17/149: loss=0.4786630343283496\n",
      "GD iter. 18/149: loss=0.47670519597709526\n",
      "GD iter. 19/149: loss=0.47482370811209124\n",
      "GD iter. 20/149: loss=0.4730119100752334\n",
      "GD iter. 21/149: loss=0.4712641791452172\n",
      "GD iter. 22/149: loss=0.46957571502033085\n",
      "GD iter. 23/149: loss=0.4679423760214227\n",
      "GD iter. 24/149: loss=0.46636055341464805\n",
      "GD iter. 25/149: loss=0.46482707409069834\n",
      "GD iter. 26/149: loss=0.4633391245237325\n",
      "GD iter. 27/149: loss=0.46189419083406547\n",
      "GD iter. 28/149: loss=0.46049001113665355\n",
      "GD iter. 29/149: loss=0.4591245373363691\n",
      "GD iter. 30/149: loss=0.45779590424276\n",
      "GD iter. 31/149: loss=0.4565024043984974\n",
      "GD iter. 32/149: loss=0.4552424674007857\n",
      "GD iter. 33/149: loss=0.4540146427813271\n",
      "GD iter. 34/149: loss=0.45281758572482594\n",
      "GD iter. 35/149: loss=0.45165004506758744\n",
      "GD iter. 36/149: loss=0.45051085314030026\n",
      "GD iter. 37/149: loss=0.44939891711260255\n",
      "GD iter. 38/149: loss=0.44831321156880577\n",
      "GD iter. 39/149: loss=0.44725277209957254\n",
      "GD iter. 40/149: loss=0.4462166897373745\n",
      "GD iter. 41/149: loss=0.44520410609715866\n",
      "GD iter. 42/149: loss=0.44421420911002957\n",
      "GD iter. 43/149: loss=0.44324622925858287\n",
      "GD iter. 44/149: loss=0.44229943623904794\n",
      "GD iter. 45/149: loss=0.44137313598859085\n",
      "GD iter. 46/149: loss=0.44046666802670187\n",
      "GD iter. 47/149: loss=0.43957940306811927\n",
      "GD iter. 48/149: loss=0.4387107408716559\n",
      "GD iter. 49/149: loss=0.43786010829492666\n",
      "GD iter. 50/149: loss=0.4370269575295809\n",
      "GD iter. 51/149: loss=0.4362107644954456\n",
      "GD iter. 52/149: loss=0.43541102737511905\n",
      "GD iter. 53/149: loss=0.43462726527316703\n",
      "GD iter. 54/149: loss=0.43385901698625046\n",
      "GD iter. 55/149: loss=0.4331058398723444\n",
      "GD iter. 56/149: loss=0.43236730880875107\n",
      "GD iter. 57/149: loss=0.4316430152299126\n",
      "GD iter. 58/149: loss=0.4309325662371464\n",
      "GD iter. 59/149: loss=0.43023558377336896\n",
      "GD iter. 60/149: loss=0.4295517038566971\n",
      "GD iter. 61/149: loss=0.4288805758675087\n",
      "GD iter. 62/149: loss=0.42822186188415984\n",
      "GD iter. 63/149: loss=0.4275752360630768\n",
      "GD iter. 64/149: loss=0.4269403840594051\n",
      "GD iter. 65/149: loss=0.4263170024847946\n",
      "GD iter. 66/149: loss=0.42570479839925257\n",
      "GD iter. 67/149: loss=0.4251034888343088\n",
      "GD iter. 68/149: loss=0.4245128003450055\n",
      "GD iter. 69/149: loss=0.4239324685884669\n",
      "GD iter. 70/149: loss=0.4233622379270203\n",
      "GD iter. 71/149: loss=0.422801861054024\n",
      "GD iter. 72/149: loss=0.4222510986407348\n",
      "GD iter. 73/149: loss=0.42170971900268994\n",
      "GD iter. 74/149: loss=0.4211774977842209\n",
      "GD iter. 75/149: loss=0.420654217659833\n",
      "GD iter. 76/149: loss=0.4201396680512947\n",
      "GD iter. 77/149: loss=0.4196336448593785\n",
      "GD iter. 78/149: loss=0.4191359502092827\n",
      "GD iter. 79/149: loss=0.418646392208843\n",
      "GD iter. 80/149: loss=0.41816478471871565\n",
      "GD iter. 81/149: loss=0.417690947133776\n",
      "GD iter. 82/149: loss=0.41722470417503893\n",
      "GD iter. 83/149: loss=0.4167658856914577\n",
      "GD iter. 84/149: loss=0.41631432647100947\n",
      "GD iter. 85/149: loss=0.41586986606051635\n",
      "GD iter. 86/149: loss=0.4154323485936952\n",
      "GD iter. 87/149: loss=0.41500162262696216\n",
      "GD iter. 88/149: loss=0.41457754098255595\n",
      "GD iter. 89/149: loss=0.4141599605985684\n",
      "GD iter. 90/149: loss=0.4137487423855077\n",
      "GD iter. 91/149: loss=0.4133437510890354\n",
      "GD iter. 92/149: loss=0.41294485515855195\n",
      "GD iter. 93/149: loss=0.4125519266213187\n",
      "GD iter. 94/149: loss=0.41216484096183076\n",
      "GD iter. 95/149: loss=0.4117834770061707\n",
      "GD iter. 96/149: loss=0.41140771681108856\n",
      "GD iter. 97/149: loss=0.4110374455575751\n",
      "GD iter. 98/149: loss=0.4106725514487012\n",
      "GD iter. 99/149: loss=0.41031292561152016\n",
      "GD iter. 100/149: loss=0.40995846200283187\n",
      "GD iter. 101/149: loss=0.40960905731862673\n",
      "GD iter. 102/149: loss=0.4092646109070345\n",
      "GD iter. 103/149: loss=0.4089250246846155\n",
      "GD iter. 104/149: loss=0.4085902030558366\n",
      "GD iter. 105/149: loss=0.40826005283559075\n",
      "GD iter. 106/149: loss=0.40793448317461584\n",
      "GD iter. 107/149: loss=0.4076134054876898\n",
      "GD iter. 108/149: loss=0.4072967333844724\n",
      "GD iter. 109/149: loss=0.40698438260288095\n",
      "GD iter. 110/149: loss=0.40667627094488773\n",
      "GD iter. 111/149: loss=0.4063723182146347\n",
      "GD iter. 112/149: loss=0.4060724461587666\n",
      "GD iter. 113/149: loss=0.40577657840888715\n",
      "GD iter. 114/149: loss=0.40548464042605\n",
      "GD iter. 115/149: loss=0.4051965594471984\n",
      "GD iter. 116/149: loss=0.4049122644334738\n",
      "GD iter. 117/149: loss=0.4046316860203153\n",
      "GD iter. 118/149: loss=0.4043547564692781\n",
      "GD iter. 119/149: loss=0.4040814096214998\n",
      "GD iter. 120/149: loss=0.40381158085275\n",
      "GD iter. 121/149: loss=0.4035452070299991\n",
      "GD iter. 122/149: loss=0.4032822264694453\n",
      "GD iter. 123/149: loss=0.40302257889594434\n",
      "GD iter. 124/149: loss=0.4027662054037853\n",
      "GD iter. 125/149: loss=0.4025130484187617\n",
      "GD iter. 126/149: loss=0.402263051661486\n",
      "GD iter. 127/149: loss=0.4020161601119028\n",
      "GD iter. 128/149: loss=0.4017723199749519\n",
      "GD iter. 129/149: loss=0.4015314786473404\n",
      "GD iter. 130/149: loss=0.40129358468537996\n",
      "GD iter. 131/149: loss=0.4010585877738515\n",
      "GD iter. 132/149: loss=0.4008264386958576\n",
      "GD iter. 133/149: loss=0.4005970893036267\n",
      "GD iter. 134/149: loss=0.4003704924902347\n",
      "GD iter. 135/149: loss=0.4001466021622097\n",
      "GD iter. 136/149: loss=0.3999253732129879\n",
      "GD iter. 137/149: loss=0.39970676149719125\n",
      "GD iter. 138/149: loss=0.3994907238056951\n",
      "GD iter. 139/149: loss=0.39927721784146025\n",
      "GD iter. 140/149: loss=0.39906620219609984\n",
      "GD iter. 141/149: loss=0.39885763632715765\n",
      "GD iter. 142/149: loss=0.39865148053607047\n",
      "GD iter. 143/149: loss=0.3984476959467921\n",
      "GD iter. 144/149: loss=0.3982462444850564\n",
      "GD iter. 145/149: loss=0.39804708885825524\n",
      "GD iter. 146/149: loss=0.39785019253591297\n",
      "GD iter. 147/149: loss=0.39765551973073493\n",
      "GD iter. 148/149: loss=0.3974630353802112\n",
      "GD iter. 149/149: loss=0.39727270512875734\n",
      "The Accuracy is: 0.8489\n",
      "The F1 score is: 0.4026\n",
      "The precision is: 0.3523\n",
      "The recall is: 0.4697\n",
      "GD iter. 0/149: loss=0.7063317466368692\n",
      "GD iter. 1/149: loss=0.5559360013540239\n",
      "GD iter. 2/149: loss=0.5370487083294785\n",
      "GD iter. 3/149: loss=0.5268549713823182\n",
      "GD iter. 4/149: loss=0.5195126907464482\n",
      "GD iter. 5/149: loss=0.513605552687091\n",
      "GD iter. 6/149: loss=0.5086009520610504\n",
      "GD iter. 7/149: loss=0.5042328887839624\n",
      "GD iter. 8/149: loss=0.5003421416237681\n",
      "GD iter. 9/149: loss=0.49682291890362607\n",
      "GD iter. 10/149: loss=0.4936006931244231\n",
      "GD iter. 11/149: loss=0.49062107856106335\n",
      "GD iter. 12/149: loss=0.48784341161994443\n",
      "GD iter. 13/149: loss=0.4852366978539444\n",
      "GD iter. 14/149: loss=0.482776911994448\n",
      "GD iter. 15/149: loss=0.48044513930080873\n",
      "GD iter. 16/149: loss=0.47822626774569543\n",
      "GD iter. 17/149: loss=0.4761080526127399\n",
      "GD iter. 18/149: loss=0.4740804383567696\n",
      "GD iter. 19/149: loss=0.4721350610459529\n",
      "GD iter. 20/149: loss=0.47026487922533067\n",
      "GD iter. 21/149: loss=0.4684638971544937\n",
      "GD iter. 22/149: loss=0.4667269551853531\n",
      "GD iter. 23/149: loss=0.4650495694172944\n",
      "GD iter. 24/149: loss=0.463427807856719\n",
      "GD iter. 25/149: loss=0.461858193861369\n",
      "GD iter. 26/149: loss=0.4603376301555319\n",
      "GD iter. 27/149: loss=0.45886333848553307\n",
      "GD iter. 28/149: loss=0.4574328112651428\n",
      "GD iter. 29/149: loss=0.4560437724871771\n",
      "GD iter. 30/149: loss=0.45469414585355733\n",
      "GD iter. 31/149: loss=0.45338202857293763\n",
      "GD iter. 32/149: loss=0.45210566964282195\n",
      "GD iter. 33/149: loss=0.450863451707275\n",
      "GD iter. 34/149: loss=0.4496538757871031\n",
      "GD iter. 35/149: loss=0.4484755483348189\n",
      "GD iter. 36/149: loss=0.44732717018486823\n",
      "GD iter. 37/149: loss=0.4462075270600002\n",
      "GD iter. 38/149: loss=0.445115481364237\n",
      "GD iter. 39/149: loss=0.44404996504678496\n",
      "GD iter. 40/149: loss=0.44300997336320747\n",
      "GD iter. 41/149: loss=0.4419945593930785\n",
      "GD iter. 42/149: loss=0.44100282919928074\n",
      "GD iter. 43/149: loss=0.44003393753468123\n",
      "GD iter. 44/149: loss=0.4390870840183344\n",
      "GD iter. 45/149: loss=0.4381615097165345\n",
      "GD iter. 46/149: loss=0.437256494074671\n",
      "GD iter. 47/149: loss=0.43637135215447853\n",
      "GD iter. 48/149: loss=0.43550543213831305\n",
      "GD iter. 49/149: loss=0.4346581130678834\n",
      "GD iter. 50/149: loss=0.4338288027896402\n",
      "GD iter. 51/149: loss=0.4330169360829916\n",
      "GD iter. 52/149: loss=0.43222197295082493\n",
      "GD iter. 53/149: loss=0.4314433970545894\n",
      "GD iter. 54/149: loss=0.43068071427853005\n",
      "GD iter. 55/149: loss=0.4299334514096459\n",
      "GD iter. 56/149: loss=0.4292011549216233\n",
      "GD iter. 57/149: loss=0.42848338985243783\n",
      "GD iter. 58/149: loss=0.4277797387665449\n",
      "GD iter. 59/149: loss=0.42708980079363984\n",
      "GD iter. 60/149: loss=0.4264131907368869\n",
      "GD iter. 61/149: loss=0.42574953824430645\n",
      "GD iter. 62/149: loss=0.4250984870377008\n",
      "GD iter. 63/149: loss=0.4244596941941013\n",
      "GD iter. 64/149: loss=0.4238328294752462\n",
      "GD iter. 65/149: loss=0.4232175747010598\n",
      "GD iter. 66/149: loss=0.42261362316351125\n",
      "GD iter. 67/149: loss=0.42202067907758933\n",
      "GD iter. 68/149: loss=0.42143845706644667\n",
      "GD iter. 69/149: loss=0.4208666816780499\n",
      "GD iter. 70/149: loss=0.4203050869309193\n",
      "GD iter. 71/149: loss=0.41975341588676635\n",
      "GD iter. 72/149: loss=0.4192114202480372\n",
      "GD iter. 73/149: loss=0.41867885997854276\n",
      "GD iter. 74/149: loss=0.4181555029455229\n",
      "GD iter. 75/149: loss=0.41764112458162767\n",
      "GD iter. 76/149: loss=0.4171355075654334\n",
      "GD iter. 77/149: loss=0.41663844151922236\n",
      "GD iter. 78/149: loss=0.41614972272286266\n",
      "GD iter. 79/149: loss=0.4156691538427177\n",
      "GD iter. 80/149: loss=0.41519654367460157\n",
      "GD iter. 81/149: loss=0.4147317068998728\n",
      "GD iter. 82/149: loss=0.41427446385382993\n",
      "GD iter. 83/149: loss=0.41382464030563754\n",
      "GD iter. 84/149: loss=0.4133820672490689\n",
      "GD iter. 85/149: loss=0.41294658070340373\n",
      "GD iter. 86/149: loss=0.41251802152386974\n",
      "GD iter. 87/149: loss=0.41209623522106054\n",
      "GD iter. 88/149: loss=0.41168107178880353\n",
      "GD iter. 89/149: loss=0.41127238553998613\n",
      "GD iter. 90/149: loss=0.41087003494988794\n",
      "GD iter. 91/149: loss=0.41047388250659367\n",
      "GD iter. 92/149: loss=0.4100837945680898\n",
      "GD iter. 93/149: loss=0.4096996412256811\n",
      "GD iter. 94/149: loss=0.4093212961733781\n",
      "GD iter. 95/149: loss=0.40894863658293784\n",
      "GD iter. 96/149: loss=0.4085815429842561\n",
      "GD iter. 97/149: loss=0.408219899150829\n",
      "GD iter. 98/149: loss=0.4078635919900224\n",
      "GD iter. 99/149: loss=0.4075125114379014\n",
      "GD iter. 100/149: loss=0.4071665503583876\n",
      "GD iter. 101/149: loss=0.40682560444652854\n",
      "GD iter. 102/149: loss=0.40648957213567305\n",
      "GD iter. 103/149: loss=0.4061583545083624\n",
      "GD iter. 104/149: loss=0.4058318552107557\n",
      "GD iter. 105/149: loss=0.4055099803704189\n",
      "GD iter. 106/149: loss=0.4051926385173184\n",
      "GD iter. 107/149: loss=0.4048797405078679\n",
      "GD iter. 108/149: loss=0.40457119945188574\n",
      "GD iter. 109/149: loss=0.4042669306423278\n",
      "GD iter. 110/149: loss=0.4039668514876701\n",
      "GD iter. 111/149: loss=0.4036708814468209\n",
      "GD iter. 112/149: loss=0.40337894196644686\n",
      "GD iter. 113/149: loss=0.403090956420609\n",
      "GD iter. 114/149: loss=0.4028068500526034\n",
      "GD iter. 115/149: loss=0.4025265499189136\n",
      "GD iter. 116/149: loss=0.4022499848351801\n",
      "GD iter. 117/149: loss=0.401977085324103\n",
      "GD iter. 118/149: loss=0.40170778356519365\n",
      "GD iter. 119/149: loss=0.40144201334629936\n",
      "GD iter. 120/149: loss=0.401179710016824\n",
      "GD iter. 121/149: loss=0.4009208104425771\n",
      "GD iter. 122/149: loss=0.40066525296218325\n",
      "GD iter. 123/149: loss=0.4004129773449867\n",
      "GD iter. 124/149: loss=0.4001639247503936\n",
      "GD iter. 125/149: loss=0.3999180376885902\n",
      "GD iter. 126/149: loss=0.39967525998258663\n",
      "GD iter. 127/149: loss=0.3994355367315287\n",
      "GD iter. 128/149: loss=0.3991988142752323\n",
      "GD iter. 129/149: loss=0.3989650401598899\n",
      "GD iter. 130/149: loss=0.39873416310490445\n",
      "GD iter. 131/149: loss=0.3985061329708074\n",
      "GD iter. 132/149: loss=0.39828090072821964\n",
      "GD iter. 133/149: loss=0.39805841842781475\n",
      "GD iter. 134/149: loss=0.39783863917124745\n",
      "GD iter. 135/149: loss=0.39762151708301113\n",
      "GD iter. 136/149: loss=0.3974070072831895\n",
      "GD iter. 137/149: loss=0.39719506586106856\n",
      "GD iter. 138/149: loss=0.3969856498495791\n",
      "GD iter. 139/149: loss=0.3967787172005377\n",
      "GD iter. 140/149: loss=0.3965742267606569\n",
      "GD iter. 141/149: loss=0.3963721382482987\n",
      "GD iter. 142/149: loss=0.3961724122309428\n",
      "GD iter. 143/149: loss=0.3959750101033445\n",
      "GD iter. 144/149: loss=0.3957798940663583\n",
      "GD iter. 145/149: loss=0.395587027106403\n",
      "GD iter. 146/149: loss=0.39539637297554575\n",
      "GD iter. 147/149: loss=0.39520789617218466\n",
      "GD iter. 148/149: loss=0.39502156192230725\n",
      "GD iter. 149/149: loss=0.3948373361613052\n",
      "The Accuracy is: 0.8456\n",
      "The F1 score is: 0.3896\n",
      "The precision is: 0.2885\n",
      "The recall is: 0.6000\n",
      "GD iter. 0/149: loss=0.6854552374715007\n",
      "GD iter. 1/149: loss=0.5563518375476204\n",
      "GD iter. 2/149: loss=0.5380065955961353\n",
      "GD iter. 3/149: loss=0.5279573539483827\n",
      "GD iter. 4/149: loss=0.5206813683581358\n",
      "GD iter. 5/149: loss=0.5148305169011412\n",
      "GD iter. 6/149: loss=0.5098894248085902\n",
      "GD iter. 7/149: loss=0.5055947254131082\n",
      "GD iter. 8/149: loss=0.5017855778162409\n",
      "GD iter. 9/149: loss=0.49835370335639373\n",
      "GD iter. 10/149: loss=0.49522229669515827\n",
      "GD iter. 11/149: loss=0.4923351642517212\n",
      "GD iter. 12/149: loss=0.4896502915397352\n",
      "GD iter. 13/149: loss=0.4871357037580637\n",
      "GD iter. 14/149: loss=0.4847666756803411\n",
      "GD iter. 15/149: loss=0.48252379904637743\n",
      "GD iter. 16/149: loss=0.48039161917838147\n",
      "GD iter. 17/149: loss=0.4783576593249037\n",
      "GD iter. 18/149: loss=0.47641171371101004\n",
      "GD iter. 19/149: loss=0.4745453293226106\n",
      "GD iter. 20/149: loss=0.47275142180296886\n",
      "GD iter. 21/149: loss=0.47102398767635834\n",
      "GD iter. 22/149: loss=0.4693578864716979\n",
      "GD iter. 23/149: loss=0.46774867407429854\n",
      "GD iter. 24/149: loss=0.4661924739853273\n",
      "GD iter. 25/149: loss=0.4646858768973492\n",
      "GD iter. 26/149: loss=0.4632258616164949\n",
      "GD iter. 27/149: loss=0.4618097322222831\n",
      "GD iter. 28/149: loss=0.4604350676876114\n",
      "GD iter. 29/149: loss=0.4590996811423397\n",
      "GD iter. 30/149: loss=0.4578015866630637\n",
      "GD iter. 31/149: loss=0.45653897198445825\n",
      "GD iter. 32/149: loss=0.45531017590659845\n",
      "GD iter. 33/149: loss=0.4541136694549519\n",
      "GD iter. 34/149: loss=0.4529480400615468\n",
      "GD iter. 35/149: loss=0.4518119781958947\n",
      "GD iter. 36/149: loss=0.45070426599610414\n",
      "GD iter. 37/149: loss=0.44962376754400435\n",
      "GD iter. 38/149: loss=0.4485694205001759\n",
      "GD iter. 39/149: loss=0.4475402288707653\n",
      "GD iter. 40/149: loss=0.4465352567217391\n",
      "GD iter. 41/149: loss=0.4455536226906777\n",
      "GD iter. 42/149: loss=0.4445944951734821\n",
      "GD iter. 43/149: loss=0.4436570880850934\n",
      "GD iter. 44/149: loss=0.4427406571107393\n",
      "GD iter. 45/149: loss=0.44184449637825196\n",
      "GD iter. 46/149: loss=0.44096793549338403\n",
      "GD iter. 47/149: loss=0.4401103368893208\n",
      "GD iter. 48/149: loss=0.4392710934491922\n",
      "GD iter. 49/149: loss=0.4384496263666489\n",
      "GD iter. 50/149: loss=0.4376453832147565\n",
      "GD iter. 51/149: loss=0.43685783619777846\n",
      "GD iter. 52/149: loss=0.43608648056402904\n",
      "GD iter. 53/149: loss=0.43533083316100935\n",
      "GD iter. 54/149: loss=0.434590431116597\n",
      "GD iter. 55/149: loss=0.4338648306322302\n",
      "GD iter. 56/149: loss=0.4331536058758625\n",
      "GD iter. 57/149: loss=0.4324563479640423\n",
      "GD iter. 58/149: loss=0.43177266402381337\n",
      "GD iter. 59/149: loss=0.4311021763262849\n",
      "GD iter. 60/149: loss=0.4304445214847207\n",
      "GD iter. 61/149: loss=0.4297993497108472\n",
      "GD iter. 62/149: loss=0.4291663241238285\n",
      "GD iter. 63/149: loss=0.4285451201069978\n",
      "GD iter. 64/149: loss=0.4279354247079946\n",
      "GD iter. 65/149: loss=0.42733693607844475\n",
      "GD iter. 66/149: loss=0.4267493629497438\n",
      "GD iter. 67/149: loss=0.42617242414187867\n",
      "GD iter. 68/149: loss=0.4256058481025456\n",
      "GD iter. 69/149: loss=0.4250493724741065\n",
      "GD iter. 70/149: loss=0.424502743686181\n",
      "GD iter. 71/149: loss=0.4239657165718869\n",
      "GD iter. 72/149: loss=0.4234380540059419\n",
      "GD iter. 73/149: loss=0.42291952656300424\n",
      "GD iter. 74/149: loss=0.42240991219479046\n",
      "GD iter. 75/149: loss=0.42190899592463743\n",
      "GD iter. 76/149: loss=0.4214165695582976\n",
      "GD iter. 77/149: loss=0.4209324314098635\n",
      "GD iter. 78/149: loss=0.4204563860418131\n",
      "GD iter. 79/149: loss=0.41998824401825263\n",
      "GD iter. 80/149: loss=0.41952782167050806\n",
      "GD iter. 81/149: loss=0.41907494087428837\n",
      "GD iter. 82/149: loss=0.4186294288377019\n",
      "GD iter. 83/149: loss=0.4181911178994664\n",
      "GD iter. 84/149: loss=0.4177598453366988\n",
      "GD iter. 85/149: loss=0.41733545318172177\n",
      "GD iter. 86/149: loss=0.41691778804736024\n",
      "GD iter. 87/149: loss=0.416506700960243\n",
      "GD iter. 88/149: loss=0.4161020472016562\n",
      "GD iter. 89/149: loss=0.41570368615552583\n",
      "GD iter. 90/149: loss=0.41531148116313926\n",
      "GD iter. 91/149: loss=0.4149252993842371\n",
      "GD iter. 92/149: loss=0.41454501166413377\n",
      "GD iter. 93/149: loss=0.4141704924065465\n",
      "GD iter. 94/149: loss=0.413801619451833\n",
      "GD iter. 95/149: loss=0.413438273960357\n",
      "GD iter. 96/149: loss=0.41308034030071655\n",
      "GD iter. 97/149: loss=0.4127277059425915\n",
      "GD iter. 98/149: loss=0.4123802613539732\n",
      "GD iter. 99/149: loss=0.412037899902562\n",
      "GD iter. 100/149: loss=0.4117005177611235\n",
      "GD iter. 101/149: loss=0.4113680138166141\n",
      "GD iter. 102/149: loss=0.4110402895828894\n",
      "GD iter. 103/149: loss=0.41071724911682833\n",
      "GD iter. 104/149: loss=0.4103987989377059\n",
      "GD iter. 105/149: loss=0.4100848479496667\n",
      "GD iter. 106/149: loss=0.40977530736715184\n",
      "GD iter. 107/149: loss=0.40947009064314505\n",
      "GD iter. 108/149: loss=0.4091691134001054\n",
      "GD iter. 109/149: loss=0.408872293363472\n",
      "GD iter. 110/149: loss=0.40857955029761717\n",
      "GD iter. 111/149: loss=0.4082908059441459\n",
      "GD iter. 112/149: loss=0.40800598396243437\n",
      "GD iter. 113/149: loss=0.40772500987231164\n",
      "GD iter. 114/149: loss=0.40744781099879135\n",
      "GD iter. 115/149: loss=0.40717431641876495\n",
      "GD iter. 116/149: loss=0.406904456909575\n",
      "GD iter. 117/149: loss=0.40663816489938714\n",
      "GD iter. 118/149: loss=0.4063753744192874\n",
      "GD iter. 119/149: loss=0.40611602105703215\n",
      "GD iter. 120/149: loss=0.40586004191238456\n",
      "GD iter. 121/149: loss=0.4056073755539713\n",
      "GD iter. 122/149: loss=0.40535796197759916\n",
      "GD iter. 123/149: loss=0.4051117425659729\n",
      "GD iter. 124/149: loss=0.40486866004975913\n",
      "GD iter. 125/149: loss=0.4046286584699428\n",
      "GD iter. 126/149: loss=0.40439168314142543\n",
      "GD iter. 127/149: loss=0.4041576806178184\n",
      "GD iter. 128/149: loss=0.40392659865738395\n",
      "GD iter. 129/149: loss=0.40369838619007986\n",
      "GD iter. 130/149: loss=0.4034729932856684\n",
      "GD iter. 131/149: loss=0.40325037112284584\n",
      "GD iter. 132/149: loss=0.40303047195935754\n",
      "GD iter. 133/149: loss=0.402813249103061\n",
      "GD iter. 134/149: loss=0.4025986568839002\n",
      "GD iter. 135/149: loss=0.4023866506267614\n",
      "GD iter. 136/149: loss=0.4021771866251755\n",
      "GD iter. 137/149: loss=0.40197022211583594\n",
      "GD iter. 138/149: loss=0.4017657152539069\n",
      "GD iter. 139/149: loss=0.40156362508908816\n",
      "GD iter. 140/149: loss=0.4013639115424137\n",
      "GD iter. 141/149: loss=0.4011665353837563\n",
      "GD iter. 142/149: loss=0.4009714582100146\n",
      "GD iter. 143/149: loss=0.40077864242395667\n",
      "GD iter. 144/149: loss=0.4005880512137006\n",
      "GD iter. 145/149: loss=0.4003996485328061\n",
      "GD iter. 146/149: loss=0.40021339908096004\n",
      "GD iter. 147/149: loss=0.400029268285233\n",
      "GD iter. 148/149: loss=0.3998472222818878\n",
      "GD iter. 149/149: loss=0.3996672278987229\n",
      "The Accuracy is: 0.8539\n",
      "The F1 score is: 0.4258\n",
      "The precision is: 0.3173\n",
      "The recall is: 0.6471\n",
      "GD iter. 0/149: loss=0.7027678367235388\n",
      "GD iter. 1/149: loss=0.5575932590485764\n",
      "GD iter. 2/149: loss=0.5394299879530555\n",
      "GD iter. 3/149: loss=0.529472531945762\n",
      "GD iter. 4/149: loss=0.522272026482183\n",
      "GD iter. 5/149: loss=0.5164855638593812\n",
      "GD iter. 6/149: loss=0.5115979681604974\n",
      "GD iter. 7/149: loss=0.50734651378051\n",
      "GD iter. 8/149: loss=0.5035713632064602\n",
      "GD iter. 9/149: loss=0.5001652775241213\n",
      "GD iter. 10/149: loss=0.4970523933566885\n",
      "GD iter. 11/149: loss=0.4941773407117391\n",
      "GD iter. 12/149: loss=0.4914988288380198\n",
      "GD iter. 13/149: loss=0.4889855304030239\n",
      "GD iter. 14/149: loss=0.4866133092612466\n",
      "GD iter. 15/149: loss=0.4843632988652062\n",
      "GD iter. 16/149: loss=0.48222054481009896\n",
      "GD iter. 17/149: loss=0.4801730320654014\n",
      "GD iter. 18/149: loss=0.47821097942218077\n",
      "GD iter. 19/149: loss=0.4763263221580083\n",
      "GD iter. 20/149: loss=0.47451232882660066\n",
      "GD iter. 21/149: loss=0.472763314624285\n",
      "GD iter. 22/149: loss=0.4710744249712646\n",
      "GD iter. 23/149: loss=0.46944147061009256\n",
      "GD iter. 24/149: loss=0.4678608008345152\n",
      "GD iter. 25/149: loss=0.46632920517873727\n",
      "GD iter. 26/149: loss=0.46484383652277383\n",
      "GD iter. 27/149: loss=0.46340215044057087\n",
      "GD iter. 28/149: loss=0.4620018569620222\n",
      "GD iter. 29/149: loss=0.4606408818937837\n",
      "GD iter. 30/149: loss=0.4593173355544894\n",
      "GD iter. 31/149: loss=0.45802948730247706\n",
      "GD iter. 32/149: loss=0.4567757446209867\n",
      "GD iter. 33/149: loss=0.4555546358141555\n",
      "GD iter. 34/149: loss=0.45436479558348286\n",
      "GD iter. 35/149: loss=0.453204952917791\n",
      "GD iter. 36/149: loss=0.45207392085379106\n",
      "GD iter. 37/149: loss=0.45097058775918447\n",
      "GD iter. 38/149: loss=0.4498939098631045\n",
      "GD iter. 39/149: loss=0.4488429048150298\n",
      "GD iter. 40/149: loss=0.4478166460970727\n",
      "GD iter. 41/149: loss=0.44681425814875125\n",
      "GD iter. 42/149: loss=0.4458349120902236\n",
      "GD iter. 43/149: loss=0.444877821951178\n",
      "GD iter. 44/149: loss=0.44394224132942023\n",
      "GD iter. 45/149: loss=0.4430274604166314\n",
      "GD iter. 46/149: loss=0.44213280333955157\n",
      "GD iter. 47/149: loss=0.4412576257735301\n",
      "GD iter. 48/149: loss=0.4404013127924178\n",
      "GD iter. 49/149: loss=0.43956327692450936\n",
      "GD iter. 50/149: loss=0.4387429563889264\n",
      "GD iter. 51/149: loss=0.4379398134906928\n",
      "GD iter. 52/149: loss=0.4371533331559307\n",
      "GD iter. 53/149: loss=0.43638302159125747\n",
      "GD iter. 54/149: loss=0.4356284050536645\n",
      "GD iter. 55/149: loss=0.43488902871900903\n",
      "GD iter. 56/149: loss=0.43416445563881156\n",
      "GD iter. 57/149: loss=0.4334542657763624\n",
      "GD iter. 58/149: loss=0.432758055114267\n",
      "GD iter. 59/149: loss=0.43207543482651023\n",
      "GD iter. 60/149: loss=0.43140603050894527\n",
      "GD iter. 61/149: loss=0.4307494814628088\n",
      "GD iter. 62/149: loss=0.4301054400264853\n",
      "GD iter. 63/149: loss=0.42947357095126004\n",
      "GD iter. 64/149: loss=0.42885355081726834\n",
      "GD iter. 65/149: loss=0.4282450674862486\n",
      "GD iter. 66/149: loss=0.42764781958805453\n",
      "GD iter. 67/149: loss=0.42706151603820003\n",
      "GD iter. 68/149: loss=0.4264858755839709\n",
      "GD iter. 69/149: loss=0.42592062637689304\n",
      "GD iter. 70/149: loss=0.425365505569545\n",
      "GD iter. 71/149: loss=0.4248202589349049\n",
      "GD iter. 72/149: loss=0.4242846405065833\n",
      "GD iter. 73/149: loss=0.4237584122384452\n",
      "GD iter. 74/149: loss=0.4232413436822614\n",
      "GD iter. 75/149: loss=0.42273321168214445\n",
      "GD iter. 76/149: loss=0.4222338000846394\n",
      "GD iter. 77/149: loss=0.42174289946342686\n",
      "GD iter. 78/149: loss=0.4212603068576927\n",
      "GD iter. 79/149: loss=0.42078582552329014\n",
      "GD iter. 80/149: loss=0.4203192646958923\n",
      "GD iter. 81/149: loss=0.41986043936539924\n",
      "GD iter. 82/149: loss=0.41940917006091755\n",
      "GD iter. 83/149: loss=0.4189652826456864\n",
      "GD iter. 84/149: loss=0.4185286081213689\n",
      "GD iter. 85/149: loss=0.41809898244117005\n",
      "GD iter. 86/149: loss=0.4176762463312851\n",
      "GD iter. 87/149: loss=0.4172602451202144\n",
      "GD iter. 88/149: loss=0.4168508285755151\n",
      "GD iter. 89/149: loss=0.4164478507475872\n",
      "GD iter. 90/149: loss=0.4160511698201238\n",
      "GD iter. 91/149: loss=0.4156606479668746\n",
      "GD iter. 92/149: loss=0.41527615121439615\n",
      "GD iter. 93/149: loss=0.4148975493104879\n",
      "GD iter. 94/149: loss=0.41452471559802345\n",
      "GD iter. 95/149: loss=0.4141575268939136\n",
      "GD iter. 96/149: loss=0.4137958633729468\n",
      "GD iter. 97/149: loss=0.4134396084562719\n",
      "GD iter. 98/149: loss=0.4130886487043021\n",
      "GD iter. 99/149: loss=0.4127428737138276\n",
      "GD iter. 100/149: loss=0.4124021760191438\n",
      "GD iter. 101/149: loss=0.4120664509970057\n",
      "GD iter. 102/149: loss=0.41173559677523397\n",
      "GD iter. 103/149: loss=0.4114095141448082\n",
      "GD iter. 104/149: loss=0.41108810647528726\n",
      "GD iter. 105/149: loss=0.4107712796334115\n",
      "GD iter. 106/149: loss=0.41045894190474436\n",
      "GD iter. 107/149: loss=0.4101510039182203\n",
      "GD iter. 108/149: loss=0.40984737857347303\n",
      "GD iter. 109/149: loss=0.4095479809708252\n",
      "GD iter. 110/149: loss=0.40925272834382287\n",
      "GD iter. 111/149: loss=0.40896153999421103\n",
      "GD iter. 112/149: loss=0.4086743372292423\n",
      "GD iter. 113/149: loss=0.40839104330122683\n",
      "GD iter. 114/149: loss=0.40811158334922515\n",
      "GD iter. 115/149: loss=0.4078358843427998\n",
      "GD iter. 116/149: loss=0.4075638750277388\n",
      "GD iter. 117/149: loss=0.4072954858736719\n",
      "GD iter. 118/149: loss=0.40703064902350433\n",
      "GD iter. 119/149: loss=0.40676929824459185\n",
      "GD iter. 120/149: loss=0.40651136888159123\n",
      "GD iter. 121/149: loss=0.4062567978109185\n",
      "GD iter. 122/149: loss=0.40600552339675033\n",
      "GD iter. 123/149: loss=0.40575748544851076\n",
      "GD iter. 124/149: loss=0.40551262517978326\n",
      "GD iter. 125/149: loss=0.4052708851685939\n",
      "GD iter. 126/149: loss=0.4050322093190137\n",
      "GD iter. 127/149: loss=0.40479654282402805\n",
      "GD iter. 128/149: loss=0.40456383212962693\n",
      "GD iter. 129/149: loss=0.4043340249000677\n",
      "GD iter. 130/149: loss=0.4041070699842689\n",
      "GD iter. 131/149: loss=0.40388291738328996\n",
      "GD iter. 132/149: loss=0.40366151821885965\n",
      "GD iter. 133/149: loss=0.40344282470291076\n",
      "GD iter. 134/149: loss=0.4032267901080878\n",
      "GD iter. 135/149: loss=0.403013368739189\n",
      "GD iter. 136/149: loss=0.40280251590551097\n",
      "GD iter. 137/149: loss=0.402594187894061\n",
      "GD iter. 138/149: loss=0.40238834194360834\n",
      "GD iter. 139/149: loss=0.40218493621954166\n",
      "GD iter. 140/149: loss=0.4019839297895066\n",
      "GD iter. 141/149: loss=0.4017852825997931\n",
      "GD iter. 142/149: loss=0.4015889554524481\n",
      "GD iter. 143/149: loss=0.4013949099830873\n",
      "GD iter. 144/149: loss=0.4012031086393804\n",
      "GD iter. 145/149: loss=0.4010135146601894\n",
      "GD iter. 146/149: loss=0.40082609205533365\n",
      "GD iter. 147/149: loss=0.4006408055859619\n",
      "GD iter. 148/149: loss=0.4004576207455118\n",
      "GD iter. 149/149: loss=0.400276503741233\n",
      "The Accuracy is: 0.8588\n",
      "The F1 score is: 0.3857\n",
      "The precision is: 0.2842\n",
      "The recall is: 0.6000\n",
      "GD iter. 0/149: loss=0.7148874838516752\n",
      "GD iter. 1/149: loss=0.549523622083641\n",
      "GD iter. 2/149: loss=0.5317994455334503\n",
      "GD iter. 3/149: loss=0.5223808468163146\n",
      "GD iter. 4/149: loss=0.5155984692553189\n",
      "GD iter. 5/149: loss=0.5101183309219594\n",
      "GD iter. 6/149: loss=0.5054512132679848\n",
      "GD iter. 7/149: loss=0.5013565274755366\n",
      "GD iter. 8/149: loss=0.4976914230840065\n",
      "GD iter. 9/149: loss=0.4943614220056997\n",
      "GD iter. 10/149: loss=0.49130017414205684\n",
      "GD iter. 11/149: loss=0.48845936481264324\n",
      "GD iter. 12/149: loss=0.4858029113467226\n",
      "GD iter. 13/149: loss=0.48330331325823134\n",
      "GD iter. 14/149: loss=0.4809392322380819\n",
      "GD iter. 15/149: loss=0.4786938348917414\n",
      "GD iter. 16/149: loss=0.4765536327160238\n",
      "GD iter. 17/149: loss=0.4745076564295413\n",
      "GD iter. 18/149: loss=0.47254685992104806\n",
      "GD iter. 19/149: loss=0.4706636844351322\n",
      "GD iter. 20/149: loss=0.46885173608305836\n",
      "GD iter. 21/149: loss=0.4671055444598127\n",
      "GD iter. 22/149: loss=0.4654203799513972\n",
      "GD iter. 23/149: loss=0.46379211395786396\n",
      "GD iter. 24/149: loss=0.4622171108146918\n",
      "GD iter. 25/149: loss=0.4606921433570371\n",
      "GD iter. 26/149: loss=0.45921432628765746\n",
      "GD iter. 27/149: loss=0.4577810630774052\n",
      "GD iter. 28/149: loss=0.45639000324665957\n",
      "GD iter. 29/149: loss=0.4550390076820777\n",
      "GD iter. 30/149: loss=0.4537261202281689\n",
      "GD iter. 31/149: loss=0.4524495442213951\n",
      "GD iter. 32/149: loss=0.45120762295027755\n",
      "GD iter. 33/149: loss=0.44999882325970136\n",
      "GD iter. 34/149: loss=0.44882172169334555\n",
      "GD iter. 35/149: loss=0.4476749927007485\n",
      "GD iter. 36/149: loss=0.4465573985362865\n",
      "GD iter. 37/149: loss=0.4454677805544638\n",
      "GD iter. 38/149: loss=0.44440505166539057\n",
      "GD iter. 39/149: loss=0.44336818976049314\n",
      "GD iter. 40/149: loss=0.44235623195460766\n",
      "GD iter. 41/149: loss=0.4413682695190237\n",
      "GD iter. 42/149: loss=0.4404034434025691\n",
      "GD iter. 43/149: loss=0.43946094025578897\n",
      "GD iter. 44/149: loss=0.43853998888769086\n",
      "GD iter. 45/149: loss=0.43763985709617415\n",
      "GD iter. 46/149: loss=0.43675984882272173\n",
      "GD iter. 47/149: loss=0.43589930158966367\n",
      "GD iter. 48/149: loss=0.4350575841846718\n",
      "GD iter. 49/149: loss=0.43423409456239465\n",
      "GD iter. 50/149: loss=0.4334282579374917\n",
      "GD iter. 51/149: loss=0.43263952504696385\n",
      "GD iter. 52/149: loss=0.4318673705627221\n",
      "GD iter. 53/149: loss=0.4311112916378957\n",
      "GD iter. 54/149: loss=0.4303708065725525\n",
      "GD iter. 55/149: loss=0.42964545358634465\n",
      "GD iter. 56/149: loss=0.4289347896871549\n",
      "GD iter. 57/149: loss=0.4282383896261665\n",
      "GD iter. 58/149: loss=0.4275558449309262\n",
      "GD iter. 59/149: loss=0.42688676300896333\n",
      "GD iter. 60/149: loss=0.4262307663153816\n",
      "GD iter. 61/149: loss=0.4255874915785878\n",
      "GD iter. 62/149: loss=0.4249565890789609\n",
      "GD iter. 63/149: loss=0.4243377219758319\n",
      "GD iter. 64/149: loss=0.42373056567863504\n",
      "GD iter. 65/149: loss=0.4231348072585223\n",
      "GD iter. 66/149: loss=0.42255014489711135\n",
      "GD iter. 67/149: loss=0.42197628736937337\n",
      "GD iter. 68/149: loss=0.4214129535579574\n",
      "GD iter. 69/149: loss=0.42085987199651353\n",
      "GD iter. 70/149: loss=0.42031678043980647\n",
      "GD iter. 71/149: loss=0.41978342545861513\n",
      "GD iter. 72/149: loss=0.4192595620576005\n",
      "GD iter. 73/149: loss=0.41874495331448575\n",
      "GD iter. 74/149: loss=0.4182393700390383\n",
      "GD iter. 75/149: loss=0.4177425904504768\n",
      "GD iter. 76/149: loss=0.41725439987204094\n",
      "GD iter. 77/149: loss=0.41677459044157034\n",
      "GD iter. 78/149: loss=0.4163029608370335\n",
      "GD iter. 79/149: loss=0.41583931601603247\n",
      "GD iter. 80/149: loss=0.41538346696838824\n",
      "GD iter. 81/149: loss=0.4149352304809827\n",
      "GD iter. 82/149: loss=0.4144944289140947\n",
      "GD iter. 83/149: loss=0.414060889988529\n",
      "GD iter. 84/149: loss=0.413634446582886\n",
      "GD iter. 85/149: loss=0.4132149365403708\n",
      "GD iter. 86/149: loss=0.41280220248458466\n",
      "GD iter. 87/149: loss=0.41239609164377883\n",
      "GD iter. 88/149: loss=0.41199645568308996\n",
      "GD iter. 89/149: loss=0.41160315054430924\n",
      "GD iter. 90/149: loss=0.41121603629276793\n",
      "GD iter. 91/149: loss=0.41083497697094995\n",
      "GD iter. 92/149: loss=0.41045984045847034\n",
      "GD iter. 93/149: loss=0.4100904983380773\n",
      "GD iter. 94/149: loss=0.4097268257673639\n",
      "GD iter. 95/149: loss=0.40936870135589154\n",
      "GD iter. 96/149: loss=0.40901600704744645\n",
      "GD iter. 97/149: loss=0.40866862800717063\n",
      "GD iter. 98/149: loss=0.40832645251332034\n",
      "GD iter. 99/149: loss=0.4079893718534245\n",
      "GD iter. 100/149: loss=0.4076572802246262\n",
      "GD iter. 101/149: loss=0.40733007463800375\n",
      "GD iter. 102/149: loss=0.407007654826682\n",
      "GD iter. 103/149: loss=0.40668992315755154\n",
      "GD iter. 104/149: loss=0.4063767845464273\n",
      "GD iter. 105/149: loss=0.40606814637648697\n",
      "GD iter. 106/149: loss=0.4057639184198357\n",
      "GD iter. 107/149: loss=0.4054640127620574\n",
      "GD iter. 108/149: loss=0.4051683437296137\n",
      "GD iter. 109/149: loss=0.4048768278199672\n",
      "GD iter. 110/149: loss=0.404589383634304\n",
      "GD iter. 111/149: loss=0.4043059318127435\n",
      "GD iter. 112/149: loss=0.40402639497192594\n",
      "GD iter. 113/149: loss=0.40375069764487465\n",
      "GD iter. 114/149: loss=0.40347876622303586\n",
      "GD iter. 115/149: loss=0.40321052890040254\n",
      "GD iter. 116/149: loss=0.4029459156196355\n",
      "GD iter. 117/149: loss=0.4026848580200958\n",
      "GD iter. 118/149: loss=0.402427289387712\n",
      "GD iter. 119/149: loss=0.4021731446066033\n",
      "GD iter. 120/149: loss=0.401922360112389\n",
      "GD iter. 121/149: loss=0.4016748738471148\n",
      "GD iter. 122/149: loss=0.4014306252157299\n",
      "GD iter. 123/149: loss=0.4011895550440538\n",
      "GD iter. 124/149: loss=0.4009516055381732\n",
      "GD iter. 125/149: loss=0.4007167202452119\n",
      "GD iter. 126/149: loss=0.4004848440154187\n",
      "GD iter. 127/149: loss=0.40025592296552664\n",
      "GD iter. 128/149: loss=0.4000299044433263\n",
      "GD iter. 129/149: loss=0.39980673699341324\n",
      "GD iter. 130/149: loss=0.39958637032406163\n",
      "GD iter. 131/149: loss=0.39936875527518123\n",
      "GD iter. 132/149: loss=0.3991538437873165\n",
      "GD iter. 133/149: loss=0.39894158887164977\n",
      "GD iter. 134/149: loss=0.3987319445809699\n",
      "GD iter. 135/149: loss=0.39852486598157033\n",
      "GD iter. 136/149: loss=0.3983203091260426\n",
      "GD iter. 137/149: loss=0.39811823102693256\n",
      "GD iter. 138/149: loss=0.3979185896312269\n",
      "GD iter. 139/149: loss=0.3977213437956405\n",
      "GD iter. 140/149: loss=0.3975264532626747\n",
      "GD iter. 141/149: loss=0.3973338786374202\n",
      "GD iter. 142/149: loss=0.397143581365076\n",
      "GD iter. 143/149: loss=0.39695552370916026\n",
      "GD iter. 144/149: loss=0.39676966873038816\n",
      "GD iter. 145/149: loss=0.3965859802661926\n",
      "GD iter. 146/149: loss=0.39640442291086603\n",
      "GD iter. 147/149: loss=0.3962249619963012\n",
      "GD iter. 148/149: loss=0.39604756357331\n",
      "GD iter. 149/149: loss=0.39587219439350063\n",
      "The Accuracy is: 0.8574\n",
      "The F1 score is: 0.3803\n",
      "The precision is: 0.2903\n",
      "The recall is: 0.5510\n",
      "Average accuracy score is:  0.850433662538955\n",
      "Average f1 score is:  0.3977953498619105\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using all the features except for those having NaN values over 50% ##\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_train_processed), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(10):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = logistic_regression(y_t, x_t, initial_w, max_iters=150, gamma=0.1)\n",
    "    y_pred = sigmoid(x_v @ w)\n",
    "    y_pred = (y_pred >= 0.7).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/499: loss=0.7458818538124562\n",
      "GD iter. 1/499: loss=0.5440900163993346\n",
      "GD iter. 2/499: loss=0.5275033008894628\n",
      "GD iter. 3/499: loss=0.5174744504849189\n",
      "GD iter. 4/499: loss=0.5101360126567559\n",
      "GD iter. 5/499: loss=0.5043193512308672\n",
      "GD iter. 6/499: loss=0.49951428800683956\n",
      "GD iter. 7/499: loss=0.49542793520658995\n",
      "GD iter. 8/499: loss=0.49187647185804834\n",
      "GD iter. 9/499: loss=0.48873909280862937\n",
      "GD iter. 10/499: loss=0.4859331165027785\n",
      "GD iter. 11/499: loss=0.4833997268121569\n",
      "GD iter. 12/499: loss=0.48109556870031484\n",
      "GD iter. 13/499: loss=0.478987652171511\n",
      "GD iter. 14/499: loss=0.4770501683558637\n",
      "GD iter. 15/499: loss=0.47526243907610677\n",
      "GD iter. 16/499: loss=0.47360755668680443\n",
      "GD iter. 17/499: loss=0.4720714559068971\n",
      "GD iter. 18/499: loss=0.47064226334928727\n",
      "GD iter. 19/499: loss=0.46930983024082623\n",
      "GD iter. 20/499: loss=0.4680653890121086\n",
      "GD iter. 21/499: loss=0.46690129562753463\n",
      "GD iter. 22/499: loss=0.46581083258014827\n",
      "GD iter. 23/499: loss=0.4647880556953955\n",
      "GD iter. 24/499: loss=0.46382767317441265\n",
      "GD iter. 25/499: loss=0.4629249487776622\n",
      "GD iter. 26/499: loss=0.4620756233729865\n",
      "GD iter. 27/499: loss=0.4612758506571309\n",
      "GD iter. 28/499: loss=0.46052214396070174\n",
      "GD iter. 29/499: loss=0.4598113318243271\n",
      "GD iter. 30/499: loss=0.45914052059216054\n",
      "GD iter. 31/499: loss=0.4585070626757316\n",
      "GD iter. 32/499: loss=0.4579085294417707\n",
      "GD iter. 33/499: loss=0.45734268790258825\n",
      "GD iter. 34/499: loss=0.4568074805579536\n",
      "GD iter. 35/499: loss=0.4563010078678182\n",
      "GD iter. 36/499: loss=0.45582151293606693\n",
      "GD iter. 37/499: loss=0.4553673680641694\n",
      "GD iter. 38/499: loss=0.45493706289554325\n",
      "GD iter. 39/499: loss=0.45452919392058216\n",
      "GD iter. 40/499: loss=0.45414245515158164\n",
      "GD iter. 41/499: loss=0.4537756298084175\n",
      "GD iter. 42/499: loss=0.45342758288145113\n",
      "GD iter. 43/499: loss=0.4530972544590221\n",
      "GD iter. 44/499: loss=0.45278365372402885\n",
      "GD iter. 45/499: loss=0.4524858535382297\n",
      "GD iter. 46/499: loss=0.45220298554461436\n",
      "GD iter. 47/499: loss=0.45193423572797814\n",
      "GD iter. 48/499: loss=0.4516788403820086\n",
      "GD iter. 49/499: loss=0.4514360824380964\n",
      "GD iter. 50/499: loss=0.4512052881169051\n",
      "GD iter. 51/499: loss=0.4509858238686879\n",
      "GD iter. 52/499: loss=0.45077709357256446\n",
      "GD iter. 53/499: loss=0.45057853596858277\n",
      "GD iter. 54/499: loss=0.4503896222995047\n",
      "GD iter. 55/499: loss=0.45020985414193093\n",
      "GD iter. 56/499: loss=0.4500387614087086\n",
      "GD iter. 57/499: loss=0.4498759005065741\n",
      "GD iter. 58/499: loss=0.4497208526347499\n",
      "GD iter. 59/499: loss=0.4495732222117401\n",
      "GD iter. 60/499: loss=0.44943263541892464\n",
      "GD iter. 61/499: loss=0.4492987388507272\n",
      "GD iter. 62/499: loss=0.4491711982621786\n",
      "GD iter. 63/499: loss=0.4490496974056162\n",
      "GD iter. 64/499: loss=0.44893393694907197\n",
      "GD iter. 65/499: loss=0.4488236334696316\n",
      "GD iter. 66/499: loss=0.4487185185156798\n",
      "GD iter. 67/499: loss=0.44861833773252885\n",
      "GD iter. 68/499: loss=0.4485228500464302\n",
      "GD iter. 69/499: loss=0.44843182690243316\n",
      "GD iter. 70/499: loss=0.4483450515519573\n",
      "GD iter. 71/499: loss=0.4482623183863164\n",
      "GD iter. 72/499: loss=0.44818343231276175\n",
      "GD iter. 73/499: loss=0.44810820816990604\n",
      "GD iter. 74/499: loss=0.4480364701796616\n",
      "GD iter. 75/499: loss=0.4479680514330642\n",
      "GD iter. 76/499: loss=0.4479027934075757\n",
      "GD iter. 77/499: loss=0.4478405455136537\n",
      "GD iter. 78/499: loss=0.44778116466855605\n",
      "GD iter. 79/499: loss=0.4477245148955145\n",
      "GD iter. 80/499: loss=0.4476704669465535\n",
      "GD iter. 81/499: loss=0.44761889794736853\n",
      "GD iter. 82/499: loss=0.44756969106280237\n",
      "GD iter. 83/499: loss=0.44752273518156355\n",
      "GD iter. 84/499: loss=0.44747792461894004\n",
      "GD iter. 85/499: loss=0.44743515883634905\n",
      "GD iter. 86/499: loss=0.44739434217665236\n",
      "GD iter. 87/499: loss=0.44735538361424376\n",
      "GD iter. 88/499: loss=0.4473181965189862\n",
      "GD iter. 89/499: loss=0.44728269843314145\n",
      "GD iter. 90/499: loss=0.4472488108604989\n",
      "GD iter. 91/499: loss=0.44721645906696006\n",
      "GD iter. 92/499: loss=0.44718557189189195\n",
      "GD iter. 93/499: loss=0.44715608156960557\n",
      "GD iter. 94/499: loss=0.4471279235603605\n",
      "GD iter. 95/499: loss=0.44710103639033827\n",
      "GD iter. 96/499: loss=0.44707536150006105\n",
      "GD iter. 97/499: loss=0.4470508431007687\n",
      "GD iter. 98/499: loss=0.447027428038298\n",
      "GD iter. 99/499: loss=0.44700506566403725\n",
      "GD iter. 100/499: loss=0.4469837077125577\n",
      "GD iter. 101/499: loss=0.44696330818554525\n",
      "GD iter. 102/499: loss=0.4469438232416843\n",
      "GD iter. 103/499: loss=0.44692521109216143\n",
      "GD iter. 104/499: loss=0.44690743190148396\n",
      "GD iter. 105/499: loss=0.4468904476933177\n",
      "GD iter. 106/499: loss=0.44687422226107754\n",
      "GD iter. 107/499: loss=0.4468587210830084\n",
      "GD iter. 108/499: loss=0.44684391124152034\n",
      "GD iter. 109/499: loss=0.4468297613465483\n",
      "GD iter. 110/499: loss=0.4468162414627225\n",
      "GD iter. 111/499: loss=0.4468033230401503\n",
      "GD iter. 112/499: loss=0.44679097884861774\n",
      "GD iter. 113/499: loss=0.44677918291503277\n",
      "GD iter. 114/499: loss=0.4467679104639423\n",
      "GD iter. 115/499: loss=0.44675713786096166\n",
      "GD iter. 116/499: loss=0.44674684255896874\n",
      "GD iter. 117/499: loss=0.4467370030469193\n",
      "GD iter. 118/499: loss=0.4467275988011493\n",
      "GD iter. 119/499: loss=0.4467186102390388\n",
      "GD iter. 120/499: loss=0.44671001867491633\n",
      "GD iter. 121/499: loss=0.4467018062780912\n",
      "GD iter. 122/499: loss=0.44669395603290635\n",
      "GD iter. 123/499: loss=0.44668645170071014\n",
      "GD iter. 124/499: loss=0.44667927778365124\n",
      "GD iter. 125/499: loss=0.4466724194902067\n",
      "GD iter. 126/499: loss=0.44666586270235376\n",
      "GD iter. 127/499: loss=0.4466595939443096\n",
      "GD iter. 128/499: loss=0.4466536003527554\n",
      "GD iter. 129/499: loss=0.44664786964847575\n",
      "GD iter. 130/499: loss=0.44664239010934265\n",
      "GD iter. 131/499: loss=0.4466371505445777\n",
      "GD iter. 132/499: loss=0.4466321402702304\n",
      "GD iter. 133/499: loss=0.4466273490858145\n",
      "GD iter. 134/499: loss=0.4466227672520429\n",
      "GD iter. 135/499: loss=0.44661838546961136\n",
      "GD iter. 136/499: loss=0.44661419485897885\n",
      "GD iter. 137/499: loss=0.4466101869410946\n",
      "GD iter. 138/499: loss=0.4466063536190304\n",
      "GD iter. 139/499: loss=0.4466026871604693\n",
      "GD iter. 140/499: loss=0.44659918018101463\n",
      "GD iter. 141/499: loss=0.44659582562827643\n",
      "GD iter. 142/499: loss=0.44659261676670065\n",
      "GD iter. 143/499: loss=0.44658954716310306\n",
      "GD iter. 144/499: loss=0.44658661067287686\n",
      "GD iter. 145/499: loss=0.44658380142684034\n",
      "GD iter. 146/499: loss=0.4465811138186948\n",
      "GD iter. 147/499: loss=0.44657854249306334\n",
      "GD iter. 148/499: loss=0.4465760823340833\n",
      "GD iter. 149/499: loss=0.44657372845452714\n",
      "GD iter. 150/499: loss=0.4465714761854241\n",
      "GD iter. 151/499: loss=0.44656932106616265\n",
      "GD iter. 152/499: loss=0.4465672588350479\n",
      "GD iter. 153/499: loss=0.4465652854202945\n",
      "GD iter. 154/499: loss=0.446563396931434\n",
      "GD iter. 155/499: loss=0.4465615896511169\n",
      "GD iter. 156/499: loss=0.446559860027292\n",
      "GD iter. 157/499: loss=0.4465582046657433\n",
      "GD iter. 158/499: loss=0.44655662032297083\n",
      "GD iter. 159/499: loss=0.4465551038993963\n",
      "GD iter. 160/499: loss=0.44655365243287964\n",
      "GD iter. 161/499: loss=0.4465522630925339\n",
      "GD iter. 162/499: loss=0.4465509331728199\n",
      "GD iter. 163/499: loss=0.44654966008791364\n",
      "GD iter. 164/499: loss=0.44654844136632776\n",
      "GD iter. 165/499: loss=0.44654727464578087\n",
      "GD iter. 166/499: loss=0.44654615766829814\n",
      "GD iter. 167/499: loss=0.44654508827553707\n",
      "GD iter. 168/499: loss=0.44654406440432404\n",
      "GD iter. 169/499: loss=0.4465430840823957\n",
      "GD iter. 170/499: loss=0.4465421454243311\n",
      "GD iter. 171/499: loss=0.44654124662767153\n",
      "GD iter. 172/499: loss=0.44654038596921386\n",
      "GD iter. 173/499: loss=0.44653956180147286\n",
      "GD iter. 174/499: loss=0.4465387725493038\n",
      "GD iter. 175/499: loss=0.44653801670667775\n",
      "GD iter. 176/499: loss=0.4465372928336027\n",
      "GD iter. 177/499: loss=0.44653659955318364\n",
      "GD iter. 178/499: loss=0.4465359355488157\n",
      "GD iter. 179/499: loss=0.44653529956150373\n",
      "GD iter. 180/499: loss=0.4465346903873028\n",
      "GD iter. 181/499: loss=0.44653410687487466\n",
      "GD iter. 182/499: loss=0.4465335479231528\n",
      "GD iter. 183/499: loss=0.446533012479114\n",
      "GD iter. 184/499: loss=0.44653249953564966\n",
      "GD iter. 185/499: loss=0.4465320081295329\n",
      "GD iter. 186/499: loss=0.4465315373394761\n",
      "GD iter. 187/499: loss=0.44653108628427696\n",
      "GD iter. 188/499: loss=0.4465306541210473\n",
      "GD iter. 189/499: loss=0.4465302400435204\n",
      "GD iter. 190/499: loss=0.44652984328043516\n",
      "GD iter. 191/499: loss=0.44652946309399283\n",
      "GD iter. 192/499: loss=0.4465290987783811\n",
      "GD iter. 193/499: loss=0.4465287496583664\n",
      "GD iter. 194/499: loss=0.446528415087948\n",
      "GD iter. 195/499: loss=0.44652809444907116\n",
      "GD iter. 196/499: loss=0.44652778715039976\n",
      "GD iter. 197/499: loss=0.4465274926261429\n",
      "GD iter. 198/499: loss=0.44652721033493253\n",
      "GD iter. 199/499: loss=0.4465269397587531\n",
      "GD iter. 200/499: loss=0.4465266804019178\n",
      "GD iter. 201/499: loss=0.4465264317900898\n",
      "GD iter. 202/499: loss=0.44652619346934835\n",
      "GD iter. 203/499: loss=0.446525965005295\n",
      "GD iter. 204/499: loss=0.446525745982201\n",
      "GD iter. 205/499: loss=0.44652553600219086\n",
      "GD iter. 206/499: loss=0.4465253346844632\n",
      "GD iter. 207/499: loss=0.4465251416645464\n",
      "GD iter. 208/499: loss=0.446524956593586\n",
      "GD iter. 209/499: loss=0.4465247791376642\n",
      "GD iter. 210/499: loss=0.44652460897715046\n",
      "GD iter. 211/499: loss=0.446524445806079\n",
      "GD iter. 212/499: loss=0.4465242893315555\n",
      "GD iter. 213/499: loss=0.44652413927318824\n",
      "GD iter. 214/499: loss=0.4465239953625464\n",
      "GD iter. 215/499: loss=0.44652385734264033\n",
      "GD iter. 216/499: loss=0.44652372496742565\n",
      "GD iter. 217/499: loss=0.44652359800132946\n",
      "GD iter. 218/499: loss=0.44652347621879657\n",
      "GD iter. 219/499: loss=0.4465233594038563\n",
      "GD iter. 220/499: loss=0.44652324734970816\n",
      "GD iter. 221/499: loss=0.44652313985832537\n",
      "GD iter. 222/499: loss=0.4465230367400769\n",
      "GD iter. 223/499: loss=0.44652293781336394\n",
      "GD iter. 224/499: loss=0.44652284290427496\n",
      "GD iter. 225/499: loss=0.44652275184625395\n",
      "GD iter. 226/499: loss=0.44652266447978384\n",
      "GD iter. 227/499: loss=0.4465225806520836\n",
      "GD iter. 228/499: loss=0.4465225002168192\n",
      "GD iter. 229/499: loss=0.44652242303382583\n",
      "GD iter. 230/499: loss=0.446522348968844\n",
      "GD iter. 231/499: loss=0.44652227789326593\n",
      "GD iter. 232/499: loss=0.44652220968389295\n",
      "GD iter. 233/499: loss=0.4465221442227046\n",
      "GD iter. 234/499: loss=0.4465220813966359\n",
      "GD iter. 235/499: loss=0.4465220210973671\n",
      "GD iter. 236/499: loss=0.44652196322111926\n",
      "GD iter. 237/499: loss=0.44652190766846117\n",
      "GD iter. 238/499: loss=0.446521854344124\n",
      "GD iter. 239/499: loss=0.446521803156823\n",
      "GD iter. 240/499: loss=0.44652175401908906\n",
      "GD iter. 241/499: loss=0.44652170684710485\n",
      "GD iter. 242/499: loss=0.44652166156055056\n",
      "GD iter. 243/499: loss=0.4465216180824548\n",
      "GD iter. 244/499: loss=0.44652157633905215\n",
      "GD iter. 245/499: loss=0.4465215362596476\n",
      "GD iter. 246/499: loss=0.44652149777648603\n",
      "GD iter. 247/499: loss=0.4465214608246274\n",
      "GD iter. 248/499: loss=0.44652142534182837\n",
      "GD iter. 249/499: loss=0.4465213912684272\n",
      "GD iter. 250/499: loss=0.4465213585472352\n",
      "GD iter. 251/499: loss=0.446521327123432\n",
      "GD iter. 252/499: loss=0.4465212969444659\n",
      "GD iter. 253/499: loss=0.44652126795995783\n",
      "GD iter. 254/499: loss=0.4465212401216099\n",
      "GD iter. 255/499: loss=0.44652121338311757\n",
      "GD iter. 256/499: loss=0.44652118770008636\n",
      "GD iter. 257/499: loss=0.44652116302995093\n",
      "GD iter. 258/499: loss=0.44652113933189813\n",
      "GD iter. 259/499: loss=0.4465211165667942\n",
      "GD iter. 260/499: loss=0.4465210946971137\n",
      "GD iter. 261/499: loss=0.4465210736868722\n",
      "GD iter. 262/499: loss=0.4465210535015617\n",
      "GD iter. 263/499: loss=0.4465210341080892\n",
      "GD iter. 264/499: loss=0.4465210154747171\n",
      "GD iter. 265/499: loss=0.44652099757100683\n",
      "GD iter. 266/499: loss=0.44652098036776433\n",
      "GD iter. 267/499: loss=0.4465209638369888\n",
      "GD iter. 268/499: loss=0.44652094795182184\n",
      "GD iter. 269/499: loss=0.44652093268650106\n",
      "GD iter. 270/499: loss=0.44652091801631394\n",
      "GD iter. 271/499: loss=0.4465209039175539\n",
      "GD iter. 272/499: loss=0.4465208903674789\n",
      "GD iter. 273/499: loss=0.44652087734427165\n",
      "GD iter. 274/499: loss=0.44652086482700026\n",
      "GD iter. 275/499: loss=0.4465208527955828\n",
      "GD iter. 276/499: loss=0.44652084123075125\n",
      "GD iter. 277/499: loss=0.44652083011401783\n",
      "GD iter. 278/499: loss=0.4465208194276437\n",
      "GD iter. 279/499: loss=0.44652080915460657\n",
      "GD iter. 280/499: loss=0.44652079927857224\n",
      "GD iter. 281/499: loss=0.44652078978386583\n",
      "GD iter. 282/499: loss=0.4465207806554447\n",
      "GD iter. 283/499: loss=0.4465207718788721\n",
      "GD iter. 284/499: loss=0.44652076344029307\n",
      "GD iter. 285/499: loss=0.4465207553264093\n",
      "GD iter. 286/499: loss=0.44652074752445764\n",
      "GD iter. 287/499: loss=0.4465207400221866\n",
      "GD iter. 288/499: loss=0.4465207328078372\n",
      "GD iter. 289/499: loss=0.44652072587012104\n",
      "GD iter. 290/499: loss=0.4465207191982023\n",
      "GD iter. 291/499: loss=0.4465207127816786\n",
      "GD iter. 292/499: loss=0.4465207066105635\n",
      "GD iter. 293/499: loss=0.4465207006752692\n",
      "GD iter. 294/499: loss=0.4465206949665911\n",
      "GD iter. 295/499: loss=0.446520689475691\n",
      "GD iter. 296/499: loss=0.44652068419408336\n",
      "GD iter. 297/499: loss=0.4465206791136201\n",
      "GD iter. 298/499: loss=0.44652067422647745\n",
      "GD iter. 299/499: loss=0.44652066952514213\n",
      "GD iter. 300/499: loss=0.4465206650023995\n",
      "GD iter. 301/499: loss=0.44652066065132084\n",
      "GD iter. 302/499: loss=0.4465206564652526\n",
      "GD iter. 303/499: loss=0.44652065243780364\n",
      "GD iter. 304/499: loss=0.44652064856283685\n",
      "GD iter. 305/499: loss=0.44652064483445664\n",
      "GD iter. 306/499: loss=0.44652064124700086\n",
      "GD iter. 307/499: loss=0.446520637795031\n",
      "GD iter. 308/499: loss=0.44652063447332224\n",
      "GD iter. 309/499: loss=0.4465206312768561\n",
      "GD iter. 310/499: loss=0.4465206282008114\n",
      "GD iter. 311/499: loss=0.44652062524055663\n",
      "GD iter. 312/499: loss=0.4465206223916421\n",
      "GD iter. 313/499: loss=0.44652061964979284\n",
      "GD iter. 314/499: loss=0.4465206170109017\n",
      "GD iter. 315/499: loss=0.4465206144710219\n",
      "GD iter. 316/499: loss=0.4465206120263619\n",
      "GD iter. 317/499: loss=0.44652060967327806\n",
      "GD iter. 318/499: loss=0.4465206074082695\n",
      "GD iter. 319/499: loss=0.4465206052279712\n",
      "GD iter. 320/499: loss=0.44652060312915026\n",
      "GD iter. 321/499: loss=0.44652060110869946\n",
      "GD iter. 322/499: loss=0.4465205991636326\n",
      "GD iter. 323/499: loss=0.4465205972910798\n",
      "GD iter. 324/499: loss=0.4465205954882827\n",
      "GD iter. 325/499: loss=0.44652059375258984\n",
      "GD iter. 326/499: loss=0.44652059208145295\n",
      "GD iter. 327/499: loss=0.4465205904724226\n",
      "GD iter. 328/499: loss=0.446520588923144\n",
      "GD iter. 329/499: loss=0.446520587431354\n",
      "GD iter. 330/499: loss=0.446520585994876\n",
      "GD iter. 331/499: loss=0.44652058461161864\n",
      "GD iter. 332/499: loss=0.4465205832795707\n",
      "GD iter. 333/499: loss=0.4465205819967988\n",
      "GD iter. 334/499: loss=0.44652058076144385\n",
      "GD iter. 335/499: loss=0.4465205795717183\n",
      "GD iter. 336/499: loss=0.44652057842590376\n",
      "GD iter. 337/499: loss=0.4465205773223477\n",
      "GD iter. 338/499: loss=0.4465205762594609\n",
      "GD iter. 339/499: loss=0.4465205752357153\n",
      "GD iter. 340/499: loss=0.44652057424964153\n",
      "GD iter. 341/499: loss=0.4465205732998263\n",
      "GD iter. 342/499: loss=0.4465205723849108\n",
      "GD iter. 343/499: loss=0.44652057150358754\n",
      "GD iter. 344/499: loss=0.44652057065459977\n",
      "GD iter. 345/499: loss=0.446520569836738\n",
      "GD iter. 346/499: loss=0.4465205690488396\n",
      "GD iter. 347/499: loss=0.44652056828978554\n",
      "GD iter. 348/499: loss=0.4465205675585002\n",
      "GD iter. 349/499: loss=0.4465205668539482\n",
      "GD iter. 350/499: loss=0.4465205661751339\n",
      "GD iter. 351/499: loss=0.4465205655210992\n",
      "GD iter. 352/499: loss=0.44652056489092296\n",
      "GD iter. 353/499: loss=0.44652056428371834\n",
      "GD iter. 354/499: loss=0.4465205636986324\n",
      "GD iter. 355/499: loss=0.4465205631348446\n",
      "GD iter. 356/499: loss=0.44652056259156503\n",
      "GD iter. 357/499: loss=0.4465205620680344\n",
      "GD iter. 358/499: loss=0.44652056156352127\n",
      "GD iter. 359/499: loss=0.4465205610773223\n",
      "GD iter. 360/499: loss=0.4465205606087606\n",
      "GD iter. 361/499: loss=0.44652056015718466\n",
      "GD iter. 362/499: loss=0.4465205597219677\n",
      "GD iter. 363/499: loss=0.4465205593025063\n",
      "GD iter. 364/499: loss=0.4465205588982199\n",
      "GD iter. 365/499: loss=0.44652055850854977\n",
      "GD iter. 366/499: loss=0.446520558132958\n",
      "GD iter. 367/499: loss=0.4465205577709269\n",
      "GD iter. 368/499: loss=0.44652055742195834\n",
      "GD iter. 369/499: loss=0.44652055708557264\n",
      "GD iter. 370/499: loss=0.446520556761308\n",
      "GD iter. 371/499: loss=0.4465205564487203\n",
      "GD iter. 372/499: loss=0.44652055614738156\n",
      "GD iter. 373/499: loss=0.44652055585687994\n",
      "GD iter. 374/499: loss=0.446520555576819\n",
      "GD iter. 375/499: loss=0.4465205553068172\n",
      "GD iter. 376/499: loss=0.44652055504650684\n",
      "GD iter. 377/499: loss=0.4465205547955343\n",
      "GD iter. 378/499: loss=0.4465205545535589\n",
      "GD iter. 379/499: loss=0.44652055432025256\n",
      "GD iter. 380/499: loss=0.44652055409529945\n",
      "GD iter. 381/499: loss=0.44652055387839557\n",
      "GD iter. 382/499: loss=0.4465205536692482\n",
      "GD iter. 383/499: loss=0.4465205534675752\n",
      "GD iter. 384/499: loss=0.4465205532731051\n",
      "GD iter. 385/499: loss=0.44652055308557637\n",
      "GD iter. 386/499: loss=0.44652055290473724\n",
      "GD iter. 387/499: loss=0.44652055273034513\n",
      "GD iter. 388/499: loss=0.44652055256216644\n",
      "GD iter. 389/499: loss=0.4465205523999763\n",
      "GD iter. 390/499: loss=0.4465205522435579\n",
      "GD iter. 391/499: loss=0.44652055209270286\n",
      "GD iter. 392/499: loss=0.44652055194720974\n",
      "GD iter. 393/499: loss=0.44652055180688527\n",
      "GD iter. 394/499: loss=0.44652055167154264\n",
      "GD iter. 395/499: loss=0.44652055154100245\n",
      "GD iter. 396/499: loss=0.44652055141509145\n",
      "GD iter. 397/499: loss=0.446520551293643\n",
      "GD iter. 398/499: loss=0.4465205511764964\n",
      "GD iter. 399/499: loss=0.44652055106349703\n",
      "GD iter. 400/499: loss=0.4465205509544958\n",
      "GD iter. 401/499: loss=0.446520550849349\n",
      "GD iter. 402/499: loss=0.4465205507479183\n",
      "GD iter. 403/499: loss=0.44652055065007035\n",
      "GD iter. 404/499: loss=0.4465205505556768\n",
      "GD iter. 405/499: loss=0.44652055046461375\n",
      "GD iter. 406/499: loss=0.4465205503767619\n",
      "GD iter. 407/499: loss=0.4465205502920064\n",
      "GD iter. 408/499: loss=0.4465205502102365\n",
      "GD iter. 409/499: loss=0.4465205501313455\n",
      "GD iter. 410/499: loss=0.4465205500552306\n",
      "GD iter. 411/499: loss=0.4465205499817924\n",
      "GD iter. 412/499: loss=0.4465205499109358\n",
      "GD iter. 413/499: loss=0.4465205498425685\n",
      "GD iter. 414/499: loss=0.4465205497766018\n",
      "GD iter. 415/499: loss=0.44652054971295035\n",
      "GD iter. 416/499: loss=0.4465205496515315\n",
      "GD iter. 417/499: loss=0.446520549592266\n",
      "GD iter. 418/499: loss=0.44652054953507725\n",
      "GD iter. 419/499: loss=0.4465205494798914\n",
      "GD iter. 420/499: loss=0.44652054942663744\n",
      "GD iter. 421/499: loss=0.44652054937524677\n",
      "GD iter. 422/499: loss=0.44652054932565327\n",
      "GD iter. 423/499: loss=0.44652054927779317\n",
      "GD iter. 424/499: loss=0.44652054923160506\n",
      "GD iter. 425/499: loss=0.4465205491870299\n",
      "GD iter. 426/499: loss=0.4465205491440104\n",
      "GD iter. 427/499: loss=0.4465205491024916\n",
      "GD iter. 428/499: loss=0.44652054906242056\n",
      "GD iter. 429/499: loss=0.446520549023746\n",
      "GD iter. 430/499: loss=0.4465205489864186\n",
      "GD iter. 431/499: loss=0.4465205489503908\n",
      "GD iter. 432/499: loss=0.4465205489156169\n",
      "GD iter. 433/499: loss=0.4465205488820526\n",
      "GD iter. 434/499: loss=0.44652054884965525\n",
      "GD iter. 435/499: loss=0.44652054881838377\n",
      "GD iter. 436/499: loss=0.4465205487881985\n",
      "GD iter. 437/499: loss=0.44652054875906133\n",
      "GD iter. 438/499: loss=0.44652054873093533\n",
      "GD iter. 439/499: loss=0.44652054870378494\n",
      "GD iter. 440/499: loss=0.44652054867757596\n",
      "GD iter. 441/499: loss=0.4465205486522753\n",
      "GD iter. 442/499: loss=0.44652054862785107\n",
      "GD iter. 443/499: loss=0.44652054860427265\n",
      "GD iter. 444/499: loss=0.44652054858151025\n",
      "GD iter. 445/499: loss=0.4465205485595353\n",
      "GD iter. 446/499: loss=0.44652054853832035\n",
      "GD iter. 447/499: loss=0.44652054851783857\n",
      "GD iter. 448/499: loss=0.4465205484980644\n",
      "GD iter. 449/499: loss=0.4465205484789733\n",
      "GD iter. 450/499: loss=0.4465205484605413\n",
      "GD iter. 451/499: loss=0.446520548442745\n",
      "GD iter. 452/499: loss=0.44652054842556294\n",
      "GD iter. 453/499: loss=0.44652054840897304\n",
      "GD iter. 454/499: loss=0.44652054839295485\n",
      "GD iter. 455/499: loss=0.44652054837748856\n",
      "GD iter. 456/499: loss=0.4465205483625548\n",
      "GD iter. 457/499: loss=0.4465205483481351\n",
      "GD iter. 458/499: loss=0.4465205483342114\n",
      "GD iter. 459/499: loss=0.44652054832076665\n",
      "GD iter. 460/499: loss=0.44652054830778404\n",
      "GD iter. 461/499: loss=0.4465205482952477\n",
      "GD iter. 462/499: loss=0.4465205482831419\n",
      "GD iter. 463/499: loss=0.4465205482714519\n",
      "GD iter. 464/499: loss=0.4465205482601629\n",
      "GD iter. 465/499: loss=0.4465205482492614\n",
      "GD iter. 466/499: loss=0.44652054823873377\n",
      "GD iter. 467/499: loss=0.44652054822856696\n",
      "GD iter. 468/499: loss=0.44652054821874865\n",
      "GD iter. 469/499: loss=0.4465205482092667\n",
      "GD iter. 470/499: loss=0.4465205482001094\n",
      "GD iter. 471/499: loss=0.4465205481912656\n",
      "GD iter. 472/499: loss=0.4465205481827244\n",
      "GD iter. 473/499: loss=0.44652054817447545\n",
      "GD iter. 474/499: loss=0.44652054816650844\n",
      "GD iter. 475/499: loss=0.44652054815881376\n",
      "GD iter. 476/499: loss=0.4465205481513821\n",
      "GD iter. 477/499: loss=0.44652054814420405\n",
      "GD iter. 478/499: loss=0.4465205481372712\n",
      "GD iter. 479/499: loss=0.44652054813057496\n",
      "GD iter. 480/499: loss=0.44652054812410713\n",
      "GD iter. 481/499: loss=0.44652054811786\n",
      "GD iter. 482/499: loss=0.4465205481118258\n",
      "GD iter. 483/499: loss=0.4465205481059972\n",
      "GD iter. 484/499: loss=0.44652054810036723\n",
      "GD iter. 485/499: loss=0.44652054809492897\n",
      "GD iter. 486/499: loss=0.44652054808967584\n",
      "GD iter. 487/499: loss=0.44652054808460157\n",
      "GD iter. 488/499: loss=0.44652054807969993\n",
      "GD iter. 489/499: loss=0.446520548074965\n",
      "GD iter. 490/499: loss=0.44652054807039093\n",
      "GD iter. 491/499: loss=0.4465205480659724\n",
      "GD iter. 492/499: loss=0.446520548061704\n",
      "GD iter. 493/499: loss=0.4465205480575807\n",
      "GD iter. 494/499: loss=0.4465205480535972\n",
      "GD iter. 495/499: loss=0.44652054804974894\n",
      "GD iter. 496/499: loss=0.44652054804603136\n",
      "GD iter. 497/499: loss=0.44652054804243974\n",
      "GD iter. 498/499: loss=0.44652054803896996\n",
      "GD iter. 499/499: loss=0.44652054803561775\n",
      "The Accuracy is: 0.8456\n",
      "The F1 score is: 0.4471\n",
      "The precision is: 0.3363\n",
      "The recall is: 0.6667\n",
      "GD iter. 0/499: loss=0.6921015217944188\n",
      "GD iter. 1/499: loss=0.5424063666916873\n",
      "GD iter. 2/499: loss=0.527730347693296\n",
      "GD iter. 3/499: loss=0.5178835851993852\n",
      "GD iter. 4/499: loss=0.5104263542195537\n",
      "GD iter. 5/499: loss=0.5044465182732808\n",
      "GD iter. 6/499: loss=0.4994735771194914\n",
      "GD iter. 7/499: loss=0.4952258055880943\n",
      "GD iter. 8/499: loss=0.4915237566494123\n",
      "GD iter. 9/499: loss=0.48824795116002306\n",
      "GD iter. 10/499: loss=0.4853155562135034\n",
      "GD iter. 11/499: loss=0.4826669578157294\n",
      "GD iter. 12/499: loss=0.48025777997294355\n",
      "GD iter. 13/499: loss=0.47805399066729876\n",
      "GD iter. 14/499: loss=0.47602880588812224\n",
      "GD iter. 15/499: loss=0.47416067081938823\n",
      "GD iter. 16/499: loss=0.4724319047633978\n",
      "GD iter. 17/499: loss=0.4708277664910044\n",
      "GD iter. 18/499: loss=0.46933579305736783\n",
      "GD iter. 19/499: loss=0.4679453210331334\n",
      "GD iter. 20/499: loss=0.4666471323403772\n",
      "GD iter. 21/499: loss=0.4654331871165804\n",
      "GD iter. 22/499: loss=0.46429641863166565\n",
      "GD iter. 23/499: loss=0.463230573305598\n",
      "GD iter. 24/499: loss=0.46223008408938665\n",
      "GD iter. 25/499: loss=0.461289968931016\n",
      "GD iter. 26/499: loss=0.4604057483853327\n",
      "GD iter. 27/499: loss=0.45957337803513965\n",
      "GD iter. 28/499: loss=0.4587891925160272\n",
      "GD iter. 29/499: loss=0.458049858737373\n",
      "GD iter. 30/499: loss=0.45735233646905255\n",
      "GD iter. 31/499: loss=0.45669384488557146\n",
      "GD iter. 32/499: loss=0.4560718339721787\n",
      "GD iter. 33/499: loss=0.45548395993215285\n",
      "GD iter. 34/499: loss=0.4549280639124143\n",
      "GD iter. 35/499: loss=0.4544021535009958\n",
      "GD iter. 36/499: loss=0.4539043865554451\n",
      "GD iter. 37/499: loss=0.4534330570036488\n",
      "GD iter. 38/499: loss=0.4529865823234633\n",
      "GD iter. 39/499: loss=0.45256349245906446\n",
      "GD iter. 40/499: loss=0.45216241997312134\n",
      "GD iter. 41/499: loss=0.45178209126708646\n",
      "GD iter. 42/499: loss=0.45142131872879604\n",
      "GD iter. 43/499: loss=0.4510789936885221\n",
      "GD iter. 44/499: loss=0.45075408008263845\n",
      "GD iter. 45/499: loss=0.45044560873893125\n",
      "GD iter. 46/499: loss=0.45015267220993227\n",
      "GD iter. 47/499: loss=0.4498744200909518\n",
      "GD iter. 48/499: loss=0.4496100547681233\n",
      "GD iter. 49/499: loss=0.44935882754905027\n",
      "GD iter. 50/499: loss=0.4491200351348006\n",
      "GD iter. 51/499: loss=0.44889301639722984\n",
      "GD iter. 52/499: loss=0.4486771494300792\n",
      "GD iter. 53/499: loss=0.448471848846125\n",
      "GD iter. 54/499: loss=0.44827656329594356\n",
      "GD iter. 55/499: loss=0.44809077318670165\n",
      "GD iter. 56/499: loss=0.447913988581838\n",
      "GD iter. 57/499: loss=0.44774574726464506\n",
      "GD iter. 58/499: loss=0.4475856129506183\n",
      "GD iter. 59/499: loss=0.447433173635072\n",
      "GD iter. 60/499: loss=0.4472880400639473\n",
      "GD iter. 61/499: loss=0.44714984431699334\n",
      "GD iter. 62/499: loss=0.4470182384936061\n",
      "GD iter. 63/499: loss=0.44689289349258693\n",
      "GD iter. 64/499: loss=0.44677349787794746\n",
      "GD iter. 65/499: loss=0.44665975682365183\n",
      "GD iter. 66/499: loss=0.4465513911308717\n",
      "GD iter. 67/499: loss=0.44644813631193203\n",
      "GD iter. 68/499: loss=0.44634974173567177\n",
      "GD iter. 69/499: loss=0.44625596982942484\n",
      "GD iter. 70/499: loss=0.44616659533326036\n",
      "GD iter. 71/499: loss=0.44608140460251255\n",
      "GD iter. 72/499: loss=0.4460001949549749\n",
      "GD iter. 73/499: loss=0.4459227740594548\n",
      "GD iter. 74/499: loss=0.445848959362662\n",
      "GD iter. 75/499: loss=0.44577857755166245\n",
      "GD iter. 76/499: loss=0.44571146404936046\n",
      "GD iter. 77/499: loss=0.44564746254068033\n",
      "GD iter. 78/499: loss=0.4455864245273096\n",
      "GD iter. 79/499: loss=0.44552820890903533\n",
      "GD iter. 80/499: loss=0.4454726815898634\n",
      "GD iter. 81/499: loss=0.44541971510725314\n",
      "GD iter. 82/499: loss=0.44536918828292515\n",
      "GD iter. 83/499: loss=0.4453209858938233\n",
      "GD iter. 84/499: loss=0.4452749983619165\n",
      "GD iter. 85/499: loss=0.4452311214616232\n",
      "GD iter. 86/499: loss=0.44518925604373444\n",
      "GD iter. 87/499: loss=0.4451493077747918\n",
      "GD iter. 88/499: loss=0.44511118689095175\n",
      "GD iter. 89/499: loss=0.44507480796543797\n",
      "GD iter. 90/499: loss=0.44504008968874703\n",
      "GD iter. 91/499: loss=0.4450069546608299\n",
      "GD iter. 92/499: loss=0.4449753291945268\n",
      "GD iter. 93/499: loss=0.44494514312958205\n",
      "GD iter. 94/499: loss=0.44491632965661115\n",
      "GD iter. 95/499: loss=0.44488882515043393\n",
      "GD iter. 96/499: loss=0.44486256901222787\n",
      "GD iter. 97/499: loss=0.44483750351998996\n",
      "GD iter. 98/499: loss=0.4448135736868305\n",
      "GD iter. 99/499: loss=0.44479072712665074\n",
      "GD iter. 100/499: loss=0.44476891392678797\n",
      "GD iter. 101/499: loss=0.4447480865272344\n",
      "GD iter. 102/499: loss=0.444728199606064\n",
      "GD iter. 103/499: loss=0.4447092099707228\n",
      "GD iter. 104/499: loss=0.44469107645485806\n",
      "GD iter. 105/499: loss=0.4446737598203856\n",
      "GD iter. 106/499: loss=0.44465722266450775\n",
      "GD iter. 107/499: loss=0.4446414293314138\n",
      "GD iter. 108/499: loss=0.4446263458284146\n",
      "GD iter. 109/499: loss=0.4446119397462693\n",
      "GD iter. 110/499: loss=0.44459818018348446\n",
      "GD iter. 111/499: loss=0.44458503767437374\n",
      "GD iter. 112/499: loss=0.4445724841206812\n",
      "GD iter. 113/499: loss=0.44456049272657944\n",
      "GD iter. 114/499: loss=0.4445490379368699\n",
      "GD iter. 115/499: loss=0.444538095378215\n",
      "GD iter. 116/499: loss=0.4445276418032487\n",
      "GD iter. 117/499: loss=0.44451765503741625\n",
      "GD iter. 118/499: loss=0.44450811392840167\n",
      "GD iter. 119/499: loss=0.44449899829801426\n",
      "GD iter. 120/499: loss=0.4444902888964056\n",
      "GD iter. 121/499: loss=0.44448196735850126\n",
      "GD iter. 122/499: loss=0.44447401616253446\n",
      "GD iter. 123/499: loss=0.4444664185905767\n",
      "GD iter. 124/499: loss=0.44445915869096403\n",
      "GD iter. 125/499: loss=0.4444522212425254\n",
      "GD iter. 126/499: loss=0.4444455917205235\n",
      "GD iter. 127/499: loss=0.4444392562642213\n",
      "GD iter. 128/499: loss=0.4444332016459969\n",
      "GD iter. 129/499: loss=0.44442741524192625\n",
      "GD iter. 130/499: loss=0.4444218850037656\n",
      "GD iter. 131/499: loss=0.4444165994322621\n",
      "GD iter. 132/499: loss=0.44441154755172846\n",
      "GD iter. 133/499: loss=0.44440671888582084\n",
      "GD iter. 134/499: loss=0.44440210343445913\n",
      "GD iter. 135/499: loss=0.44439769165183657\n",
      "GD iter. 136/499: loss=0.4443934744254641\n",
      "GD iter. 137/499: loss=0.44438944305620093\n",
      "GD iter. 138/499: loss=0.44438558923922106\n",
      "GD iter. 139/499: loss=0.44438190504587477\n",
      "GD iter. 140/499: loss=0.44437838290639753\n",
      "GD iter. 141/499: loss=0.444375015593429\n",
      "GD iter. 142/499: loss=0.44437179620630174\n",
      "GD iter. 143/499: loss=0.44436871815606316\n",
      "GD iter. 144/499: loss=0.444365775151196\n",
      "GD iter. 145/499: loss=0.44436296118400354\n",
      "GD iter. 146/499: loss=0.44436027051762794\n",
      "GD iter. 147/499: loss=0.4443576976736721\n",
      "GD iter. 148/499: loss=0.4443552374203965\n",
      "GD iter. 149/499: loss=0.44435288476146256\n",
      "GD iter. 150/499: loss=0.4443506349251985\n",
      "GD iter. 151/499: loss=0.444348483354361\n",
      "GD iter. 152/499: loss=0.4443464256963714\n",
      "GD iter. 153/499: loss=0.44434445779400294\n",
      "GD iter. 154/499: loss=0.4443425756764963\n",
      "GD iter. 155/499: loss=0.44434077555108753\n",
      "GD iter. 156/499: loss=0.4443390537949242\n",
      "GD iter. 157/499: loss=0.44433740694735485\n",
      "GD iter. 158/499: loss=0.44433583170257324\n",
      "GD iter. 159/499: loss=0.4443343249026\n",
      "GD iter. 160/499: loss=0.44433288353058753\n",
      "GD iter. 161/499: loss=0.44433150470443195\n",
      "GD iter. 162/499: loss=0.4443301856706777\n",
      "GD iter. 163/499: loss=0.4443289237987018\n",
      "GD iter. 164/499: loss=0.44432771657516495\n",
      "GD iter. 165/499: loss=0.44432656159871514\n",
      "GD iter. 166/499: loss=0.4443254565749353\n",
      "GD iter. 167/499: loss=0.44432439931152057\n",
      "GD iter. 168/499: loss=0.44432338771367635\n",
      "GD iter. 169/499: loss=0.44432241977972636\n",
      "GD iter. 170/499: loss=0.4443214935969208\n",
      "GD iter. 171/499: loss=0.4443206073374356\n",
      "GD iter. 172/499: loss=0.4443197592545546\n",
      "GD iter. 173/499: loss=0.4443189476790239\n",
      "GD iter. 174/499: loss=0.44431817101557447\n",
      "GD iter. 175/499: loss=0.4443174277396001\n",
      "GD iter. 176/499: loss=0.44431671639398884\n",
      "GD iter. 177/499: loss=0.4443160355860962\n",
      "GD iter. 178/499: loss=0.44431538398485754\n",
      "GD iter. 179/499: loss=0.4443147603180294\n",
      "GD iter. 180/499: loss=0.44431416336955837\n",
      "GD iter. 181/499: loss=0.4443135919770669\n",
      "GD iter. 182/499: loss=0.4443130450294537\n",
      "GD iter. 183/499: loss=0.44431252146460354\n",
      "GD iter. 184/499: loss=0.4443120202671992\n",
      "GD iter. 185/499: loss=0.44431154046663324\n",
      "GD iter. 186/499: loss=0.44431108113501416\n",
      "GD iter. 187/499: loss=0.44431064138526166\n",
      "GD iter. 188/499: loss=0.4443102203692888\n",
      "GD iter. 189/499: loss=0.44430981727626556\n",
      "GD iter. 190/499: loss=0.4443094313309611\n",
      "GD iter. 191/499: loss=0.4443090617921603\n",
      "GD iter. 192/499: loss=0.4443087079511515\n",
      "GD iter. 193/499: loss=0.44430836913028265\n",
      "GD iter. 194/499: loss=0.44430804468158214\n",
      "GD iter. 195/499: loss=0.444307733985442\n",
      "GD iter. 196/499: loss=0.4443074364493593\n",
      "GD iter. 197/499: loss=0.4443071515067354\n",
      "GD iter. 198/499: loss=0.4443068786157275\n",
      "GD iter. 199/499: loss=0.44430661725815357\n",
      "GD iter. 200/499: loss=0.4443063669384445\n",
      "GD iter. 201/499: loss=0.44430612718264395\n",
      "GD iter. 202/499: loss=0.44430589753745403\n",
      "GD iter. 203/499: loss=0.44430567756932154\n",
      "GD iter. 204/499: loss=0.44430546686356687\n",
      "GD iter. 205/499: loss=0.44430526502355083\n",
      "GD iter. 206/499: loss=0.4443050716698796\n",
      "GD iter. 207/499: loss=0.4443048864396438\n",
      "GD iter. 208/499: loss=0.4443047089856933\n",
      "GD iter. 209/499: loss=0.4443045389759428\n",
      "GD iter. 210/499: loss=0.4443043760927087\n",
      "GD iter. 211/499: loss=0.4443042200320767\n",
      "GD iter. 212/499: loss=0.4443040705032963\n",
      "GD iter. 213/499: loss=0.44430392722820206\n",
      "GD iter. 214/499: loss=0.4443037899406618\n",
      "GD iter. 215/499: loss=0.44430365838604846\n",
      "GD iter. 216/499: loss=0.4443035323207356\n",
      "GD iter. 217/499: loss=0.4443034115116155\n",
      "GD iter. 218/499: loss=0.4443032957356382\n",
      "GD iter. 219/499: loss=0.4443031847793717\n",
      "GD iter. 220/499: loss=0.44430307843858075\n",
      "GD iter. 221/499: loss=0.44430297651782524\n",
      "GD iter. 222/499: loss=0.4443028788300761\n",
      "GD iter. 223/499: loss=0.44430278519634686\n",
      "GD iter. 224/499: loss=0.4443026954453443\n",
      "GD iter. 225/499: loss=0.4443026094131317\n",
      "GD iter. 226/499: loss=0.4443025269428088\n",
      "GD iter. 227/499: loss=0.44430244788420514\n",
      "GD iter. 228/499: loss=0.4443023720935872\n",
      "GD iter. 229/499: loss=0.44430229943337796\n",
      "GD iter. 230/499: loss=0.4443022297718897\n",
      "GD iter. 231/499: loss=0.444302162983068\n",
      "GD iter. 232/499: loss=0.44430209894624667\n",
      "GD iter. 233/499: loss=0.4443020375459144\n",
      "GD iter. 234/499: loss=0.444301978671491\n",
      "GD iter. 235/499: loss=0.4443019222171138\n",
      "GD iter. 236/499: loss=0.44430186808143285\n",
      "GD iter. 237/499: loss=0.44430181616741615\n",
      "GD iter. 238/499: loss=0.4443017663821622\n",
      "GD iter. 239/499: loss=0.4443017186367222\n",
      "GD iter. 240/499: loss=0.44430167284592836\n",
      "GD iter. 241/499: loss=0.4443016289282315\n",
      "GD iter. 242/499: loss=0.44430158680554394\n",
      "GD iter. 243/499: loss=0.44430154640309116\n",
      "GD iter. 244/499: loss=0.44430150764926857\n",
      "GD iter. 245/499: loss=0.44430147047550467\n",
      "GD iter. 246/499: loss=0.44430143481613105\n",
      "GD iter. 247/499: loss=0.4443014006082573\n",
      "GD iter. 248/499: loss=0.4443013677916513\n",
      "GD iter. 249/499: loss=0.44430133630862545\n",
      "GD iter. 250/499: loss=0.4443013061039275\n",
      "GD iter. 251/499: loss=0.44430127712463524\n",
      "GD iter. 252/499: loss=0.44430124932005743\n",
      "GD iter. 253/499: loss=0.4443012226416383\n",
      "GD iter. 254/499: loss=0.44430119704286525\n",
      "GD iter. 255/499: loss=0.4443011724791823\n",
      "GD iter. 256/499: loss=0.44430114890790584\n",
      "GD iter. 257/499: loss=0.444301126288145\n",
      "GD iter. 258/499: loss=0.4443011045807247\n",
      "GD iter. 259/499: loss=0.44430108374811295\n",
      "GD iter. 260/499: loss=0.44430106375435036\n",
      "GD iter. 261/499: loss=0.44430104456498315\n",
      "GD iter. 262/499: loss=0.4443010261469991\n",
      "GD iter. 263/499: loss=0.44430100846876663\n",
      "GD iter. 264/499: loss=0.4443009914999751\n",
      "GD iter. 265/499: loss=0.44430097521157985\n",
      "GD iter. 266/499: loss=0.4443009595757475\n",
      "GD iter. 267/499: loss=0.4443009445658047\n",
      "GD iter. 268/499: loss=0.4443009301561893\n",
      "GD iter. 269/499: loss=0.4443009163224032\n",
      "GD iter. 270/499: loss=0.444300903040967\n",
      "GD iter. 271/499: loss=0.444300890289377\n",
      "GD iter. 272/499: loss=0.44430087804606405\n",
      "GD iter. 273/499: loss=0.4443008662903541\n",
      "GD iter. 274/499: loss=0.4443008550024298\n",
      "GD iter. 275/499: loss=0.44430084416329524\n",
      "GD iter. 276/499: loss=0.44430083375474083\n",
      "GD iter. 277/499: loss=0.4443008237593103\n",
      "GD iter. 278/499: loss=0.44430081416026845\n",
      "GD iter. 279/499: loss=0.44430080494157176\n",
      "GD iter. 280/499: loss=0.4443007960878384\n",
      "GD iter. 281/499: loss=0.4443007875843212\n",
      "GD iter. 282/499: loss=0.44430077941688034\n",
      "GD iter. 283/499: loss=0.44430077157195813\n",
      "GD iter. 284/499: loss=0.44430076403655494\n",
      "GD iter. 285/499: loss=0.4443007567982052\n",
      "GD iter. 286/499: loss=0.4443007498449553\n",
      "GD iter. 287/499: loss=0.4443007431653425\n",
      "GD iter. 288/499: loss=0.44430073674837395\n",
      "GD iter. 289/499: loss=0.4443007305835071\n",
      "GD iter. 290/499: loss=0.4443007246606314\n",
      "GD iter. 291/499: loss=0.4443007189700494\n",
      "GD iter. 292/499: loss=0.4443007135024606\n",
      "GD iter. 293/499: loss=0.4443007082489442\n",
      "GD iter. 294/499: loss=0.44430070320094306\n",
      "GD iter. 295/499: loss=0.4443006983502496\n",
      "GD iter. 296/499: loss=0.4443006936889903\n",
      "GD iter. 297/499: loss=0.4443006892096122\n",
      "GD iter. 298/499: loss=0.4443006849048695\n",
      "GD iter. 299/499: loss=0.44430068076781143\n",
      "GD iter. 300/499: loss=0.4443006767917688\n",
      "GD iter. 301/499: loss=0.4443006729703437\n",
      "GD iter. 302/499: loss=0.4443006692973971\n",
      "GD iter. 303/499: loss=0.444300665767039\n",
      "GD iter. 304/499: loss=0.4443006623736177\n",
      "GD iter. 305/499: loss=0.44430065911171024\n",
      "GD iter. 306/499: loss=0.4443006559761127\n",
      "GD iter. 307/499: loss=0.4443006529618313\n",
      "GD iter. 308/499: loss=0.44430065006407365\n",
      "GD iter. 309/499: loss=0.4443006472782409\n",
      "GD iter. 310/499: loss=0.44430064459991925\n",
      "GD iter. 311/499: loss=0.44430064202487235\n",
      "GD iter. 312/499: loss=0.4443006395490346\n",
      "GD iter. 313/499: loss=0.44430063716850343\n",
      "GD iter. 314/499: loss=0.44430063487953303\n",
      "GD iter. 315/499: loss=0.44430063267852793\n",
      "GD iter. 316/499: loss=0.44430063056203684\n",
      "GD iter. 317/499: loss=0.44430062852674657\n",
      "GD iter. 318/499: loss=0.4443006265694765\n",
      "GD iter. 319/499: loss=0.444300624687173\n",
      "GD iter. 320/499: loss=0.44430062287690497\n",
      "GD iter. 321/499: loss=0.44430062113585783\n",
      "GD iter. 322/499: loss=0.4443006194613293\n",
      "GD iter. 323/499: loss=0.4443006178507251\n",
      "GD iter. 324/499: loss=0.44430061630155376\n",
      "GD iter. 325/499: loss=0.4443006148114232\n",
      "GD iter. 326/499: loss=0.4443006133780365\n",
      "GD iter. 327/499: loss=0.4443006119991878\n",
      "GD iter. 328/499: loss=0.44430061067275883\n",
      "GD iter. 329/499: loss=0.44430060939671545\n",
      "GD iter. 330/499: loss=0.44430060816910383\n",
      "GD iter. 331/499: loss=0.44430060698804746\n",
      "GD iter. 332/499: loss=0.4443006058517445\n",
      "GD iter. 333/499: loss=0.4443006047584639\n",
      "GD iter. 334/499: loss=0.44430060370654284\n",
      "GD iter. 335/499: loss=0.4443006026943846\n",
      "GD iter. 336/499: loss=0.4443006017204551\n",
      "GD iter. 337/499: loss=0.4443006007832805\n",
      "GD iter. 338/499: loss=0.4443005998814452\n",
      "GD iter. 339/499: loss=0.4443005990135894\n",
      "GD iter. 340/499: loss=0.44430059817840634\n",
      "GD iter. 341/499: loss=0.44430059737464084\n",
      "GD iter. 342/499: loss=0.4443005966010869\n",
      "GD iter. 343/499: loss=0.4443005958565854\n",
      "GD iter. 344/499: loss=0.4443005951400233\n",
      "GD iter. 345/499: loss=0.44430059445033065\n",
      "GD iter. 346/499: loss=0.4443005937864791\n",
      "GD iter. 347/499: loss=0.4443005931474813\n",
      "GD iter. 348/499: loss=0.4443005925323874\n",
      "GD iter. 349/499: loss=0.4443005919402854\n",
      "GD iter. 350/499: loss=0.4443005913702983\n",
      "GD iter. 351/499: loss=0.4443005908215836\n",
      "GD iter. 352/499: loss=0.4443005902933312\n",
      "GD iter. 353/499: loss=0.4443005897847629\n",
      "GD iter. 354/499: loss=0.44430058929513044\n",
      "GD iter. 355/499: loss=0.4443005888237146\n",
      "GD iter. 356/499: loss=0.44430058836982395\n",
      "GD iter. 357/499: loss=0.4443005879327943\n",
      "GD iter. 358/499: loss=0.44430058751198664\n",
      "GD iter. 359/499: loss=0.4443005871067869\n",
      "GD iter. 360/499: loss=0.4443005867166046\n",
      "GD iter. 361/499: loss=0.44430058634087244\n",
      "GD iter. 362/499: loss=0.44430058597904437\n",
      "GD iter. 363/499: loss=0.44430058563059593\n",
      "GD iter. 364/499: loss=0.44430058529502253\n",
      "GD iter. 365/499: loss=0.4443005849718392\n",
      "GD iter. 366/499: loss=0.44430058466057953\n",
      "GD iter. 367/499: loss=0.44430058436079495\n",
      "GD iter. 368/499: loss=0.4443005840720542\n",
      "GD iter. 369/499: loss=0.44430058379394266\n",
      "GD iter. 370/499: loss=0.44430058352606117\n",
      "GD iter. 371/499: loss=0.44430058326802646\n",
      "GD iter. 372/499: loss=0.4443005830194695\n",
      "GD iter. 373/499: loss=0.44430058278003554\n",
      "GD iter. 374/499: loss=0.44430058254938304\n",
      "GD iter. 375/499: loss=0.44430058232718417\n",
      "GD iter. 376/499: loss=0.4443005821131228\n",
      "GD iter. 377/499: loss=0.4443005819068956\n",
      "GD iter. 378/499: loss=0.44430058170821013\n",
      "GD iter. 379/499: loss=0.4443005815167857\n",
      "GD iter. 380/499: loss=0.4443005813323515\n",
      "GD iter. 381/499: loss=0.4443005811546478\n",
      "GD iter. 382/499: loss=0.4443005809834243\n",
      "GD iter. 383/499: loss=0.4443005808184403\n",
      "GD iter. 384/499: loss=0.4443005806594642\n",
      "GD iter. 385/499: loss=0.44430058050627297\n",
      "GD iter. 386/499: loss=0.44430058035865233\n",
      "GD iter. 387/499: loss=0.44430058021639623\n",
      "GD iter. 388/499: loss=0.4443005800793057\n",
      "GD iter. 389/499: loss=0.44430057994718997\n",
      "GD iter. 390/499: loss=0.44430057981986515\n",
      "GD iter. 391/499: loss=0.4443005796971544\n",
      "GD iter. 392/499: loss=0.44430057957888736\n",
      "GD iter. 393/499: loss=0.44430057946490026\n",
      "GD iter. 394/499: loss=0.4443005793550355\n",
      "GD iter. 395/499: loss=0.4443005792491411\n",
      "GD iter. 396/499: loss=0.44430057914707116\n",
      "GD iter. 397/499: loss=0.4443005790486849\n",
      "GD iter. 398/499: loss=0.4443005789538474\n",
      "GD iter. 399/499: loss=0.444300578862428\n",
      "GD iter. 400/499: loss=0.4443005787743015\n",
      "GD iter. 401/499: loss=0.4443005786893472\n",
      "GD iter. 402/499: loss=0.44430057860744876\n",
      "GD iter. 403/499: loss=0.44430057852849464\n",
      "GD iter. 404/499: loss=0.4443005784523768\n",
      "GD iter. 405/499: loss=0.4443005783789917\n",
      "GD iter. 406/499: loss=0.4443005783082396\n",
      "GD iter. 407/499: loss=0.4443005782400244\n",
      "GD iter. 408/499: loss=0.44430057817425345\n",
      "GD iter. 409/499: loss=0.44430057811083784\n",
      "GD iter. 410/499: loss=0.44430057804969153\n",
      "GD iter. 411/499: loss=0.4443005779907321\n",
      "GD iter. 412/499: loss=0.44430057793388006\n",
      "GD iter. 413/499: loss=0.44430057787905874\n",
      "GD iter. 414/499: loss=0.4443005778261944\n",
      "GD iter. 415/499: loss=0.444300577775216\n",
      "GD iter. 416/499: loss=0.4443005777260552\n",
      "GD iter. 417/499: loss=0.44430057767864595\n",
      "GD iter. 418/499: loss=0.4443005776329249\n",
      "GD iter. 419/499: loss=0.444300577588831\n",
      "GD iter. 420/499: loss=0.4443005775463054\n",
      "GD iter. 421/499: loss=0.4443005775052914\n",
      "GD iter. 422/499: loss=0.44430057746573426\n",
      "GD iter. 423/499: loss=0.44430057742758156\n",
      "GD iter. 424/499: loss=0.4443005773907824\n",
      "GD iter. 425/499: loss=0.44430057735528816\n",
      "GD iter. 426/499: loss=0.44430057732105194\n",
      "GD iter. 427/499: loss=0.4443005772880282\n",
      "GD iter. 428/499: loss=0.4443005772561735\n",
      "GD iter. 429/499: loss=0.44430057722544564\n",
      "GD iter. 430/499: loss=0.4443005771958043\n",
      "GD iter. 431/499: loss=0.4443005771672103\n",
      "GD iter. 432/499: loss=0.444300577139626\n",
      "GD iter. 433/499: loss=0.4443005771130154\n",
      "GD iter. 434/499: loss=0.4443005770873435\n",
      "GD iter. 435/499: loss=0.4443005770625767\n",
      "GD iter. 436/499: loss=0.44430057703868264\n",
      "GD iter. 437/499: loss=0.44430057701563\n",
      "GD iter. 438/499: loss=0.4443005769933888\n",
      "GD iter. 439/499: loss=0.4443005769719299\n",
      "GD iter. 440/499: loss=0.4443005769512254\n",
      "GD iter. 441/499: loss=0.44430057693124847\n",
      "GD iter. 442/499: loss=0.44430057691197317\n",
      "GD iter. 443/499: loss=0.4443005768933744\n",
      "GD iter. 444/499: loss=0.44430057687542807\n",
      "GD iter. 445/499: loss=0.4443005768581111\n",
      "GD iter. 446/499: loss=0.44430057684140084\n",
      "GD iter. 447/499: loss=0.44430057682527596\n",
      "GD iter. 448/499: loss=0.4443005768097154\n",
      "GD iter. 449/499: loss=0.44430057679469936\n",
      "GD iter. 450/499: loss=0.4443005767802085\n",
      "GD iter. 451/499: loss=0.44430057676622403\n",
      "GD iter. 452/499: loss=0.44430057675272805\n",
      "GD iter. 453/499: loss=0.44430057673970347\n",
      "GD iter. 454/499: loss=0.44430057672713313\n",
      "GD iter. 455/499: loss=0.4443005767150015\n",
      "GD iter. 456/499: loss=0.4443005767032927\n",
      "GD iter. 457/499: loss=0.44430057669199186\n",
      "GD iter. 458/499: loss=0.44430057668108464\n",
      "GD iter. 459/499: loss=0.44430057667055706\n",
      "GD iter. 460/499: loss=0.4443005766603958\n",
      "GD iter. 461/499: loss=0.44430057665058775\n",
      "GD iter. 462/499: loss=0.44430057664112066\n",
      "GD iter. 463/499: loss=0.4443005766319826\n",
      "GD iter. 464/499: loss=0.4443005766231617\n",
      "GD iter. 465/499: loss=0.44430057661464695\n",
      "GD iter. 466/499: loss=0.44430057660642763\n",
      "GD iter. 467/499: loss=0.44430057659849326\n",
      "GD iter. 468/499: loss=0.44430057659083394\n",
      "GD iter. 469/499: loss=0.44430057658343985\n",
      "GD iter. 470/499: loss=0.44430057657630184\n",
      "GD iter. 471/499: loss=0.4443005765694108\n",
      "GD iter. 472/499: loss=0.4443005765627582\n",
      "GD iter. 473/499: loss=0.44430057655633565\n",
      "GD iter. 474/499: loss=0.44430057655013494\n",
      "GD iter. 475/499: loss=0.4443005765441485\n",
      "GD iter. 476/499: loss=0.4443005765383688\n",
      "GD iter. 477/499: loss=0.4443005765327884\n",
      "GD iter. 478/499: loss=0.44430057652740057\n",
      "GD iter. 479/499: loss=0.4443005765221986\n",
      "GD iter. 480/499: loss=0.4443005765171759\n",
      "GD iter. 481/499: loss=0.44430057651232624\n",
      "GD iter. 482/499: loss=0.4443005765076436\n",
      "GD iter. 483/499: loss=0.44430057650312205\n",
      "GD iter. 484/499: loss=0.4443005764987561\n",
      "GD iter. 485/499: loss=0.4443005764945402\n",
      "GD iter. 486/499: loss=0.4443005764904694\n",
      "GD iter. 487/499: loss=0.4443005764865385\n",
      "GD iter. 488/499: loss=0.4443005764827424\n",
      "GD iter. 489/499: loss=0.4443005764790766\n",
      "GD iter. 490/499: loss=0.44430057647553683\n",
      "GD iter. 491/499: loss=0.44430057647211824\n",
      "GD iter. 492/499: loss=0.444300576468817\n",
      "GD iter. 493/499: loss=0.4443005764656289\n",
      "GD iter. 494/499: loss=0.44430057646255\n",
      "GD iter. 495/499: loss=0.44430057645957655\n",
      "GD iter. 496/499: loss=0.4443005764567048\n",
      "GD iter. 497/499: loss=0.44430057645393145\n",
      "GD iter. 498/499: loss=0.44430057645125276\n",
      "GD iter. 499/499: loss=0.4443005764486659\n",
      "The Accuracy is: 0.8292\n",
      "The F1 score is: 0.4222\n",
      "The precision is: 0.3363\n",
      "The recall is: 0.5672\n",
      "GD iter. 0/499: loss=0.7113857012133933\n",
      "GD iter. 1/499: loss=0.541998351426433\n",
      "GD iter. 2/499: loss=0.5283883546466732\n",
      "GD iter. 3/499: loss=0.5191754631650971\n",
      "GD iter. 4/499: loss=0.5121193148996126\n",
      "GD iter. 5/499: loss=0.5064018444867422\n",
      "GD iter. 6/499: loss=0.5016093806597913\n",
      "GD iter. 7/499: loss=0.4974931000282703\n",
      "GD iter. 8/499: loss=0.4938924276713785\n",
      "GD iter. 9/499: loss=0.49069885668871743\n",
      "GD iter. 10/499: loss=0.48783604944873366\n",
      "GD iter. 11/499: loss=0.48524830711002825\n",
      "GD iter. 12/499: loss=0.4828936582346584\n",
      "GD iter. 13/499: loss=0.4807395794500196\n",
      "GD iter. 14/499: loss=0.47876025824471485\n",
      "GD iter. 15/499: loss=0.476934785939934\n",
      "GD iter. 16/499: loss=0.4752459286557962\n",
      "GD iter. 17/499: loss=0.4736792681291628\n",
      "GD iter. 18/499: loss=0.47222258594223154\n",
      "GD iter. 19/499: loss=0.47086541222402184\n",
      "GD iter. 20/499: loss=0.46959868821829776\n",
      "GD iter. 21/499: loss=0.4684145094453422\n",
      "GD iter. 22/499: loss=0.4673059270587202\n",
      "GD iter. 23/499: loss=0.4662667919857605\n",
      "GD iter. 24/499: loss=0.4652916310347763\n",
      "GD iter. 25/499: loss=0.4643755472382572\n",
      "GD iter. 26/499: loss=0.46351413881600684\n",
      "GD iter. 27/499: loss=0.4627034326178555\n",
      "GD iter. 28/499: loss=0.4619398289524745\n",
      "GD iter. 29/499: loss=0.46122005546273426\n",
      "GD iter. 30/499: loss=0.4605411282583854\n",
      "GD iter. 31/499: loss=0.4599003189235393\n",
      "GD iter. 32/499: loss=0.459295126320364\n",
      "GD iter. 33/499: loss=0.45872325233988365\n",
      "GD iter. 34/499: loss=0.4581825809257063\n",
      "GD iter. 35/499: loss=0.4576711598310164\n",
      "GD iter. 36/499: loss=0.45718718467350855\n",
      "GD iter. 37/499: loss=0.45672898493448105\n",
      "GD iter. 38/499: loss=0.4562950116125482\n",
      "GD iter. 39/499: loss=0.45588382629339297\n",
      "GD iter. 40/499: loss=0.4554940914377011\n",
      "GD iter. 41/499: loss=0.4551245617221788\n",
      "GD iter. 42/499: loss=0.45477407629507877\n",
      "GD iter. 43/499: loss=0.454441551829275\n",
      "GD iter. 44/499: loss=0.4541259762736502\n",
      "GD iter. 45/499: loss=0.4538264032181775\n",
      "GD iter. 46/499: loss=0.4535419468001974\n",
      "GD iter. 47/499: loss=0.453271777089508\n",
      "GD iter. 48/499: loss=0.4530151158983602\n",
      "GD iter. 49/499: loss=0.45277123296959204\n",
      "GD iter. 50/499: loss=0.4525394425021846\n",
      "GD iter. 51/499: loss=0.4523190999786606\n",
      "GD iter. 52/499: loss=0.4521095992631435\n",
      "GD iter. 53/499: loss=0.4519103699426539\n",
      "GD iter. 54/499: loss=0.45172087488746826\n",
      "GD iter. 55/499: loss=0.4515406080091619\n",
      "GD iter. 56/499: loss=0.45136909219738636\n",
      "GD iter. 57/499: loss=0.45120587741854545\n",
      "GD iter. 58/499: loss=0.45105053896137426\n",
      "GD iter. 59/499: loss=0.45090267581603544\n",
      "GD iter. 60/499: loss=0.45076190917476816\n",
      "GD iter. 61/499: loss=0.4506278810433591\n",
      "GD iter. 62/499: loss=0.4505002529538077\n",
      "GD iter. 63/499: loss=0.45037870476952657\n",
      "GD iter. 64/499: loss=0.45026293357527236\n",
      "GD iter. 65/499: loss=0.4501526526447675\n",
      "GD iter. 66/499: loss=0.4500475904796488\n",
      "GD iter. 67/499: loss=0.4499474899139814\n",
      "GD iter. 68/499: loss=0.4498521072791177\n",
      "GD iter. 69/499: loss=0.4497612116241579\n",
      "GD iter. 70/499: loss=0.4496745839877059\n",
      "GD iter. 71/499: loss=0.44959201671699267\n",
      "GD iter. 72/499: loss=0.44951331283079576\n",
      "GD iter. 73/499: loss=0.4494382854228872\n",
      "GD iter. 74/499: loss=0.4493667571030309\n",
      "GD iter. 75/499: loss=0.4492985594727979\n",
      "GD iter. 76/499: loss=0.4492335326337046\n",
      "GD iter. 77/499: loss=0.4491715247253764\n",
      "GD iter. 78/499: loss=0.44911239149163984\n",
      "GD iter. 79/499: loss=0.44905599587260303\n",
      "GD iter. 80/499: loss=0.44900220762095094\n",
      "GD iter. 81/499: loss=0.44895090294081313\n",
      "GD iter. 82/499: loss=0.4489019641476956\n",
      "GD iter. 83/499: loss=0.4488552793480824\n",
      "GD iter. 84/499: loss=0.44881074213741873\n",
      "GD iter. 85/499: loss=0.448768251315285\n",
      "GD iter. 86/499: loss=0.44872771061665945\n",
      "GD iter. 87/499: loss=0.4486890284582492\n",
      "GD iter. 88/499: loss=0.4486521176989415\n",
      "GD iter. 89/499: loss=0.4486168954134982\n",
      "GD iter. 90/499: loss=0.4485832826786756\n",
      "GD iter. 91/499: loss=0.44855120437101276\n",
      "GD iter. 92/499: loss=0.44852058897557984\n",
      "GD iter. 93/499: loss=0.4484913684050324\n",
      "GD iter. 94/499: loss=0.44846347782835666\n",
      "GD iter. 95/499: loss=0.44843685550873313\n",
      "GD iter. 96/499: loss=0.44841144264999105\n",
      "GD iter. 97/499: loss=0.4483871832511481\n",
      "GD iter. 98/499: loss=0.4483640239685761\n",
      "GD iter. 99/499: loss=0.44834191398535356\n",
      "GD iter. 100/499: loss=0.44832080488739917\n",
      "GD iter. 101/499: loss=0.4483006505460046\n",
      "GD iter. 102/499: loss=0.4482814070064088\n",
      "GD iter. 103/499: loss=0.4482630323820766\n",
      "GD iter. 104/499: loss=0.4482454867543718\n",
      "GD iter. 105/499: loss=0.4482287320773222\n",
      "GD iter. 106/499: loss=0.4482127320872067\n",
      "GD iter. 107/499: loss=0.4481974522166974\n",
      "GD iter. 108/499: loss=0.44818285951331427\n",
      "GD iter. 109/499: loss=0.4481689225619625\n",
      "GD iter. 110/499: loss=0.44815561141133275\n",
      "GD iter. 111/499: loss=0.4481428975039613\n",
      "GD iter. 112/499: loss=0.4481307536097571\n",
      "GD iter. 113/499: loss=0.44811915376281386\n",
      "GD iter. 114/499: loss=0.44810807320133555\n",
      "GD iter. 115/499: loss=0.4480974883105143\n",
      "GD iter. 116/499: loss=0.4480873765682073\n",
      "GD iter. 117/499: loss=0.44807771649326866\n",
      "GD iter. 118/499: loss=0.4480684875964015\n",
      "GD iter. 119/499: loss=0.4480596703333989\n",
      "GD iter. 120/499: loss=0.44805124606065516\n",
      "GD iter. 121/499: loss=0.44804319699283046\n",
      "GD iter. 122/499: loss=0.44803550616256066\n",
      "GD iter. 123/499: loss=0.44802815738210866\n",
      "GD iter. 124/499: loss=0.4480211352068629\n",
      "GD iter. 125/499: loss=0.4480144249005856\n",
      "GD iter. 126/499: loss=0.44800801240232874\n",
      "GD iter. 127/499: loss=0.44800188429493126\n",
      "GD iter. 128/499: loss=0.44799602777502134\n",
      "GD iter. 129/499: loss=0.44799043062444766\n",
      "GD iter. 130/499: loss=0.44798508118307157\n",
      "GD iter. 131/499: loss=0.4479799683228503\n",
      "GD iter. 132/499: loss=0.44797508142315157\n",
      "GD iter. 133/499: loss=0.4479704103472352\n",
      "GD iter. 134/499: loss=0.4479659454198495\n",
      "GD iter. 135/499: loss=0.4479616774058829\n",
      "GD iter. 136/499: loss=0.4479575974900257\n",
      "GD iter. 137/499: loss=0.447953697257387\n",
      "GD iter. 138/499: loss=0.44794996867502473\n",
      "GD iter. 139/499: loss=0.44794640407434333\n",
      "GD iter. 140/499: loss=0.4479429961343172\n",
      "GD iter. 141/499: loss=0.44793973786549995\n",
      "GD iter. 142/499: loss=0.4479366225947831\n",
      "GD iter. 143/499: loss=0.44793364395086727\n",
      "GD iter. 144/499: loss=0.44793079585041035\n",
      "GD iter. 145/499: loss=0.4479280724848247\n",
      "GD iter. 146/499: loss=0.44792546830768737\n",
      "GD iter. 147/499: loss=0.44792297802273784\n",
      "GD iter. 148/499: loss=0.4479205965724332\n",
      "GD iter. 149/499: loss=0.4479183191270358\n",
      "GD iter. 150/499: loss=0.447916141074207\n",
      "GD iter. 151/499: loss=0.4479140580090837\n",
      "GD iter. 152/499: loss=0.4479120657248149\n",
      "GD iter. 153/499: loss=0.4479101602035362\n",
      "GD iter. 154/499: loss=0.44790833760776183\n",
      "GD iter. 155/499: loss=0.44790659427217444\n",
      "GD iter. 156/499: loss=0.4479049266957939\n",
      "GD iter. 157/499: loss=0.44790333153450784\n",
      "GD iter. 158/499: loss=0.44790180559394643\n",
      "GD iter. 159/499: loss=0.4479003458226847\n",
      "GD iter. 160/499: loss=0.44789894930575874\n",
      "GD iter. 161/499: loss=0.44789761325847965\n",
      "GD iter. 162/499: loss=0.44789633502053156\n",
      "GD iter. 163/499: loss=0.4478951120503412\n",
      "GD iter. 164/499: loss=0.44789394191970533\n",
      "GD iter. 165/499: loss=0.44789282230866506\n",
      "GD iter. 166/499: loss=0.4478917510006145\n",
      "GD iter. 167/499: loss=0.4478907258776344\n",
      "GD iter. 168/499: loss=0.4478897449160374\n",
      "GD iter. 169/499: loss=0.4478888061821195\n",
      "GD iter. 170/499: loss=0.4478879078281034\n",
      "GD iter. 171/499: loss=0.4478870480882688\n",
      "GD iter. 172/499: loss=0.447886225275258\n",
      "GD iter. 173/499: loss=0.4478854377765508\n",
      "GD iter. 174/499: loss=0.4478846840511004\n",
      "GD iter. 175/499: loss=0.4478839626261204\n",
      "GD iter. 176/499: loss=0.44788327209402173\n",
      "GD iter. 177/499: loss=0.447882611109485\n",
      "GD iter. 178/499: loss=0.44788197838666866\n",
      "GD iter. 179/499: loss=0.4478813726965428\n",
      "GD iter. 180/499: loss=0.44788079286434446\n",
      "GD iter. 181/499: loss=0.4478802377671478\n",
      "GD iter. 182/499: loss=0.44787970633154567\n",
      "GD iter. 183/499: loss=0.4478791975314346\n",
      "GD iter. 184/499: loss=0.4478787103859015\n",
      "GD iter. 185/499: loss=0.4478782439572054\n",
      "GD iter. 186/499: loss=0.44787779734885047\n",
      "GD iter. 187/499: loss=0.4478773697037467\n",
      "GD iter. 188/499: loss=0.4478769602024528\n",
      "GD iter. 189/499: loss=0.44787656806149895\n",
      "GD iter. 190/499: loss=0.4478761925317858\n",
      "GD iter. 191/499: loss=0.44787583289705435\n",
      "GD iter. 192/499: loss=0.447875488472426\n",
      "GD iter. 193/499: loss=0.4478751586030077\n",
      "GD iter. 194/499: loss=0.4478748426625606\n",
      "GD iter. 195/499: loss=0.4478745400522275\n",
      "GD iter. 196/499: loss=0.44787425019931854\n",
      "GD iter. 197/499: loss=0.44787397255615113\n",
      "GD iter. 198/499: loss=0.4478737065989419\n",
      "GD iter. 199/499: loss=0.44787345182674815\n",
      "GD iter. 200/499: loss=0.44787320776045775\n",
      "GD iter. 201/499: loss=0.44787297394182307\n",
      "GD iter. 202/499: loss=0.44787274993253934\n",
      "GD iter. 203/499: loss=0.44787253531336324\n",
      "GD iter. 204/499: loss=0.4478723296832718\n",
      "GD iter. 205/499: loss=0.4478721326586587\n",
      "GD iter. 206/499: loss=0.4478719438725661\n",
      "GD iter. 207/499: loss=0.4478717629739516\n",
      "GD iter. 208/499: loss=0.44787158962698653\n",
      "GD iter. 209/499: loss=0.4478714235103878\n",
      "GD iter. 210/499: loss=0.447871264316777\n",
      "GD iter. 211/499: loss=0.44787111175206973\n",
      "GD iter. 212/499: loss=0.44787096553489236\n",
      "GD iter. 213/499: loss=0.4478708253960232\n",
      "GD iter. 214/499: loss=0.4478706910778597\n",
      "GD iter. 215/499: loss=0.44787056233390954\n",
      "GD iter. 216/499: loss=0.44787043892830325\n",
      "GD iter. 217/499: loss=0.4478703206353298\n",
      "GD iter. 218/499: loss=0.44787020723899157\n",
      "GD iter. 219/499: loss=0.4478700985325799\n",
      "GD iter. 220/499: loss=0.4478699943182693\n",
      "GD iter. 221/499: loss=0.44786989440672953\n",
      "GD iter. 222/499: loss=0.44786979861675474\n",
      "GD iter. 223/499: loss=0.447869706774909\n",
      "GD iter. 224/499: loss=0.4478696187151884\n",
      "GD iter. 225/499: loss=0.44786953427869663\n",
      "GD iter. 226/499: loss=0.4478694533133359\n",
      "GD iter. 227/499: loss=0.44786937567351126\n",
      "GD iter. 228/499: loss=0.44786930121984797\n",
      "GD iter. 229/499: loss=0.4478692298189216\n",
      "GD iter. 230/499: loss=0.447869161342999\n",
      "GD iter. 231/499: loss=0.44786909566979205\n",
      "GD iter. 232/499: loss=0.44786903268222183\n",
      "GD iter. 233/499: loss=0.4478689722681922\n",
      "GD iter. 234/499: loss=0.44786891432037473\n",
      "GD iter. 235/499: loss=0.44786885873600224\n",
      "GD iter. 236/499: loss=0.44786880541667173\n",
      "GD iter. 237/499: loss=0.44786875426815564\n",
      "GD iter. 238/499: loss=0.4478687052002217\n",
      "GD iter. 239/499: loss=0.44786865812646093\n",
      "GD iter. 240/499: loss=0.4478686129641221\n",
      "GD iter. 241/499: loss=0.44786856963395516\n",
      "GD iter. 242/499: loss=0.4478685280600591\n",
      "GD iter. 243/499: loss=0.44786848816973956\n",
      "GD iter. 244/499: loss=0.4478684498933698\n",
      "GD iter. 245/499: loss=0.44786841316425907\n",
      "GD iter. 246/499: loss=0.44786837791852735\n",
      "GD iter. 247/499: loss=0.4478683440949838\n",
      "GD iter. 248/499: loss=0.44786831163501206\n",
      "GD iter. 249/499: loss=0.4478682804824598\n",
      "GD iter. 250/499: loss=0.44786825058353347\n",
      "GD iter. 251/499: loss=0.44786822188669717\n",
      "GD iter. 252/499: loss=0.44786819434257613\n",
      "GD iter. 253/499: loss=0.44786816790386474\n",
      "GD iter. 254/499: loss=0.4478681425252379\n",
      "GD iter. 255/499: loss=0.4478681181632668\n",
      "GD iter. 256/499: loss=0.4478680947763378\n",
      "GD iter. 257/499: loss=0.4478680723245757\n",
      "GD iter. 258/499: loss=0.44786805076976893\n",
      "GD iter. 259/499: loss=0.4478680300753\n",
      "GD iter. 260/499: loss=0.4478680102060765\n",
      "GD iter. 261/499: loss=0.44786799112846704\n",
      "GD iter. 262/499: loss=0.44786797281023955\n",
      "GD iter. 263/499: loss=0.44786795522050094\n",
      "GD iter. 264/499: loss=0.44786793832964183\n",
      "GD iter. 265/499: loss=0.44786792210928067\n",
      "GD iter. 266/499: loss=0.44786790653221303\n",
      "GD iter. 267/499: loss=0.4478678915723613\n",
      "GD iter. 268/499: loss=0.44786787720472687\n",
      "GD iter. 269/499: loss=0.4478678634053456\n",
      "GD iter. 270/499: loss=0.447867850151243\n",
      "GD iter. 271/499: loss=0.44786783742039316\n",
      "GD iter. 272/499: loss=0.447867825191679\n",
      "GD iter. 273/499: loss=0.4478678134448535\n",
      "GD iter. 274/499: loss=0.4478678021605033\n",
      "GD iter. 275/499: loss=0.4478677913200141\n",
      "GD iter. 276/499: loss=0.44786778090553697\n",
      "GD iter. 277/499: loss=0.4478677708999555\n",
      "GD iter. 278/499: loss=0.4478677612868564\n",
      "GD iter. 279/499: loss=0.44786775205049884\n",
      "GD iter. 280/499: loss=0.44786774317578726\n",
      "GD iter. 281/499: loss=0.44786773464824386\n",
      "GD iter. 282/499: loss=0.447867726453983\n",
      "GD iter. 283/499: loss=0.4478677185796864\n",
      "GD iter. 284/499: loss=0.4478677110125796\n",
      "GD iter. 285/499: loss=0.447867703740409\n",
      "GD iter. 286/499: loss=0.4478676967514209\n",
      "GD iter. 287/499: loss=0.4478676900343397\n",
      "GD iter. 288/499: loss=0.44786768357834883\n",
      "GD iter. 289/499: loss=0.44786767737307154\n",
      "GD iter. 290/499: loss=0.4478676714085521\n",
      "GD iter. 291/499: loss=0.44786766567523917\n",
      "GD iter. 292/499: loss=0.4478676601639686\n",
      "GD iter. 293/499: loss=0.4478676548659471\n",
      "GD iter. 294/499: loss=0.4478676497727377\n",
      "GD iter. 295/499: loss=0.44786764487624436\n",
      "GD iter. 296/499: loss=0.44786764016869834\n",
      "GD iter. 297/499: loss=0.44786763564264404\n",
      "GD iter. 298/499: loss=0.44786763129092694\n",
      "GD iter. 299/499: loss=0.44786762710668065\n",
      "GD iter. 300/499: loss=0.44786762308331535\n",
      "GD iter. 301/499: loss=0.44786761921450596\n",
      "GD iter. 302/499: loss=0.4478676154941818\n",
      "GD iter. 303/499: loss=0.447867611916516\n",
      "GD iter. 304/499: loss=0.4478676084759152\n",
      "GD iter. 305/499: loss=0.4478676051670105\n",
      "GD iter. 306/499: loss=0.44786760198464776\n",
      "GD iter. 307/499: loss=0.44786759892387906\n",
      "GD iter. 308/499: loss=0.44786759597995424\n",
      "GD iter. 309/499: loss=0.4478675931483132\n",
      "GD iter. 310/499: loss=0.44786759042457713\n",
      "GD iter. 311/499: loss=0.4478675878045422\n",
      "GD iter. 312/499: loss=0.4478675852841722\n",
      "GD iter. 313/499: loss=0.4478675828595911\n",
      "GD iter. 314/499: loss=0.44786758052707704\n",
      "GD iter. 315/499: loss=0.4478675782830564\n",
      "GD iter. 316/499: loss=0.4478675761240968\n",
      "GD iter. 317/499: loss=0.4478675740469024\n",
      "GD iter. 318/499: loss=0.4478675720483076\n",
      "GD iter. 319/499: loss=0.4478675701252725\n",
      "GD iter. 320/499: loss=0.4478675682748769\n",
      "GD iter. 321/499: loss=0.4478675664943164\n",
      "GD iter. 322/499: loss=0.44786756478089695\n",
      "GD iter. 323/499: loss=0.447867563132031\n",
      "GD iter. 324/499: loss=0.4478675615452329\n",
      "GD iter. 325/499: loss=0.44786756001811484\n",
      "GD iter. 326/499: loss=0.4478675585483828\n",
      "GD iter. 327/499: loss=0.4478675571338329\n",
      "GD iter. 328/499: loss=0.4478675557723481\n",
      "GD iter. 329/499: loss=0.44786755446189414\n",
      "GD iter. 330/499: loss=0.4478675532005163\n",
      "GD iter. 331/499: loss=0.4478675519863369\n",
      "GD iter. 332/499: loss=0.44786755081755114\n",
      "GD iter. 333/499: loss=0.4478675496924252\n",
      "GD iter. 334/499: loss=0.44786754860929273\n",
      "GD iter. 335/499: loss=0.4478675475665525\n",
      "GD iter. 336/499: loss=0.44786754656266575\n",
      "GD iter. 337/499: loss=0.4478675455961532\n",
      "GD iter. 338/499: loss=0.44786754466559386\n",
      "GD iter. 339/499: loss=0.44786754376962135\n",
      "GD iter. 340/499: loss=0.44786754290692266\n",
      "GD iter. 341/499: loss=0.44786754207623564\n",
      "GD iter. 342/499: loss=0.44786754127634715\n",
      "GD iter. 343/499: loss=0.44786754050609084\n",
      "GD iter. 344/499: loss=0.44786753976434596\n",
      "GD iter. 345/499: loss=0.4478675390500343\n",
      "GD iter. 346/499: loss=0.4478675383621202\n",
      "GD iter. 347/499: loss=0.44786753769960724\n",
      "GD iter. 348/499: loss=0.44786753706153803\n",
      "GD iter. 349/499: loss=0.44786753644699157\n",
      "GD iter. 350/499: loss=0.4478675358550825\n",
      "GD iter. 351/499: loss=0.44786753528495954\n",
      "GD iter. 352/499: loss=0.44786753473580415\n",
      "GD iter. 353/499: loss=0.4478675342068292\n",
      "GD iter. 354/499: loss=0.44786753369727755\n",
      "GD iter. 355/499: loss=0.4478675332064216\n",
      "GD iter. 356/499: loss=0.4478675327335608\n",
      "GD iter. 357/499: loss=0.4478675322780222\n",
      "GD iter. 358/499: loss=0.447867531839158\n",
      "GD iter. 359/499: loss=0.4478675314163455\n",
      "GD iter. 360/499: loss=0.4478675310089856\n",
      "GD iter. 361/499: loss=0.4478675306165021\n",
      "GD iter. 362/499: loss=0.4478675302383405\n",
      "GD iter. 363/499: loss=0.4478675298739679\n",
      "GD iter. 364/499: loss=0.4478675295228713\n",
      "GD iter. 365/499: loss=0.4478675291845572\n",
      "GD iter. 366/499: loss=0.4478675288585508\n",
      "GD iter. 367/499: loss=0.4478675285443957\n",
      "GD iter. 368/499: loss=0.4478675282416523\n",
      "GD iter. 369/499: loss=0.44786752794989776\n",
      "GD iter. 370/499: loss=0.44786752766872556\n",
      "GD iter. 371/499: loss=0.44786752739774416\n",
      "GD iter. 372/499: loss=0.4478675271365768\n",
      "GD iter. 373/499: loss=0.4478675268848613\n",
      "GD iter. 374/499: loss=0.4478675266422488\n",
      "GD iter. 375/499: loss=0.4478675264084037\n",
      "GD iter. 376/499: loss=0.4478675261830032\n",
      "GD iter. 377/499: loss=0.4478675259657362\n",
      "GD iter. 378/499: loss=0.44786752575630373\n",
      "GD iter. 379/499: loss=0.4478675255544176\n",
      "GD iter. 380/499: loss=0.4478675253598011\n",
      "GD iter. 381/499: loss=0.44786752517218714\n",
      "GD iter. 382/499: loss=0.44786752499131915\n",
      "GD iter. 383/499: loss=0.44786752481694986\n",
      "GD iter. 384/499: loss=0.4478675246488415\n",
      "GD iter. 385/499: loss=0.447867524486765\n",
      "GD iter. 386/499: loss=0.4478675243304997\n",
      "GD iter. 387/499: loss=0.44786752417983355\n",
      "GD iter. 388/499: loss=0.44786752403456237\n",
      "GD iter. 389/499: loss=0.447867523894489\n",
      "GD iter. 390/499: loss=0.4478675237594242\n",
      "GD iter. 391/499: loss=0.44786752362918575\n",
      "GD iter. 392/499: loss=0.4478675235035979\n",
      "GD iter. 393/499: loss=0.44786752338249136\n",
      "GD iter. 394/499: loss=0.4478675232657037\n",
      "GD iter. 395/499: loss=0.44786752315307793\n",
      "GD iter. 396/499: loss=0.4478675230444631\n",
      "GD iter. 397/499: loss=0.44786752293971377\n",
      "GD iter. 398/499: loss=0.4478675228386899\n",
      "GD iter. 399/499: loss=0.4478675227412567\n",
      "GD iter. 400/499: loss=0.44786752264728424\n",
      "GD iter. 401/499: loss=0.44786752255664747\n",
      "GD iter. 402/499: loss=0.44786752246922573\n",
      "GD iter. 403/499: loss=0.4478675223849032\n",
      "GD iter. 404/499: loss=0.4478675223035681\n",
      "GD iter. 405/499: loss=0.44786752222511256\n",
      "GD iter. 406/499: loss=0.44786752214943304\n",
      "GD iter. 407/499: loss=0.4478675220764296\n",
      "GD iter. 408/499: loss=0.44786752200600594\n",
      "GD iter. 409/499: loss=0.44786752193806934\n",
      "GD iter. 410/499: loss=0.44786752187253054\n",
      "GD iter. 411/499: loss=0.4478675218093035\n",
      "GD iter. 412/499: loss=0.4478675217483052\n",
      "GD iter. 413/499: loss=0.4478675216894557\n",
      "GD iter. 414/499: loss=0.4478675216326783\n",
      "GD iter. 415/499: loss=0.4478675215778986\n",
      "GD iter. 416/499: loss=0.4478675215250452\n",
      "GD iter. 417/499: loss=0.44786752147404935\n",
      "GD iter. 418/499: loss=0.44786752142484454\n",
      "GD iter. 419/499: loss=0.44786752137736696\n",
      "GD iter. 420/499: loss=0.447867521331555\n",
      "GD iter. 421/499: loss=0.44786752128734914\n",
      "GD iter. 422/499: loss=0.44786752124469237\n",
      "GD iter. 423/499: loss=0.4478675212035294\n",
      "GD iter. 424/499: loss=0.44786752116380696\n",
      "GD iter. 425/499: loss=0.4478675211254742\n",
      "GD iter. 426/499: loss=0.44786752108848144\n",
      "GD iter. 427/499: loss=0.44786752105278116\n",
      "GD iter. 428/499: loss=0.4478675210183275\n",
      "GD iter. 429/499: loss=0.4478675209850763\n",
      "GD iter. 430/499: loss=0.447867520952985\n",
      "GD iter. 431/499: loss=0.44786752092201226\n",
      "GD iter. 432/499: loss=0.4478675208921188\n",
      "GD iter. 433/499: loss=0.44786752086326614\n",
      "GD iter. 434/499: loss=0.44786752083541775\n",
      "GD iter. 435/499: loss=0.44786752080853803\n",
      "GD iter. 436/499: loss=0.4478675207825928\n",
      "GD iter. 437/499: loss=0.44786752075754893\n",
      "GD iter. 438/499: loss=0.447867520733375\n",
      "GD iter. 439/499: loss=0.4478675207100401\n",
      "GD iter. 440/499: loss=0.44786752068751456\n",
      "GD iter. 441/499: loss=0.4478675206657702\n",
      "GD iter. 442/499: loss=0.44786752064477925\n",
      "GD iter. 443/499: loss=0.4478675206245155\n",
      "GD iter. 444/499: loss=0.4478675206049531\n",
      "GD iter. 445/499: loss=0.44786752058606755\n",
      "GD iter. 446/499: loss=0.44786752056783524\n",
      "GD iter. 447/499: loss=0.447867520550233\n",
      "GD iter. 448/499: loss=0.4478675205332389\n",
      "GD iter. 449/499: loss=0.44786752051683165\n",
      "GD iter. 450/499: loss=0.44786752050099055\n",
      "GD iter. 451/499: loss=0.44786752048569584\n",
      "GD iter. 452/499: loss=0.4478675204709285\n",
      "GD iter. 453/499: loss=0.44786752045667017\n",
      "GD iter. 454/499: loss=0.4478675204429029\n",
      "GD iter. 455/499: loss=0.4478675204296095\n",
      "GD iter. 456/499: loss=0.4478675204167736\n",
      "GD iter. 457/499: loss=0.4478675204043792\n",
      "GD iter. 458/499: loss=0.44786752039241084\n",
      "GD iter. 459/499: loss=0.4478675203808537\n",
      "GD iter. 460/499: loss=0.4478675203696936\n",
      "GD iter. 461/499: loss=0.4478675203589164\n",
      "GD iter. 462/499: loss=0.447867520348509\n",
      "GD iter. 463/499: loss=0.44786752033845867\n",
      "GD iter. 464/499: loss=0.4478675203287528\n",
      "GD iter. 465/499: loss=0.44786752031937954\n",
      "GD iter. 466/499: loss=0.44786752031032717\n",
      "GD iter. 467/499: loss=0.4478675203015848\n",
      "GD iter. 468/499: loss=0.4478675202931415\n",
      "GD iter. 469/499: loss=0.44786752028498694\n",
      "GD iter. 470/499: loss=0.44786752027711124\n",
      "GD iter. 471/499: loss=0.4478675202695046\n",
      "GD iter. 472/499: loss=0.44786752026215776\n",
      "GD iter. 473/499: loss=0.44786752025506177\n",
      "GD iter. 474/499: loss=0.4478675202482079\n",
      "GD iter. 475/499: loss=0.44786752024158777\n",
      "GD iter. 476/499: loss=0.4478675202351934\n",
      "GD iter. 477/499: loss=0.44786752022901694\n",
      "GD iter. 478/499: loss=0.44786752022305093\n",
      "GD iter. 479/499: loss=0.44786752021728804\n",
      "GD iter. 480/499: loss=0.4478675202117213\n",
      "GD iter. 481/499: loss=0.4478675202063439\n",
      "GD iter. 482/499: loss=0.44786752020114945\n",
      "GD iter. 483/499: loss=0.44786752019613146\n",
      "GD iter. 484/499: loss=0.4478675201912842\n",
      "GD iter. 485/499: loss=0.44786752018660136\n",
      "GD iter. 486/499: loss=0.4478675201820777\n",
      "GD iter. 487/499: loss=0.44786752017770737\n",
      "GD iter. 488/499: loss=0.4478675201734853\n",
      "GD iter. 489/499: loss=0.44786752016940656\n",
      "GD iter. 490/499: loss=0.4478675201654659\n",
      "GD iter. 491/499: loss=0.44786752016165887\n",
      "GD iter. 492/499: loss=0.4478675201579808\n",
      "GD iter. 493/499: loss=0.44786752015442716\n",
      "GD iter. 494/499: loss=0.4478675201509938\n",
      "GD iter. 495/499: loss=0.44786752014767667\n",
      "GD iter. 496/499: loss=0.4478675201444716\n",
      "GD iter. 497/499: loss=0.44786752014137493\n",
      "GD iter. 498/499: loss=0.4478675201383828\n",
      "GD iter. 499/499: loss=0.44786752013549186\n",
      "The Accuracy is: 0.8489\n",
      "The F1 score is: 0.3947\n",
      "The precision is: 0.2941\n",
      "The recall is: 0.6000\n",
      "GD iter. 0/499: loss=0.6913162655855795\n",
      "GD iter. 1/499: loss=0.5376227108587301\n",
      "GD iter. 2/499: loss=0.5243719557834571\n",
      "GD iter. 3/499: loss=0.5154562116377195\n",
      "GD iter. 4/499: loss=0.5085826357006088\n",
      "GD iter. 5/499: loss=0.5029839331621079\n",
      "GD iter. 6/499: loss=0.4982741187534388\n",
      "GD iter. 7/499: loss=0.49421832190704507\n",
      "GD iter. 8/499: loss=0.490663866316884\n",
      "GD iter. 9/499: loss=0.48750705956619655\n",
      "GD iter. 10/499: loss=0.4846746118087111\n",
      "GD iter. 11/499: loss=0.4821127936679106\n",
      "GD iter. 12/499: loss=0.47978092155373464\n",
      "GD iter. 13/499: loss=0.47764732013227523\n",
      "GD iter. 14/499: loss=0.47568673775967024\n",
      "GD iter. 15/499: loss=0.4738786380387995\n",
      "GD iter. 16/499: loss=0.4722060354061066\n",
      "GD iter. 17/499: loss=0.4706546786691179\n",
      "GD iter. 18/499: loss=0.469212463572771\n",
      "GD iter. 19/499: loss=0.46786900027885564\n",
      "GD iter. 20/499: loss=0.46661528831298155\n",
      "GD iter. 21/499: loss=0.4654434678116143\n",
      "GD iter. 22/499: loss=0.464346626087485\n",
      "GD iter. 23/499: loss=0.4633186450620551\n",
      "GD iter. 24/499: loss=0.4623540793996682\n",
      "GD iter. 25/499: loss=0.4614480580542056\n",
      "GD iter. 26/499: loss=0.4605962039099156\n",
      "GD iter. 27/499: loss=0.4597945675749903\n",
      "GD iter. 28/499: loss=0.4590395723656874\n",
      "GD iter. 29/499: loss=0.45832796822655636\n",
      "GD iter. 30/499: loss=0.45765679285144606\n",
      "GD iter. 31/499: loss=0.4570233386557852\n",
      "GD iter. 32/499: loss=0.4564251245408002\n",
      "GD iter. 33/499: loss=0.45585987161094\n",
      "GD iter. 34/499: loss=0.4553254821751177\n",
      "GD iter. 35/499: loss=0.4548200214935357\n",
      "GD iter. 36/499: loss=0.45434170183426986\n",
      "GD iter. 37/499: loss=0.4538888684843496\n",
      "GD iter. 38/499: loss=0.4534599874238986\n",
      "GD iter. 39/499: loss=0.45305363442280466\n",
      "GD iter. 40/499: loss=0.45266848536024\n",
      "GD iter. 41/499: loss=0.4523033076003377\n",
      "GD iter. 42/499: loss=0.4519569522841182\n",
      "GD iter. 43/499: loss=0.45162834741963037\n",
      "GD iter. 44/499: loss=0.45131649167024107\n",
      "GD iter. 45/499: loss=0.4510204487558259\n",
      "GD iter. 46/499: loss=0.45073934239391755\n",
      "GD iter. 47/499: loss=0.4504723517181239\n",
      "GD iter. 48/499: loss=0.45021870711972045\n",
      "GD iter. 49/499: loss=0.4499776864655488\n",
      "GD iter. 50/499: loss=0.4497486116514756\n",
      "GD iter. 51/499: loss=0.44953084545585253\n",
      "GD iter. 52/499: loss=0.4493237886618437\n",
      "GD iter. 53/499: loss=0.44912687742128393\n",
      "GD iter. 54/499: loss=0.4489395808359816\n",
      "GD iter. 55/499: loss=0.44876139873519155\n",
      "GD iter. 56/499: loss=0.44859185963041737\n",
      "GD iter. 57/499: loss=0.4484305188308085\n",
      "GD iter. 58/499: loss=0.4482769567042627\n",
      "GD iter. 59/499: loss=0.44813077707094845\n",
      "GD iter. 60/499: loss=0.44799160571736896\n",
      "GD iter. 61/499: loss=0.4478590890203306\n",
      "GD iter. 62/499: loss=0.44773289267126565\n",
      "GD iter. 63/499: loss=0.44761270049232044\n",
      "GD iter. 64/499: loss=0.44749821333647455\n",
      "GD iter. 65/499: loss=0.44738914806471203\n",
      "GD iter. 66/499: loss=0.44728523659393143\n",
      "GD iter. 67/499: loss=0.44718622500988897\n",
      "GD iter. 68/499: loss=0.44709187273999107\n",
      "GD iter. 69/499: loss=0.4470019517812423\n",
      "GD iter. 70/499: loss=0.4469162459790669\n",
      "GD iter. 71/499: loss=0.44683455035311975\n",
      "GD iter. 72/499: loss=0.4467566704665354\n",
      "GD iter. 73/499: loss=0.4466824218353783\n",
      "GD iter. 74/499: loss=0.44661162937533616\n",
      "GD iter. 75/499: loss=0.4465441268829479\n",
      "GD iter. 76/499: loss=0.4464797565488866\n",
      "GD iter. 77/499: loss=0.4464183685010219\n",
      "GD iter. 78/499: loss=0.44635982037517513\n",
      "GD iter. 79/499: loss=0.446303976911645\n",
      "GD iter. 80/499: loss=0.4462507095757401\n",
      "GD iter. 81/499: loss=0.4461998962006904\n",
      "GD iter. 82/499: loss=0.446151420651437\n",
      "GD iter. 83/499: loss=0.4461051725079171\n",
      "GD iter. 84/499: loss=0.44606104676656516\n",
      "GD iter. 85/499: loss=0.44601894355884725\n",
      "GD iter. 86/499: loss=0.4459787678857334\n",
      "GD iter. 87/499: loss=0.4459404293670971\n",
      "GD iter. 88/499: loss=0.4459038420050976\n",
      "GD iter. 89/499: loss=0.44586892396067584\n",
      "GD iter. 90/499: loss=0.4458355973423527\n",
      "GD iter. 91/499: loss=0.4458037880065754\n",
      "GD iter. 92/499: loss=0.4457734253689132\n",
      "GD iter. 93/499: loss=0.44574444222544785\n",
      "GD iter. 94/499: loss=0.44571677458375336\n",
      "GD iter. 95/499: loss=0.4456903615028943\n",
      "GD iter. 96/499: loss=0.4456651449419194\n",
      "GD iter. 97/499: loss=0.4456410696163494\n",
      "GD iter. 98/499: loss=0.4456180828622043\n",
      "GD iter. 99/499: loss=0.44559613450713315\n",
      "GD iter. 100/499: loss=0.44557517674824576\n",
      "GD iter. 101/499: loss=0.4455551640362657\n",
      "GD iter. 102/499: loss=0.44553605296565146\n",
      "GD iter. 103/499: loss=0.44551780217035347\n",
      "GD iter. 104/499: loss=0.44550037222489475\n",
      "GD iter. 105/499: loss=0.44548372555048293\n",
      "GD iter. 106/499: loss=0.4454678263258785\n",
      "GD iter. 107/499: loss=0.4454526404027616\n",
      "GD iter. 108/499: loss=0.44543813522535375\n",
      "GD iter. 109/499: loss=0.44542427975406823\n",
      "GD iter. 110/499: loss=0.4454110443929703\n",
      "GD iter. 111/499: loss=0.4453984009208486\n",
      "GD iter. 112/499: loss=0.4453863224257067\n",
      "GD iter. 113/499: loss=0.4453747832424916\n",
      "GD iter. 114/499: loss=0.44536375889389535\n",
      "GD iter. 115/499: loss=0.4453532260340644\n",
      "GD iter. 116/499: loss=0.4453431623950698\n",
      "GD iter. 117/499: loss=0.44533354673599557\n",
      "GD iter. 118/499: loss=0.4453243587945069\n",
      "GD iter. 119/499: loss=0.44531557924077747\n",
      "GD iter. 120/499: loss=0.4453071896336502\n",
      "GD iter. 121/499: loss=0.4452991723789213\n",
      "GD iter. 122/499: loss=0.44529151068963935\n",
      "GD iter. 123/499: loss=0.4452841885483171\n",
      "GD iter. 124/499: loss=0.4452771906709609\n",
      "GD iter. 125/499: loss=0.44527050247282746\n",
      "GD iter. 126/499: loss=0.4452641100358198\n",
      "GD iter. 127/499: loss=0.4452580000774416\n",
      "GD iter. 128/499: loss=0.4452521599212345\n",
      "GD iter. 129/499: loss=0.4452465774686231\n",
      "GD iter. 130/499: loss=0.44524124117209735\n",
      "GD iter. 131/499: loss=0.44523614000966977\n",
      "GD iter. 132/499: loss=0.4452312634605431\n",
      "GD iter. 133/499: loss=0.4452266014819278\n",
      "GD iter. 134/499: loss=0.4452221444869568\n",
      "GD iter. 135/499: loss=0.44521788332364004\n",
      "GD iter. 136/499: loss=0.4452138092548129\n",
      "GD iter. 137/499: loss=0.44520991393902565\n",
      "GD iter. 138/499: loss=0.4452061894123316\n",
      "GD iter. 139/499: loss=0.4452026280709295\n",
      "GD iter. 140/499: loss=0.4451992226546197\n",
      "GD iter. 141/499: loss=0.44519596623103397\n",
      "GD iter. 142/499: loss=0.44519285218060234\n",
      "GD iter. 143/499: loss=0.44518987418222167\n",
      "GD iter. 144/499: loss=0.4451870261995932\n",
      "GD iter. 145/499: loss=0.44518430246819557\n",
      "GD iter. 146/499: loss=0.4451816974828643\n",
      "GD iter. 147/499: loss=0.44517920598594857\n",
      "GD iter. 148/499: loss=0.44517682295601807\n",
      "GD iter. 149/499: loss=0.4451745435970925\n",
      "GD iter. 150/499: loss=0.4451723633283718\n",
      "GD iter. 151/499: loss=0.44517027777443996\n",
      "GD iter. 152/499: loss=0.4451682827559234\n",
      "GD iter. 153/499: loss=0.44516637428057965\n",
      "GD iter. 154/499: loss=0.44516454853479853\n",
      "GD iter. 155/499: loss=0.44516280187549384\n",
      "GD iter. 156/499: loss=0.4451611308223702\n",
      "GD iter. 157/499: loss=0.44515953205054454\n",
      "GD iter. 158/499: loss=0.44515800238350794\n",
      "GD iter. 159/499: loss=0.44515653878641\n",
      "GD iter. 160/499: loss=0.44515513835965287\n",
      "GD iter. 161/499: loss=0.4451537983327779\n",
      "GD iter. 162/499: loss=0.4451525160586333\n",
      "GD iter. 163/499: loss=0.44515128900780854\n",
      "GD iter. 164/499: loss=0.44515011476332417\n",
      "GD iter. 165/499: loss=0.44514899101556366\n",
      "GD iter. 166/499: loss=0.4451479155574379\n",
      "GD iter. 167/499: loss=0.44514688627976956\n",
      "GD iter. 168/499: loss=0.4451459011668888\n",
      "GD iter. 169/499: loss=0.4451449582924294\n",
      "GD iter. 170/499: loss=0.44514405581531685\n",
      "GD iter. 171/499: loss=0.44514319197593855\n",
      "GD iter. 172/499: loss=0.4451423650924897\n",
      "GD iter. 173/499: loss=0.44514157355748313\n",
      "GD iter. 174/499: loss=0.4451408158344201\n",
      "GD iter. 175/499: loss=0.4451400904546107\n",
      "GD iter. 176/499: loss=0.4451393960141391\n",
      "GD iter. 177/499: loss=0.4451387311709667\n",
      "GD iter. 178/499: loss=0.445138094642166\n",
      "GD iter. 179/499: loss=0.44513748520128066\n",
      "GD iter. 180/499: loss=0.44513690167580383\n",
      "GD iter. 181/499: loss=0.44513634294477183\n",
      "GD iter. 182/499: loss=0.4451358079364656\n",
      "GD iter. 183/499: loss=0.4451352956262167\n",
      "GD iter. 184/499: loss=0.44513480503431224\n",
      "GD iter. 185/499: loss=0.4451343352239939\n",
      "GD iter. 186/499: loss=0.4451338852995477\n",
      "GD iter. 187/499: loss=0.4451334544044803\n",
      "GD iter. 188/499: loss=0.44513304171977514\n",
      "GD iter. 189/499: loss=0.4451326464622307\n",
      "GD iter. 190/499: loss=0.44513226788287064\n",
      "GD iter. 191/499: loss=0.4451319052654265\n",
      "GD iter. 192/499: loss=0.44513155792488823\n",
      "GD iter. 193/499: loss=0.4451312252061207\n",
      "GD iter. 194/499: loss=0.44513090648254083\n",
      "GD iter. 195/499: loss=0.4451306011548556\n",
      "GD iter. 196/499: loss=0.4451303086498549\n",
      "GD iter. 197/499: loss=0.4451300284192607\n",
      "GD iter. 198/499: loss=0.44512975993862536\n",
      "GD iter. 199/499: loss=0.44512950270628115\n",
      "GD iter. 200/499: loss=0.44512925624233535\n",
      "GD iter. 201/499: loss=0.44512902008771127\n",
      "GD iter. 202/499: loss=0.44512879380323134\n",
      "GD iter. 203/499: loss=0.445128576968742\n",
      "GD iter. 204/499: loss=0.4451283691822762\n",
      "GD iter. 205/499: loss=0.4451281700592555\n",
      "GD iter. 206/499: loss=0.4451279792317247\n",
      "GD iter. 207/499: loss=0.44512779634762384\n",
      "GD iter. 208/499: loss=0.44512762107008974\n",
      "GD iter. 209/499: loss=0.44512745307679064\n",
      "GD iter. 210/499: loss=0.4451272920592889\n",
      "GD iter. 211/499: loss=0.44512713772243345\n",
      "GD iter. 212/499: loss=0.4451269897837782\n",
      "GD iter. 213/499: loss=0.44512684797302604\n",
      "GD iter. 214/499: loss=0.44512671203149895\n",
      "GD iter. 215/499: loss=0.44512658171163016\n",
      "GD iter. 216/499: loss=0.44512645677647883\n",
      "GD iter. 217/499: loss=0.4451263369992675\n",
      "GD iter. 218/499: loss=0.44512622216293884\n",
      "GD iter. 219/499: loss=0.44512611205973185\n",
      "GD iter. 220/499: loss=0.44512600649077816\n",
      "GD iter. 221/499: loss=0.4451259052657145\n",
      "GD iter. 222/499: loss=0.44512580820231323\n",
      "GD iter. 223/499: loss=0.44512571512612953\n",
      "GD iter. 224/499: loss=0.44512562587016236\n",
      "GD iter. 225/499: loss=0.445125540274533\n",
      "GD iter. 226/499: loss=0.445125458186175\n",
      "GD iter. 227/499: loss=0.4451253794585401\n",
      "GD iter. 228/499: loss=0.4451253039513154\n",
      "GD iter. 229/499: loss=0.44512523153015404\n",
      "GD iter. 230/499: loss=0.4451251620664167\n",
      "GD iter. 231/499: loss=0.44512509543692597\n",
      "GD iter. 232/499: loss=0.44512503152372945\n",
      "GD iter. 233/499: loss=0.44512497021387537\n",
      "GD iter. 234/499: loss=0.4451249113991962\n",
      "GD iter. 235/499: loss=0.4451248549761027\n",
      "GD iter. 236/499: loss=0.44512480084538747\n",
      "GD iter. 237/499: loss=0.4451247489120353\n",
      "GD iter. 238/499: loss=0.4451246990850442\n",
      "GD iter. 239/499: loss=0.4451246512772517\n",
      "GD iter. 240/499: loss=0.445124605405171\n",
      "GD iter. 241/499: loss=0.44512456138883294\n",
      "GD iter. 242/499: loss=0.44512451915163515\n",
      "GD iter. 243/499: loss=0.44512447862019755\n",
      "GD iter. 244/499: loss=0.44512443972422505\n",
      "GD iter. 245/499: loss=0.4451244023963751\n",
      "GD iter. 246/499: loss=0.44512436657213156\n",
      "GD iter. 247/499: loss=0.44512433218968395\n",
      "GD iter. 248/499: loss=0.44512429918981206\n",
      "GD iter. 249/499: loss=0.4451242675157753\n",
      "GD iter. 250/499: loss=0.4451242371132072\n",
      "GD iter. 251/499: loss=0.4451242079300142\n",
      "GD iter. 252/499: loss=0.4451241799162786\n",
      "GD iter. 253/499: loss=0.44512415302416686\n",
      "GD iter. 254/499: loss=0.4451241272078396\n",
      "GD iter. 255/499: loss=0.44512410242336853\n",
      "GD iter. 256/499: loss=0.4451240786286542\n",
      "GD iter. 257/499: loss=0.4451240557833491\n",
      "GD iter. 258/499: loss=0.4451240338487829\n",
      "GD iter. 259/499: loss=0.4451240127878916\n",
      "GD iter. 260/499: loss=0.4451239925651501\n",
      "GD iter. 261/499: loss=0.44512397314650604\n",
      "GD iter. 262/499: loss=0.44512395449931874\n",
      "GD iter. 263/499: loss=0.4451239365922988\n",
      "GD iter. 264/499: loss=0.44512391939545165\n",
      "GD iter. 265/499: loss=0.4451239028800223\n",
      "GD iter. 266/499: loss=0.4451238870184437\n",
      "GD iter. 267/499: loss=0.44512387178428664\n",
      "GD iter. 268/499: loss=0.4451238571522118\n",
      "GD iter. 269/499: loss=0.44512384309792397\n",
      "GD iter. 270/499: loss=0.44512382959812813\n",
      "GD iter. 271/499: loss=0.4451238166304874\n",
      "GD iter. 272/499: loss=0.4451238041735835\n",
      "GD iter. 273/499: loss=0.4451237922068773\n",
      "GD iter. 274/499: loss=0.44512378071067304\n",
      "GD iter. 275/499: loss=0.4451237696660819\n",
      "GD iter. 276/499: loss=0.4451237590549892\n",
      "GD iter. 277/499: loss=0.4451237488600219\n",
      "GD iter. 278/499: loss=0.44512373906451724\n",
      "GD iter. 279/499: loss=0.44512372965249364\n",
      "GD iter. 280/499: loss=0.44512372060862193\n",
      "GD iter. 281/499: loss=0.44512371191819844\n",
      "GD iter. 282/499: loss=0.4451237035671185\n",
      "GD iter. 283/499: loss=0.44512369554185266\n",
      "GD iter. 284/499: loss=0.44512368782942124\n",
      "GD iter. 285/499: loss=0.4451236804173728\n",
      "GD iter. 286/499: loss=0.44512367329376185\n",
      "GD iter. 287/499: loss=0.44512366644712725\n",
      "GD iter. 288/499: loss=0.44512365986647345\n",
      "GD iter. 289/499: loss=0.44512365354125\n",
      "GD iter. 290/499: loss=0.44512364746133365\n",
      "GD iter. 291/499: loss=0.44512364161701073\n",
      "GD iter. 292/499: loss=0.44512363599896054\n",
      "GD iter. 293/499: loss=0.4451236305982384\n",
      "GD iter. 294/499: loss=0.4451236254062607\n",
      "GD iter. 295/499: loss=0.4451236204147902\n",
      "GD iter. 296/499: loss=0.4451236156159211\n",
      "GD iter. 297/499: loss=0.44512361100206604\n",
      "GD iter. 298/499: loss=0.4451236065659426\n",
      "GD iter. 299/499: loss=0.44512360230056136\n",
      "GD iter. 300/499: loss=0.4451235981992132\n",
      "GD iter. 301/499: loss=0.4451235942554582\n",
      "GD iter. 302/499: loss=0.4451235904631145\n",
      "GD iter. 303/499: loss=0.44512358681624803\n",
      "GD iter. 304/499: loss=0.4451235833091619\n",
      "GD iter. 305/499: loss=0.44512357993638724\n",
      "GD iter. 306/499: loss=0.4451235766926731\n",
      "GD iter. 307/499: loss=0.4451235735729789\n",
      "GD iter. 308/499: loss=0.4451235705724642\n",
      "GD iter. 309/499: loss=0.4451235676864822\n",
      "GD iter. 310/499: loss=0.4451235649105705\n",
      "GD iter. 311/499: loss=0.44512356224044436\n",
      "GD iter. 312/499: loss=0.44512355967198963\n",
      "GD iter. 313/499: loss=0.4451235572012549\n",
      "GD iter. 314/499: loss=0.4451235548244461\n",
      "GD iter. 315/499: loss=0.44512355253791935\n",
      "GD iter. 316/499: loss=0.4451235503381748\n",
      "GD iter. 317/499: loss=0.4451235482218517\n",
      "GD iter. 318/499: loss=0.4451235461857214\n",
      "GD iter. 319/499: loss=0.4451235442266837\n",
      "GD iter. 320/499: loss=0.44512354234175994\n",
      "GD iter. 321/499: loss=0.44512354052808956\n",
      "GD iter. 322/499: loss=0.4451235387829241\n",
      "GD iter. 323/499: loss=0.44512353710362385\n",
      "GD iter. 324/499: loss=0.4451235354876528\n",
      "GD iter. 325/499: loss=0.4451235339325742\n",
      "GD iter. 326/499: loss=0.44512353243604785\n",
      "GD iter. 327/499: loss=0.44512353099582425\n",
      "GD iter. 328/499: loss=0.44512352960974316\n",
      "GD iter. 329/499: loss=0.4451235282757282\n",
      "GD iter. 330/499: loss=0.44512352699178476\n",
      "GD iter. 331/499: loss=0.4451235257559958\n",
      "GD iter. 332/499: loss=0.4451235245665196\n",
      "GD iter. 333/499: loss=0.44512352342158595\n",
      "GD iter. 334/499: loss=0.4451235223194937\n",
      "GD iter. 335/499: loss=0.44512352125860855\n",
      "GD iter. 336/499: loss=0.44512352023735907\n",
      "GD iter. 337/499: loss=0.44512351925423566\n",
      "GD iter. 338/499: loss=0.4451235183077868\n",
      "GD iter. 339/499: loss=0.44512351739661793\n",
      "GD iter. 340/499: loss=0.4451235165193879\n",
      "GD iter. 341/499: loss=0.4451235156748084\n",
      "GD iter. 342/499: loss=0.4451235148616403\n",
      "GD iter. 343/499: loss=0.4451235140786929\n",
      "GD iter. 344/499: loss=0.4451235133248214\n",
      "GD iter. 345/499: loss=0.4451235125989253\n",
      "GD iter. 346/499: loss=0.44512351189994637\n",
      "GD iter. 347/499: loss=0.44512351122686755\n",
      "GD iter. 348/499: loss=0.4451235105787107\n",
      "GD iter. 349/499: loss=0.44512350995453553\n",
      "GD iter. 350/499: loss=0.4451235093534379\n",
      "GD iter. 351/499: loss=0.4451235087745484\n",
      "GD iter. 352/499: loss=0.44512350821703106\n",
      "GD iter. 353/499: loss=0.4451235076800821\n",
      "GD iter. 354/499: loss=0.44512350716292837\n",
      "GD iter. 355/499: loss=0.4451235066648263\n",
      "GD iter. 356/499: loss=0.4451235061850614\n",
      "GD iter. 357/499: loss=0.44512350572294557\n",
      "GD iter. 358/499: loss=0.4451235052778178\n",
      "GD iter. 359/499: loss=0.44512350484904206\n",
      "GD iter. 360/499: loss=0.44512350443600646\n",
      "GD iter. 361/499: loss=0.4451235040381228\n",
      "GD iter. 362/499: loss=0.44512350365482495\n",
      "GD iter. 363/499: loss=0.44512350328556854\n",
      "GD iter. 364/499: loss=0.4451235029298297\n",
      "GD iter. 365/499: loss=0.4451235025871048\n",
      "GD iter. 366/499: loss=0.44512350225690916\n",
      "GD iter. 367/499: loss=0.44512350193877637\n",
      "GD iter. 368/499: loss=0.445123501632258\n",
      "GD iter. 369/499: loss=0.44512350133692247\n",
      "GD iter. 370/499: loss=0.4451235010523543\n",
      "GD iter. 371/499: loss=0.4451235007781543\n",
      "GD iter. 372/499: loss=0.44512350051393784\n",
      "GD iter. 373/499: loss=0.4451235002593351\n",
      "GD iter. 374/499: loss=0.44512350001399026\n",
      "GD iter. 375/499: loss=0.4451234997775605\n",
      "GD iter. 376/499: loss=0.44512349954971653\n",
      "GD iter. 377/499: loss=0.4451234993301411\n",
      "GD iter. 378/499: loss=0.4451234991185288\n",
      "GD iter. 379/499: loss=0.4451234989145862\n",
      "GD iter. 380/499: loss=0.4451234987180302\n",
      "GD iter. 381/499: loss=0.44512349852858885\n",
      "GD iter. 382/499: loss=0.4451234983460003\n",
      "GD iter. 383/499: loss=0.44512349817001234\n",
      "GD iter. 384/499: loss=0.44512349800038253\n",
      "GD iter. 385/499: loss=0.44512349783687716\n",
      "GD iter. 386/499: loss=0.44512349767927134\n",
      "GD iter. 387/499: loss=0.4451234975273488\n",
      "GD iter. 388/499: loss=0.4451234973809012\n",
      "GD iter. 389/499: loss=0.44512349723972783\n",
      "GD iter. 390/499: loss=0.44512349710363575\n",
      "GD iter. 391/499: loss=0.44512349697243897\n",
      "GD iter. 392/499: loss=0.44512349684595875\n",
      "GD iter. 393/499: loss=0.4451234967240225\n",
      "GD iter. 394/499: loss=0.44512349660646444\n",
      "GD iter. 395/499: loss=0.44512349649312494\n",
      "GD iter. 396/499: loss=0.44512349638384996\n",
      "GD iter. 397/499: loss=0.4451234962784915\n",
      "GD iter. 398/499: loss=0.44512349617690716\n",
      "GD iter. 399/499: loss=0.4451234960789594\n",
      "GD iter. 400/499: loss=0.44512349598451595\n",
      "GD iter. 401/499: loss=0.44512349589344957\n",
      "GD iter. 402/499: loss=0.44512349580563754\n",
      "GD iter. 403/499: loss=0.4451234957209618\n",
      "GD iter. 404/499: loss=0.44512349563930853\n",
      "GD iter. 405/499: loss=0.44512349556056835\n",
      "GD iter. 406/499: loss=0.44512349548463565\n",
      "GD iter. 407/499: loss=0.44512349541140867\n",
      "GD iter. 408/499: loss=0.44512349534078977\n",
      "GD iter. 409/499: loss=0.44512349527268436\n",
      "GD iter. 410/499: loss=0.4451234952070019\n",
      "GD iter. 411/499: loss=0.44512349514365473\n",
      "GD iter. 412/499: loss=0.4451234950825587\n",
      "GD iter. 413/499: loss=0.4451234950236324\n",
      "GD iter. 414/499: loss=0.44512349496679793\n",
      "GD iter. 415/499: loss=0.44512349491197967\n",
      "GD iter. 416/499: loss=0.44512349485910535\n",
      "GD iter. 417/499: loss=0.44512349480810476\n",
      "GD iter. 418/499: loss=0.4451234947589105\n",
      "GD iter. 419/499: loss=0.44512349471145796\n",
      "GD iter. 420/499: loss=0.4451234946656842\n",
      "GD iter. 421/499: loss=0.44512349462152934\n",
      "GD iter. 422/499: loss=0.44512349457893513\n",
      "GD iter. 423/499: loss=0.44512349453784567\n",
      "GD iter. 424/499: loss=0.445123494498207\n",
      "GD iter. 425/499: loss=0.4451234944599673\n",
      "GD iter. 426/499: loss=0.44512349442307636\n",
      "GD iter. 427/499: loss=0.445123494387486\n",
      "GD iter. 428/499: loss=0.4451234943531496\n",
      "GD iter. 429/499: loss=0.4451234943200226\n",
      "GD iter. 430/499: loss=0.44512349428806175\n",
      "GD iter. 431/499: loss=0.44512349425722536\n",
      "GD iter. 432/499: loss=0.44512349422747344\n",
      "GD iter. 433/499: loss=0.4451234941987673\n",
      "GD iter. 434/499: loss=0.4451234941710695\n",
      "GD iter. 435/499: loss=0.4451234941443444\n",
      "GD iter. 436/499: loss=0.44512349411855734\n",
      "GD iter. 437/499: loss=0.44512349409367474\n",
      "GD iter. 438/499: loss=0.4451234940696647\n",
      "GD iter. 439/499: loss=0.4451234940464962\n",
      "GD iter. 440/499: loss=0.44512349402413937\n",
      "GD iter. 441/499: loss=0.4451234940025653\n",
      "GD iter. 442/499: loss=0.44512349398174617\n",
      "GD iter. 443/499: loss=0.4451234939616554\n",
      "GD iter. 444/499: loss=0.445123493942267\n",
      "GD iter. 445/499: loss=0.4451234939235563\n",
      "GD iter. 446/499: loss=0.4451234939054992\n",
      "GD iter. 447/499: loss=0.44512349388807265\n",
      "GD iter. 448/499: loss=0.4451234938712542\n",
      "GD iter. 449/499: loss=0.44512349385502253\n",
      "GD iter. 450/499: loss=0.4451234938393569\n",
      "GD iter. 451/499: loss=0.44512349382423716\n",
      "GD iter. 452/499: loss=0.4451234938096443\n",
      "GD iter. 453/499: loss=0.44512349379555943\n",
      "GD iter. 454/499: loss=0.4451234937819649\n",
      "GD iter. 455/499: loss=0.44512349376884336\n",
      "GD iter. 456/499: loss=0.4451234937561782\n",
      "GD iter. 457/499: loss=0.4451234937439533\n",
      "GD iter. 458/499: loss=0.4451234937321532\n",
      "GD iter. 459/499: loss=0.44512349372076293\n",
      "GD iter. 460/499: loss=0.44512349370976817\n",
      "GD iter. 461/499: loss=0.4451234936991548\n",
      "GD iter. 462/499: loss=0.4451234936889098\n",
      "GD iter. 463/499: loss=0.44512349367901977\n",
      "GD iter. 464/499: loss=0.4451234936694728\n",
      "GD iter. 465/499: loss=0.44512349366025633\n",
      "GD iter. 466/499: loss=0.4451234936513593\n",
      "GD iter. 467/499: loss=0.44512349364277004\n",
      "GD iter. 468/499: loss=0.44512349363447806\n",
      "GD iter. 469/499: loss=0.44512349362647297\n",
      "GD iter. 470/499: loss=0.44512349361874465\n",
      "GD iter. 471/499: loss=0.44512349361128334\n",
      "GD iter. 472/499: loss=0.4451234936040798\n",
      "GD iter. 473/499: loss=0.4451234935971251\n",
      "GD iter. 474/499: loss=0.4451234935904105\n",
      "GD iter. 475/499: loss=0.4451234935839275\n",
      "GD iter. 476/499: loss=0.4451234935776681\n",
      "GD iter. 477/499: loss=0.4451234935716246\n",
      "GD iter. 478/499: loss=0.44512349356578934\n",
      "GD iter. 479/499: loss=0.4451234935601551\n",
      "GD iter. 480/499: loss=0.44512349355471503\n",
      "GD iter. 481/499: loss=0.4451234935494621\n",
      "GD iter. 482/499: loss=0.4451234935443901\n",
      "GD iter. 483/499: loss=0.4451234935394925\n",
      "GD iter. 484/499: loss=0.44512349353476344\n",
      "GD iter. 485/499: loss=0.44512349353019676\n",
      "GD iter. 486/499: loss=0.4451234935257871\n",
      "GD iter. 487/499: loss=0.44512349352152897\n",
      "GD iter. 488/499: loss=0.4451234935174169\n",
      "GD iter. 489/499: loss=0.4451234935134462\n",
      "GD iter. 490/499: loss=0.44512349350961156\n",
      "GD iter. 491/499: loss=0.4451234935059086\n",
      "GD iter. 492/499: loss=0.4451234935023325\n",
      "GD iter. 493/499: loss=0.44512349349887903\n",
      "GD iter. 494/499: loss=0.4451234934955439\n",
      "GD iter. 495/499: loss=0.44512349349232294\n",
      "GD iter. 496/499: loss=0.4451234934892123\n",
      "GD iter. 497/499: loss=0.44512349348620817\n",
      "GD iter. 498/499: loss=0.4451234934833069\n",
      "GD iter. 499/499: loss=0.4451234934805048\n",
      "The Accuracy is: 0.8374\n",
      "The F1 score is: 0.3926\n",
      "The precision is: 0.2832\n",
      "The recall is: 0.6400\n",
      "GD iter. 0/499: loss=0.7038973144880609\n",
      "GD iter. 1/499: loss=0.538378382828109\n",
      "GD iter. 2/499: loss=0.5250762792062936\n",
      "GD iter. 3/499: loss=0.5163791423446208\n",
      "GD iter. 4/499: loss=0.5097173781820105\n",
      "GD iter. 5/499: loss=0.5042836318981488\n",
      "GD iter. 6/499: loss=0.49969876713412126\n",
      "GD iter. 7/499: loss=0.49573834024818286\n",
      "GD iter. 8/499: loss=0.49225768186277236\n",
      "GD iter. 9/499: loss=0.4891587913865104\n",
      "GD iter. 10/499: loss=0.48637242256723373\n",
      "GD iter. 11/499: loss=0.48384775668856217\n",
      "GD iter. 12/499: loss=0.481546235509918\n",
      "GD iter. 13/499: loss=0.4794377557788034\n",
      "GD iter. 14/499: loss=0.47749824176917954\n",
      "GD iter. 15/499: loss=0.47570804518203064\n",
      "GD iter. 16/499: loss=0.4740508567776542\n",
      "GD iter. 17/499: loss=0.4725129441349078\n",
      "GD iter. 18/499: loss=0.47108260341649\n",
      "GD iter. 19/499: loss=0.4697497555340864\n",
      "GD iter. 20/499: loss=0.4685056423163583\n",
      "GD iter. 21/499: loss=0.467342593608572\n",
      "GD iter. 22/499: loss=0.4662538457850503\n",
      "GD iter. 23/499: loss=0.4652333982573156\n",
      "GD iter. 24/499: loss=0.4642758985522186\n",
      "GD iter. 25/499: loss=0.4633765492056033\n",
      "GD iter. 26/499: loss=0.46253103154368036\n",
      "GD iter. 27/499: loss=0.46173544269863337\n",
      "GD iter. 28/499: loss=0.4609862431105612\n",
      "GD iter. 29/499: loss=0.4602802124222179\n",
      "GD iter. 30/499: loss=0.4596144121530407\n",
      "GD iter. 31/499: loss=0.45898615389588465\n",
      "GD iter. 32/499: loss=0.4583929720485101\n",
      "GD iter. 33/499: loss=0.45783260029627015\n",
      "GD iter. 34/499: loss=0.4573029512194812\n",
      "GD iter. 35/499: loss=0.45680209852072723\n",
      "GD iter. 36/499: loss=0.4563282614625131\n",
      "GD iter. 37/499: loss=0.45587979118064675\n",
      "GD iter. 38/499: loss=0.45545515859819635\n",
      "GD iter. 39/499: loss=0.45505294371235855\n",
      "GD iter. 40/499: loss=0.4546718260647556\n",
      "GD iter. 41/499: loss=0.4543105762365496\n",
      "GD iter. 42/499: loss=0.4539680482348885\n",
      "GD iter. 43/499: loss=0.45364317265775705\n",
      "GD iter. 44/499: loss=0.4533349505412184\n",
      "GD iter. 45/499: loss=0.4530424478070378\n",
      "GD iter. 46/499: loss=0.4527647902403129\n",
      "GD iter. 47/499: loss=0.452501158936473\n",
      "GD iter. 48/499: loss=0.45225078616518133\n",
      "GD iter. 49/499: loss=0.4520129516055763\n",
      "GD iter. 50/499: loss=0.45178697891313424\n",
      "GD iter. 51/499: loss=0.4515722325834225\n",
      "GD iter. 52/499: loss=0.45136811508226404\n",
      "GD iter. 53/499: loss=0.45117406421549666\n",
      "GD iter. 54/499: loss=0.4509895507146583\n",
      "GD iter. 55/499: loss=0.45081407601765083\n",
      "GD iter. 56/499: loss=0.4506471702258048\n",
      "GD iter. 57/499: loss=0.45048839022081855\n",
      "GD iter. 58/499: loss=0.4503373179268461\n",
      "GD iter. 59/499: loss=0.4501935587045789\n",
      "GD iter. 60/499: loss=0.4500567398655475\n",
      "GD iter. 61/499: loss=0.44992650929608596\n",
      "GD iter. 62/499: loss=0.4498025341814727\n",
      "GD iter. 63/499: loss=0.4496844998217124\n",
      "GD iter. 64/499: loss=0.44957210853126095\n",
      "GD iter. 65/499: loss=0.4494650786157453\n",
      "GD iter. 66/499: loss=0.44936314341939354\n",
      "GD iter. 67/499: loss=0.44926605043748113\n",
      "GD iter. 68/499: loss=0.4491735604886312\n",
      "GD iter. 69/499: loss=0.44908544694227903\n",
      "GD iter. 70/499: loss=0.4490014949970333\n",
      "GD iter. 71/499: loss=0.4489215010060512\n",
      "GD iter. 72/499: loss=0.44884527184588224\n",
      "GD iter. 73/499: loss=0.44877262432554815\n",
      "GD iter. 74/499: loss=0.4487033846329001\n",
      "GD iter. 75/499: loss=0.44863738781554835\n",
      "GD iter. 76/499: loss=0.44857447729388306\n",
      "GD iter. 77/499: loss=0.4485145044039128\n",
      "GD iter. 78/499: loss=0.4484573279678315\n",
      "GD iter. 79/499: loss=0.4484028138903933\n",
      "GD iter. 80/499: loss=0.44835083477932786\n",
      "GD iter. 81/499: loss=0.4483012695881695\n",
      "GD iter. 82/499: loss=0.44825400327999726\n",
      "GD iter. 83/499: loss=0.448208926510701\n",
      "GD iter. 84/499: loss=0.44816593533049437\n",
      "GD iter. 85/499: loss=0.44812493090248856\n",
      "GD iter. 86/499: loss=0.44808581923723245\n",
      "GD iter. 87/499: loss=0.4480485109422028\n",
      "GD iter. 88/499: loss=0.4480129209853041\n",
      "GD iter. 89/499: loss=0.44797896847150287\n",
      "GD iter. 90/499: loss=0.4479465764317854\n",
      "GD iter. 91/499: loss=0.44791567162368445\n",
      "GD iter. 92/499: loss=0.4478861843426716\n",
      "GD iter. 93/499: loss=0.4478580482437625\n",
      "GD iter. 94/499: loss=0.44783120017272715\n",
      "GD iter. 95/499: loss=0.4478055800063322\n",
      "GD iter. 96/499: loss=0.44778113050109153\n",
      "GD iter. 97/499: loss=0.44775779715002384\n",
      "GD iter. 98/499: loss=0.4477355280469597\n",
      "GD iter. 99/499: loss=0.4477142737579609\n",
      "GD iter. 100/499: loss=0.4476939871994499\n",
      "GD iter. 101/499: loss=0.44767462352266846\n",
      "GD iter. 102/499: loss=0.4476561400041118\n",
      "GD iter. 103/499: loss=0.4476384959416029\n",
      "GD iter. 104/499: loss=0.447621652555696\n",
      "GD iter. 105/499: loss=0.447605572896116\n",
      "GD iter. 106/499: loss=0.4475902217529562\n",
      "GD iter. 107/499: loss=0.44757556557237815\n",
      "GD iter. 108/499: loss=0.4475615723765684\n",
      "GD iter. 109/499: loss=0.4475482116877226\n",
      "GD iter. 110/499: loss=0.4475354544558446\n",
      "GD iter. 111/499: loss=0.44752327299015293\n",
      "GD iter. 112/499: loss=0.4475116408939079\n",
      "GD iter. 113/499: loss=0.44750053300247616\n",
      "GD iter. 114/499: loss=0.4474899253244642\n",
      "GD iter. 115/499: loss=0.4474797949857607\n",
      "GD iter. 116/499: loss=0.44747012017633575\n",
      "GD iter. 117/499: loss=0.4474608800996549\n",
      "GD iter. 118/499: loss=0.4474520549245725\n",
      "GD iter. 119/499: loss=0.4474436257395791\n",
      "GD iter. 120/499: loss=0.4474355745092786\n",
      "GD iter. 121/499: loss=0.4474278840329861\n",
      "GD iter. 122/499: loss=0.44742053790533587\n",
      "GD iter. 123/499: loss=0.44741352047879845\n",
      "GD iter. 124/499: loss=0.4474068168280108\n",
      "GD iter. 125/499: loss=0.4474004127158277\n",
      "GD iter. 126/499: loss=0.44739429456101054\n",
      "GD iter. 127/499: loss=0.44738844940746775\n",
      "GD iter. 128/499: loss=0.4473828648949737\n",
      "GD iter. 129/499: loss=0.447377529231289\n",
      "GD iter. 130/499: loss=0.4473724311656162\n",
      "GD iter. 131/499: loss=0.44736755996332245\n",
      "GD iter. 132/499: loss=0.44736290538186685\n",
      "GD iter. 133/499: loss=0.4473584576478764\n",
      "GD iter. 134/499: loss=0.4473542074353085\n",
      "GD iter. 135/499: loss=0.4473501458446531\n",
      "GD iter. 136/499: loss=0.4473462643831189\n",
      "GD iter. 137/499: loss=0.447342554945759\n",
      "GD iter. 138/499: loss=0.44733900979748753\n",
      "GD iter. 139/499: loss=0.44733562155594697\n",
      "GD iter. 140/499: loss=0.4473323831751826\n",
      "GD iter. 141/499: loss=0.4473292879300855\n",
      "GD iter. 142/499: loss=0.44732632940156924\n",
      "GD iter. 143/499: loss=0.4473235014624414\n",
      "GD iter. 144/499: loss=0.44732079826393906\n",
      "GD iter. 145/499: loss=0.4473182142228959\n",
      "GD iter. 146/499: loss=0.44731574400951035\n",
      "GD iter. 147/499: loss=0.4473133825356855\n",
      "GD iter. 148/499: loss=0.4473111249439149\n",
      "GD iter. 149/499: loss=0.4473089665966871\n",
      "GD iter. 150/499: loss=0.44730690306638426\n",
      "GD iter. 151/499: loss=0.4473049301256523\n",
      "GD iter. 152/499: loss=0.4473030437382185\n",
      "GD iter. 153/499: loss=0.44730124005013716\n",
      "GD iter. 154/499: loss=0.4472995153814424\n",
      "GD iter. 155/499: loss=0.4472978662181877\n",
      "GD iter. 156/499: loss=0.4472962892048553\n",
      "GD iter. 157/499: loss=0.44729478113711807\n",
      "GD iter. 158/499: loss=0.44729333895493606\n",
      "GD iter. 159/499: loss=0.4472919597359732\n",
      "GD iter. 160/499: loss=0.44729064068931806\n",
      "GD iter. 161/499: loss=0.4472893791494944\n",
      "GD iter. 162/499: loss=0.4472881725707504\n",
      "GD iter. 163/499: loss=0.44728701852160835\n",
      "GD iter. 164/499: loss=0.44728591467966866\n",
      "GD iter. 165/499: loss=0.4472848588266517\n",
      "GD iter. 166/499: loss=0.4472838488436678\n",
      "GD iter. 167/499: loss=0.4472828827067061\n",
      "GD iter. 168/499: loss=0.44728195848233004\n",
      "GD iter. 169/499: loss=0.44728107432357034\n",
      "GD iter. 170/499: loss=0.44728022846600796\n",
      "GD iter. 171/499: loss=0.4472794192240352\n",
      "GD iter. 172/499: loss=0.44727864498728936\n",
      "GD iter. 173/499: loss=0.4472779042172496\n",
      "GD iter. 174/499: loss=0.4472771954439894\n",
      "GD iter. 175/499: loss=0.4472765172630781\n",
      "GD iter. 176/499: loss=0.4472758683326238\n",
      "GD iter. 177/499: loss=0.4472752473704519\n",
      "GD iter. 178/499: loss=0.44727465315141185\n",
      "GD iter. 179/499: loss=0.44727408450480766\n",
      "GD iter. 180/499: loss=0.44727354031194544\n",
      "GD iter. 181/499: loss=0.4472730195037921\n",
      "GD iter. 182/499: loss=0.4472725210587416\n",
      "GD iter. 183/499: loss=0.44727204400048304\n",
      "GD iter. 184/499: loss=0.4472715873959647\n",
      "GD iter. 185/499: loss=0.44727115035345205\n",
      "GD iter. 186/499: loss=0.44727073202067297\n",
      "GD iter. 187/499: loss=0.44727033158304813\n",
      "GD iter. 188/499: loss=0.44726994826200095\n",
      "GD iter. 189/499: loss=0.4472695813133451\n",
      "GD iter. 190/499: loss=0.44726923002574437\n",
      "GD iter. 191/499: loss=0.4472688937192424\n",
      "GD iter. 192/499: loss=0.4472685717438601\n",
      "GD iter. 193/499: loss=0.44726826347825527\n",
      "GD iter. 194/499: loss=0.4472679683284436\n",
      "GD iter. 195/499: loss=0.4472676857265774\n",
      "GD iter. 196/499: loss=0.4472674151297791\n",
      "GD iter. 197/499: loss=0.4472671560190282\n",
      "GD iter. 198/499: loss=0.4472669078980984\n",
      "GD iter. 199/499: loss=0.4472666702925418\n",
      "GD iter. 200/499: loss=0.44726644274871985\n",
      "GD iter. 201/499: loss=0.4472662248328774\n",
      "GD iter. 202/499: loss=0.4472660161302598\n",
      "GD iter. 203/499: loss=0.4472658162442668\n",
      "GD iter. 204/499: loss=0.44726562479564835\n",
      "GD iter. 205/499: loss=0.4472654414217334\n",
      "GD iter. 206/499: loss=0.4472652657756951\n",
      "GD iter. 207/499: loss=0.4472650975258487\n",
      "GD iter. 208/499: loss=0.4472649363549801\n",
      "GD iter. 209/499: loss=0.4472647819597061\n",
      "GD iter. 210/499: loss=0.4472646340498619\n",
      "GD iter. 211/499: loss=0.44726449234791676\n",
      "GD iter. 212/499: loss=0.447264356588416\n",
      "GD iter. 213/499: loss=0.44726422651744724\n",
      "GD iter. 214/499: loss=0.4472641018921314\n",
      "GD iter. 215/499: loss=0.4472639824801355\n",
      "GD iter. 216/499: loss=0.44726386805920876\n",
      "GD iter. 217/499: loss=0.44726375841673716\n",
      "GD iter. 218/499: loss=0.4472636533493206\n",
      "GD iter. 219/499: loss=0.44726355266236634\n",
      "GD iter. 220/499: loss=0.4472634561697021\n",
      "GD iter. 221/499: loss=0.4472633636932065\n",
      "GD iter. 222/499: loss=0.4472632750624549\n",
      "GD iter. 223/499: loss=0.4472631901143818\n",
      "GD iter. 224/499: loss=0.4472631086929585\n",
      "GD iter. 225/499: loss=0.44726303064888384\n",
      "GD iter. 226/499: loss=0.44726295583929027\n",
      "GD iter. 227/499: loss=0.447262884127462\n",
      "GD iter. 228/499: loss=0.44726281538256574\n",
      "GD iter. 229/499: loss=0.44726274947939354\n",
      "GD iter. 230/499: loss=0.44726268629811716\n",
      "GD iter. 231/499: loss=0.4472626257240533\n",
      "GD iter. 232/499: loss=0.4472625676474385\n",
      "GD iter. 233/499: loss=0.44726251196321565\n",
      "GD iter. 234/499: loss=0.44726245857082797\n",
      "GD iter. 235/499: loss=0.4472624073740234\n",
      "GD iter. 236/499: loss=0.4472623582806675\n",
      "GD iter. 237/499: loss=0.44726231120256377\n",
      "GD iter. 238/499: loss=0.44726226605528335\n",
      "GD iter. 239/499: loss=0.44726222275800054\n",
      "GD iter. 240/499: loss=0.44726218123333716\n",
      "GD iter. 241/499: loss=0.4472621414072125\n",
      "GD iter. 242/499: loss=0.4472621032087006\n",
      "GD iter. 243/499: loss=0.4472620665698936\n",
      "GD iter. 244/499: loss=0.4472620314257713\n",
      "GD iter. 245/499: loss=0.44726199771407604\n",
      "GD iter. 246/499: loss=0.447261965375193\n",
      "GD iter. 247/499: loss=0.447261934352037\n",
      "GD iter. 248/499: loss=0.4472619045899419\n",
      "GD iter. 249/499: loss=0.4472618760365581\n",
      "GD iter. 250/499: loss=0.44726184864175067\n",
      "GD iter. 251/499: loss=0.44726182235750556\n",
      "GD iter. 252/499: loss=0.44726179713783754\n",
      "GD iter. 253/499: loss=0.44726177293870323\n",
      "GD iter. 254/499: loss=0.44726174971791727\n",
      "GD iter. 255/499: loss=0.44726172743507303\n",
      "GD iter. 256/499: loss=0.4472617060514657\n",
      "GD iter. 257/499: loss=0.4472616855300201\n",
      "GD iter. 258/499: loss=0.44726166583522003\n",
      "GD iter. 259/499: loss=0.44726164693304166\n",
      "GD iter. 260/499: loss=0.44726162879089026\n",
      "GD iter. 261/499: loss=0.44726161137753856\n",
      "GD iter. 262/499: loss=0.44726159466306814\n",
      "GD iter. 263/499: loss=0.4472615786188145\n",
      "GD iter. 264/499: loss=0.44726156321731175\n",
      "GD iter. 265/499: loss=0.4472615484322439\n",
      "GD iter. 266/499: loss=0.44726153423839377\n",
      "GD iter. 267/499: loss=0.44726152061159735\n",
      "GD iter. 268/499: loss=0.44726150752869887\n",
      "GD iter. 269/499: loss=0.44726149496750733\n",
      "GD iter. 270/499: loss=0.4472614829067564\n",
      "GD iter. 271/499: loss=0.44726147132606453\n",
      "GD iter. 272/499: loss=0.4472614602058977\n",
      "GD iter. 273/499: loss=0.44726144952753333\n",
      "GD iter. 274/499: loss=0.4472614392730261\n",
      "GD iter. 275/499: loss=0.447261429425175\n",
      "GD iter. 276/499: loss=0.44726141996749175\n",
      "GD iter. 277/499: loss=0.447261410884171\n",
      "GD iter. 278/499: loss=0.44726140216006094\n",
      "GD iter. 279/499: loss=0.44726139378063645\n",
      "GD iter. 280/499: loss=0.44726138573197194\n",
      "GD iter. 281/499: loss=0.4472613780007167\n",
      "GD iter. 282/499: loss=0.4472613705740707\n",
      "GD iter. 283/499: loss=0.44726136343976053\n",
      "GD iter. 284/499: loss=0.4472613565860186\n",
      "GD iter. 285/499: loss=0.4472613500015608\n",
      "GD iter. 286/499: loss=0.4472613436755671\n",
      "GD iter. 287/499: loss=0.4472613375976613\n",
      "GD iter. 288/499: loss=0.4472613317578928\n",
      "GD iter. 289/499: loss=0.4472613261467193\n",
      "GD iter. 290/499: loss=0.4472613207549885\n",
      "GD iter. 291/499: loss=0.44726131557392335\n",
      "GD iter. 292/499: loss=0.4472613105951047\n",
      "GD iter. 293/499: loss=0.44726130581045787\n",
      "GD iter. 294/499: loss=0.44726130121223745\n",
      "GD iter. 295/499: loss=0.44726129679301363\n",
      "GD iter. 296/499: loss=0.44726129254565944\n",
      "GD iter. 297/499: loss=0.447261288463338\n",
      "GD iter. 298/499: loss=0.4472612845394903\n",
      "GD iter. 299/499: loss=0.4472612807678237\n",
      "GD iter. 300/499: loss=0.44726127714230113\n",
      "GD iter. 301/499: loss=0.4472612736571304\n",
      "GD iter. 302/499: loss=0.4472612703067537\n",
      "GD iter. 303/499: loss=0.4472612670858385\n",
      "GD iter. 304/499: loss=0.44726126398926774\n",
      "GD iter. 305/499: loss=0.44726126101213115\n",
      "GD iter. 306/499: loss=0.4472612581497171\n",
      "GD iter. 307/499: loss=0.44726125539750333\n",
      "GD iter. 308/499: loss=0.44726125275115025\n",
      "GD iter. 309/499: loss=0.44726125020649304\n",
      "GD iter. 310/499: loss=0.4472612477595342\n",
      "GD iter. 311/499: loss=0.44726124540643697\n",
      "GD iter. 312/499: loss=0.44726124314351884\n",
      "GD iter. 313/499: loss=0.4472612409672446\n",
      "GD iter. 314/499: loss=0.44726123887422126\n",
      "GD iter. 315/499: loss=0.44726123686119135\n",
      "GD iter. 316/499: loss=0.4472612349250279\n",
      "GD iter. 317/499: loss=0.44726123306272897\n",
      "GD iter. 318/499: loss=0.4472612312714122\n",
      "GD iter. 319/499: loss=0.44726122954831066\n",
      "GD iter. 320/499: loss=0.4472612278907675\n",
      "GD iter. 321/499: loss=0.44726122629623183\n",
      "GD iter. 322/499: loss=0.4472612247622539\n",
      "GD iter. 323/499: loss=0.4472612232864818\n",
      "GD iter. 324/499: loss=0.4472612218666569\n",
      "GD iter. 325/499: loss=0.4472612205006102\n",
      "GD iter. 326/499: loss=0.4472612191862584\n",
      "GD iter. 327/499: loss=0.4472612179216012\n",
      "GD iter. 328/499: loss=0.44726121670471725\n",
      "GD iter. 329/499: loss=0.447261215533761\n",
      "GD iter. 330/499: loss=0.4472612144069603\n",
      "GD iter. 331/499: loss=0.4472612133226124\n",
      "GD iter. 332/499: loss=0.44726121227908205\n",
      "GD iter. 333/499: loss=0.4472612112747981\n",
      "GD iter. 334/499: loss=0.44726121030825167\n",
      "GD iter. 335/499: loss=0.44726120937799274\n",
      "GD iter. 336/499: loss=0.4472612084826286\n",
      "GD iter. 337/499: loss=0.4472612076208212\n",
      "GD iter. 338/499: loss=0.44726120679128484\n",
      "GD iter. 339/499: loss=0.44726120599278407\n",
      "GD iter. 340/499: loss=0.4472612052241322\n",
      "GD iter. 341/499: loss=0.44726120448418855\n",
      "GD iter. 342/499: loss=0.44726120377185713\n",
      "GD iter. 343/499: loss=0.4472612030860849\n",
      "GD iter. 344/499: loss=0.4472612024258597\n",
      "GD iter. 345/499: loss=0.4472612017902089\n",
      "GD iter. 346/499: loss=0.4472612011781977\n",
      "GD iter. 347/499: loss=0.4472612005889277\n",
      "GD iter. 348/499: loss=0.44726120002153535\n",
      "GD iter. 349/499: loss=0.44726119947519083\n",
      "GD iter. 350/499: loss=0.4472611989490961\n",
      "GD iter. 351/499: loss=0.44726119844248446\n",
      "GD iter. 352/499: loss=0.44726119795461866\n",
      "GD iter. 353/499: loss=0.44726119748479004\n",
      "GD iter. 354/499: loss=0.4472611970323173\n",
      "GD iter. 355/499: loss=0.4472611965965456\n",
      "GD iter. 356/499: loss=0.4472611961768453\n",
      "GD iter. 357/499: loss=0.4472611957726107\n",
      "GD iter. 358/499: loss=0.4472611953832599\n",
      "GD iter. 359/499: loss=0.447261195008233\n",
      "GD iter. 360/499: loss=0.4472611946469921\n",
      "GD iter. 361/499: loss=0.4472611942990196\n",
      "GD iter. 362/499: loss=0.44726119396381764\n",
      "GD iter. 363/499: loss=0.4472611936409077\n",
      "GD iter. 364/499: loss=0.4472611933298297\n",
      "GD iter. 365/499: loss=0.4472611930301408\n",
      "GD iter. 366/499: loss=0.4472611927414153\n",
      "GD iter. 367/499: loss=0.4472611924632439\n",
      "GD iter. 368/499: loss=0.4472611921952325\n",
      "GD iter. 369/499: loss=0.4472611919370027\n",
      "GD iter. 370/499: loss=0.4472611916881898\n",
      "GD iter. 371/499: loss=0.4472611914484434\n",
      "GD iter. 372/499: loss=0.44726119121742625\n",
      "GD iter. 373/499: loss=0.4472611909948141\n",
      "GD iter. 374/499: loss=0.4472611907802949\n",
      "GD iter. 375/499: loss=0.4472611905735682\n",
      "GD iter. 376/499: loss=0.44726119037434553\n",
      "GD iter. 377/499: loss=0.4472611901823487\n",
      "GD iter. 378/499: loss=0.4472611899973105\n",
      "GD iter. 379/499: loss=0.4472611898189735\n",
      "GD iter. 380/499: loss=0.4472611896470904\n",
      "GD iter. 381/499: loss=0.4472611894814225\n",
      "GD iter. 382/499: loss=0.447261189321741\n",
      "GD iter. 383/499: loss=0.44726118916782526\n",
      "GD iter. 384/499: loss=0.44726118901946266\n",
      "GD iter. 385/499: loss=0.4472611888764493\n",
      "GD iter. 386/499: loss=0.44726118873858806\n",
      "GD iter. 387/499: loss=0.4472611886056901\n",
      "GD iter. 388/499: loss=0.44726118847757307\n",
      "GD iter. 389/499: loss=0.4472611883540616\n",
      "GD iter. 390/499: loss=0.44726118823498695\n",
      "GD iter. 391/499: loss=0.4472611881201866\n",
      "GD iter. 392/499: loss=0.4472611880095043\n",
      "GD iter. 393/499: loss=0.4472611879027895\n",
      "GD iter. 394/499: loss=0.44726118779989715\n",
      "GD iter. 395/499: loss=0.4472611877006878\n",
      "GD iter. 396/499: loss=0.4472611876050271\n",
      "GD iter. 397/499: loss=0.4472611875127857\n",
      "GD iter. 398/499: loss=0.4472611874238391\n",
      "GD iter. 399/499: loss=0.44726118733806736\n",
      "GD iter. 400/499: loss=0.4472611872553551\n",
      "GD iter. 401/499: loss=0.44726118717559116\n",
      "GD iter. 402/499: loss=0.44726118709866847\n",
      "GD iter. 403/499: loss=0.4472611870244839\n",
      "GD iter. 404/499: loss=0.44726118695293837\n",
      "GD iter. 405/499: loss=0.4472611868839362\n",
      "GD iter. 406/499: loss=0.44726118681738514\n",
      "GD iter. 407/499: loss=0.4472611867531968\n",
      "GD iter. 408/499: loss=0.4472611866912857\n",
      "GD iter. 409/499: loss=0.4472611866315695\n",
      "GD iter. 410/499: loss=0.4472611865739693\n",
      "GD iter. 411/499: loss=0.4472611865184083\n",
      "GD iter. 412/499: loss=0.4472611864648135\n",
      "GD iter. 413/499: loss=0.4472611864131137\n",
      "GD iter. 414/499: loss=0.4472611863632409\n",
      "GD iter. 415/499: loss=0.44726118631512946\n",
      "GD iter. 416/499: loss=0.447261186268716\n",
      "GD iter. 417/499: loss=0.4472611862239396\n",
      "GD iter. 418/499: loss=0.44726118618074145\n",
      "GD iter. 419/499: loss=0.447261186139065\n",
      "GD iter. 420/499: loss=0.44726118609885585\n",
      "GD iter. 421/499: loss=0.44726118606006127\n",
      "GD iter. 422/499: loss=0.4472611860226307\n",
      "GD iter. 423/499: loss=0.44726118598651543\n",
      "GD iter. 424/499: loss=0.4472611859516684\n",
      "GD iter. 425/499: loss=0.44726118591804437\n",
      "GD iter. 426/499: loss=0.4472611858855997\n",
      "GD iter. 427/499: loss=0.4472611858542923\n",
      "GD iter. 428/499: loss=0.44726118582408164\n",
      "GD iter. 429/499: loss=0.4472611857949288\n",
      "GD iter. 430/499: loss=0.447261185766796\n",
      "GD iter. 431/499: loss=0.44726118573964696\n",
      "GD iter. 432/499: loss=0.4472611857134468\n",
      "GD iter. 433/499: loss=0.4472611856881618\n",
      "GD iter. 434/499: loss=0.44726118566375933\n",
      "GD iter. 435/499: loss=0.4472611856402084\n",
      "GD iter. 436/499: loss=0.44726118561747863\n",
      "GD iter. 437/499: loss=0.4472611855955408\n",
      "GD iter. 438/499: loss=0.44726118557436706\n",
      "GD iter. 439/499: loss=0.4472611855539305\n",
      "GD iter. 440/499: loss=0.4472611855342049\n",
      "GD iter. 441/499: loss=0.44726118551516514\n",
      "GD iter. 442/499: loss=0.447261185496787\n",
      "GD iter. 443/499: loss=0.44726118547904714\n",
      "GD iter. 444/499: loss=0.44726118546192306\n",
      "GD iter. 445/499: loss=0.44726118544539306\n",
      "GD iter. 446/499: loss=0.44726118542943627\n",
      "GD iter. 447/499: loss=0.4472611854140324\n",
      "GD iter. 448/499: loss=0.4472611853991621\n",
      "GD iter. 449/499: loss=0.4472611853848067\n",
      "GD iter. 450/499: loss=0.44726118537094783\n",
      "GD iter. 451/499: loss=0.4472611853575683\n",
      "GD iter. 452/499: loss=0.4472611853446513\n",
      "GD iter. 453/499: loss=0.44726118533218046\n",
      "GD iter. 454/499: loss=0.44726118532014025\n",
      "GD iter. 455/499: loss=0.4472611853085154\n",
      "GD iter. 456/499: loss=0.4472611852972916\n",
      "GD iter. 457/499: loss=0.44726118528645464\n",
      "GD iter. 458/499: loss=0.4472611852759913\n",
      "GD iter. 459/499: loss=0.44726118526588815\n",
      "GD iter. 460/499: loss=0.44726118525613273\n",
      "GD iter. 461/499: loss=0.44726118524671293\n",
      "GD iter. 462/499: loss=0.44726118523761726\n",
      "GD iter. 463/499: loss=0.44726118522883407\n",
      "GD iter. 464/499: loss=0.4472611852203526\n",
      "GD iter. 465/499: loss=0.4472611852121624\n",
      "GD iter. 466/499: loss=0.4472611852042533\n",
      "GD iter. 467/499: loss=0.44726118519661545\n",
      "GD iter. 468/499: loss=0.4472611851892395\n",
      "GD iter. 469/499: loss=0.4472611851821164\n",
      "GD iter. 470/499: loss=0.4472611851752373\n",
      "GD iter. 471/499: loss=0.44726118516859364\n",
      "GD iter. 472/499: loss=0.4472611851621774\n",
      "GD iter. 473/499: loss=0.4472611851559806\n",
      "GD iter. 474/499: loss=0.44726118514999563\n",
      "GD iter. 475/499: loss=0.44726118514421526\n",
      "GD iter. 476/499: loss=0.44726118513863233\n",
      "GD iter. 477/499: loss=0.4472611851332401\n",
      "GD iter. 478/499: loss=0.4472611851280318\n",
      "GD iter. 479/499: loss=0.4472611851230012\n",
      "GD iter. 480/499: loss=0.44726118511814217\n",
      "GD iter. 481/499: loss=0.4472611851134487\n",
      "GD iter. 482/499: loss=0.44726118510891505\n",
      "GD iter. 483/499: loss=0.4472611851045359\n",
      "GD iter. 484/499: loss=0.44726118510030577\n",
      "GD iter. 485/499: loss=0.44726118509621965\n",
      "GD iter. 486/499: loss=0.44726118509227236\n",
      "GD iter. 487/499: loss=0.44726118508845925\n",
      "GD iter. 488/499: loss=0.4472611850847757\n",
      "GD iter. 489/499: loss=0.4472611850812174\n",
      "GD iter. 490/499: loss=0.44726118507777973\n",
      "GD iter. 491/499: loss=0.44726118507445883\n",
      "GD iter. 492/499: loss=0.4472611850712506\n",
      "GD iter. 493/499: loss=0.4472611850681511\n",
      "GD iter. 494/499: loss=0.44726118506515666\n",
      "GD iter. 495/499: loss=0.44726118506226376\n",
      "GD iter. 496/499: loss=0.44726118505946866\n",
      "GD iter. 497/499: loss=0.44726118505676826\n",
      "GD iter. 498/499: loss=0.4472611850541592\n",
      "GD iter. 499/499: loss=0.4472611850516385\n",
      "The Accuracy is: 0.8325\n",
      "The F1 score is: 0.3855\n",
      "The precision is: 0.2759\n",
      "The recall is: 0.6400\n",
      "GD iter. 0/499: loss=0.6757677771604391\n",
      "GD iter. 1/499: loss=0.5345732598030863\n",
      "GD iter. 2/499: loss=0.5208258969871737\n",
      "GD iter. 3/499: loss=0.5119072230769113\n",
      "GD iter. 4/499: loss=0.505192661866686\n",
      "GD iter. 5/499: loss=0.49979547199807917\n",
      "GD iter. 6/499: loss=0.495289160296792\n",
      "GD iter. 7/499: loss=0.4914243151749746\n",
      "GD iter. 8/499: loss=0.48804319993904094\n",
      "GD iter. 9/499: loss=0.4850410596676194\n",
      "GD iter. 10/499: loss=0.4823453421118333\n",
      "GD iter. 11/499: loss=0.47990383356971195\n",
      "GD iter. 12/499: loss=0.47767763267762015\n",
      "GD iter. 13/499: loss=0.47563685387269466\n",
      "GD iter. 14/499: loss=0.473757917503125\n",
      "GD iter. 15/499: loss=0.47202178855916865\n",
      "GD iter. 16/499: loss=0.47041279842651273\n",
      "GD iter. 17/499: loss=0.46891783471505566\n",
      "GD iter. 18/499: loss=0.46752576956773\n",
      "GD iter. 19/499: loss=0.46622704638342116\n",
      "GD iter. 20/499: loss=0.46501337430925654\n",
      "GD iter. 21/499: loss=0.46387749773206466\n",
      "GD iter. 22/499: loss=0.4628130190956582\n",
      "GD iter. 23/499: loss=0.4618142604046417\n",
      "GD iter. 24/499: loss=0.460876153324675\n",
      "GD iter. 25/499: loss=0.45999415078914585\n",
      "GD iter. 26/499: loss=0.45916415503824565\n",
      "GD iter. 27/499: loss=0.4583824583961723\n",
      "GD iter. 28/499: loss=0.4576456940531648\n",
      "GD iter. 29/499: loss=0.4569507947997358\n",
      "GD iter. 30/499: loss=0.45629495815033333\n",
      "GD iter. 31/499: loss=0.4556756166515532\n",
      "GD iter. 32/499: loss=0.45509041243519277\n",
      "GD iter. 33/499: loss=0.4545371752754918\n",
      "GD iter. 34/499: loss=0.4540139035611355\n",
      "GD iter. 35/499: loss=0.4535187477087779\n",
      "GD iter. 36/499: loss=0.4530499956350138\n",
      "GD iter. 37/499: loss=0.4526060599743651\n",
      "GD iter. 38/499: loss=0.45218546678666705\n",
      "GD iter. 39/499: loss=0.4517868455416966\n",
      "GD iter. 40/499: loss=0.4514089202045599\n",
      "GD iter. 41/499: loss=0.4510505012741769\n",
      "GD iter. 42/499: loss=0.45071047865063973\n",
      "GD iter. 43/499: loss=0.45038781522638993\n",
      "GD iter. 44/499: loss=0.4500815411119401\n",
      "GD iter. 45/499: loss=0.44979074841991373\n",
      "GD iter. 46/499: loss=0.44951458654203114\n",
      "GD iter. 47/499: loss=0.4492522578627406\n",
      "GD iter. 48/499: loss=0.449003013860809\n",
      "GD iter. 49/499: loss=0.4487661515566135\n",
      "GD iter. 50/499: loss=0.44854101026831716\n",
      "GD iter. 51/499: loss=0.4483269686447476\n",
      "GD iter. 52/499: loss=0.44812344194674886\n",
      "GD iter. 53/499: loss=0.4479298795521749\n",
      "GD iter. 54/499: loss=0.44774576266261296\n",
      "GD iter. 55/499: loss=0.4475706021924461\n",
      "GD iter. 56/499: loss=0.4474039368230544\n",
      "GD iter. 57/499: loss=0.44724533120685467\n",
      "GD iter. 58/499: loss=0.4470943743075371\n",
      "GD iter. 59/499: loss=0.44695067786431175\n",
      "GD iter. 60/499: loss=0.44681387496924685\n",
      "GD iter. 61/499: loss=0.4466836187479046\n",
      "GD iter. 62/499: loss=0.4465595811344688\n",
      "GD iter. 63/499: loss=0.44644145173342825\n",
      "GD iter. 64/499: loss=0.44632893676065954\n",
      "GD iter. 65/499: loss=0.4462217580574364\n",
      "GD iter. 66/499: loss=0.44611965217150606\n",
      "GD iter. 67/499: loss=0.44602236949992075\n",
      "GD iter. 68/499: loss=0.4459296734887975\n",
      "GD iter. 69/499: loss=0.4458413398856173\n",
      "GD iter. 70/499: loss=0.44575715604006727\n",
      "GD iter. 71/499: loss=0.44567692024977884\n",
      "GD iter. 72/499: loss=0.4456004411476339\n",
      "GD iter. 73/499: loss=0.44552753712759435\n",
      "GD iter. 74/499: loss=0.4454580358062684\n",
      "GD iter. 75/499: loss=0.4453917735176606\n",
      "GD iter. 76/499: loss=0.4453285948387608\n",
      "GD iter. 77/499: loss=0.44526835214381844\n",
      "GD iter. 78/499: loss=0.44521090518532624\n",
      "GD iter. 79/499: loss=0.4451561206998861\n",
      "GD iter. 80/499: loss=0.44510387203728047\n",
      "GD iter. 81/499: loss=0.4450540388111997\n",
      "GD iter. 82/499: loss=0.44500650657019425\n",
      "GD iter. 83/499: loss=0.4449611664875295\n",
      "GD iter. 84/499: loss=0.4449179150687208\n",
      "GD iter. 85/499: loss=0.44487665387561603\n",
      "GD iter. 86/499: loss=0.4448372892659756\n",
      "GD iter. 87/499: loss=0.44479973214757657\n",
      "GD iter. 88/499: loss=0.4447638977459374\n",
      "GD iter. 89/499: loss=0.4447297053848229\n",
      "GD iter. 90/499: loss=0.4446970782787504\n",
      "GD iter. 91/499: loss=0.44466594333676523\n",
      "GD iter. 92/499: loss=0.44463623097681737\n",
      "GD iter. 93/499: loss=0.4446078749500992\n",
      "GD iter. 94/499: loss=0.4445808121747623\n",
      "GD iter. 95/499: loss=0.4445549825784615\n",
      "GD iter. 96/499: loss=0.4445303289492137\n",
      "GD iter. 97/499: loss=0.44450679679409133\n",
      "GD iter. 98/499: loss=0.4444843342053038\n",
      "GD iter. 99/499: loss=0.44446289173324466\n",
      "GD iter. 100/499: loss=0.44444242226611336\n",
      "GD iter. 101/499: loss=0.44442288091574267\n",
      "GD iter. 102/499: loss=0.44440422490928516\n",
      "GD iter. 103/499: loss=0.44438641348643765\n",
      "GD iter. 104/499: loss=0.4443694078018951\n",
      "GD iter. 105/499: loss=0.4443531708327527\n",
      "GD iter. 106/499: loss=0.4443376672905847\n",
      "GD iter. 107/499: loss=0.44432286353794764\n",
      "GD iter. 108/499: loss=0.44430872750907263\n",
      "GD iter. 109/499: loss=0.4442952286345207\n",
      "GD iter. 110/499: loss=0.4442823377695916\n",
      "GD iter. 111/499: loss=0.44427002712628894\n",
      "GD iter. 112/499: loss=0.44425827020865266\n",
      "GD iter. 113/499: loss=0.44424704175128377\n",
      "GD iter. 114/499: loss=0.44423631766089566\n",
      "GD iter. 115/499: loss=0.44422607496073446\n",
      "GD iter. 116/499: loss=0.4442162917377187\n",
      "GD iter. 117/499: loss=0.44420694709216324\n",
      "GD iter. 118/499: loss=0.4441980210899491\n",
      "GD iter. 119/499: loss=0.4441894947170195\n",
      "GD iter. 120/499: loss=0.4441813498360813\n",
      "GD iter. 121/499: loss=0.4441735691454006\n",
      "GD iter. 122/499: loss=0.44416613613958783\n",
      "GD iter. 123/499: loss=0.44415903507227145\n",
      "GD iter. 124/499: loss=0.4441522509205658\n",
      "GD iter. 125/499: loss=0.44414576935124433\n",
      "GD iter. 126/499: loss=0.44413957668853177\n",
      "GD iter. 127/499: loss=0.4441336598834362\n",
      "GD iter. 128/499: loss=0.4441280064845459\n",
      "GD iter. 129/499: loss=0.444122604610214\n",
      "GD iter. 130/499: loss=0.44411744292206934\n",
      "GD iter. 131/499: loss=0.44411251059978324\n",
      "GD iter. 132/499: loss=0.44410779731703287\n",
      "GD iter. 133/499: loss=0.44410329321860204\n",
      "GD iter. 134/499: loss=0.4440989888985659\n",
      "GD iter. 135/499: loss=0.44409487537950243\n",
      "GD iter. 136/499: loss=0.44409094409268746\n",
      "GD iter. 137/499: loss=0.44408718685921916\n",
      "GD iter. 138/499: loss=0.44408359587203095\n",
      "GD iter. 139/499: loss=0.4440801636787492\n",
      "GD iter. 140/499: loss=0.4440768831653551\n",
      "GD iter. 141/499: loss=0.44407374754061063\n",
      "GD iter. 142/499: loss=0.4440707503212146\n",
      "GD iter. 143/499: loss=0.4440678853176521\n",
      "GD iter. 144/499: loss=0.444065146620704\n",
      "GD iter. 145/499: loss=0.44406252858858647\n",
      "GD iter. 146/499: loss=0.444060025834689\n",
      "GD iter. 147/499: loss=0.44405763321588393\n",
      "GD iter. 148/499: loss=0.44405534582137846\n",
      "GD iter. 149/499: loss=0.44405315896208564\n",
      "GD iter. 150/499: loss=0.44405106816048834\n",
      "GD iter. 151/499: loss=0.4440490691409727\n",
      "GD iter. 152/499: loss=0.4440471578206102\n",
      "GD iter. 153/499: loss=0.44404533030036614\n",
      "GD iter. 154/499: loss=0.4440435828567141\n",
      "GD iter. 155/499: loss=0.4440419119336394\n",
      "GD iter. 156/499: loss=0.44404031413500994\n",
      "GD iter. 157/499: loss=0.44403878621730103\n",
      "GD iter. 158/499: loss=0.44403732508265387\n",
      "GD iter. 159/499: loss=0.44403592777225437\n",
      "GD iter. 160/499: loss=0.4440345914600179\n",
      "GD iter. 161/499: loss=0.44403331344656277\n",
      "GD iter. 162/499: loss=0.4440320911534619\n",
      "GD iter. 163/499: loss=0.44403092211775813\n",
      "GD iter. 164/499: loss=0.4440298039867314\n",
      "GD iter. 165/499: loss=0.4440287345129059\n",
      "GD iter. 166/499: loss=0.4440277115492862\n",
      "GD iter. 167/499: loss=0.44402673304481166\n",
      "GD iter. 168/499: loss=0.4440257970400184\n",
      "GD iter. 169/499: loss=0.44402490166290076\n",
      "GD iter. 170/499: loss=0.4440240451249613\n",
      "GD iter. 171/499: loss=0.4440232257174409\n",
      "GD iter. 172/499: loss=0.4440224418077221\n",
      "GD iter. 173/499: loss=0.44402169183589607\n",
      "GD iter. 174/499: loss=0.44402097431148563\n",
      "GD iter. 175/499: loss=0.4440202878103184\n",
      "GD iter. 176/499: loss=0.4440196309715422\n",
      "GD iter. 177/499: loss=0.4440190024947762\n",
      "GD iter. 178/499: loss=0.4440184011373919\n",
      "GD iter. 179/499: loss=0.44401782571191734\n",
      "GD iter. 180/499: loss=0.44401727508356\n",
      "GD iter. 181/499: loss=0.44401674816784165\n",
      "GD iter. 182/499: loss=0.4440162439283408\n",
      "GD iter. 183/499: loss=0.4440157613745377\n",
      "GD iter. 184/499: loss=0.4440152995597566\n",
      "GD iter. 185/499: loss=0.4440148575792023\n",
      "GD iter. 186/499: loss=0.4440144345680841\n",
      "GD iter. 187/499: loss=0.4440140296998262\n",
      "GD iter. 188/499: loss=0.44401364218435846\n",
      "GD iter. 189/499: loss=0.4440132712664843\n",
      "GD iter. 190/499: loss=0.44401291622432254\n",
      "GD iter. 191/499: loss=0.44401257636781977\n",
      "GD iter. 192/499: loss=0.44401225103733\n",
      "GD iter. 193/499: loss=0.44401193960225754\n",
      "GD iter. 194/499: loss=0.444011641459763\n",
      "GD iter. 195/499: loss=0.4440113560335251\n",
      "GD iter. 196/499: loss=0.4440110827725606\n",
      "GD iter. 197/499: loss=0.44401082115009555\n",
      "GD iter. 198/499: loss=0.44401057066248906\n",
      "GD iter. 199/499: loss=0.4440103308282044\n",
      "GD iter. 200/499: loss=0.4440101011868261\n",
      "GD iter. 201/499: loss=0.4440098812981223\n",
      "GD iter. 202/499: loss=0.4440096707411484\n",
      "GD iter. 203/499: loss=0.4440094691133917\n",
      "GD iter. 204/499: loss=0.444009276029953\n",
      "GD iter. 205/499: loss=0.44400909112276726\n",
      "GD iter. 206/499: loss=0.4440089140398566\n",
      "GD iter. 207/499: loss=0.4440087444446186\n",
      "GD iter. 208/499: loss=0.44400858201514587\n",
      "GD iter. 209/499: loss=0.4440084264435761\n",
      "GD iter. 210/499: loss=0.44400827743547083\n",
      "GD iter. 211/499: loss=0.44400813470922285\n",
      "GD iter. 212/499: loss=0.4440079979954897\n",
      "GD iter. 213/499: loss=0.4440078670366517\n",
      "GD iter. 214/499: loss=0.4440077415862954\n",
      "GD iter. 215/499: loss=0.4440076214087198\n",
      "GD iter. 216/499: loss=0.44400750627846364\n",
      "GD iter. 217/499: loss=0.44400739597985506\n",
      "GD iter. 218/499: loss=0.4440072903065807\n",
      "GD iter. 219/499: loss=0.4440071890612737\n",
      "GD iter. 220/499: loss=0.4440070920551209\n",
      "GD iter. 221/499: loss=0.4440069991074863\n",
      "GD iter. 222/499: loss=0.4440069100455532\n",
      "GD iter. 223/499: loss=0.4440068247039796\n",
      "GD iter. 224/499: loss=0.4440067429245712\n",
      "GD iter. 225/499: loss=0.44400666455596816\n",
      "GD iter. 226/499: loss=0.4440065894533449\n",
      "GD iter. 227/499: loss=0.44400651747812503\n",
      "GD iter. 228/499: loss=0.4440064484977068\n",
      "GD iter. 229/499: loss=0.4440063823852025\n",
      "GD iter. 230/499: loss=0.44400631901918863\n",
      "GD iter. 231/499: loss=0.44400625828346674\n",
      "GD iter. 232/499: loss=0.44400620006683605\n",
      "GD iter. 233/499: loss=0.44400614426287455\n",
      "GD iter. 234/499: loss=0.4440060907697311\n",
      "GD iter. 235/499: loss=0.4440060394899262\n",
      "GD iter. 236/499: loss=0.4440059903301614\n",
      "GD iter. 237/499: loss=0.4440059432011375\n",
      "GD iter. 238/499: loss=0.44400589801738033\n",
      "GD iter. 239/499: loss=0.4440058546970746\n",
      "GD iter. 240/499: loss=0.44400581316190474\n",
      "GD iter. 241/499: loss=0.44400577333690305\n",
      "GD iter. 242/499: loss=0.4440057351503047\n",
      "GD iter. 243/499: loss=0.4440056985334083\n",
      "GD iter. 244/499: loss=0.44400566342044306\n",
      "GD iter. 245/499: loss=0.4440056297484423\n",
      "GD iter. 246/499: loss=0.44400559745712215\n",
      "GD iter. 247/499: loss=0.4440055664887647\n",
      "GD iter. 248/499: loss=0.44400553678810756\n",
      "GD iter. 249/499: loss=0.44400550830223784\n",
      "GD iter. 250/499: loss=0.44400548098049064\n",
      "GD iter. 251/499: loss=0.4440054547743516\n",
      "GD iter. 252/499: loss=0.44400542963736483\n",
      "GD iter. 253/499: loss=0.4440054055250436\n",
      "GD iter. 254/499: loss=0.4440053823947859\n",
      "GD iter. 255/499: loss=0.44400536020579284\n",
      "GD iter. 256/499: loss=0.44400533891899185\n",
      "GD iter. 257/499: loss=0.44400531849696173\n",
      "GD iter. 258/499: loss=0.4440052989038621\n",
      "GD iter. 259/499: loss=0.4440052801053653\n",
      "GD iter. 260/499: loss=0.4440052620685913\n",
      "GD iter. 261/499: loss=0.44400524476204667\n",
      "GD iter. 262/499: loss=0.44400522815556365\n",
      "GD iter. 263/499: loss=0.4440052122202445\n",
      "GD iter. 264/499: loss=0.4440051969284067\n",
      "GD iter. 265/499: loss=0.4440051822535308\n",
      "GD iter. 266/499: loss=0.44400516817021146\n",
      "GD iter. 267/499: loss=0.44400515465410884\n",
      "GD iter. 268/499: loss=0.4440051416819042\n",
      "GD iter. 269/499: loss=0.4440051292312554\n",
      "GD iter. 270/499: loss=0.44400511728075587\n",
      "GD iter. 271/499: loss=0.4440051058098943\n",
      "GD iter. 272/499: loss=0.4440050947990173\n",
      "GD iter. 273/499: loss=0.44400508422929197\n",
      "GD iter. 274/499: loss=0.44400507408267165\n",
      "GD iter. 275/499: loss=0.4440050643418625\n",
      "GD iter. 276/499: loss=0.44400505499029097\n",
      "GD iter. 277/499: loss=0.44400504601207413\n",
      "GD iter. 278/499: loss=0.44400503739198954\n",
      "GD iter. 279/499: loss=0.444005029115448\n",
      "GD iter. 280/499: loss=0.44400502116846624\n",
      "GD iter. 281/499: loss=0.4440050135376415\n",
      "GD iter. 282/499: loss=0.44400500621012695\n",
      "GD iter. 283/499: loss=0.4440049991736084\n",
      "GD iter. 284/499: loss=0.4440049924162813\n",
      "GD iter. 285/499: loss=0.4440049859268299\n",
      "GD iter. 286/499: loss=0.4440049796944063\n",
      "GD iter. 287/499: loss=0.4440049737086106\n",
      "GD iter. 288/499: loss=0.4440049679594723\n",
      "GD iter. 289/499: loss=0.4440049624374325\n",
      "GD iter. 290/499: loss=0.4440049571333258\n",
      "GD iter. 291/499: loss=0.4440049520383643\n",
      "GD iter. 292/499: loss=0.4440049471441224\n",
      "GD iter. 293/499: loss=0.44400494244252\n",
      "GD iter. 294/499: loss=0.44400493792580953\n",
      "GD iter. 295/499: loss=0.44400493358656135\n",
      "GD iter. 296/499: loss=0.4440049294176504\n",
      "GD iter. 297/499: loss=0.4440049254122441\n",
      "GD iter. 298/499: loss=0.44400492156378885\n",
      "GD iter. 299/499: loss=0.4440049178659998\n",
      "GD iter. 300/499: loss=0.4440049143128489\n",
      "GD iter. 301/499: loss=0.44400491089855426\n",
      "GD iter. 302/499: loss=0.4440049076175701\n",
      "GD iter. 303/499: loss=0.4440049044645766\n",
      "GD iter. 304/499: loss=0.44400490143447074\n",
      "GD iter. 305/499: loss=0.4440048985223575\n",
      "GD iter. 306/499: loss=0.4440048957235408\n",
      "GD iter. 307/499: loss=0.4440048930335154\n",
      "GD iter. 308/499: loss=0.4440048904479588\n",
      "GD iter. 309/499: loss=0.4440048879627245\n",
      "GD iter. 310/499: loss=0.4440048855738335\n",
      "GD iter. 311/499: loss=0.4440048832774683\n",
      "GD iter. 312/499: loss=0.4440048810699656\n",
      "GD iter. 313/499: loss=0.44400487894781077\n",
      "GD iter. 314/499: loss=0.4440048769076307\n",
      "GD iter. 315/499: loss=0.44400487494618857\n",
      "GD iter. 316/499: loss=0.4440048730603783\n",
      "GD iter. 317/499: loss=0.44400487124721877\n",
      "GD iter. 318/499: loss=0.44400486950384904\n",
      "GD iter. 319/499: loss=0.444004867827523\n",
      "GD iter. 320/499: loss=0.4440048662156054\n",
      "GD iter. 321/499: loss=0.44400486466556643\n",
      "GD iter. 322/499: loss=0.4440048631749781\n",
      "GD iter. 323/499: loss=0.44400486174150916\n",
      "GD iter. 324/499: loss=0.44400486036292275\n",
      "GD iter. 325/499: loss=0.44400485903707065\n",
      "GD iter. 326/499: loss=0.4440048577618908\n",
      "GD iter. 327/499: loss=0.4440048565354038\n",
      "GD iter. 328/499: loss=0.44400485535570894\n",
      "GD iter. 329/499: loss=0.4440048542209815\n",
      "GD iter. 330/499: loss=0.44400485312946936\n",
      "GD iter. 331/499: loss=0.4440048520794903\n",
      "GD iter. 332/499: loss=0.4440048510694289\n",
      "GD iter. 333/499: loss=0.44400485009773394\n",
      "GD iter. 334/499: loss=0.444004849162916\n",
      "GD iter. 335/499: loss=0.4440048482635445\n",
      "GD iter. 336/499: loss=0.4440048473982457\n",
      "GD iter. 337/499: loss=0.4440048465657003\n",
      "GD iter. 338/499: loss=0.4440048457646411\n",
      "GD iter. 339/499: loss=0.44400484499385107\n",
      "GD iter. 340/499: loss=0.4440048442521613\n",
      "GD iter. 341/499: loss=0.44400484353844893\n",
      "GD iter. 342/499: loss=0.44400484285163544\n",
      "GD iter. 343/499: loss=0.44400484219068487\n",
      "GD iter. 344/499: loss=0.44400484155460174\n",
      "GD iter. 345/499: loss=0.4440048409424301\n",
      "GD iter. 346/499: loss=0.44400484035325133\n",
      "GD iter. 347/499: loss=0.44400483978618294\n",
      "GD iter. 348/499: loss=0.44400483924037704\n",
      "GD iter. 349/499: loss=0.44400483871501917\n",
      "GD iter. 350/499: loss=0.44400483820932646\n",
      "GD iter. 351/499: loss=0.44400483772254673\n",
      "GD iter. 352/499: loss=0.44400483725395734\n",
      "GD iter. 353/499: loss=0.44400483680286384\n",
      "GD iter. 354/499: loss=0.44400483636859867\n",
      "GD iter. 355/499: loss=0.4440048359505207\n",
      "GD iter. 356/499: loss=0.4440048355480133\n",
      "GD iter. 357/499: loss=0.44400483516048433\n",
      "GD iter. 358/499: loss=0.4440048347873642\n",
      "GD iter. 359/499: loss=0.4440048344281057\n",
      "GD iter. 360/499: loss=0.4440048340821827\n",
      "GD iter. 361/499: loss=0.44400483374909\n",
      "GD iter. 362/499: loss=0.44400483342834124\n",
      "GD iter. 363/499: loss=0.44400483311946926\n",
      "GD iter. 364/499: loss=0.44400483282202485\n",
      "GD iter. 365/499: loss=0.44400483253557643\n",
      "GD iter. 366/499: loss=0.4440048322597088\n",
      "GD iter. 367/499: loss=0.44400483199402285\n",
      "GD iter. 368/499: loss=0.44400483173813476\n",
      "GD iter. 369/499: loss=0.4440048314916758\n",
      "GD iter. 370/499: loss=0.44400483125429113\n",
      "GD iter. 371/499: loss=0.4440048310256397\n",
      "GD iter. 372/499: loss=0.4440048308053933\n",
      "GD iter. 373/499: loss=0.4440048305932367\n",
      "GD iter. 374/499: loss=0.4440048303888667\n",
      "GD iter. 375/499: loss=0.44400483019199116\n",
      "GD iter. 376/499: loss=0.44400483000233\n",
      "GD iter. 377/499: loss=0.4440048298196132\n",
      "GD iter. 378/499: loss=0.4440048296435811\n",
      "GD iter. 379/499: loss=0.4440048294739844\n",
      "GD iter. 380/499: loss=0.444004829310583\n",
      "GD iter. 381/499: loss=0.4440048291531455\n",
      "GD iter. 382/499: loss=0.44400482900145044\n",
      "GD iter. 383/499: loss=0.4440048288552837\n",
      "GD iter. 384/499: loss=0.4440048287144398\n",
      "GD iter. 385/499: loss=0.44400482857872103\n",
      "GD iter. 386/499: loss=0.44400482844793715\n",
      "GD iter. 387/499: loss=0.444004828321905\n",
      "GD iter. 388/499: loss=0.4440048282004486\n",
      "GD iter. 389/499: loss=0.4440048280833985\n",
      "GD iter. 390/499: loss=0.4440048279705916\n",
      "GD iter. 391/499: loss=0.444004827861871\n",
      "GD iter. 392/499: loss=0.44400482775708583\n",
      "GD iter. 393/499: loss=0.44400482765609073\n",
      "GD iter. 394/499: loss=0.444004827558746\n",
      "GD iter. 395/499: loss=0.4440048274649171\n",
      "GD iter. 396/499: loss=0.4440048273744745\n",
      "GD iter. 397/499: loss=0.4440048272872937\n",
      "GD iter. 398/499: loss=0.4440048272032547\n",
      "GD iter. 399/499: loss=0.4440048271222421\n",
      "GD iter. 400/499: loss=0.4440048270441448\n",
      "GD iter. 401/499: loss=0.444004826968856\n",
      "GD iter. 402/499: loss=0.44400482689627263\n",
      "GD iter. 403/499: loss=0.44400482682629566\n",
      "GD iter. 404/499: loss=0.44400482675882974\n",
      "GD iter. 405/499: loss=0.444004826693783\n",
      "GD iter. 406/499: loss=0.4440048266310671\n",
      "GD iter. 407/499: loss=0.44400482657059687\n",
      "GD iter. 408/499: loss=0.4440048265122905\n",
      "GD iter. 409/499: loss=0.44400482645606903\n",
      "GD iter. 410/499: loss=0.4440048264018566\n",
      "GD iter. 411/499: loss=0.44400482634958005\n",
      "GD iter. 412/499: loss=0.44400482629916893\n",
      "GD iter. 413/499: loss=0.44400482625055554\n",
      "GD iter. 414/499: loss=0.44400482620367454\n",
      "GD iter. 415/499: loss=0.444004826158463\n",
      "GD iter. 416/499: loss=0.4440048261148606\n",
      "GD iter. 417/499: loss=0.4440048260728087\n",
      "GD iter. 418/499: loss=0.4440048260322515\n",
      "GD iter. 419/499: loss=0.4440048259931348\n",
      "GD iter. 420/499: loss=0.44400482595540647\n",
      "GD iter. 421/499: loss=0.4440048259190163\n",
      "GD iter. 422/499: loss=0.4440048258839162\n",
      "GD iter. 423/499: loss=0.44400482585005946\n",
      "GD iter. 424/499: loss=0.4440048258174013\n",
      "GD iter. 425/499: loss=0.4440048257858984\n",
      "GD iter. 426/499: loss=0.4440048257555096\n",
      "GD iter. 427/499: loss=0.4440048257261944\n",
      "GD iter. 428/499: loss=0.4440048256979144\n",
      "GD iter. 429/499: loss=0.4440048256706323\n",
      "GD iter. 430/499: loss=0.4440048256443125\n",
      "GD iter. 431/499: loss=0.44400482561892013\n",
      "GD iter. 432/499: loss=0.4440048255944222\n",
      "GD iter. 433/499: loss=0.44400482557078674\n",
      "GD iter. 434/499: loss=0.4440048255479827\n",
      "GD iter. 435/499: loss=0.44400482552598036\n",
      "GD iter. 436/499: loss=0.4440048255047512\n",
      "GD iter. 437/499: loss=0.4440048254842674\n",
      "GD iter. 438/499: loss=0.44400482546450276\n",
      "GD iter. 439/499: loss=0.44400482544543096\n",
      "GD iter. 440/499: loss=0.4440048254270278\n",
      "GD iter. 441/499: loss=0.44400482540926933\n",
      "GD iter. 442/499: loss=0.4440048253921327\n",
      "GD iter. 443/499: loss=0.4440048253755955\n",
      "GD iter. 444/499: loss=0.44400482535963676\n",
      "GD iter. 445/499: loss=0.44400482534423574\n",
      "GD iter. 446/499: loss=0.4440048253293726\n",
      "GD iter. 447/499: loss=0.44400482531502844\n",
      "GD iter. 448/499: loss=0.4440048253011847\n",
      "GD iter. 449/499: loss=0.44400482528782365\n",
      "GD iter. 450/499: loss=0.44400482527492824\n",
      "GD iter. 451/499: loss=0.44400482526248203\n",
      "GD iter. 452/499: loss=0.44400482525046897\n",
      "GD iter. 453/499: loss=0.4440048252388739\n",
      "GD iter. 454/499: loss=0.4440048252276821\n",
      "GD iter. 455/499: loss=0.4440048252168791\n",
      "GD iter. 456/499: loss=0.44400482520645146\n",
      "GD iter. 457/499: loss=0.4440048251963858\n",
      "GD iter. 458/499: loss=0.44400482518666934\n",
      "GD iter. 459/499: loss=0.44400482517728995\n",
      "GD iter. 460/499: loss=0.44400482516823553\n",
      "GD iter. 461/499: loss=0.4440048251594948\n",
      "GD iter. 462/499: loss=0.44400482515105666\n",
      "GD iter. 463/499: loss=0.4440048251429105\n",
      "GD iter. 464/499: loss=0.44400482513504613\n",
      "GD iter. 465/499: loss=0.4440048251274536\n",
      "GD iter. 466/499: loss=0.4440048251201234\n",
      "GD iter. 467/499: loss=0.44400482511304634\n",
      "GD iter. 468/499: loss=0.4440048251062136\n",
      "GD iter. 469/499: loss=0.44400482509961653\n",
      "GD iter. 470/499: loss=0.4440048250932468\n",
      "GD iter. 471/499: loss=0.4440048250870969\n",
      "GD iter. 472/499: loss=0.4440048250811586\n",
      "GD iter. 473/499: loss=0.4440048250754247\n",
      "GD iter. 474/499: loss=0.4440048250698883\n",
      "GD iter. 475/499: loss=0.4440048250645422\n",
      "GD iter. 476/499: loss=0.4440048250593799\n",
      "GD iter. 477/499: loss=0.4440048250543951\n",
      "GD iter. 478/499: loss=0.44400482504958144\n",
      "GD iter. 479/499: loss=0.4440048250449329\n",
      "GD iter. 480/499: loss=0.44400482504044386\n",
      "GD iter. 481/499: loss=0.44400482503610894\n",
      "GD iter. 482/499: loss=0.44400482503192257\n",
      "GD iter. 483/499: loss=0.4440048250278795\n",
      "GD iter. 484/499: loss=0.4440048250239751\n",
      "GD iter. 485/499: loss=0.4440048250202041\n",
      "GD iter. 486/499: loss=0.4440048250165624\n",
      "GD iter. 487/499: loss=0.44400482501304517\n",
      "GD iter. 488/499: loss=0.4440048250096481\n",
      "GD iter. 489/499: loss=0.44400482500636707\n",
      "GD iter. 490/499: loss=0.4440048250031982\n",
      "GD iter. 491/499: loss=0.4440048250001375\n",
      "GD iter. 492/499: loss=0.4440048249971812\n",
      "GD iter. 493/499: loss=0.44400482499432575\n",
      "GD iter. 494/499: loss=0.44400482499156757\n",
      "GD iter. 495/499: loss=0.4440048249889035\n",
      "GD iter. 496/499: loss=0.44400482498633015\n",
      "GD iter. 497/499: loss=0.4440048249838444\n",
      "GD iter. 498/499: loss=0.44400482498144334\n",
      "GD iter. 499/499: loss=0.44400482497912386\n",
      "The Accuracy is: 0.8259\n",
      "The F1 score is: 0.3537\n",
      "The precision is: 0.2636\n",
      "The recall is: 0.5370\n",
      "GD iter. 0/499: loss=0.6886149636540967\n",
      "GD iter. 1/499: loss=0.5375741517918611\n",
      "GD iter. 2/499: loss=0.5250823872041119\n",
      "GD iter. 3/499: loss=0.5167545618203028\n",
      "GD iter. 4/499: loss=0.5103465847364106\n",
      "GD iter. 5/499: loss=0.5051261546084668\n",
      "GD iter. 6/499: loss=0.5007311100178365\n",
      "GD iter. 7/499: loss=0.49694206901522797\n",
      "GD iter. 8/499: loss=0.4936163822539115\n",
      "GD iter. 9/499: loss=0.4906572453829394\n",
      "GD iter. 10/499: loss=0.48799646388941986\n",
      "GD iter. 11/499: loss=0.48558430794342416\n",
      "GD iter. 12/499: loss=0.48338335419726525\n",
      "GD iter. 13/499: loss=0.48136464036083004\n",
      "GD iter. 14/499: loss=0.4795051952852917\n",
      "GD iter. 15/499: loss=0.4777864080087372\n",
      "GD iter. 16/499: loss=0.47619292169760385\n",
      "GD iter. 17/499: loss=0.47471186431757717\n",
      "GD iter. 18/499: loss=0.47333230062156983\n",
      "GD iter. 19/499: loss=0.4720448330178771\n",
      "GD iter. 20/499: loss=0.4708413048305033\n",
      "GD iter. 21/499: loss=0.46971457547091433\n",
      "GD iter. 22/499: loss=0.46865834712115484\n",
      "GD iter. 23/499: loss=0.46766702900419244\n",
      "GD iter. 24/499: loss=0.46673562955787895\n",
      "GD iter. 25/499: loss=0.4658596696572847\n",
      "GD iter. 26/499: loss=0.4650351119502932\n",
      "GD iter. 27/499: loss=0.46425830269719925\n",
      "GD iter. 28/499: loss=0.46352592343548593\n",
      "GD iter. 29/499: loss=0.4628349504540456\n",
      "GD iter. 30/499: loss=0.4621826205405784\n",
      "GD iter. 31/499: loss=0.4615664018173979\n",
      "GD iter. 32/499: loss=0.4609839687418611\n",
      "GD iter. 33/499: loss=0.4604331805437797\n",
      "GD iter. 34/499: loss=0.4599120625212167\n",
      "GD iter. 35/499: loss=0.45941878973052563\n",
      "GD iter. 36/499: loss=0.458951672695232\n",
      "GD iter. 37/499: loss=0.4585091448277902\n",
      "GD iter. 38/499: loss=0.4580897513130356\n",
      "GD iter. 39/499: loss=0.4576921392457273\n",
      "GD iter. 40/499: loss=0.4573150488494955\n",
      "GD iter. 41/499: loss=0.4569573056326817\n",
      "GD iter. 42/499: loss=0.456617813359459\n",
      "GD iter. 43/499: loss=0.4562955477333251\n",
      "GD iter. 44/499: loss=0.4559895507054649\n",
      "GD iter. 45/499: loss=0.4556989253332033\n",
      "GD iter. 46/499: loss=0.45542283112436815\n",
      "GD iter. 47/499: loss=0.4551604798122343\n",
      "GD iter. 48/499: loss=0.4549111315131644\n",
      "GD iter. 49/499: loss=0.4546740912253428\n",
      "GD iter. 50/499: loss=0.4544487056323294\n",
      "GD iter. 51/499: loss=0.45423436017969837\n",
      "GD iter. 52/499: loss=0.45403047639690797\n",
      "GD iter. 53/499: loss=0.4538365094398846\n",
      "GD iter. 54/499: loss=0.45365194583267243\n",
      "GD iter. 55/499: loss=0.4534763013889888\n",
      "GD iter. 56/499: loss=0.45330911929667816\n",
      "GD iter. 57/499: loss=0.4531499683499388\n",
      "GD iter. 58/499: loss=0.4529984413158312\n",
      "GD iter. 59/499: loss=0.45285415342301355\n",
      "GD iter. 60/499: loss=0.4527167409619134\n",
      "GD iter. 61/499: loss=0.45258585998664713\n",
      "GD iter. 62/499: loss=0.4524611851099851\n",
      "GD iter. 63/499: loss=0.45234240838351975\n",
      "GD iter. 64/499: loss=0.4522292382559665\n",
      "GD iter. 65/499: loss=0.4521213986032055\n",
      "GD iter. 66/499: loss=0.45201862782428226\n",
      "GD iter. 67/499: loss=0.45192067799812247\n",
      "GD iter. 68/499: loss=0.4518273140962042\n",
      "GD iter. 69/499: loss=0.45173831324686015\n",
      "GD iter. 70/499: loss=0.451653464047271\n",
      "GD iter. 71/499: loss=0.4515725659195622\n",
      "GD iter. 72/499: loss=0.4514954285077241\n",
      "GD iter. 73/499: loss=0.451421871112364\n",
      "GD iter. 74/499: loss=0.4513517221605474\n",
      "GD iter. 75/499: loss=0.45128481870822035\n",
      "GD iter. 76/499: loss=0.4512210059729075\n",
      "GD iter. 77/499: loss=0.4511601368945747\n",
      "GD iter. 78/499: loss=0.45110207172271133\n",
      "GD iter. 79/499: loss=0.4510466776278426\n",
      "GD iter. 80/499: loss=0.4509938283358261\n",
      "GD iter. 81/499: loss=0.4509434037834114\n",
      "GD iter. 82/499: loss=0.45089528979366233\n",
      "GD iter. 83/499: loss=0.4508493777699429\n",
      "GD iter. 84/499: loss=0.45080556440726977\n",
      "GD iter. 85/499: loss=0.450763751419922\n",
      "GD iter. 86/499: loss=0.45072384528427784\n",
      "GD iter. 87/499: loss=0.45068575699592633\n",
      "GD iter. 88/499: loss=0.4506494018401668\n",
      "GD iter. 89/499: loss=0.4506146991750745\n",
      "GD iter. 90/499: loss=0.450581572226368\n",
      "GD iter. 91/499: loss=0.4505499478933633\n",
      "GD iter. 92/499: loss=0.45051975656535626\n",
      "GD iter. 93/499: loss=0.4504909319478104\n",
      "GD iter. 94/499: loss=0.45046341089777736\n",
      "GD iter. 95/499: loss=0.450437133268009\n",
      "GD iter. 96/499: loss=0.45041204175925964\n",
      "GD iter. 97/499: loss=0.4503880817803089\n",
      "GD iter. 98/499: loss=0.4503652013152633\n",
      "GD iter. 99/499: loss=0.45034335079772714\n",
      "GD iter. 100/499: loss=0.45032248299145544\n",
      "GD iter. 101/499: loss=0.45030255287712895\n",
      "GD iter. 102/499: loss=0.45028351754491047\n",
      "GD iter. 103/499: loss=0.4502653360924665\n",
      "GD iter. 104/499: loss=0.4502479695281526\n",
      "GD iter. 105/499: loss=0.4502313806790855\n",
      "GD iter. 106/499: loss=0.4502155341038341\n",
      "GD iter. 107/499: loss=0.4502003960094849\n",
      "GD iter. 108/499: loss=0.4501859341728459\n",
      "GD iter. 109/499: loss=0.4501721178655698\n",
      "GD iter. 110/499: loss=0.45015891778299016\n",
      "GD iter. 111/499: loss=0.45014630597647404\n",
      "GD iter. 112/499: loss=0.45013425578910937\n",
      "GD iter. 113/499: loss=0.45012274179455003\n",
      "GD iter. 114/499: loss=0.45011173973885876\n",
      "GD iter. 115/499: loss=0.4501012264851908\n",
      "GD iter. 116/499: loss=0.4500911799611734\n",
      "GD iter. 117/499: loss=0.450081579108843\n",
      "GD iter. 118/499: loss=0.45007240383700897\n",
      "GD iter. 119/499: loss=0.45006363497592394\n",
      "GD iter. 120/499: loss=0.4500552542341388\n",
      "GD iter. 121/499: loss=0.4500472441574377\n",
      "GD iter. 122/499: loss=0.45003958808974553\n",
      "GD iter. 123/499: loss=0.4500322701359093\n",
      "GD iter. 124/499: loss=0.4500252751262621\n",
      "GD iter. 125/499: loss=0.4500185885828782\n",
      "GD iter. 126/499: loss=0.45001219668743814\n",
      "GD iter. 127/499: loss=0.450006086250622\n",
      "GD iter. 128/499: loss=0.45000024468295763\n",
      "GD iter. 129/499: loss=0.44999465996705046\n",
      "GD iter. 130/499: loss=0.4499893206311287\n",
      "GD iter. 131/499: loss=0.449984215723839\n",
      "GD iter. 132/499: loss=0.44997933479023167\n",
      "GD iter. 133/499: loss=0.44997466784887685\n",
      "GD iter. 134/499: loss=0.44997020537005916\n",
      "GD iter. 135/499: loss=0.44996593825499503\n",
      "GD iter. 136/499: loss=0.4499618578160274\n",
      "GD iter. 137/499: loss=0.4499579557577462\n",
      "GD iter. 138/499: loss=0.44995422415899494\n",
      "GD iter. 139/499: loss=0.4499506554557157\n",
      "GD iter. 140/499: loss=0.4499472424245976\n",
      "GD iter. 141/499: loss=0.4499439781674861\n",
      "GD iter. 142/499: loss=0.44994085609651835\n",
      "GD iter. 143/499: loss=0.44993786991995155\n",
      "GD iter. 144/499: loss=0.4499350136286482\n",
      "GD iter. 145/499: loss=0.44993228148318937\n",
      "GD iter. 146/499: loss=0.4499296680015861\n",
      "GD iter. 147/499: loss=0.44992716794755916\n",
      "GD iter. 148/499: loss=0.4499247763193616\n",
      "GD iter. 149/499: loss=0.44992248833911797\n",
      "GD iter. 150/499: loss=0.4499202994426557\n",
      "GD iter. 151/499: loss=0.4499182052698065\n",
      "GD iter. 152/499: loss=0.4499162016551537\n",
      "GD iter. 153/499: loss=0.44991428461920624\n",
      "GD iter. 154/499: loss=0.44991245035997957\n",
      "GD iter. 155/499: loss=0.449910695244962\n",
      "GD iter. 156/499: loss=0.4499090158034519\n",
      "GD iter. 157/499: loss=0.44990740871924484\n",
      "GD iter. 158/499: loss=0.4499058708236577\n",
      "GD iter. 159/499: loss=0.4499043990888713\n",
      "GD iter. 160/499: loss=0.4499029906215785\n",
      "GD iter. 161/499: loss=0.4499016426569224\n",
      "GD iter. 162/499: loss=0.4499003525527119\n",
      "GD iter. 163/499: loss=0.449899117783902\n",
      "GD iter. 164/499: loss=0.44989793593732474\n",
      "GD iter. 165/499: loss=0.4498968047066621\n",
      "GD iter. 166/499: loss=0.4498957218876461\n",
      "GD iter. 167/499: loss=0.4498946853734801\n",
      "GD iter. 168/499: loss=0.4498936931504651\n",
      "GD iter. 169/499: loss=0.44989274329382867\n",
      "GD iter. 170/499: loss=0.4498918339637403\n",
      "GD iter. 171/499: loss=0.44989096340150947\n",
      "GD iter. 172/499: loss=0.44989012992595584\n",
      "GD iter. 173/499: loss=0.4498893319299439\n",
      "GD iter. 174/499: loss=0.449888567877075\n",
      "GD iter. 175/499: loss=0.44988783629852835\n",
      "GD iter. 176/499: loss=0.4498871357900471\n",
      "GD iter. 177/499: loss=0.44988646500905727\n",
      "GD iter. 178/499: loss=0.4498858226719208\n",
      "GD iter. 179/499: loss=0.4498852075513097\n",
      "GD iter. 180/499: loss=0.44988461847370054\n",
      "GD iter. 181/499: loss=0.4498840543169808\n",
      "GD iter. 182/499: loss=0.4498835140081637\n",
      "GD iter. 183/499: loss=0.44988299652120667\n",
      "GD iter. 184/499: loss=0.44988250087492615\n",
      "GD iter. 185/499: loss=0.44988202613100814\n",
      "GD iter. 186/499: loss=0.4498815713921077\n",
      "GD iter. 187/499: loss=0.44988113580003297\n",
      "GD iter. 188/499: loss=0.4498807185340117\n",
      "GD iter. 189/499: loss=0.4498803188090355\n",
      "GD iter. 190/499: loss=0.4498799358742783\n",
      "GD iter. 191/499: loss=0.4498795690115849\n",
      "GD iter. 192/499: loss=0.4498792175340286\n",
      "GD iter. 193/499: loss=0.4498788807845326\n",
      "GD iter. 194/499: loss=0.44987855813455346\n",
      "GD iter. 195/499: loss=0.44987824898282275\n",
      "GD iter. 196/499: loss=0.4498779527541458\n",
      "GD iter. 197/499: loss=0.44987766889825354\n",
      "GD iter. 198/499: loss=0.4498773968887059\n",
      "GD iter. 199/499: loss=0.4498771362218442\n",
      "GD iter. 200/499: loss=0.4498768864157896\n",
      "GD iter. 201/499: loss=0.44987664700948726\n",
      "GD iter. 202/499: loss=0.44987641756179203\n",
      "GD iter. 203/499: loss=0.4498761976505951\n",
      "GD iter. 204/499: loss=0.4498759868719903\n",
      "GD iter. 205/499: loss=0.4498757848394763\n",
      "GD iter. 206/499: loss=0.44987559118319503\n",
      "GD iter. 207/499: loss=0.4498754055492033\n",
      "GD iter. 208/499: loss=0.4498752275987777\n",
      "GD iter. 209/499: loss=0.44987505700774894\n",
      "GD iter. 210/499: loss=0.449874893465867\n",
      "GD iter. 211/499: loss=0.44987473667619415\n",
      "GD iter. 212/499: loss=0.44987458635452354\n",
      "GD iter. 213/499: loss=0.4498744422288251\n",
      "GD iter. 214/499: loss=0.4498743040387159\n",
      "GD iter. 215/499: loss=0.4498741715349523\n",
      "GD iter. 216/499: loss=0.44987404447894636\n",
      "GD iter. 217/499: loss=0.4498739226423022\n",
      "GD iter. 218/499: loss=0.449873805806374\n",
      "GD iter. 219/499: loss=0.44987369376184266\n",
      "GD iter. 220/499: loss=0.44987358630831137\n",
      "GD iter. 221/499: loss=0.44987348325391857\n",
      "GD iter. 222/499: loss=0.449873384414969\n",
      "GD iter. 223/499: loss=0.44987328961557976\n",
      "GD iter. 224/499: loss=0.44987319868734266\n",
      "GD iter. 225/499: loss=0.4498731114690013\n",
      "GD iter. 226/499: loss=0.44987302780614147\n",
      "GD iter. 227/499: loss=0.44987294755089746\n",
      "GD iter. 228/499: loss=0.44987287056166747\n",
      "GD iter. 229/499: loss=0.44987279670284563\n",
      "GD iter. 230/499: loss=0.44987272584456256\n",
      "GD iter. 231/499: loss=0.4498726578624389\n",
      "GD iter. 232/499: loss=0.44987259263734913\n",
      "GD iter. 233/499: loss=0.44987253005519584\n",
      "GD iter. 234/499: loss=0.44987247000669384\n",
      "GD iter. 235/499: loss=0.44987241238716336\n",
      "GD iter. 236/499: loss=0.4498723570963328\n",
      "GD iter. 237/499: loss=0.44987230403814976\n",
      "GD iter. 238/499: loss=0.4498722531206005\n",
      "GD iter. 239/499: loss=0.44987220425553687\n",
      "GD iter. 240/499: loss=0.44987215735851077\n",
      "GD iter. 241/499: loss=0.44987211234861696\n",
      "GD iter. 242/499: loss=0.4498720691483407\n",
      "GD iter. 243/499: loss=0.44987202768341394\n",
      "GD iter. 244/499: loss=0.4498719878826763\n",
      "GD iter. 245/499: loss=0.4498719496779427\n",
      "GD iter. 246/499: loss=0.4498719130038774\n",
      "GD iter. 247/499: loss=0.4498718777978719\n",
      "GD iter. 248/499: loss=0.44987184399992936\n",
      "GD iter. 249/499: loss=0.44987181155255346\n",
      "GD iter. 250/499: loss=0.44987178040064285\n",
      "GD iter. 251/499: loss=0.4498717504913889\n",
      "GD iter. 252/499: loss=0.4498717217741787\n",
      "GD iter. 253/499: loss=0.44987169420050216\n",
      "GD iter. 254/499: loss=0.44987166772386306\n",
      "GD iter. 255/499: loss=0.4498716422996936\n",
      "GD iter. 256/499: loss=0.44987161788527363\n",
      "GD iter. 257/499: loss=0.44987159443965125\n",
      "GD iter. 258/499: loss=0.4498715719235698\n",
      "GD iter. 259/499: loss=0.44987155029939535\n",
      "GD iter. 260/499: loss=0.4498715295310487\n",
      "GD iter. 261/499: loss=0.44987150958393946\n",
      "GD iter. 262/499: loss=0.44987149042490454\n",
      "GD iter. 263/499: loss=0.44987147202214683\n",
      "GD iter. 264/499: loss=0.44987145434517867\n",
      "GD iter. 265/499: loss=0.4498714373647666\n",
      "GD iter. 266/499: loss=0.4498714210528789\n",
      "GD iter. 267/499: loss=0.44987140538263526\n",
      "GD iter. 268/499: loss=0.4498713903282582\n",
      "GD iter. 269/499: loss=0.4498713758650272\n",
      "GD iter. 270/499: loss=0.44987136196923483\n",
      "GD iter. 271/499: loss=0.44987134861814393\n",
      "GD iter. 272/499: loss=0.44987133578994715\n",
      "GD iter. 273/499: loss=0.4498713234637286\n",
      "GD iter. 274/499: loss=0.4498713116194263\n",
      "GD iter. 275/499: loss=0.44987130023779665\n",
      "GD iter. 276/499: loss=0.4498712893003807\n",
      "GD iter. 277/499: loss=0.44987127878947114\n",
      "GD iter. 278/499: loss=0.4498712686880813\n",
      "GD iter. 279/499: loss=0.4498712589799152\n",
      "GD iter. 280/499: loss=0.449871249649339\n",
      "GD iter. 281/499: loss=0.44987124068135326\n",
      "GD iter. 282/499: loss=0.4498712320615668\n",
      "GD iter. 283/499: loss=0.449871223776172\n",
      "GD iter. 284/499: loss=0.4498712158119199\n",
      "GD iter. 285/499: loss=0.4498712081560979\n",
      "GD iter. 286/499: loss=0.4498712007965065\n",
      "GD iter. 287/499: loss=0.44987119372143985\n",
      "GD iter. 288/499: loss=0.4498711869196637\n",
      "GD iter. 289/499: loss=0.4498711803803969\n",
      "GD iter. 290/499: loss=0.44987117409329286\n",
      "GD iter. 291/499: loss=0.4498711680484209\n",
      "GD iter. 292/499: loss=0.44987116223625034\n",
      "GD iter. 293/499: loss=0.4498711566476325\n",
      "GD iter. 294/499: loss=0.4498711512737869\n",
      "GD iter. 295/499: loss=0.4498711461062848\n",
      "GD iter. 296/499: loss=0.4498711411370351\n",
      "GD iter. 297/499: loss=0.4498711363582709\n",
      "GD iter. 298/499: loss=0.44987113176253624\n",
      "GD iter. 299/499: loss=0.44987112734267254\n",
      "GD iter. 300/499: loss=0.44987112309180766\n",
      "GD iter. 301/499: loss=0.4498711190033435\n",
      "GD iter. 302/499: loss=0.4498711150709449\n",
      "GD iter. 303/499: loss=0.4498711112885293\n",
      "GD iter. 304/499: loss=0.44987110765025606\n",
      "GD iter. 305/499: loss=0.4498711041505169\n",
      "GD iter. 306/499: loss=0.44987110078392634\n",
      "GD iter. 307/499: loss=0.44987109754531257\n",
      "GD iter. 308/499: loss=0.449871094429709\n",
      "GD iter. 309/499: loss=0.44987109143234577\n",
      "GD iter. 310/499: loss=0.4498710885486422\n",
      "GD iter. 311/499: loss=0.4498710857741985\n",
      "GD iter. 312/499: loss=0.44987108310478874\n",
      "GD iter. 313/499: loss=0.44987108053635416\n",
      "GD iter. 314/499: loss=0.44987107806499615\n",
      "GD iter. 315/499: loss=0.4498710756869695\n",
      "GD iter. 316/499: loss=0.4498710733986769\n",
      "GD iter. 317/499: loss=0.44987107119666214\n",
      "GD iter. 318/499: loss=0.44987106907760555\n",
      "GD iter. 319/499: loss=0.44987106703831725\n",
      "GD iter. 320/499: loss=0.4498710650757331\n",
      "GD iter. 321/499: loss=0.4498710631869088\n",
      "GD iter. 322/499: loss=0.44987106136901567\n",
      "GD iter. 323/499: loss=0.4498710596193356\n",
      "GD iter. 324/499: loss=0.449871057935257\n",
      "GD iter. 325/499: loss=0.4498710563142702\n",
      "GD iter. 326/499: loss=0.44987105475396355\n",
      "GD iter. 327/499: loss=0.44987105325201965\n",
      "GD iter. 328/499: loss=0.44987105180621145\n",
      "GD iter. 329/499: loss=0.44987105041439845\n",
      "GD iter. 330/499: loss=0.4498710490745235\n",
      "GD iter. 331/499: loss=0.44987104778460985\n",
      "GD iter. 332/499: loss=0.44987104654275695\n",
      "GD iter. 333/499: loss=0.4498710453471385\n",
      "GD iter. 334/499: loss=0.44987104419599894\n",
      "GD iter. 335/499: loss=0.44987104308765047\n",
      "GD iter. 336/499: loss=0.44987104202047096\n",
      "GD iter. 337/499: loss=0.4498710409929011\n",
      "GD iter. 338/499: loss=0.4498710400034416\n",
      "GD iter. 339/499: loss=0.449871039050651\n",
      "GD iter. 340/499: loss=0.449871038133144\n",
      "GD iter. 341/499: loss=0.44987103724958827\n",
      "GD iter. 342/499: loss=0.4498710363987029\n",
      "GD iter. 343/499: loss=0.44987103557925634\n",
      "GD iter. 344/499: loss=0.4498710347900645\n",
      "GD iter. 345/499: loss=0.44987103402998857\n",
      "GD iter. 346/499: loss=0.4498710332979336\n",
      "GD iter. 347/499: loss=0.44987103259284683\n",
      "GD iter. 348/499: loss=0.44987103191371547\n",
      "GD iter. 349/499: loss=0.4498710312595658\n",
      "GD iter. 350/499: loss=0.4498710306294613\n",
      "GD iter. 351/499: loss=0.44987103002250156\n",
      "GD iter. 352/499: loss=0.4498710294378198\n",
      "GD iter. 353/499: loss=0.4498710288745831\n",
      "GD iter. 354/499: loss=0.44987102833199005\n",
      "GD iter. 355/499: loss=0.4498710278092697\n",
      "GD iter. 356/499: loss=0.4498710273056806\n",
      "GD iter. 357/499: loss=0.44987102682050933\n",
      "GD iter. 358/499: loss=0.44987102635306986\n",
      "GD iter. 359/499: loss=0.449871025902702\n",
      "GD iter. 360/499: loss=0.44987102546877084\n",
      "GD iter. 361/499: loss=0.4498710250506653\n",
      "GD iter. 362/499: loss=0.4498710246477974\n",
      "GD iter. 363/499: loss=0.4498710242596019\n",
      "GD iter. 364/499: loss=0.4498710238855346\n",
      "GD iter. 365/499: loss=0.44987102352507174\n",
      "GD iter. 366/499: loss=0.4498710231777097\n",
      "GD iter. 367/499: loss=0.4498710228429638\n",
      "GD iter. 368/499: loss=0.4498710225203674\n",
      "GD iter. 369/499: loss=0.4498710222094715\n",
      "GD iter. 370/499: loss=0.4498710219098444\n",
      "GD iter. 371/499: loss=0.4498710216210701\n",
      "GD iter. 372/499: loss=0.44987102134274864\n",
      "GD iter. 373/499: loss=0.4498710210744948\n",
      "GD iter. 374/499: loss=0.4498710208159381\n",
      "GD iter. 375/499: loss=0.4498710205667217\n",
      "GD iter. 376/499: loss=0.44987102032650245\n",
      "GD iter. 377/499: loss=0.4498710200949497\n",
      "GD iter. 378/499: loss=0.44987101987174516\n",
      "GD iter. 379/499: loss=0.449871019656583\n",
      "GD iter. 380/499: loss=0.44987101944916813\n",
      "GD iter. 381/499: loss=0.449871019249217\n",
      "GD iter. 382/499: loss=0.4498710190564563\n",
      "GD iter. 383/499: loss=0.449871018870623\n",
      "GD iter. 384/499: loss=0.4498710186914639\n",
      "GD iter. 385/499: loss=0.4498710185187353\n",
      "GD iter. 386/499: loss=0.44987101835220233\n",
      "GD iter. 387/499: loss=0.4498710181916393\n",
      "GD iter. 388/499: loss=0.44987101803682816\n",
      "GD iter. 389/499: loss=0.44987101788755995\n",
      "GD iter. 390/499: loss=0.4498710177436325\n",
      "GD iter. 391/499: loss=0.4498710176048516\n",
      "GD iter. 392/499: loss=0.44987101747103037\n",
      "GD iter. 393/499: loss=0.4498710173419885\n",
      "GD iter. 394/499: loss=0.4498710172175525\n",
      "GD iter. 395/499: loss=0.4498710170975552\n",
      "GD iter. 396/499: loss=0.44987101698183596\n",
      "GD iter. 397/499: loss=0.44987101687023945\n",
      "GD iter. 398/499: loss=0.44987101676261676\n",
      "GD iter. 399/499: loss=0.4498710166588238\n",
      "GD iter. 400/499: loss=0.44987101655872225\n",
      "GD iter. 401/499: loss=0.4498710164621788\n",
      "GD iter. 402/499: loss=0.4498710163690649\n",
      "GD iter. 403/499: loss=0.4498710162792568\n",
      "GD iter. 404/499: loss=0.4498710161926352\n",
      "GD iter. 405/499: loss=0.44987101610908536\n",
      "GD iter. 406/499: loss=0.4498710160284966\n",
      "GD iter. 407/499: loss=0.44987101595076245\n",
      "GD iter. 408/499: loss=0.44987101587578\n",
      "GD iter. 409/499: loss=0.4498710158034506\n",
      "GD iter. 410/499: loss=0.4498710157336788\n",
      "GD iter. 411/499: loss=0.4498710156663728\n",
      "GD iter. 412/499: loss=0.4498710156014441\n",
      "GD iter. 413/499: loss=0.4498710155388075\n",
      "GD iter. 414/499: loss=0.44987101547838076\n",
      "GD iter. 415/499: loss=0.4498710154200849\n",
      "GD iter. 416/499: loss=0.44987101536384366\n",
      "GD iter. 417/499: loss=0.4498710153095834\n",
      "GD iter. 418/499: loss=0.44987101525723333\n",
      "GD iter. 419/499: loss=0.4498710152067252\n",
      "GD iter. 420/499: loss=0.4498710151579934\n",
      "GD iter. 421/499: loss=0.4498710151109744\n",
      "GD iter. 422/499: loss=0.44987101506560717\n",
      "GD iter. 423/499: loss=0.4498710150218328\n",
      "GD iter. 424/499: loss=0.44987101497959464\n",
      "GD iter. 425/499: loss=0.44987101493883785\n",
      "GD iter. 426/499: loss=0.44987101489950987\n",
      "GD iter. 427/499: loss=0.4498710148615599\n",
      "GD iter. 428/499: loss=0.4498710148249388\n",
      "GD iter. 429/499: loss=0.4498710147895996\n",
      "GD iter. 430/499: loss=0.44987101475549657\n",
      "GD iter. 431/499: loss=0.44987101472258617\n",
      "GD iter. 432/499: loss=0.4498710146908258\n",
      "GD iter. 433/499: loss=0.44987101466017476\n",
      "GD iter. 434/499: loss=0.44987101463059403\n",
      "GD iter. 435/499: loss=0.44987101460204537\n",
      "GD iter. 436/499: loss=0.4498710145744927\n",
      "GD iter. 437/499: loss=0.4498710145479002\n",
      "GD iter. 438/499: loss=0.4498710145222345\n",
      "GD iter. 439/499: loss=0.4498710144974626\n",
      "GD iter. 440/499: loss=0.4498710144735528\n",
      "GD iter. 441/499: loss=0.4498710144504753\n",
      "GD iter. 442/499: loss=0.4498710144282\n",
      "GD iter. 443/499: loss=0.44987101440669924\n",
      "GD iter. 444/499: loss=0.4498710143859454\n",
      "GD iter. 445/499: loss=0.44987101436591215\n",
      "GD iter. 446/499: loss=0.4498710143465744\n",
      "GD iter. 447/499: loss=0.4498710143279075\n",
      "GD iter. 448/499: loss=0.44987101430988785\n",
      "GD iter. 449/499: loss=0.4498710142924929\n",
      "GD iter. 450/499: loss=0.4498710142757006\n",
      "GD iter. 451/499: loss=0.4498710142594897\n",
      "GD iter. 452/499: loss=0.44987101424384\n",
      "GD iter. 453/499: loss=0.4498710142287317\n",
      "GD iter. 454/499: loss=0.449871014214146\n",
      "GD iter. 455/499: loss=0.4498710142000645\n",
      "GD iter. 456/499: loss=0.44987101418646924\n",
      "GD iter. 457/499: loss=0.44987101417334374\n",
      "GD iter. 458/499: loss=0.4498710141606713\n",
      "GD iter. 459/499: loss=0.4498710141484361\n",
      "GD iter. 460/499: loss=0.44987101413662284\n",
      "GD iter. 461/499: loss=0.4498710141252169\n",
      "GD iter. 462/499: loss=0.44987101411420405\n",
      "GD iter. 463/499: loss=0.44987101410357044\n",
      "GD iter. 464/499: loss=0.44987101409330305\n",
      "GD iter. 465/499: loss=0.44987101408338886\n",
      "GD iter. 466/499: loss=0.44987101407381586\n",
      "GD iter. 467/499: loss=0.449871014064572\n",
      "GD iter. 468/499: loss=0.44987101405564583\n",
      "GD iter. 469/499: loss=0.44987101404702634\n",
      "GD iter. 470/499: loss=0.44987101403870283\n",
      "GD iter. 471/499: loss=0.44987101403066515\n",
      "GD iter. 472/499: loss=0.44987101402290314\n",
      "GD iter. 473/499: loss=0.4498710140154074\n",
      "GD iter. 474/499: loss=0.4498710140081688\n",
      "GD iter. 475/499: loss=0.44987101400117807\n",
      "GD iter. 476/499: loss=0.4498710139944269\n",
      "GD iter. 477/499: loss=0.4498710139879069\n",
      "GD iter. 478/499: loss=0.4498710139816101\n",
      "GD iter. 479/499: loss=0.4498710139755287\n",
      "GD iter. 480/499: loss=0.44987101396965545\n",
      "GD iter. 481/499: loss=0.4498710139639828\n",
      "GD iter. 482/499: loss=0.4498710139585042\n",
      "GD iter. 483/499: loss=0.44987101395321266\n",
      "GD iter. 484/499: loss=0.4498710139481019\n",
      "GD iter. 485/499: loss=0.4498710139431654\n",
      "GD iter. 486/499: loss=0.4498710139383975\n",
      "GD iter. 487/499: loss=0.44987101393379225\n",
      "GD iter. 488/499: loss=0.449871013929344\n",
      "GD iter. 489/499: loss=0.44987101392504725\n",
      "GD iter. 490/499: loss=0.449871013920897\n",
      "GD iter. 491/499: loss=0.4498710139168881\n",
      "GD iter. 492/499: loss=0.4498710139130156\n",
      "GD iter. 493/499: loss=0.449871013909275\n",
      "GD iter. 494/499: loss=0.4498710139056616\n",
      "GD iter. 495/499: loss=0.44987101390217105\n",
      "GD iter. 496/499: loss=0.44987101389879913\n",
      "GD iter. 497/499: loss=0.4498710138955419\n",
      "GD iter. 498/499: loss=0.44987101389239526\n",
      "GD iter. 499/499: loss=0.44987101388935546\n",
      "The Accuracy is: 0.8276\n",
      "The F1 score is: 0.4000\n",
      "The precision is: 0.2869\n",
      "The recall is: 0.6604\n",
      "GD iter. 0/499: loss=0.6758577133543311\n",
      "GD iter. 1/499: loss=0.5377266871991484\n",
      "GD iter. 2/499: loss=0.5243569512465637\n",
      "GD iter. 3/499: loss=0.5153045103103687\n",
      "GD iter. 4/499: loss=0.5083724030112222\n",
      "GD iter. 5/499: loss=0.5027657368633269\n",
      "GD iter. 6/499: loss=0.49807496217133224\n",
      "GD iter. 7/499: loss=0.4940521666863549\n",
      "GD iter. 8/499: loss=0.4905375213151036\n",
      "GD iter. 9/499: loss=0.48742333101764956\n",
      "GD iter. 10/499: loss=0.48463405153101896\n",
      "GD iter. 11/499: loss=0.4821146989041274\n",
      "GD iter. 12/499: loss=0.4798239167322455\n",
      "GD iter. 13/499: loss=0.4777296973490705\n",
      "GD iter. 14/499: loss=0.47580665506540787\n",
      "GD iter. 15/499: loss=0.4740342329401205\n",
      "GD iter. 16/499: loss=0.4723954876252033\n",
      "GD iter. 17/499: loss=0.47087624265283456\n",
      "GD iter. 18/499: loss=0.4694644831838715\n",
      "GD iter. 19/499: loss=0.4681499132271128\n",
      "GD iter. 20/499: loss=0.4669236249025018\n",
      "GD iter. 21/499: loss=0.4657778467476375\n",
      "GD iter. 22/499: loss=0.4647057489579117\n",
      "GD iter. 23/499: loss=0.4637012904176031\n",
      "GD iter. 24/499: loss=0.46275909693663386\n",
      "GD iter. 25/499: loss=0.46187436315255515\n",
      "GD iter. 26/499: loss=0.4610427726326432\n",
      "GD iter. 27/499: loss=0.4602604321520469\n",
      "GD iter. 28/499: loss=0.45952381714203916\n",
      "GD iter. 29/499: loss=0.4588297260333074\n",
      "GD iter. 30/499: loss=0.4581752417516504\n",
      "GD iter. 31/499: loss=0.4575576990165819\n",
      "GD iter. 32/499: loss=0.4569746563872057\n",
      "GD iter. 33/499: loss=0.4564238722219006\n",
      "GD iter. 34/499: loss=0.4559032838880522\n",
      "GD iter. 35/499: loss=0.4554109896889565\n",
      "GD iter. 36/499: loss=0.4549452330768475\n",
      "GD iter. 37/499: loss=0.45450438880088856\n",
      "GD iter. 38/499: loss=0.45408695070211846\n",
      "GD iter. 39/499: loss=0.45369152091763093\n",
      "GD iter. 40/499: loss=0.4533168002965791\n",
      "GD iter. 41/499: loss=0.4529615798631394\n",
      "GD iter. 42/499: loss=0.4526247331879729\n",
      "GD iter. 43/499: loss=0.45230520955131326\n",
      "GD iter. 44/499: loss=0.4520020277985264\n",
      "GD iter. 45/499: loss=0.45171427080362686\n",
      "GD iter. 46/499: loss=0.4514410804683979\n",
      "GD iter. 47/499: loss=0.45118165319489617\n",
      "GD iter. 48/499: loss=0.45093523577763317\n",
      "GD iter. 49/499: loss=0.4507011216688891\n",
      "GD iter. 50/499: loss=0.450478647576675\n",
      "GD iter. 51/499: loss=0.450267190360017\n",
      "GD iter. 52/499: loss=0.4500661641906255\n",
      "GD iter. 53/499: loss=0.4498750179537837\n",
      "GD iter. 54/499: loss=0.4496932328645275\n",
      "GD iter. 55/499: loss=0.44952032027797917\n",
      "GD iter. 56/499: loss=0.4493558196751194\n",
      "GD iter. 57/499: loss=0.4491992968073846\n",
      "GD iter. 58/499: loss=0.4490503419853024\n",
      "GD iter. 59/499: loss=0.44890856849798333\n",
      "GD iter. 60/499: loss=0.4487736111516836\n",
      "GD iter. 61/499: loss=0.4486451249168897\n",
      "GD iter. 62/499: loss=0.4485227836744554\n",
      "GD iter. 63/499: loss=0.44840627905228203\n",
      "GD iter. 64/499: loss=0.4482953193448806\n",
      "GD iter. 65/499: loss=0.4481896285089009\n",
      "GD iter. 66/499: loss=0.44808894522838594\n",
      "GD iter. 67/499: loss=0.4479930220440991\n",
      "GD iter. 68/499: loss=0.44790162454180493\n",
      "GD iter. 69/499: loss=0.4478145305948544\n",
      "GD iter. 70/499: loss=0.44773152965685303\n",
      "GD iter. 71/499: loss=0.4476524221005659\n",
      "GD iter. 72/499: loss=0.4475770185995583\n",
      "GD iter. 73/499: loss=0.44750513954937626\n",
      "GD iter. 74/499: loss=0.4474366145253439\n",
      "GD iter. 75/499: loss=0.4473712817743118\n",
      "GD iter. 76/499: loss=0.4473089877379057\n",
      "GD iter. 77/499: loss=0.44724958660503644\n",
      "GD iter. 78/499: loss=0.4471929398916107\n",
      "GD iter. 79/499: loss=0.44713891604555256\n",
      "GD iter. 80/499: loss=0.4470873900753962\n",
      "GD iter. 81/499: loss=0.4470382432008468\n",
      "GD iter. 82/499: loss=0.44699136252383326\n",
      "GD iter. 83/499: loss=0.4469466407186917\n",
      "GD iter. 84/499: loss=0.4469039757402195\n",
      "GD iter. 85/499: loss=0.4468632705484378\n",
      "GD iter. 86/499: loss=0.4468244328489849\n",
      "GD iter. 87/499: loss=0.4467873748481458\n",
      "GD iter. 88/499: loss=0.44675201302158996\n",
      "GD iter. 89/499: loss=0.44671826789596375\n",
      "GD iter. 90/499: loss=0.4466860638425386\n",
      "GD iter. 91/499: loss=0.4466553288821755\n",
      "GD iter. 92/499: loss=0.44662599450091783\n",
      "GD iter. 93/499: loss=0.4465979954755708\n",
      "GD iter. 94/499: loss=0.446571269708672\n",
      "GD iter. 95/499: loss=0.44654575807229435\n",
      "GD iter. 96/499: loss=0.4465214042601641\n",
      "GD iter. 97/499: loss=0.44649815464760706\n",
      "GD iter. 98/499: loss=0.44647595815887214\n",
      "GD iter. 99/499: loss=0.446454766141407\n",
      "GD iter. 100/499: loss=0.4464345322466892\n",
      "GD iter. 101/499: loss=0.44641521231724346\n",
      "GD iter. 102/499: loss=0.4463967642794955\n",
      "GD iter. 103/499: loss=0.4463791480421382\n",
      "GD iter. 104/499: loss=0.44636232539970294\n",
      "GD iter. 105/499: loss=0.44634625994105087\n",
      "GD iter. 106/499: loss=0.4463309169625124\n",
      "GD iter. 107/499: loss=0.4463162633854254\n",
      "GD iter. 108/499: loss=0.44630226767782955\n",
      "GD iter. 109/499: loss=0.44628889978009756\n",
      "GD iter. 110/499: loss=0.44627613103429004\n",
      "GD iter. 111/499: loss=0.44626393411703585\n",
      "GD iter. 112/499: loss=0.4462522829757529\n",
      "GD iter. 113/499: loss=0.4462411527680317\n",
      "GD iter. 114/499: loss=0.44623051980401585\n",
      "GD iter. 115/499: loss=0.44622036149162536\n",
      "GD iter. 116/499: loss=0.4462106562844713\n",
      "GD iter. 117/499: loss=0.4462013836323266\n",
      "GD iter. 118/499: loss=0.4461925239340175\n",
      "GD iter. 119/499: loss=0.44618405849261505\n",
      "GD iter. 120/499: loss=0.44617596947280647\n",
      "GD iter. 121/499: loss=0.446168239860336\n",
      "GD iter. 122/499: loss=0.4461608534234123\n",
      "GD iter. 123/499: loss=0.4461537946759791\n",
      "GD iter. 124/499: loss=0.4461470488427601\n",
      "GD iter. 125/499: loss=0.44614060182598414\n",
      "GD iter. 126/499: loss=0.4461344401737105\n",
      "GD iter. 127/499: loss=0.4461285510496727\n",
      "GD iter. 128/499: loss=0.4461229222045651\n",
      "GD iter. 129/499: loss=0.4461175419487022\n",
      "GD iter. 130/499: loss=0.4461123991259806\n",
      "GD iter. 131/499: loss=0.4461074830890816\n",
      "GD iter. 132/499: loss=0.4461027836758526\n",
      "GD iter. 133/499: loss=0.4460982911868081\n",
      "GD iter. 134/499: loss=0.4460939963636988\n",
      "GD iter. 135/499: loss=0.4460898903690923\n",
      "GD iter. 136/499: loss=0.4460859647669202\n",
      "GD iter. 137/499: loss=0.4460822115039426\n",
      "GD iter. 138/499: loss=0.446078622892085\n",
      "GD iter. 139/499: loss=0.44607519159160725\n",
      "GD iter. 140/499: loss=0.4460719105950629\n",
      "GD iter. 141/499: loss=0.4460687732120111\n",
      "GD iter. 142/499: loss=0.44606577305444545\n",
      "GD iter. 143/499: loss=0.44606290402290466\n",
      "GD iter. 144/499: loss=0.44606016029323275\n",
      "GD iter. 145/499: loss=0.44605753630395795\n",
      "GD iter. 146/499: loss=0.44605502674425945\n",
      "GD iter. 147/499: loss=0.44605262654249656\n",
      "GD iter. 148/499: loss=0.4460503308552699\n",
      "GD iter. 149/499: loss=0.4460481350569928\n",
      "GD iter. 150/499: loss=0.44604603472994664\n",
      "GD iter. 151/499: loss=0.44604402565479756\n",
      "GD iter. 152/499: loss=0.446042103801553\n",
      "GD iter. 153/499: loss=0.4460402653209374\n",
      "GD iter. 154/499: loss=0.4460385065361668\n",
      "GD iter. 155/499: loss=0.44603682393510335\n",
      "GD iter. 156/499: loss=0.44603521416277286\n",
      "GD iter. 157/499: loss=0.4460336740142275\n",
      "GD iter. 158/499: loss=0.44603220042773756\n",
      "GD iter. 159/499: loss=0.4460307904782965\n",
      "GD iter. 160/499: loss=0.44602944137142586\n",
      "GD iter. 161/499: loss=0.4460281504372644\n",
      "GD iter. 162/499: loss=0.44602691512492876\n",
      "GD iter. 163/499: loss=0.4460257329971341\n",
      "GD iter. 164/499: loss=0.4460246017250603\n",
      "GD iter. 165/499: loss=0.4460235190834549\n",
      "GD iter. 166/499: loss=0.44602248294595864\n",
      "GD iter. 167/499: loss=0.4460214912806469\n",
      "GD iter. 168/499: loss=0.4460205421457736\n",
      "GD iter. 169/499: loss=0.4460196336857105\n",
      "GD iter. 170/499: loss=0.44601876412707225\n",
      "GD iter. 171/499: loss=0.4460179317750166\n",
      "GD iter. 172/499: loss=0.4460171350097157\n",
      "GD iter. 173/499: loss=0.4460163722829864\n",
      "GD iter. 174/499: loss=0.446015642115075\n",
      "GD iter. 175/499: loss=0.4460149430915883\n",
      "GD iter. 176/499: loss=0.4460142738605633\n",
      "GD iter. 177/499: loss=0.4460136331296717\n",
      "GD iter. 178/499: loss=0.4460130196635497\n",
      "GD iter. 179/499: loss=0.4460124322812503\n",
      "GD iter. 180/499: loss=0.4460118698538104\n",
      "GD iter. 181/499: loss=0.4460113313019285\n",
      "GD iter. 182/499: loss=0.4460108155937475\n",
      "GD iter. 183/499: loss=0.44601032174273814\n",
      "GD iter. 184/499: loss=0.4460098488056772\n",
      "GD iter. 185/499: loss=0.44600939588071903\n",
      "GD iter. 186/499: loss=0.4460089621055521\n",
      "GD iter. 187/499: loss=0.44600854665564027\n",
      "GD iter. 188/499: loss=0.4460081487425425\n",
      "GD iter. 189/499: loss=0.4460077676123093\n",
      "GD iter. 190/499: loss=0.44600740254394994\n",
      "GD iter. 191/499: loss=0.44600705284797065\n",
      "GD iter. 192/499: loss=0.44600671786497675\n",
      "GD iter. 193/499: loss=0.44600639696433925\n",
      "GD iter. 194/499: loss=0.44600608954292\n",
      "GD iter. 195/499: loss=0.44600579502385485\n",
      "GD iter. 196/499: loss=0.44600551285539153\n",
      "GD iter. 197/499: loss=0.44600524250977974\n",
      "GD iter. 198/499: loss=0.44600498348220985\n",
      "GD iter. 199/499: loss=0.44600473528980067\n",
      "GD iter. 200/499: loss=0.44600449747063237\n",
      "GD iter. 201/499: loss=0.4460042695828211\n",
      "GD iter. 202/499: loss=0.44600405120363684\n",
      "GD iter. 203/499: loss=0.4460038419286602\n",
      "GD iter. 204/499: loss=0.44600364137097587\n",
      "GD iter. 205/499: loss=0.4460034491604038\n",
      "GD iter. 206/499: loss=0.4460032649427628\n",
      "GD iter. 207/499: loss=0.44600308837916885\n",
      "GD iter. 208/499: loss=0.4460029191453629\n",
      "GD iter. 209/499: loss=0.44600275693106994\n",
      "GD iter. 210/499: loss=0.44600260143938625\n",
      "GD iter. 211/499: loss=0.4460024523861933\n",
      "GD iter. 212/499: loss=0.4460023094995983\n",
      "GD iter. 213/499: loss=0.4460021725193997\n",
      "GD iter. 214/499: loss=0.44600204119657594\n",
      "GD iter. 215/499: loss=0.44600191529279704\n",
      "GD iter. 216/499: loss=0.44600179457995814\n",
      "GD iter. 217/499: loss=0.44600167883973346\n",
      "GD iter. 218/499: loss=0.4460015678631496\n",
      "GD iter. 219/499: loss=0.4460014614501791\n",
      "GD iter. 220/499: loss=0.44600135940935\n",
      "GD iter. 221/499: loss=0.44600126155737424\n",
      "GD iter. 222/499: loss=0.4460011677187918\n",
      "GD iter. 223/499: loss=0.44600107772563036\n",
      "GD iter. 224/499: loss=0.4460009914170814\n",
      "GD iter. 225/499: loss=0.44600090863918757\n",
      "GD iter. 226/499: loss=0.44600082924454754\n",
      "GD iter. 227/499: loss=0.4460007530920309\n",
      "GD iter. 228/499: loss=0.44600068004650695\n",
      "GD iter. 229/499: loss=0.4460006099785858\n",
      "GD iter. 230/499: loss=0.4460005427643691\n",
      "GD iter. 231/499: loss=0.44600047828521383\n",
      "GD iter. 232/499: loss=0.44600041642750493\n",
      "GD iter. 233/499: loss=0.44600035708243896\n",
      "GD iter. 234/499: loss=0.4460003001458158\n",
      "GD iter. 235/499: loss=0.4460002455178413\n",
      "GD iter. 236/499: loss=0.44600019310293754\n",
      "GD iter. 237/499: loss=0.44600014280956063\n",
      "GD iter. 238/499: loss=0.4460000945500286\n",
      "GD iter. 239/499: loss=0.4460000482403548\n",
      "GD iter. 240/499: loss=0.4460000038000893\n",
      "GD iter. 241/499: loss=0.44599996115216767\n",
      "GD iter. 242/499: loss=0.4459999202227655\n",
      "GD iter. 243/499: loss=0.44599988094116005\n",
      "GD iter. 244/499: loss=0.4459998432395971\n",
      "GD iter. 245/499: loss=0.4459998070531644\n",
      "GD iter. 246/499: loss=0.4459997723196699\n",
      "GD iter. 247/499: loss=0.4459997389795266\n",
      "GD iter. 248/499: loss=0.44599970697563984\n",
      "GD iter. 249/499: loss=0.44599967625330245\n",
      "GD iter. 250/499: loss=0.4459996467600922\n",
      "GD iter. 251/499: loss=0.44599961844577507\n",
      "GD iter. 252/499: loss=0.4459995912622121\n",
      "GD iter. 253/499: loss=0.44599956516327016\n",
      "GD iter. 254/499: loss=0.44599954010473697\n",
      "GD iter. 255/499: loss=0.4459995160442392\n",
      "GD iter. 256/499: loss=0.4459994929411654\n",
      "GD iter. 257/499: loss=0.4459994707565903\n",
      "GD iter. 258/499: loss=0.44599944945320424\n",
      "GD iter. 259/499: loss=0.44599942899524425\n",
      "GD iter. 260/499: loss=0.44599940934842924\n",
      "GD iter. 261/499: loss=0.44599939047989673\n",
      "GD iter. 262/499: loss=0.44599937235814396\n",
      "GD iter. 263/499: loss=0.44599935495296933\n",
      "GD iter. 264/499: loss=0.4459993382354186\n",
      "GD iter. 265/499: loss=0.44599932217773197\n",
      "GD iter. 266/499: loss=0.44599930675329374\n",
      "GD iter. 267/499: loss=0.44599929193658455\n",
      "GD iter. 268/499: loss=0.4459992777031349\n",
      "GD iter. 269/499: loss=0.44599926402948165\n",
      "GD iter. 270/499: loss=0.44599925089312503\n",
      "GD iter. 271/499: loss=0.4459992382724891\n",
      "GD iter. 272/499: loss=0.4459992261468828\n",
      "GD iter. 273/499: loss=0.44599921449646246\n",
      "GD iter. 274/499: loss=0.44599920330219717\n",
      "GD iter. 275/499: loss=0.44599919254583426\n",
      "GD iter. 276/499: loss=0.445999182209867\n",
      "GD iter. 277/499: loss=0.44599917227750346\n",
      "GD iter. 278/499: loss=0.44599916273263707\n",
      "GD iter. 279/499: loss=0.44599915355981745\n",
      "GD iter. 280/499: loss=0.4459991447442236\n",
      "GD iter. 281/499: loss=0.44599913627163773\n",
      "GD iter. 282/499: loss=0.44599912812841985\n",
      "GD iter. 283/499: loss=0.4459991203014843\n",
      "GD iter. 284/499: loss=0.4459991127782763\n",
      "GD iter. 285/499: loss=0.4459991055467505\n",
      "GD iter. 286/499: loss=0.4459990985953491\n",
      "GD iter. 287/499: loss=0.4459990919129824\n",
      "GD iter. 288/499: loss=0.44599908548900935\n",
      "GD iter. 289/499: loss=0.44599907931321864\n",
      "GD iter. 290/499: loss=0.4459990733758114\n",
      "GD iter. 291/499: loss=0.44599906766738406\n",
      "GD iter. 292/499: loss=0.44599906217891183\n",
      "GD iter. 293/499: loss=0.4459990569017335\n",
      "GD iter. 294/499: loss=0.4459990518275364\n",
      "GD iter. 295/499: loss=0.4459990469483417\n",
      "GD iter. 296/499: loss=0.4459990422564914\n",
      "GD iter. 297/499: loss=0.4459990377446344\n",
      "GD iter. 298/499: loss=0.4459990334057145\n",
      "GD iter. 299/499: loss=0.44599902923295837\n",
      "GD iter. 300/499: loss=0.4459990252198634\n",
      "GD iter. 301/499: loss=0.445999021360187\n",
      "GD iter. 302/499: loss=0.4459990176479363\n",
      "GD iter. 303/499: loss=0.4459990140773572\n",
      "GD iter. 304/499: loss=0.4459990106429255\n",
      "GD iter. 305/499: loss=0.44599900733933695\n",
      "GD iter. 306/499: loss=0.4459990041614981\n",
      "GD iter. 307/499: loss=0.4459990011045186\n",
      "GD iter. 308/499: loss=0.4459989981637023\n",
      "GD iter. 309/499: loss=0.4459989953345393\n",
      "GD iter. 310/499: loss=0.4459989926126989\n",
      "GD iter. 311/499: loss=0.4459989899940217\n",
      "GD iter. 312/499: loss=0.4459989874745133\n",
      "GD iter. 313/499: loss=0.4459989850503372\n",
      "GD iter. 314/499: loss=0.4459989827178086\n",
      "GD iter. 315/499: loss=0.44599898047338815\n",
      "GD iter. 316/499: loss=0.4459989783136763\n",
      "GD iter. 317/499: loss=0.4459989762354075\n",
      "GD iter. 318/499: loss=0.4459989742354446\n",
      "GD iter. 319/499: loss=0.4459989723107741\n",
      "GD iter. 320/499: loss=0.4459989704585011\n",
      "GD iter. 321/499: loss=0.44599896867584427\n",
      "GD iter. 322/499: loss=0.44599896696013136\n",
      "GD iter. 323/499: loss=0.44599896530879524\n",
      "GD iter. 324/499: loss=0.4459989637193689\n",
      "GD iter. 325/499: loss=0.4459989621894824\n",
      "GD iter. 326/499: loss=0.44599896071685813\n",
      "GD iter. 327/499: loss=0.4459989592993076\n",
      "GD iter. 328/499: loss=0.445998957934728\n",
      "GD iter. 329/499: loss=0.44599895662109806\n",
      "GD iter. 330/499: loss=0.44599895535647577\n",
      "GD iter. 331/499: loss=0.4459989541389944\n",
      "GD iter. 332/499: loss=0.4459989529668601\n",
      "GD iter. 333/499: loss=0.4459989518383485\n",
      "GD iter. 334/499: loss=0.44599895075180246\n",
      "GD iter. 335/499: loss=0.44599894970562903\n",
      "GD iter. 336/499: loss=0.44599894869829704\n",
      "GD iter. 337/499: loss=0.4459989477283346\n",
      "GD iter. 338/499: loss=0.4459989467943268\n",
      "GD iter. 339/499: loss=0.44599894589491335\n",
      "GD iter. 340/499: loss=0.4459989450287868\n",
      "GD iter. 341/499: loss=0.44599894419468994\n",
      "GD iter. 342/499: loss=0.4459989433914141\n",
      "GD iter. 343/499: loss=0.4459989426177971\n",
      "GD iter. 344/499: loss=0.4459989418727217\n",
      "GD iter. 345/499: loss=0.4459989411551136\n",
      "GD iter. 346/499: loss=0.4459989404639396\n",
      "GD iter. 347/499: loss=0.44599893979820654\n",
      "GD iter. 348/499: loss=0.4459989391569588\n",
      "GD iter. 349/499: loss=0.445998938539278\n",
      "GD iter. 350/499: loss=0.4459989379442805\n",
      "GD iter. 351/499: loss=0.44599893737111695\n",
      "GD iter. 352/499: loss=0.4459989368189699\n",
      "GD iter. 353/499: loss=0.44599893628705356\n",
      "GD iter. 354/499: loss=0.4459989357746118\n",
      "GD iter. 355/499: loss=0.4459989352809179\n",
      "GD iter. 356/499: loss=0.44599893480527214\n",
      "GD iter. 357/499: loss=0.44599893434700183\n",
      "GD iter. 358/499: loss=0.4459989339054599\n",
      "GD iter. 359/499: loss=0.4459989334800239\n",
      "GD iter. 360/499: loss=0.4459989330700947\n",
      "GD iter. 361/499: loss=0.4459989326750962\n",
      "GD iter. 362/499: loss=0.44599893229447396\n",
      "GD iter. 363/499: loss=0.44599893192769485\n",
      "GD iter. 364/499: loss=0.4459989315742456\n",
      "GD iter. 365/499: loss=0.4459989312336325\n",
      "GD iter. 366/499: loss=0.4459989309053804\n",
      "GD iter. 367/499: loss=0.4459989305890324\n",
      "GD iter. 368/499: loss=0.4459989302841484\n",
      "GD iter. 369/499: loss=0.445998929990305\n",
      "GD iter. 370/499: loss=0.44599892970709504\n",
      "GD iter. 371/499: loss=0.44599892943412656\n",
      "GD iter. 372/499: loss=0.44599892917102196\n",
      "GD iter. 373/499: loss=0.44599892891741855\n",
      "GD iter. 374/499: loss=0.4459989286729663\n",
      "GD iter. 375/499: loss=0.4459989284373295\n",
      "GD iter. 376/499: loss=0.44599892821018394\n",
      "GD iter. 377/499: loss=0.4459989279912181\n",
      "GD iter. 378/499: loss=0.44599892778013206\n",
      "GD iter. 379/499: loss=0.44599892757663695\n",
      "GD iter. 380/499: loss=0.445998927380455\n",
      "GD iter. 381/499: loss=0.44599892719131845\n",
      "GD iter. 382/499: loss=0.44599892700896976\n",
      "GD iter. 383/499: loss=0.44599892683316084\n",
      "GD iter. 384/499: loss=0.4459989266636531\n",
      "GD iter. 385/499: loss=0.44599892650021655\n",
      "GD iter. 386/499: loss=0.4459989263426299\n",
      "GD iter. 387/499: loss=0.44599892619068005\n",
      "GD iter. 388/499: loss=0.44599892604416175\n",
      "GD iter. 389/499: loss=0.4459989259028775\n",
      "GD iter. 390/499: loss=0.44599892576663686\n",
      "GD iter. 391/499: loss=0.44599892563525684\n",
      "GD iter. 392/499: loss=0.44599892550856074\n",
      "GD iter. 393/499: loss=0.44599892538637903\n",
      "GD iter. 394/499: loss=0.4459989252685478\n",
      "GD iter. 395/499: loss=0.4459989251549096\n",
      "GD iter. 396/499: loss=0.44599892504531247\n",
      "GD iter. 397/499: loss=0.44599892493961046\n",
      "GD iter. 398/499: loss=0.4459989248376627\n",
      "GD iter. 399/499: loss=0.4459989247393336\n",
      "GD iter. 400/499: loss=0.4459989246444924\n",
      "GD iter. 401/499: loss=0.4459989245530135\n",
      "GD iter. 402/499: loss=0.44599892446477557\n",
      "GD iter. 403/499: loss=0.44599892437966204\n",
      "GD iter. 404/499: loss=0.44599892429756016\n",
      "GD iter. 405/499: loss=0.44599892421836174\n",
      "GD iter. 406/499: loss=0.44599892414196246\n",
      "GD iter. 407/499: loss=0.4459989240682618\n",
      "GD iter. 408/499: loss=0.4459989239971627\n",
      "GD iter. 409/499: loss=0.4459989239285719\n",
      "GD iter. 410/499: loss=0.4459989238623995\n",
      "GD iter. 411/499: loss=0.4459989237985589\n",
      "GD iter. 412/499: loss=0.44599892373696653\n",
      "GD iter. 413/499: loss=0.4459989236775418\n",
      "GD iter. 414/499: loss=0.4459989236202075\n",
      "GD iter. 415/499: loss=0.4459989235648889\n",
      "GD iter. 416/499: loss=0.44599892351151366\n",
      "GD iter. 417/499: loss=0.44599892346001274\n",
      "GD iter. 418/499: loss=0.4459989234103194\n",
      "GD iter. 419/499: loss=0.4459989233623688\n",
      "GD iter. 420/499: loss=0.4459989233160992\n",
      "GD iter. 421/499: loss=0.4459989232714506\n",
      "GD iter. 422/499: loss=0.44599892322836543\n",
      "GD iter. 423/499: loss=0.445998923186788\n",
      "GD iter. 424/499: loss=0.44599892314666484\n",
      "GD iter. 425/499: loss=0.4459989231079442\n",
      "GD iter. 426/499: loss=0.4459989230705764\n",
      "GD iter. 427/499: loss=0.4459989230345135\n",
      "GD iter. 428/499: loss=0.44599892299970906\n",
      "GD iter. 429/499: loss=0.4459989229661187\n",
      "GD iter. 430/499: loss=0.4459989229336993\n",
      "GD iter. 431/499: loss=0.44599892290240956\n",
      "GD iter. 432/499: loss=0.4459989228722095\n",
      "GD iter. 433/499: loss=0.4459989228430606\n",
      "GD iter. 434/499: loss=0.44599892281492565\n",
      "GD iter. 435/499: loss=0.4459989227877688\n",
      "GD iter. 436/499: loss=0.4459989227615559\n",
      "GD iter. 437/499: loss=0.44599892273625336\n",
      "GD iter. 438/499: loss=0.4459989227118291\n",
      "GD iter. 439/499: loss=0.4459989226882525\n",
      "GD iter. 440/499: loss=0.4459989226654934\n",
      "GD iter. 441/499: loss=0.44599892264352314\n",
      "GD iter. 442/499: loss=0.4459989226223141\n",
      "GD iter. 443/499: loss=0.4459989226018394\n",
      "GD iter. 444/499: loss=0.44599892258207324\n",
      "GD iter. 445/499: loss=0.4459989225629909\n",
      "GD iter. 446/499: loss=0.44599892254456847\n",
      "GD iter. 447/499: loss=0.44599892252678247\n",
      "GD iter. 448/499: loss=0.4459989225096109\n",
      "GD iter. 449/499: loss=0.44599892249303213\n",
      "GD iter. 450/499: loss=0.4459989224770255\n",
      "GD iter. 451/499: loss=0.4459989224615708\n",
      "GD iter. 452/499: loss=0.44599892244664896\n",
      "GD iter. 453/499: loss=0.44599892243224126\n",
      "GD iter. 454/499: loss=0.4459989224183297\n",
      "GD iter. 455/499: loss=0.4459989224048972\n",
      "GD iter. 456/499: loss=0.4459989223919267\n",
      "GD iter. 457/499: loss=0.4459989223794024\n",
      "GD iter. 458/499: loss=0.4459989223673085\n",
      "GD iter. 459/499: loss=0.4459989223556303\n",
      "GD iter. 460/499: loss=0.445998922344353\n",
      "GD iter. 461/499: loss=0.44599892233346294\n",
      "GD iter. 462/499: loss=0.4459989223229465\n",
      "GD iter. 463/499: loss=0.44599892231279076\n",
      "GD iter. 464/499: loss=0.44599892230298305\n",
      "GD iter. 465/499: loss=0.4459989222935115\n",
      "GD iter. 466/499: loss=0.4459989222843644\n",
      "GD iter. 467/499: loss=0.44599892227553034\n",
      "GD iter. 468/499: loss=0.4459989222669986\n",
      "GD iter. 469/499: loss=0.44599892225875876\n",
      "GD iter. 470/499: loss=0.44599892225080057\n",
      "GD iter. 471/499: loss=0.4459989222431144\n",
      "GD iter. 472/499: loss=0.44599892223569065\n",
      "GD iter. 473/499: loss=0.44599892222852044\n",
      "GD iter. 474/499: loss=0.44599892222159504\n",
      "GD iter. 475/499: loss=0.4459989222149058\n",
      "GD iter. 476/499: loss=0.44599892220844467\n",
      "GD iter. 477/499: loss=0.4459989222022038\n",
      "GD iter. 478/499: loss=0.4459989221961755\n",
      "GD iter. 479/499: loss=0.44599892219035253\n",
      "GD iter. 480/499: loss=0.44599892218472786\n",
      "GD iter. 481/499: loss=0.44599892217929454\n",
      "GD iter. 482/499: loss=0.445998922174046\n",
      "GD iter. 483/499: loss=0.44599892216897585\n",
      "GD iter. 484/499: loss=0.4459989221640782\n",
      "GD iter. 485/499: loss=0.4459989221593468\n",
      "GD iter. 486/499: loss=0.4459989221547762\n",
      "GD iter. 487/499: loss=0.44599892215036063\n",
      "GD iter. 488/499: loss=0.445998922146095\n",
      "GD iter. 489/499: loss=0.4459989221419739\n",
      "GD iter. 490/499: loss=0.44599892213799264\n",
      "GD iter. 491/499: loss=0.44599892213414627\n",
      "GD iter. 492/499: loss=0.44599892213043024\n",
      "GD iter. 493/499: loss=0.44599892212684006\n",
      "GD iter. 494/499: loss=0.4459989221233712\n",
      "GD iter. 495/499: loss=0.4459989221200201\n",
      "GD iter. 496/499: loss=0.445998922116782\n",
      "GD iter. 497/499: loss=0.44599892211365355\n",
      "GD iter. 498/499: loss=0.44599892211063086\n",
      "GD iter. 499/499: loss=0.44599892210771025\n",
      "The Accuracy is: 0.8259\n",
      "The F1 score is: 0.3908\n",
      "The precision is: 0.2810\n",
      "The recall is: 0.6415\n",
      "GD iter. 0/499: loss=0.7047228210887913\n",
      "GD iter. 1/499: loss=0.5422089760004312\n",
      "GD iter. 2/499: loss=0.527358686348465\n",
      "GD iter. 3/499: loss=0.5178491188679845\n",
      "GD iter. 4/499: loss=0.5108672170088874\n",
      "GD iter. 5/499: loss=0.5053757358036545\n",
      "GD iter. 6/499: loss=0.5008608006771049\n",
      "GD iter. 7/499: loss=0.4970273259994133\n",
      "GD iter. 8/499: loss=0.4936947265884721\n",
      "GD iter. 9/499: loss=0.490747031125022\n",
      "GD iter. 10/499: loss=0.48810627481081853\n",
      "GD iter. 11/499: loss=0.48571771993590185\n",
      "GD iter. 12/499: loss=0.48354138105336736\n",
      "GD iter. 13/499: loss=0.48154700488434304\n",
      "GD iter. 14/499: loss=0.4797109959269811\n",
      "GD iter. 15/499: loss=0.47801447004665826\n",
      "GD iter. 16/499: loss=0.47644198167402163\n",
      "GD iter. 17/499: loss=0.4749806653661807\n",
      "GD iter. 18/499: loss=0.47361963977935184\n",
      "GD iter. 19/499: loss=0.47234958256482173\n",
      "GD iter. 20/499: loss=0.47116241962872585\n",
      "GD iter. 21/499: loss=0.470051092876727\n",
      "GD iter. 22/499: loss=0.46900938310998164\n",
      "GD iter. 23/499: loss=0.46803177253207523\n",
      "GD iter. 24/499: loss=0.46711333628116347\n",
      "GD iter. 25/499: loss=0.4662496556225757\n",
      "GD iter. 26/499: loss=0.46543674757644926\n",
      "GD iter. 27/499: loss=0.4646710072049925\n",
      "GD iter. 28/499: loss=0.4639491597857675\n",
      "GD iter. 29/499: loss=0.46326822080197827\n",
      "GD iter. 30/499: loss=0.46262546218458284\n",
      "GD iter. 31/499: loss=0.4620183836068903\n",
      "GD iter. 32/499: loss=0.46144468790172105\n",
      "GD iter. 33/499: loss=0.46090225987221617\n",
      "GD iter. 34/499: loss=0.46038914791915025\n",
      "GD iter. 35/499: loss=0.45990354802348826\n",
      "GD iter. 36/499: loss=0.4594437897123023\n",
      "GD iter. 37/499: loss=0.45900832370578365\n",
      "GD iter. 38/499: loss=0.45859571099778174\n",
      "GD iter. 39/499: loss=0.4582046131656439\n",
      "GD iter. 40/499: loss=0.45783378373975386\n",
      "GD iter. 41/499: loss=0.45748206049101425\n",
      "GD iter. 42/499: loss=0.4571483585171005\n",
      "GD iter. 43/499: loss=0.45683166402672426\n",
      "GD iter. 44/499: loss=0.4565310287362657\n",
      "GD iter. 45/499: loss=0.45624556480562467\n",
      "GD iter. 46/499: loss=0.4559744402505155\n",
      "GD iter. 47/499: loss=0.45571687477709816\n",
      "GD iter. 48/499: loss=0.45547213599211744\n",
      "GD iter. 49/499: loss=0.45523953594786143\n",
      "GD iter. 50/499: loss=0.4550184279864601\n",
      "GD iter. 51/499: loss=0.45480820385247567\n",
      "GD iter. 52/499: loss=0.4546082910465324\n",
      "GD iter. 53/499: loss=0.45441815039598454\n",
      "GD iter. 54/499: loss=0.45423727382143286\n",
      "GD iter. 55/499: loss=0.45406518228032033\n",
      "GD iter. 56/499: loss=0.4539014238709504\n",
      "GD iter. 57/499: loss=0.4537455720820999\n",
      "GD iter. 58/499: loss=0.45359722417500475\n",
      "GD iter. 59/499: loss=0.453455999685893\n",
      "GD iter. 60/499: loss=0.45332153903847927\n",
      "GD iter. 61/499: loss=0.4531935022569118\n",
      "GD iter. 62/499: loss=0.45307156777062474\n",
      "GD iter. 63/499: loss=0.4529554313033944\n",
      "GD iter. 64/499: loss=0.45284480483964684\n",
      "GD iter. 65/499: loss=0.45273941566173537\n",
      "GD iter. 66/499: loss=0.4526390054524948\n",
      "GD iter. 67/499: loss=0.4525433294579151\n",
      "GD iter. 68/499: loss=0.4524521557052461\n",
      "GD iter. 69/499: loss=0.4523652642722706\n",
      "GD iter. 70/499: loss=0.4522824466038635\n",
      "GD iter. 71/499: loss=0.4522035048722941\n",
      "GD iter. 72/499: loss=0.45212825137803997\n",
      "GD iter. 73/499: loss=0.45205650798815294\n",
      "GD iter. 74/499: loss=0.45198810560947034\n",
      "GD iter. 75/499: loss=0.4519228836941891\n",
      "GD iter. 76/499: loss=0.4518606897755258\n",
      "GD iter. 77/499: loss=0.45180137903136947\n",
      "GD iter. 78/499: loss=0.45174481387400234\n",
      "GD iter. 79/499: loss=0.45169086356411714\n",
      "GD iter. 80/499: loss=0.4516394038474956\n",
      "GD iter. 81/499: loss=0.4515903166128438\n",
      "GD iter. 82/499: loss=0.45154348956938894\n",
      "GD iter. 83/499: loss=0.45149881594295416\n",
      "GD iter. 84/499: loss=0.45145619418931787\n",
      "GD iter. 85/499: loss=0.45141552772375587\n",
      "GD iter. 86/499: loss=0.45137672466574363\n",
      "GD iter. 87/499: loss=0.4513396975978698\n",
      "GD iter. 88/499: loss=0.4513043633380799\n",
      "GD iter. 89/499: loss=0.4512706427244316\n",
      "GD iter. 90/499: loss=0.4512384604115999\n",
      "GD iter. 91/499: loss=0.4512077446784212\n",
      "GD iter. 92/499: loss=0.45117842724582\n",
      "GD iter. 93/499: loss=0.45115044310449703\n",
      "GD iter. 94/499: loss=0.4511237303518077\n",
      "GD iter. 95/499: loss=0.45109823003729255\n",
      "GD iter. 96/499: loss=0.45107388601635967\n",
      "GD iter. 97/499: loss=0.45105064481164847\n",
      "GD iter. 98/499: loss=0.4510284554816374\n",
      "GD iter. 99/499: loss=0.451007269496085\n",
      "GD iter. 100/499: loss=0.4509870406179189\n",
      "GD iter. 101/499: loss=0.4509677247912123\n",
      "GD iter. 102/499: loss=0.4509492800349106\n",
      "GD iter. 103/499: loss=0.45093166634198956\n",
      "GD iter. 104/499: loss=0.4509148455837474\n",
      "GD iter. 105/499: loss=0.4508987814189508\n",
      "GD iter. 106/499: loss=0.45088343920757257\n",
      "GD iter. 107/499: loss=0.4508687859288712\n",
      "GD iter. 108/499: loss=0.4508547901035816\n",
      "GD iter. 109/499: loss=0.450841421719997\n",
      "GD iter. 110/499: loss=0.45082865216373474\n",
      "GD iter. 111/499: loss=0.45081645415099253\n",
      "GD iter. 112/499: loss=0.4508048016651118\n",
      "GD iter. 113/499: loss=0.4507936698962732\n",
      "GD iter. 114/499: loss=0.45078303518416435\n",
      "GD iter. 115/499: loss=0.45077287496346274\n",
      "GD iter. 116/499: loss=0.45076316771199065\n",
      "GD iter. 117/499: loss=0.45075389290140266\n",
      "GD iter. 118/499: loss=0.4507450309502784\n",
      "GD iter. 119/499: loss=0.4507365631794965\n",
      "GD iter. 120/499: loss=0.45072847176977243\n",
      "GD iter. 121/499: loss=0.4507207397212546\n",
      "GD iter. 122/499: loss=0.45071335081506997\n",
      "GD iter. 123/499: loss=0.4507062895767258\n",
      "GD iter. 124/499: loss=0.45069954124127043\n",
      "GD iter. 125/499: loss=0.45069309172012784\n",
      "GD iter. 126/499: loss=0.45068692756952194\n",
      "GD iter. 127/499: loss=0.4506810359604109\n",
      "GD iter. 128/499: loss=0.45067540464985817\n",
      "GD iter. 129/499: loss=0.4506700219537666\n",
      "GD iter. 130/499: loss=0.45066487672091254\n",
      "GD iter. 131/499: loss=0.4506599583082117\n",
      "GD iter. 132/499: loss=0.45065525655715977\n",
      "GD iter. 133/499: loss=0.45065076177138746\n",
      "GD iter. 134/499: loss=0.45064646469527986\n",
      "GD iter. 135/499: loss=0.4506423564936025\n",
      "GD iter. 136/499: loss=0.4506384287320902\n",
      "GD iter. 137/499: loss=0.45063467335894947\n",
      "GD iter. 138/499: loss=0.4506310826872316\n",
      "GD iter. 139/499: loss=0.45062764937803323\n",
      "GD iter. 140/499: loss=0.45062436642448456\n",
      "GD iter. 141/499: loss=0.4506212271364897\n",
      "GD iter. 142/499: loss=0.45061822512617883\n",
      "GD iter. 143/499: loss=0.45061535429404265\n",
      "GD iter. 144/499: loss=0.45061260881571263\n",
      "GD iter. 145/499: loss=0.45060998312935885\n",
      "GD iter. 146/499: loss=0.45060747192367434\n",
      "GD iter. 147/499: loss=0.4506050701264197\n",
      "GD iter. 148/499: loss=0.45060277289349876\n",
      "GD iter. 149/499: loss=0.45060057559854333\n",
      "GD iter. 150/499: loss=0.45059847382298013\n",
      "GD iter. 151/499: loss=0.4505964633465584\n",
      "GD iter. 152/499: loss=0.4505945401383165\n",
      "GD iter. 153/499: loss=0.4505927003479664\n",
      "GD iter. 154/499: loss=0.4505909402976761\n",
      "GD iter. 155/499: loss=0.45058925647423204\n",
      "GD iter. 156/499: loss=0.450587645521563\n",
      "GD iter. 157/499: loss=0.4505861042336084\n",
      "GD iter. 158/499: loss=0.4505846295475157\n",
      "GD iter. 159/499: loss=0.45058321853715055\n",
      "GD iter. 160/499: loss=0.45058186840690523\n",
      "GD iter. 161/499: loss=0.45058057648579214\n",
      "GD iter. 162/499: loss=0.45057934022180807\n",
      "GD iter. 163/499: loss=0.4505781571765572\n",
      "GD iter. 164/499: loss=0.45057702502012065\n",
      "GD iter. 165/499: loss=0.4505759415261607\n",
      "GD iter. 166/499: loss=0.45057490456724997\n",
      "GD iter. 167/499: loss=0.45057391211041264\n",
      "GD iter. 168/499: loss=0.45057296221287185\n",
      "GD iter. 169/499: loss=0.4505720530179888\n",
      "GD iter. 170/499: loss=0.45057118275138913\n",
      "GD iter. 171/499: loss=0.45057034971726484\n",
      "GD iter. 172/499: loss=0.45056955229484535\n",
      "GD iter. 173/499: loss=0.45056878893502905\n",
      "GD iter. 174/499: loss=0.45056805815716783\n",
      "GD iter. 175/499: loss=0.4505673585459989\n",
      "GD iter. 176/499: loss=0.45056668874871464\n",
      "GD iter. 177/499: loss=0.4505660474721668\n",
      "GD iter. 178/499: loss=0.45056543348019695\n",
      "GD iter. 179/499: loss=0.4505648455910881\n",
      "GD iter. 180/499: loss=0.45056428267513177\n",
      "GD iter. 181/499: loss=0.4505637436523058\n",
      "GD iter. 182/499: loss=0.4505632274900563\n",
      "GD iter. 183/499: loss=0.4505627332011811\n",
      "GD iter. 184/499: loss=0.4505622598418082\n",
      "GD iter. 185/499: loss=0.4505618065094654\n",
      "GD iter. 186/499: loss=0.4505613723412377\n",
      "GD iter. 187/499: loss=0.4505609565120076\n",
      "GD iter. 188/499: loss=0.45056055823277447\n",
      "GD iter. 189/499: loss=0.45056017674905013\n",
      "GD iter. 190/499: loss=0.450559811339326\n",
      "GD iter. 191/499: loss=0.4505594613136107\n",
      "GD iter. 192/499: loss=0.45055912601203124\n",
      "GD iter. 193/499: loss=0.4505588048034994\n",
      "GD iter. 194/499: loss=0.45055849708443635\n",
      "GD iter. 195/499: loss=0.45055820227755516\n",
      "GD iter. 196/499: loss=0.4505579198306985\n",
      "GD iter. 197/499: loss=0.45055764921572655\n",
      "GD iter. 198/499: loss=0.45055738992745786\n",
      "GD iter. 199/499: loss=0.4505571414826548\n",
      "GD iter. 200/499: loss=0.4505569034190552\n",
      "GD iter. 201/499: loss=0.45055667529444876\n",
      "GD iter. 202/499: loss=0.45055645668579275\n",
      "GD iter. 203/499: loss=0.4505562471883684\n",
      "GD iter. 204/499: loss=0.4505560464149743\n",
      "GD iter. 205/499: loss=0.45055585399515713\n",
      "GD iter. 206/499: loss=0.45055566957447385\n",
      "GD iter. 207/499: loss=0.4505554928137911\n",
      "GD iter. 208/499: loss=0.450555323388611\n",
      "GD iter. 209/499: loss=0.4505551609884311\n",
      "GD iter. 210/499: loss=0.45055500531613\n",
      "GD iter. 211/499: loss=0.4505548560873813\n",
      "GD iter. 212/499: loss=0.45055471303009437\n",
      "GD iter. 213/499: loss=0.4505545758838783\n",
      "GD iter. 214/499: loss=0.4505544443995315\n",
      "GD iter. 215/499: loss=0.4505543183385517\n",
      "GD iter. 216/499: loss=0.45055419747267\n",
      "GD iter. 217/499: loss=0.45055408158340404\n",
      "GD iter. 218/499: loss=0.45055397046163154\n",
      "GD iter. 219/499: loss=0.45055386390718216\n",
      "GD iter. 220/499: loss=0.45055376172844874\n",
      "GD iter. 221/499: loss=0.45055366374201367\n",
      "GD iter. 222/499: loss=0.45055356977229355\n",
      "GD iter. 223/499: loss=0.4505534796511991\n",
      "GD iter. 224/499: loss=0.45055339321780874\n",
      "GD iter. 225/499: loss=0.4505533103180594\n",
      "GD iter. 226/499: loss=0.4505532308044471\n",
      "GD iter. 227/499: loss=0.4505531545357447\n",
      "GD iter. 228/499: loss=0.450553081376729\n",
      "GD iter. 229/499: loss=0.45055301119792157\n",
      "GD iter. 230/499: loss=0.4505529438753401\n",
      "GD iter. 231/499: loss=0.45055287929026105\n",
      "GD iter. 232/499: loss=0.4505528173289929\n",
      "GD iter. 233/499: loss=0.450552757882659\n",
      "GD iter. 234/499: loss=0.45055270084698956\n",
      "GD iter. 235/499: loss=0.45055264612212387\n",
      "GD iter. 236/499: loss=0.45055259361242\n",
      "GD iter. 237/499: loss=0.4505525432262739\n",
      "GD iter. 238/499: loss=0.4505524948759454\n",
      "GD iter. 239/499: loss=0.450552448477392\n",
      "GD iter. 240/499: loss=0.45055240395011115\n",
      "GD iter. 241/499: loss=0.45055236121698744\n",
      "GD iter. 242/499: loss=0.45055232020414815\n",
      "GD iter. 243/499: loss=0.45055228084082377\n",
      "GD iter. 244/499: loss=0.4505522430592159\n",
      "GD iter. 245/499: loss=0.4505522067943692\n",
      "GD iter. 246/499: loss=0.45055217198405134\n",
      "GD iter. 247/499: loss=0.4505521385686355\n",
      "GD iter. 248/499: loss=0.4505521064909897\n",
      "GD iter. 249/499: loss=0.4505520756963705\n",
      "GD iter. 250/499: loss=0.45055204613232114\n",
      "GD iter. 251/499: loss=0.45055201774857395\n",
      "GD iter. 252/499: loss=0.450551990496958\n",
      "GD iter. 253/499: loss=0.45055196433130934\n",
      "GD iter. 254/499: loss=0.45055193920738545\n",
      "GD iter. 255/499: loss=0.45055191508278497\n",
      "GD iter. 256/499: loss=0.45055189191686806\n",
      "GD iter. 257/499: loss=0.45055186967068317\n",
      "GD iter. 258/499: loss=0.4505518483068947\n",
      "GD iter. 259/499: loss=0.45055182778971464\n",
      "GD iter. 260/499: loss=0.4505518080848377\n",
      "GD iter. 261/499: loss=0.4505517891593781\n",
      "GD iter. 262/499: loss=0.4505517709818103\n",
      "GD iter. 263/499: loss=0.450551753521911\n",
      "GD iter. 264/499: loss=0.4505517367507045\n",
      "GD iter. 265/499: loss=0.45055172064041027\n",
      "GD iter. 266/499: loss=0.4505517051643927\n",
      "GD iter. 267/499: loss=0.4505516902971127\n",
      "GD iter. 268/499: loss=0.4505516760140818\n",
      "GD iter. 269/499: loss=0.45055166229181837\n",
      "GD iter. 270/499: loss=0.4505516491078046\n",
      "GD iter. 271/499: loss=0.45055163644044727\n",
      "GD iter. 272/499: loss=0.45055162426903766\n",
      "GD iter. 273/499: loss=0.45055161257371573\n",
      "GD iter. 274/499: loss=0.45055160133543404\n",
      "GD iter. 275/499: loss=0.45055159053592403\n",
      "GD iter. 276/499: loss=0.45055158015766344\n",
      "GD iter. 277/499: loss=0.4505515701838451\n",
      "GD iter. 278/499: loss=0.4505515605983473\n",
      "GD iter. 279/499: loss=0.4505515513857051\n",
      "GD iter. 280/499: loss=0.45055154253108326\n",
      "GD iter. 281/499: loss=0.45055153402024983\n",
      "GD iter. 282/499: loss=0.45055152583955094\n",
      "GD iter. 283/499: loss=0.45055151797588777\n",
      "GD iter. 284/499: loss=0.45055151041669195\n",
      "GD iter. 285/499: loss=0.450551503149905\n",
      "GD iter. 286/499: loss=0.4505514961639565\n",
      "GD iter. 287/499: loss=0.45055148944774437\n",
      "GD iter. 288/499: loss=0.4505514829906148\n",
      "GD iter. 289/499: loss=0.45055147678234486\n",
      "GD iter. 290/499: loss=0.45055147081312324\n",
      "GD iter. 291/499: loss=0.4505514650735348\n",
      "GD iter. 292/499: loss=0.4505514595545431\n",
      "GD iter. 293/499: loss=0.4505514542474758\n",
      "GD iter. 294/499: loss=0.45055144914400885\n",
      "GD iter. 295/499: loss=0.45055144423615245\n",
      "GD iter. 296/499: loss=0.45055143951623766\n",
      "GD iter. 297/499: loss=0.4505514349769029\n",
      "GD iter. 298/499: loss=0.4505514306110815\n",
      "GD iter. 299/499: loss=0.4505514264119894\n",
      "GD iter. 300/499: loss=0.45055142237311413\n",
      "GD iter. 301/499: loss=0.450551418488203\n",
      "GD iter. 302/499: loss=0.45055141475125315\n",
      "GD iter. 303/499: loss=0.4505514111565009\n",
      "GD iter. 304/499: loss=0.45055140769841223\n",
      "GD iter. 305/499: loss=0.4505514043716735\n",
      "GD iter. 306/499: loss=0.450551401171182\n",
      "GD iter. 307/499: loss=0.45055139809203815\n",
      "GD iter. 308/499: loss=0.4505513951295366\n",
      "GD iter. 309/499: loss=0.45055139227915897\n",
      "GD iter. 310/499: loss=0.4505513895365653\n",
      "GD iter. 311/499: loss=0.4505513868975879\n",
      "GD iter. 312/499: loss=0.4505513843582234\n",
      "GD iter. 313/499: loss=0.4505513819146275\n",
      "GD iter. 314/499: loss=0.4505513795631066\n",
      "GD iter. 315/499: loss=0.4505513773001135\n",
      "GD iter. 316/499: loss=0.45055137512224047\n",
      "GD iter. 317/499: loss=0.4505513730262141\n",
      "GD iter. 318/499: loss=0.4505513710088894\n",
      "GD iter. 319/499: loss=0.45055136906724536\n",
      "GD iter. 320/499: loss=0.4505513671983794\n",
      "GD iter. 321/499: loss=0.45055136539950263\n",
      "GD iter. 322/499: loss=0.45055136366793574\n",
      "GD iter. 323/499: loss=0.4505513620011042\n",
      "GD iter. 324/499: loss=0.450551360396534\n",
      "GD iter. 325/499: loss=0.4505513588518481\n",
      "GD iter. 326/499: loss=0.4505513573647621\n",
      "GD iter. 327/499: loss=0.4505513559330809\n",
      "GD iter. 328/499: loss=0.45055135455469464\n",
      "GD iter. 329/499: loss=0.45055135322757583\n",
      "GD iter. 330/499: loss=0.45055135194977575\n",
      "GD iter. 331/499: loss=0.4505513507194216\n",
      "GD iter. 332/499: loss=0.4505513495347131\n",
      "GD iter. 333/499: loss=0.45055134839391986\n",
      "GD iter. 334/499: loss=0.45055134729537866\n",
      "GD iter. 335/499: loss=0.45055134623749066\n",
      "GD iter. 336/499: loss=0.4505513452187187\n",
      "GD iter. 337/499: loss=0.45055134423758525\n",
      "GD iter. 338/499: loss=0.4505513432926699\n",
      "GD iter. 339/499: loss=0.4505513423826066\n",
      "GD iter. 340/499: loss=0.45055134150608256\n",
      "GD iter. 341/499: loss=0.4505513406618351\n",
      "GD iter. 342/499: loss=0.45055133984865053\n",
      "GD iter. 343/499: loss=0.4505513390653614\n",
      "GD iter. 344/499: loss=0.4505513383108453\n",
      "GD iter. 345/499: loss=0.45055133758402294\n",
      "GD iter. 346/499: loss=0.4505513368838563\n",
      "GD iter. 347/499: loss=0.4505513362093474\n",
      "GD iter. 348/499: loss=0.45055133555953586\n",
      "GD iter. 349/499: loss=0.45055133493349886\n",
      "GD iter. 350/499: loss=0.45055133433034816\n",
      "GD iter. 351/499: loss=0.45055133374922945\n",
      "GD iter. 352/499: loss=0.45055133318932156\n",
      "GD iter. 353/499: loss=0.4505513326498341\n",
      "GD iter. 354/499: loss=0.4505513321300069\n",
      "GD iter. 355/499: loss=0.4505513316291087\n",
      "GD iter. 356/499: loss=0.45055133114643614\n",
      "GD iter. 357/499: loss=0.4505513306813125\n",
      "GD iter. 358/499: loss=0.4505513302330865\n",
      "GD iter. 359/499: loss=0.45055132980113205\n",
      "GD iter. 360/499: loss=0.4505513293848462\n",
      "GD iter. 361/499: loss=0.45055132898364925\n",
      "GD iter. 362/499: loss=0.4505513285969832\n",
      "GD iter. 363/499: loss=0.4505513282243114\n",
      "GD iter. 364/499: loss=0.45055132786511676\n",
      "GD iter. 365/499: loss=0.45055132751890253\n",
      "GD iter. 366/499: loss=0.4505513271851903\n",
      "GD iter. 367/499: loss=0.45055132686351956\n",
      "GD iter. 368/499: loss=0.4505513265534472\n",
      "GD iter. 369/499: loss=0.4505513262545468\n",
      "GD iter. 370/499: loss=0.4505513259664079\n",
      "GD iter. 371/499: loss=0.4505513256886356\n",
      "GD iter. 372/499: loss=0.45055132542084947\n",
      "GD iter. 373/499: loss=0.4505513251626837\n",
      "GD iter. 374/499: loss=0.450551324913786\n",
      "GD iter. 375/499: loss=0.45055132467381714\n",
      "GD iter. 376/499: loss=0.4505513244424506\n",
      "GD iter. 377/499: loss=0.4505513242193723\n",
      "GD iter. 378/499: loss=0.4505513240042794\n",
      "GD iter. 379/499: loss=0.4505513237968808\n",
      "GD iter. 380/499: loss=0.4505513235968961\n",
      "GD iter. 381/499: loss=0.45055132340405524\n",
      "GD iter. 382/499: loss=0.4505513232180982\n",
      "GD iter. 383/499: loss=0.45055132303877465\n",
      "GD iter. 384/499: loss=0.45055132286584354\n",
      "GD iter. 385/499: loss=0.4505513226990727\n",
      "GD iter. 386/499: loss=0.45055132253823893\n",
      "GD iter. 387/499: loss=0.4505513223831265\n",
      "GD iter. 388/499: loss=0.4505513222335287\n",
      "GD iter. 389/499: loss=0.45055132208924553\n",
      "GD iter. 390/499: loss=0.45055132195008474\n",
      "GD iter. 391/499: loss=0.45055132181586144\n",
      "GD iter. 392/499: loss=0.45055132168639705\n",
      "GD iter. 393/499: loss=0.4505513215615199\n",
      "GD iter. 394/499: loss=0.45055132144106447\n",
      "GD iter. 395/499: loss=0.45055132132487147\n",
      "GD iter. 396/499: loss=0.45055132121278746\n",
      "GD iter. 397/499: loss=0.4505513211046644\n",
      "GD iter. 398/499: loss=0.45055132100036005\n",
      "GD iter. 399/499: loss=0.45055132089973704\n",
      "GD iter. 400/499: loss=0.45055132080266314\n",
      "GD iter. 401/499: loss=0.45055132070901116\n",
      "GD iter. 402/499: loss=0.4505513206186584\n",
      "GD iter. 403/499: loss=0.45055132053148644\n",
      "GD iter. 404/499: loss=0.45055132044738144\n",
      "GD iter. 405/499: loss=0.45055132036623374\n",
      "GD iter. 406/499: loss=0.4505513202879376\n",
      "GD iter. 407/499: loss=0.450551320212391\n",
      "GD iter. 408/499: loss=0.4505513201394959\n",
      "GD iter. 409/499: loss=0.45055132006915766\n",
      "GD iter. 410/499: loss=0.45055132000128495\n",
      "GD iter. 411/499: loss=0.45055131993579006\n",
      "GD iter. 412/499: loss=0.45055131987258823\n",
      "GD iter. 413/499: loss=0.45055131981159796\n",
      "GD iter. 414/499: loss=0.4505513197527405\n",
      "GD iter. 415/499: loss=0.45055131969594014\n",
      "GD iter. 416/499: loss=0.45055131964112366\n",
      "GD iter. 417/499: loss=0.45055131958822087\n",
      "GD iter. 418/499: loss=0.4505513195371637\n",
      "GD iter. 419/499: loss=0.4505513194878868\n",
      "GD iter. 420/499: loss=0.4505513194403272\n",
      "GD iter. 421/499: loss=0.450551319394424\n",
      "GD iter. 422/499: loss=0.4505513193501186\n",
      "GD iter. 423/499: loss=0.45055131930735454\n",
      "GD iter. 424/499: loss=0.45055131926607733\n",
      "GD iter. 425/499: loss=0.45055131922623454\n",
      "GD iter. 426/499: loss=0.4505513191877755\n",
      "GD iter. 427/499: loss=0.45055131915065155\n",
      "GD iter. 428/499: loss=0.45055131911481533\n",
      "GD iter. 429/499: loss=0.45055131908022167\n",
      "GD iter. 430/499: loss=0.45055131904682705\n",
      "GD iter. 431/499: loss=0.45055131901458895\n",
      "GD iter. 432/499: loss=0.4505513189834667\n",
      "GD iter. 433/499: loss=0.4505513189534214\n",
      "GD iter. 434/499: loss=0.45055131892441513\n",
      "GD iter. 435/499: loss=0.45055131889641126\n",
      "GD iter. 436/499: loss=0.4505513188693748\n",
      "GD iter. 437/499: loss=0.45055131884327165\n",
      "GD iter. 438/499: loss=0.4505513188180695\n",
      "GD iter. 439/499: loss=0.4505513187937364\n",
      "GD iter. 440/499: loss=0.45055131877024207\n",
      "GD iter. 441/499: loss=0.4505513187475574\n",
      "GD iter. 442/499: loss=0.45055131872565374\n",
      "GD iter. 443/499: loss=0.4505513187045041\n",
      "GD iter. 444/499: loss=0.450551318684082\n",
      "GD iter. 445/499: loss=0.45055131866436215\n",
      "GD iter. 446/499: loss=0.45055131864532\n",
      "GD iter. 447/499: loss=0.45055131862693193\n",
      "GD iter. 448/499: loss=0.45055131860917524\n",
      "GD iter. 449/499: loss=0.45055131859202796\n",
      "GD iter. 450/499: loss=0.4505513185754688\n",
      "GD iter. 451/499: loss=0.45055131855947733\n",
      "GD iter. 452/499: loss=0.4505513185440339\n",
      "GD iter. 453/499: loss=0.4505513185291194\n",
      "GD iter. 454/499: loss=0.45055131851471547\n",
      "GD iter. 455/499: loss=0.4505513185008044\n",
      "GD iter. 456/499: loss=0.4505513184873691\n",
      "GD iter. 457/499: loss=0.45055131847439317\n",
      "GD iter. 458/499: loss=0.45055131846186075\n",
      "GD iter. 459/499: loss=0.4505513184497561\n",
      "GD iter. 460/499: loss=0.4505513184380649\n",
      "GD iter. 461/499: loss=0.45055131842677254\n",
      "GD iter. 462/499: loss=0.4505513184158655\n",
      "GD iter. 463/499: loss=0.4505513184053301\n",
      "GD iter. 464/499: loss=0.4505513183951539\n",
      "GD iter. 465/499: loss=0.4505513183853242\n",
      "GD iter. 466/499: loss=0.4505513183758292\n",
      "GD iter. 467/499: loss=0.45055131836665735\n",
      "GD iter. 468/499: loss=0.45055131835779755\n",
      "GD iter. 469/499: loss=0.45055131834923895\n",
      "GD iter. 470/499: loss=0.4505513183409713\n",
      "GD iter. 471/499: loss=0.45055131833298434\n",
      "GD iter. 472/499: loss=0.45055131832526885\n",
      "GD iter. 473/499: loss=0.45055131831781503\n",
      "GD iter. 474/499: loss=0.45055131831061423\n",
      "GD iter. 475/499: loss=0.45055131830365763\n",
      "GD iter. 476/499: loss=0.45055131829693695\n",
      "GD iter. 477/499: loss=0.4505513182904439\n",
      "GD iter. 478/499: loss=0.45055131828417083\n",
      "GD iter. 479/499: loss=0.4505513182781101\n",
      "GD iter. 480/499: loss=0.45055131827225464\n",
      "GD iter. 481/499: loss=0.45055131826659717\n",
      "GD iter. 482/499: loss=0.450551318261131\n",
      "GD iter. 483/499: loss=0.45055131825584965\n",
      "GD iter. 484/499: loss=0.4505513182507467\n",
      "GD iter. 485/499: loss=0.45055131824581623\n",
      "GD iter. 486/499: loss=0.45055131824105227\n",
      "GD iter. 487/499: loss=0.4505513182364489\n",
      "GD iter. 488/499: loss=0.45055131823200084\n",
      "GD iter. 489/499: loss=0.45055131822770295\n",
      "GD iter. 490/499: loss=0.4505513182235499\n",
      "GD iter. 491/499: loss=0.45055131821953676\n",
      "GD iter. 492/499: loss=0.4505513182156589\n",
      "GD iter. 493/499: loss=0.4505513182119115\n",
      "GD iter. 494/499: loss=0.45055131820829025\n",
      "GD iter. 495/499: loss=0.45055131820479094\n",
      "GD iter. 496/499: loss=0.4505513182014092\n",
      "GD iter. 497/499: loss=0.45055131819814137\n",
      "GD iter. 498/499: loss=0.45055131819498323\n",
      "GD iter. 499/499: loss=0.45055131819193106\n",
      "The Accuracy is: 0.8342\n",
      "The F1 score is: 0.3648\n",
      "The precision is: 0.2710\n",
      "The recall is: 0.5577\n",
      "GD iter. 0/499: loss=0.6735961646305242\n",
      "GD iter. 1/499: loss=0.5379470077067181\n",
      "GD iter. 2/499: loss=0.5248126519825086\n",
      "GD iter. 3/499: loss=0.5158936685552916\n",
      "GD iter. 4/499: loss=0.5090331653834893\n",
      "GD iter. 5/499: loss=0.5034583645013593\n",
      "GD iter. 6/499: loss=0.498770146002747\n",
      "GD iter. 7/499: loss=0.4947281783475571\n",
      "GD iter. 8/499: loss=0.49117866004857047\n",
      "GD iter. 9/499: loss=0.4880186113454392\n",
      "GD iter. 10/499: loss=0.4851761076082559\n",
      "GD iter. 11/499: loss=0.4825989041251825\n",
      "GD iter. 12/499: loss=0.48024769213816326\n",
      "GD iter. 13/499: loss=0.47809197612528576\n",
      "GD iter. 14/499: loss=0.47610747238223367\n",
      "GD iter. 15/499: loss=0.4742744146082368\n",
      "GD iter. 16/499: loss=0.47257641557046176\n",
      "GD iter. 17/499: loss=0.47099967943849197\n",
      "GD iter. 18/499: loss=0.4695324415059016\n",
      "GD iter. 19/499: loss=0.46816455941579826\n",
      "GD iter. 20/499: loss=0.46688720799572847\n",
      "GD iter. 21/499: loss=0.4656926467151543\n",
      "GD iter. 22/499: loss=0.46457403922694074\n",
      "GD iter. 23/499: loss=0.46352531105771344\n",
      "GD iter. 24/499: loss=0.4625410357792063\n",
      "GD iter. 25/499: loss=0.46161634281119757\n",
      "GD iter. 26/499: loss=0.4607468419081697\n",
      "GD iter. 27/499: loss=0.459928560691203\n",
      "GD iter. 28/499: loss=0.4591578925058485\n",
      "GD iter. 29/499: loss=0.45843155254387974\n",
      "GD iter. 30/499: loss=0.4577465406445325\n",
      "GD iter. 31/499: loss=0.45710010954347075\n",
      "GD iter. 32/499: loss=0.45648973760165085\n",
      "GD iter. 33/499: loss=0.45591310524629414\n",
      "GD iter. 34/499: loss=0.455368074509509\n",
      "GD iter. 35/499: loss=0.454852671168844\n",
      "GD iter. 36/499: loss=0.4543650690868474\n",
      "GD iter. 37/499: loss=0.4539035764198497\n",
      "GD iter. 38/499: loss=0.4534666234242812\n",
      "GD iter. 39/499: loss=0.45305275163530306\n",
      "GD iter. 40/499: loss=0.45266060422996873\n",
      "GD iter. 41/499: loss=0.4522889174174562\n",
      "GD iter. 42/499: loss=0.45193651272365176\n",
      "GD iter. 43/499: loss=0.45160229005764174\n",
      "GD iter. 44/499: loss=0.45128522146439215\n",
      "GD iter. 45/499: loss=0.4509843454817524\n",
      "GD iter. 46/499: loss=0.4506987620314703\n",
      "GD iter. 47/499: loss=0.45042762778356366\n",
      "GD iter. 48/499: loss=0.45017015194153165\n",
      "GD iter. 49/499: loss=0.44992559240274943\n",
      "GD iter. 50/499: loss=0.4496932522542272\n",
      "GD iter. 51/499: loss=0.44947247656887646\n",
      "GD iter. 52/499: loss=0.4492626494716798\n",
      "GD iter. 53/499: loss=0.44906319144881207\n",
      "GD iter. 54/499: loss=0.448873556875907\n",
      "GD iter. 55/499: loss=0.4486932317443898\n",
      "GD iter. 56/499: loss=0.4485217315671561\n",
      "GD iter. 57/499: loss=0.4483585994469434\n",
      "GD iter. 58/499: loss=0.44820340429253247\n",
      "GD iter. 59/499: loss=0.4480557391694971\n",
      "GD iter. 60/499: loss=0.4479152197736037\n",
      "GD iter. 61/499: loss=0.44778148301618026\n",
      "GD iter. 62/499: loss=0.4476541857118507\n",
      "GD iter. 63/499: loss=0.44753300335998514\n",
      "GD iter. 64/499: loss=0.44741762901205906\n",
      "GD iter. 65/499: loss=0.4473077722178673\n",
      "GD iter. 66/499: loss=0.4472031580442053\n",
      "GD iter. 67/499: loss=0.4471035261602297\n",
      "GD iter. 68/499: loss=0.4470086299842408\n",
      "GD iter. 69/499: loss=0.4469182358871102\n",
      "GD iter. 70/499: loss=0.44683212244800113\n",
      "GD iter. 71/499: loss=0.4467500797584186\n",
      "GD iter. 72/499: loss=0.4466719087709695\n",
      "GD iter. 73/499: loss=0.44659742068952635\n",
      "GD iter. 74/499: loss=0.4465264363977679\n",
      "GD iter. 75/499: loss=0.446458785923328\n",
      "GD iter. 76/499: loss=0.4463943079350094\n",
      "GD iter. 77/499: loss=0.4463328492707327\n",
      "GD iter. 78/499: loss=0.4462742644940744\n",
      "GD iter. 79/499: loss=0.44621841547742497\n",
      "GD iter. 80/499: loss=0.44616517100994946\n",
      "GD iter. 81/499: loss=0.44611440642867856\n",
      "GD iter. 82/499: loss=0.44606600327118573\n",
      "GD iter. 83/499: loss=0.4460198489484236\n",
      "GD iter. 84/499: loss=0.44597583643640315\n",
      "GD iter. 85/499: loss=0.44593386398549495\n",
      "GD iter. 86/499: loss=0.4458938348462225\n",
      "GD iter. 87/499: loss=0.44585565701050367\n",
      "GD iter. 88/499: loss=0.4458192429673654\n",
      "GD iter. 89/499: loss=0.4457845094722326\n",
      "GD iter. 90/499: loss=0.44575137732895315\n",
      "GD iter. 91/499: loss=0.4457197711837795\n",
      "GD iter. 92/499: loss=0.4456896193305814\n",
      "GD iter. 93/499: loss=0.4456608535266163\n",
      "GD iter. 94/499: loss=0.4456334088182254\n",
      "GD iter. 95/499: loss=0.4456072233758706\n",
      "GD iter. 96/499: loss=0.4455822383379626\n",
      "GD iter. 97/499: loss=0.4455583976629702\n",
      "GD iter. 98/499: loss=0.44553564798932965\n",
      "GD iter. 99/499: loss=0.4455139385027093\n",
      "GD iter. 100/499: loss=0.4454932208102095\n",
      "GD iter. 101/499: loss=0.4454734488211039\n",
      "GD iter. 102/499: loss=0.4454545786337595\n",
      "GD iter. 103/499: loss=0.4454365684283834\n",
      "GD iter. 104/499: loss=0.44541937836528084\n",
      "GD iter. 105/499: loss=0.44540297048831495\n",
      "GD iter. 106/499: loss=0.44538730863328596\n",
      "GD iter. 107/499: loss=0.4453723583409623\n",
      "GD iter. 108/499: loss=0.44535808677451\n",
      "GD iter. 109/499: loss=0.44534446264108496\n",
      "GD iter. 110/499: loss=0.4453314561173641\n",
      "GD iter. 111/499: loss=0.4453190387788064\n",
      "GD iter. 112/499: loss=0.44530718353244636\n",
      "GD iter. 113/499: loss=0.44529586455303083\n",
      "GD iter. 114/499: loss=0.44528505722232725\n",
      "GD iter. 115/499: loss=0.44527473807143564\n",
      "GD iter. 116/499: loss=0.4452648847259472\n",
      "GD iter. 117/499: loss=0.4452554758538037\n",
      "GD iter. 118/499: loss=0.4452464911157178\n",
      "GD iter. 119/499: loss=0.44523791111802136\n",
      "GD iter. 120/499: loss=0.4452297173678187\n",
      "GD iter. 121/499: loss=0.4452218922303277\n",
      "GD iter. 122/499: loss=0.44521441888829544\n",
      "GD iter. 123/499: loss=0.4452072813033848\n",
      "GD iter. 124/499: loss=0.44520046417943293\n",
      "GD iter. 125/499: loss=0.44519395292748537\n",
      "GD iter. 126/499: loss=0.44518773363251885\n",
      "GD iter. 127/499: loss=0.4451817930217674\n",
      "GD iter. 128/499: loss=0.4451761184345713\n",
      "GD iter. 129/499: loss=0.4451706977936734\n",
      "GD iter. 130/499: loss=0.44516551957789263\n",
      "GD iter. 131/499: loss=0.4451605727961037\n",
      "GD iter. 132/499: loss=0.4451558469624606\n",
      "GD iter. 133/499: loss=0.4451513320728016\n",
      "GD iter. 134/499: loss=0.4451470185821798\n",
      "GD iter. 135/499: loss=0.44514289738346036\n",
      "GD iter. 136/499: loss=0.445138959786935\n",
      "GD iter. 137/499: loss=0.44513519750090547\n",
      "GD iter. 138/499: loss=0.4451316026131835\n",
      "GD iter. 139/499: loss=0.44512816757346935\n",
      "GD iter. 140/499: loss=0.44512488517656057\n",
      "GD iter. 141/499: loss=0.44512174854635517\n",
      "GD iter. 142/499: loss=0.4451187511206077\n",
      "GD iter. 143/499: loss=0.44511588663640417\n",
      "GD iter. 144/499: loss=0.44511314911631916\n",
      "GD iter. 145/499: loss=0.4451105328552244\n",
      "GD iter. 146/499: loss=0.4451080324077163\n",
      "GD iter. 147/499: loss=0.44510564257613255\n",
      "GD iter. 148/499: loss=0.4451033583991305\n",
      "GD iter. 149/499: loss=0.4451011751408004\n",
      "GD iter. 150/499: loss=0.44509908828028655\n",
      "GD iter. 151/499: loss=0.44509709350189497\n",
      "GD iter. 152/499: loss=0.44509518668566117\n",
      "GD iter. 153/499: loss=0.4450933638983581\n",
      "GD iter. 154/499: loss=0.4450916213849241\n",
      "GD iter. 155/499: loss=0.44508995556028863\n",
      "GD iter. 156/499: loss=0.4450883630015784\n",
      "GD iter. 157/499: loss=0.44508684044068636\n",
      "GD iter. 158/499: loss=0.44508538475718495\n",
      "GD iter. 159/499: loss=0.4450839929715689\n",
      "GD iter. 160/499: loss=0.44508266223881116\n",
      "GD iter. 161/499: loss=0.4450813898422162\n",
      "GD iter. 162/499: loss=0.4450801731875597\n",
      "GD iter. 163/499: loss=0.4450790097974988\n",
      "GD iter. 164/499: loss=0.44507789730623937\n",
      "GD iter. 165/499: loss=0.44507683345445154\n",
      "GD iter. 166/499: loss=0.44507581608441915\n",
      "GD iter. 167/499: loss=0.44507484313541223\n",
      "GD iter. 168/499: loss=0.44507391263927426\n",
      "GD iter. 169/499: loss=0.4450730227162116\n",
      "GD iter. 170/499: loss=0.4450721715707772\n",
      "GD iter. 171/499: loss=0.44507135748803933\n",
      "GD iter. 172/499: loss=0.4450705788299257\n",
      "GD iter. 173/499: loss=0.4450698340317363\n",
      "GD iter. 174/499: loss=0.44506912159881645\n",
      "GD iter. 175/499: loss=0.4450684401033817\n",
      "GD iter. 176/499: loss=0.44506778818148923\n",
      "GD iter. 177/499: loss=0.4450671645301476\n",
      "GD iter. 178/499: loss=0.44506656790455945\n",
      "GD iter. 179/499: loss=0.44506599711548916\n",
      "GD iter. 180/499: loss=0.4450654510267527\n",
      "GD iter. 181/499: loss=0.4450649285528213\n",
      "GD iter. 182/499: loss=0.44506442865653384\n",
      "GD iter. 183/499: loss=0.4450639503469159\n",
      "GD iter. 184/499: loss=0.44506349267709616\n",
      "GD iter. 185/499: loss=0.4450630547423195\n",
      "GD iter. 186/499: loss=0.4450626356780493\n",
      "GD iter. 187/499: loss=0.44506223465815764\n",
      "GD iter. 188/499: loss=0.4450618508931965\n",
      "GD iter. 189/499: loss=0.44506148362874914\n",
      "GD iter. 190/499: loss=0.4450611321438555\n",
      "GD iter. 191/499: loss=0.44506079574950874\n",
      "GD iter. 192/499: loss=0.44506047378722174\n",
      "GD iter. 193/499: loss=0.4450601656276575\n",
      "GD iter. 194/499: loss=0.44505987066932196\n",
      "GD iter. 195/499: loss=0.44505958833731574\n",
      "GD iter. 196/499: loss=0.4450593180821439\n",
      "GD iter. 197/499: loss=0.4450590593785775\n",
      "GD iter. 198/499: loss=0.44505881172456896\n",
      "GD iter. 199/499: loss=0.4450585746402147\n",
      "GD iter. 200/499: loss=0.4450583476667658\n",
      "GD iter. 201/499: loss=0.445058130365683\n",
      "GD iter. 202/499: loss=0.445057922317735\n",
      "GD iter. 203/499: loss=0.44505772312213604\n",
      "GD iter. 204/499: loss=0.44505753239572415\n",
      "GD iter. 205/499: loss=0.4450573497721756\n",
      "GD iter. 206/499: loss=0.4450571749012547\n",
      "GD iter. 207/499: loss=0.44505700744809773\n",
      "GD iter. 208/499: loss=0.44505684709252924\n",
      "GD iter. 209/499: loss=0.44505669352840854\n",
      "GD iter. 210/499: loss=0.44505654646300635\n",
      "GD iter. 211/499: loss=0.44505640561640897\n",
      "GD iter. 212/499: loss=0.4450562707209495\n",
      "GD iter. 213/499: loss=0.4450561415206645\n",
      "GD iter. 214/499: loss=0.44505601777077514\n",
      "GD iter. 215/499: loss=0.44505589923719135\n",
      "GD iter. 216/499: loss=0.44505578569603904\n",
      "GD iter. 217/499: loss=0.44505567693320713\n",
      "GD iter. 218/499: loss=0.44505557274391644\n",
      "GD iter. 219/499: loss=0.4450554729323067\n",
      "GD iter. 220/499: loss=0.4450553773110425\n",
      "GD iter. 221/499: loss=0.44505528570093744\n",
      "GD iter. 222/499: loss=0.4450551979305936\n",
      "GD iter. 223/499: loss=0.4450551138360587\n",
      "GD iter. 224/499: loss=0.4450550332604977\n",
      "GD iter. 225/499: loss=0.44505495605387946\n",
      "GD iter. 226/499: loss=0.44505488207267696\n",
      "GD iter. 227/499: loss=0.44505481117958096\n",
      "GD iter. 228/499: loss=0.4450547432432273\n",
      "GD iter. 229/499: loss=0.4450546781379351\n",
      "GD iter. 230/499: loss=0.4450546157434569\n",
      "GD iter. 231/499: loss=0.44505455594474136\n",
      "GD iter. 232/499: loss=0.44505449863170377\n",
      "GD iter. 233/499: loss=0.44505444369900937\n",
      "GD iter. 234/499: loss=0.4450543910458652\n",
      "GD iter. 235/499: loss=0.4450543405758208\n",
      "GD iter. 236/499: loss=0.44505429219657866\n",
      "GD iter. 237/499: loss=0.44505424581981223\n",
      "GD iter. 238/499: loss=0.44505420136099255\n",
      "GD iter. 239/499: loss=0.4450541587392225\n",
      "GD iter. 240/499: loss=0.44505411787707866\n",
      "GD iter. 241/499: loss=0.44505407870045843\n",
      "GD iter. 242/499: loss=0.44505404113843705\n",
      "GD iter. 243/499: loss=0.44505400512312787\n",
      "GD iter. 244/499: loss=0.44505397058955065\n",
      "GD iter. 245/499: loss=0.44505393747550537\n",
      "GD iter. 246/499: loss=0.4450539057214502\n",
      "GD iter. 247/499: loss=0.44505387527038764\n",
      "GD iter. 248/499: loss=0.4450538460677526\n",
      "GD iter. 249/499: loss=0.4450538180613081\n",
      "GD iter. 250/499: loss=0.4450537912010432\n",
      "GD iter. 251/499: loss=0.44505376543907726\n",
      "GD iter. 252/499: loss=0.44505374072956727\n",
      "GD iter. 253/499: loss=0.4450537170286201\n",
      "GD iter. 254/499: loss=0.4450536942942078\n",
      "GD iter. 255/499: loss=0.4450536724860871\n",
      "GD iter. 256/499: loss=0.4450536515657225\n",
      "GD iter. 257/499: loss=0.44505363149621247\n",
      "GD iter. 258/499: loss=0.44505361224221857\n",
      "GD iter. 259/499: loss=0.4450535937698989\n",
      "GD iter. 260/499: loss=0.44505357604684276\n",
      "GD iter. 261/499: loss=0.4450535590420099\n",
      "GD iter. 262/499: loss=0.4450535427256709\n",
      "GD iter. 263/499: loss=0.44505352706935125\n",
      "GD iter. 264/499: loss=0.4450535120457771\n",
      "GD iter. 265/499: loss=0.44505349762882396\n",
      "GD iter. 266/499: loss=0.44505348379346754\n",
      "GD iter. 267/499: loss=0.4450534705157362\n",
      "GD iter. 268/499: loss=0.44505345777266625\n",
      "GD iter. 269/499: loss=0.44505344554225845\n",
      "GD iter. 270/499: loss=0.4450534338034373\n",
      "GD iter. 271/499: loss=0.44505342253601077\n",
      "GD iter. 272/499: loss=0.44505341172063345\n",
      "GD iter. 273/499: loss=0.44505340133876964\n",
      "GD iter. 274/499: loss=0.4450533913726594\n",
      "GD iter. 275/499: loss=0.4450533818052852\n",
      "GD iter. 276/499: loss=0.44505337262034017\n",
      "GD iter. 277/499: loss=0.44505336380219834\n",
      "GD iter. 278/499: loss=0.4450533553358854\n",
      "GD iter. 279/499: loss=0.4450533472070509\n",
      "GD iter. 280/499: loss=0.44505333940194186\n",
      "GD iter. 281/499: loss=0.4450533319073775\n",
      "GD iter. 282/499: loss=0.44505332471072506\n",
      "GD iter. 283/499: loss=0.44505331779987617\n",
      "GD iter. 284/499: loss=0.445053311163225\n",
      "GD iter. 285/499: loss=0.44505330478964755\n",
      "GD iter. 286/499: loss=0.44505329866847965\n",
      "GD iter. 287/499: loss=0.44505329278949923\n",
      "GD iter. 288/499: loss=0.44505328714290704\n",
      "GD iter. 289/499: loss=0.4450532817193085\n",
      "GD iter. 290/499: loss=0.4450532765096971\n",
      "GD iter. 291/499: loss=0.4450532715054377\n",
      "GD iter. 292/499: loss=0.44505326669825146\n",
      "GD iter. 293/499: loss=0.4450532620802001\n",
      "GD iter. 294/499: loss=0.44505325764367243\n",
      "GD iter. 295/499: loss=0.4450532533813697\n",
      "GD iter. 296/499: loss=0.4450532492862935\n",
      "GD iter. 297/499: loss=0.4450532453517322\n",
      "GD iter. 298/499: loss=0.44505324157124976\n",
      "GD iter. 299/499: loss=0.4450532379386737\n",
      "GD iter. 300/499: loss=0.44505323444808426\n",
      "GD iter. 301/499: loss=0.44505323109380374\n",
      "GD iter. 302/499: loss=0.44505322787038654\n",
      "GD iter. 303/499: loss=0.44505322477260945\n",
      "GD iter. 304/499: loss=0.4450532217954621\n",
      "GD iter. 305/499: loss=0.44505321893413885\n",
      "GD iter. 306/499: loss=0.44505321618402927\n",
      "GD iter. 307/499: loss=0.44505321354071065\n",
      "GD iter. 308/499: loss=0.4450532109999402\n",
      "GD iter. 309/499: loss=0.44505320855764746\n",
      "GD iter. 310/499: loss=0.445053206209927\n",
      "GD iter. 311/499: loss=0.4450532039530317\n",
      "GD iter. 312/499: loss=0.4450532017833665\n",
      "GD iter. 313/499: loss=0.4450531996974816\n",
      "GD iter. 314/499: loss=0.4450531976920668\n",
      "GD iter. 315/499: loss=0.4450531957639457\n",
      "GD iter. 316/499: loss=0.4450531939100699\n",
      "GD iter. 317/499: loss=0.44505319212751415\n",
      "GD iter. 318/499: loss=0.44505319041347086\n",
      "GD iter. 319/499: loss=0.44505318876524586\n",
      "GD iter. 320/499: loss=0.4450531871802529\n",
      "GD iter. 321/499: loss=0.4450531856560096\n",
      "GD iter. 322/499: loss=0.44505318419013373\n",
      "GD iter. 323/499: loss=0.4450531827803383\n",
      "GD iter. 324/499: loss=0.445053181424428\n",
      "GD iter. 325/499: loss=0.4450531801202953\n",
      "GD iter. 326/499: loss=0.4450531788659172\n",
      "GD iter. 327/499: loss=0.4450531776593516\n",
      "GD iter. 328/499: loss=0.4450531764987336\n",
      "GD iter. 329/499: loss=0.44505317538227285\n",
      "GD iter. 330/499: loss=0.44505317430825064\n",
      "GD iter. 331/499: loss=0.4450531732750163\n",
      "GD iter. 332/499: loss=0.44505317228098507\n",
      "GD iter. 333/499: loss=0.44505317132463507\n",
      "GD iter. 334/499: loss=0.44505317040450476\n",
      "GD iter. 335/499: loss=0.44505316951919094\n",
      "GD iter. 336/499: loss=0.44505316866734557\n",
      "GD iter. 337/499: loss=0.4450531678476745\n",
      "GD iter. 338/499: loss=0.4450531670589344\n",
      "GD iter. 339/499: loss=0.445053166299931\n",
      "GD iter. 340/499: loss=0.44505316556951774\n",
      "GD iter. 341/499: loss=0.4450531648665928\n",
      "GD iter. 342/499: loss=0.4450531641900979\n",
      "GD iter. 343/499: loss=0.4450531635390166\n",
      "GD iter. 344/499: loss=0.4450531629123722\n",
      "GD iter. 345/499: loss=0.4450531623092268\n",
      "GD iter. 346/499: loss=0.44505316172867876\n",
      "GD iter. 347/499: loss=0.44505316116986243\n",
      "GD iter. 348/499: loss=0.44505316063194555\n",
      "GD iter. 349/499: loss=0.44505316011412893\n",
      "GD iter. 350/499: loss=0.4450531596156444\n",
      "GD iter. 351/499: loss=0.4450531591357536\n",
      "GD iter. 352/499: loss=0.4450531586737477\n",
      "GD iter. 353/499: loss=0.44505315822894476\n",
      "GD iter. 354/499: loss=0.4450531578006898\n",
      "GD iter. 355/499: loss=0.4450531573883536\n",
      "GD iter. 356/499: loss=0.4450531569913308\n",
      "GD iter. 357/499: loss=0.4450531566090402\n",
      "GD iter. 358/499: loss=0.44505315624092273\n",
      "GD iter. 359/499: loss=0.4450531558864411\n",
      "GD iter. 360/499: loss=0.44505315554507907\n",
      "GD iter. 361/499: loss=0.44505315521634037\n",
      "GD iter. 362/499: loss=0.4450531548997476\n",
      "GD iter. 363/499: loss=0.4450531545948421\n",
      "GD iter. 364/499: loss=0.44505315430118303\n",
      "GD iter. 365/499: loss=0.4450531540183463\n",
      "GD iter. 366/499: loss=0.4450531537459242\n",
      "GD iter. 367/499: loss=0.445053153483525\n",
      "GD iter. 368/499: loss=0.44505315323077166\n",
      "GD iter. 369/499: loss=0.4450531529873019\n",
      "GD iter. 370/499: loss=0.44505315275276736\n",
      "GD iter. 371/499: loss=0.44505315252683286\n",
      "GD iter. 372/499: loss=0.4450531523091763\n",
      "GD iter. 373/499: loss=0.4450531520994878\n",
      "GD iter. 374/499: loss=0.44505315189746936\n",
      "GD iter. 375/499: loss=0.4450531517028343\n",
      "GD iter. 376/499: loss=0.44505315151530705\n",
      "GD iter. 377/499: loss=0.4450531513346224\n",
      "GD iter. 378/499: loss=0.4450531511605256\n",
      "GD iter. 379/499: loss=0.44505315099277093\n",
      "GD iter. 380/499: loss=0.4450531508311227\n",
      "GD iter. 381/499: loss=0.44505315067535384\n",
      "GD iter. 382/499: loss=0.4450531505252459\n",
      "GD iter. 383/499: loss=0.44505315038058896\n",
      "GD iter. 384/499: loss=0.4450531502411809\n",
      "GD iter. 385/499: loss=0.4450531501068272\n",
      "GD iter. 386/499: loss=0.44505314997734086\n",
      "GD iter. 387/499: loss=0.4450531498525418\n",
      "GD iter. 388/499: loss=0.4450531497322569\n",
      "GD iter. 389/499: loss=0.44505314961631937\n",
      "GD iter. 390/499: loss=0.445053149504569\n",
      "GD iter. 391/499: loss=0.4450531493968515\n",
      "GD iter. 392/499: loss=0.44505314929301804\n",
      "GD iter. 393/499: loss=0.44505314919292593\n",
      "GD iter. 394/499: loss=0.4450531490964376\n",
      "GD iter. 395/499: loss=0.44505314900342063\n",
      "GD iter. 396/499: loss=0.4450531489137478\n",
      "GD iter. 397/499: loss=0.44505314882729613\n",
      "GD iter. 398/499: loss=0.4450531487439478\n",
      "GD iter. 399/499: loss=0.44505314866358914\n",
      "GD iter. 400/499: loss=0.4450531485861108\n",
      "GD iter. 401/499: loss=0.44505314851140737\n",
      "GD iter. 402/499: loss=0.4450531484393777\n",
      "GD iter. 403/499: loss=0.44505314836992405\n",
      "GD iter. 404/499: loss=0.4450531483029525\n",
      "GD iter. 405/499: loss=0.4450531482383726\n",
      "GD iter. 406/499: loss=0.44505314817609753\n",
      "GD iter. 407/499: loss=0.445053148116043\n",
      "GD iter. 408/499: loss=0.4450531480581286\n",
      "GD iter. 409/499: loss=0.4450531480022765\n",
      "GD iter. 410/499: loss=0.44505314794841194\n",
      "GD iter. 411/499: loss=0.4450531478964627\n",
      "GD iter. 412/499: loss=0.4450531478463596\n",
      "GD iter. 413/499: loss=0.44505314779803545\n",
      "GD iter. 414/499: loss=0.4450531477514261\n",
      "GD iter. 415/499: loss=0.4450531477064697\n",
      "GD iter. 416/499: loss=0.4450531476631063\n",
      "GD iter. 417/499: loss=0.44505314762127834\n",
      "GD iter. 418/499: loss=0.4450531475809308\n",
      "GD iter. 419/499: loss=0.4450531475420099\n",
      "GD iter. 420/499: loss=0.44505314750446456\n",
      "GD iter. 421/499: loss=0.4450531474682452\n",
      "GD iter. 422/499: loss=0.44505314743330404\n",
      "GD iter. 423/499: loss=0.44505314739959534\n",
      "GD iter. 424/499: loss=0.4450531473670746\n",
      "GD iter. 425/499: loss=0.4450531473356993\n",
      "GD iter. 426/499: loss=0.4450531473054285\n",
      "GD iter. 427/499: loss=0.44505314727622247\n",
      "GD iter. 428/499: loss=0.4450531472480431\n",
      "GD iter. 429/499: loss=0.4450531472208539\n",
      "GD iter. 430/499: loss=0.44505314719461914\n",
      "GD iter. 431/499: loss=0.4450531471693048\n",
      "GD iter. 432/499: loss=0.44505314714487815\n",
      "GD iter. 433/499: loss=0.44505314712130745\n",
      "GD iter. 434/499: loss=0.4450531470985621\n",
      "GD iter. 435/499: loss=0.4450531470766129\n",
      "GD iter. 436/499: loss=0.4450531470554315\n",
      "GD iter. 437/499: loss=0.4450531470349902\n",
      "GD iter. 438/499: loss=0.4450531470152632\n",
      "GD iter. 439/499: loss=0.4450531469962249\n",
      "GD iter. 440/499: loss=0.4450531469778508\n",
      "GD iter. 441/499: loss=0.4450531469601175\n",
      "GD iter. 442/499: loss=0.4450531469430021\n",
      "GD iter. 443/499: loss=0.4450531469264828\n",
      "GD iter. 444/499: loss=0.4450531469105384\n",
      "GD iter. 445/499: loss=0.4450531468951488\n",
      "GD iter. 446/499: loss=0.445053146880294\n",
      "GD iter. 447/499: loss=0.44505314686595565\n",
      "GD iter. 448/499: loss=0.44505314685211494\n",
      "GD iter. 449/499: loss=0.44505314683875485\n",
      "GD iter. 450/499: loss=0.44505314682585795\n",
      "GD iter. 451/499: loss=0.4450531468134082\n",
      "GD iter. 452/499: loss=0.44505314680138985\n",
      "GD iter. 453/499: loss=0.44505314678978763\n",
      "GD iter. 454/499: loss=0.4450531467785869\n",
      "GD iter. 455/499: loss=0.4450531467677736\n",
      "GD iter. 456/499: loss=0.44505314675733404\n",
      "GD iter. 457/499: loss=0.4450531467472552\n",
      "GD iter. 458/499: loss=0.4450531467375244\n",
      "GD iter. 459/499: loss=0.44505314672812946\n",
      "GD iter. 460/499: loss=0.4450531467190586\n",
      "GD iter. 461/499: loss=0.44505314671030033\n",
      "GD iter. 462/499: loss=0.44505314670184387\n",
      "GD iter. 463/499: loss=0.44505314669367874\n",
      "GD iter. 464/499: loss=0.44505314668579454\n",
      "GD iter. 465/499: loss=0.4450531466781815\n",
      "GD iter. 466/499: loss=0.44505314667083046\n",
      "GD iter. 467/499: loss=0.44505314666373186\n",
      "GD iter. 468/499: loss=0.44505314665687723\n",
      "GD iter. 469/499: loss=0.44505314665025775\n",
      "GD iter. 470/499: loss=0.4450531466438655\n",
      "GD iter. 471/499: loss=0.44505314663769246\n",
      "GD iter. 472/499: loss=0.44505314663173096\n",
      "GD iter. 473/499: loss=0.4450531466259738\n",
      "GD iter. 474/499: loss=0.4450531466204137\n",
      "GD iter. 475/499: loss=0.445053146615044\n",
      "GD iter. 476/499: loss=0.445053146609858\n",
      "GD iter. 477/499: loss=0.44505314660484924\n",
      "GD iter. 478/499: loss=0.44505314660001166\n",
      "GD iter. 479/499: loss=0.4450531465953393\n",
      "GD iter. 480/499: loss=0.4450531465908266\n",
      "GD iter. 481/499: loss=0.4450531465864679\n",
      "GD iter. 482/499: loss=0.4450531465822578\n",
      "GD iter. 483/499: loss=0.44505314657819117\n",
      "GD iter. 484/499: loss=0.44505314657426326\n",
      "GD iter. 485/499: loss=0.4450531465704691\n",
      "GD iter. 486/499: loss=0.4450531465668041\n",
      "GD iter. 487/499: loss=0.44505314656326384\n",
      "GD iter. 488/499: loss=0.44505314655984407\n",
      "GD iter. 489/499: loss=0.4450531465565405\n",
      "GD iter. 490/499: loss=0.44505314655334915\n",
      "GD iter. 491/499: loss=0.44505314655026634\n",
      "GD iter. 492/499: loss=0.4450531465472881\n",
      "GD iter. 493/499: loss=0.44505314654441097\n",
      "GD iter. 494/499: loss=0.44505314654163153\n",
      "GD iter. 495/499: loss=0.4450531465389463\n",
      "GD iter. 496/499: loss=0.44505314653635186\n",
      "GD iter. 497/499: loss=0.4450531465338454\n",
      "GD iter. 498/499: loss=0.445053146531424\n",
      "GD iter. 499/499: loss=0.44505314652908456\n",
      "The Accuracy is: 0.8152\n",
      "The F1 score is: 0.3596\n",
      "The precision is: 0.2500\n",
      "The recall is: 0.6400\n",
      "Average accuracy score is:  0.8322624170665304\n",
      "Average f1 score is:  0.39109916636395164\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_train_processed), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(10):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = reg_logistic_regression(y_t, x_t, lambda_=0.1, initial_w=initial_w, max_iters=500, gamma=0.15)\n",
    "    y_pred = (x_v @ w >= 0.75).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is: 0.5911\n",
      "The F1 score is: 0.2477\n",
      "The precision is: 0.1449\n",
      "The recall is: 0.8542\n",
      "The Accuracy is: 0.5928\n",
      "The F1 score is: 0.2663\n",
      "The precision is: 0.1573\n",
      "The recall is: 0.8654\n",
      "The Accuracy is: 0.6076\n",
      "The F1 score is: 0.3032\n",
      "The precision is: 0.1851\n",
      "The recall is: 0.8387\n",
      "The Accuracy is: 0.5632\n",
      "The F1 score is: 0.2400\n",
      "The precision is: 0.1395\n",
      "The recall is: 0.8571\n",
      "The Accuracy is: 0.5780\n",
      "The F1 score is: 0.2720\n",
      "The precision is: 0.1611\n",
      "The recall is: 0.8727\n",
      "The Accuracy is: 0.5911\n",
      "The F1 score is: 0.2523\n",
      "The precision is: 0.1484\n",
      "The recall is: 0.8400\n",
      "The Accuracy is: 0.6338\n",
      "The F1 score is: 0.2875\n",
      "The precision is: 0.1711\n",
      "The recall is: 0.9000\n",
      "The Accuracy is: 0.6026\n",
      "The F1 score is: 0.3046\n",
      "The precision is: 0.1828\n",
      "The recall is: 0.9138\n",
      "The Accuracy is: 0.6125\n",
      "The F1 score is: 0.2761\n",
      "The precision is: 0.1636\n",
      "The recall is: 0.8824\n",
      "The Accuracy is: 0.6305\n",
      "The F1 score is: 0.3410\n",
      "The precision is: 0.2070\n",
      "The recall is: 0.9672\n",
      "Average accuracy score is:  0.6003212216535864\n",
      "Average f1 score is:  0.27906719711909955\n"
     ]
    }
   ],
   "source": [
    "# ridge regression using all the features except for those having NaN values over 50% ##\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_train_processed), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(10):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    w, loss = ridge_regression(y_t, x_t, lambda_=0.01)\n",
    "    y_pred = x_v @ w\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    y_pred = (y_pred > y_pred_mean).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinge loss gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/499: loss=0.9882648115725106\n",
      "GD iter. 1/499: loss=0.9017428895774816\n",
      "GD iter. 2/499: loss=0.8205296310724131\n",
      "GD iter. 3/499: loss=0.7567389924849213\n",
      "GD iter. 4/499: loss=0.7161450276239179\n",
      "GD iter. 5/499: loss=0.6916835125463995\n",
      "GD iter. 6/499: loss=0.674830776082536\n",
      "GD iter. 7/499: loss=0.6617622683538288\n",
      "GD iter. 8/499: loss=0.6511291955138866\n",
      "GD iter. 9/499: loss=0.6422804981456199\n",
      "GD iter. 10/499: loss=0.6357545707120806\n",
      "GD iter. 11/499: loss=0.6307853555004956\n",
      "GD iter. 12/499: loss=0.6265068610116423\n",
      "GD iter. 13/499: loss=0.6226183312022762\n",
      "GD iter. 14/499: loss=0.6191291359529025\n",
      "GD iter. 15/499: loss=0.6163425963744044\n",
      "GD iter. 16/499: loss=0.6137343866532734\n",
      "GD iter. 17/499: loss=0.6113623006051477\n",
      "GD iter. 18/499: loss=0.6091657213327429\n",
      "GD iter. 19/499: loss=0.6071578584479335\n",
      "GD iter. 20/499: loss=0.6052336413552044\n",
      "GD iter. 21/499: loss=0.603383732134022\n",
      "GD iter. 22/499: loss=0.6017281927962144\n",
      "GD iter. 23/499: loss=0.6001379940498583\n",
      "GD iter. 24/499: loss=0.5985960906627239\n",
      "GD iter. 25/499: loss=0.5971123020084972\n",
      "GD iter. 26/499: loss=0.5957160370509491\n",
      "GD iter. 27/499: loss=0.5944009211974973\n",
      "GD iter. 28/499: loss=0.5931296694690454\n",
      "GD iter. 29/499: loss=0.5918629218513535\n",
      "GD iter. 30/499: loss=0.590605255016963\n",
      "GD iter. 31/499: loss=0.5893521080907663\n",
      "GD iter. 32/499: loss=0.5881160530849344\n",
      "GD iter. 33/499: loss=0.5868982037594306\n",
      "GD iter. 34/499: loss=0.5857229976301974\n",
      "GD iter. 35/499: loss=0.5845664213713883\n",
      "GD iter. 36/499: loss=0.5834397356739778\n",
      "GD iter. 37/499: loss=0.5823792710875374\n",
      "GD iter. 38/499: loss=0.5813787856621951\n",
      "GD iter. 39/499: loss=0.5804384445541151\n",
      "GD iter. 40/499: loss=0.5795275515612315\n",
      "GD iter. 41/499: loss=0.5786516998850756\n",
      "GD iter. 42/499: loss=0.5778192096500864\n",
      "GD iter. 43/499: loss=0.5770230647484982\n",
      "GD iter. 44/499: loss=0.5762452896095236\n",
      "GD iter. 45/499: loss=0.5755248997552187\n",
      "GD iter. 46/499: loss=0.5747981392336547\n",
      "GD iter. 47/499: loss=0.5740838991629539\n",
      "GD iter. 48/499: loss=0.5733893442412052\n",
      "GD iter. 49/499: loss=0.5727138226966006\n",
      "GD iter. 50/499: loss=0.572044208152427\n",
      "GD iter. 51/499: loss=0.571373712353319\n",
      "GD iter. 52/499: loss=0.5707382159783382\n",
      "GD iter. 53/499: loss=0.5701183626426639\n",
      "GD iter. 54/499: loss=0.5695106182187415\n",
      "GD iter. 55/499: loss=0.5689400430154473\n",
      "GD iter. 56/499: loss=0.5683943082058632\n",
      "GD iter. 57/499: loss=0.5678699370546543\n",
      "GD iter. 58/499: loss=0.5673795147881826\n",
      "GD iter. 59/499: loss=0.5669056614500573\n",
      "GD iter. 60/499: loss=0.5664301813867318\n",
      "GD iter. 61/499: loss=0.5659624625779602\n",
      "GD iter. 62/499: loss=0.56551513959788\n",
      "GD iter. 63/499: loss=0.5650547219803719\n",
      "GD iter. 64/499: loss=0.5645984297085163\n",
      "GD iter. 65/499: loss=0.5641520457854804\n",
      "GD iter. 66/499: loss=0.563712968169677\n",
      "GD iter. 67/499: loss=0.563301796098994\n",
      "GD iter. 68/499: loss=0.5628779610671162\n",
      "GD iter. 69/499: loss=0.5624663979649717\n",
      "GD iter. 70/499: loss=0.562063780678132\n",
      "GD iter. 71/499: loss=0.5616559585103984\n",
      "GD iter. 72/499: loss=0.5612488642951541\n",
      "GD iter. 73/499: loss=0.5608614868496274\n",
      "GD iter. 74/499: loss=0.5604948448918587\n",
      "GD iter. 75/499: loss=0.5601339747051426\n",
      "GD iter. 76/499: loss=0.5597984031464384\n",
      "GD iter. 77/499: loss=0.559440900285456\n",
      "GD iter. 78/499: loss=0.5590964903601402\n",
      "GD iter. 79/499: loss=0.5587648242663712\n",
      "GD iter. 80/499: loss=0.5584539328602359\n",
      "GD iter. 81/499: loss=0.5581390400564316\n",
      "GD iter. 82/499: loss=0.5578177165502399\n",
      "GD iter. 83/499: loss=0.5574951796580992\n",
      "GD iter. 84/499: loss=0.5571880581775491\n",
      "GD iter. 85/499: loss=0.5568977510440629\n",
      "GD iter. 86/499: loss=0.5566258056629889\n",
      "GD iter. 87/499: loss=0.5563428153340981\n",
      "GD iter. 88/499: loss=0.5560422139548434\n",
      "GD iter. 89/499: loss=0.5557495277363728\n",
      "GD iter. 90/499: loss=0.5554974132571698\n",
      "GD iter. 91/499: loss=0.5551946911403941\n",
      "GD iter. 92/499: loss=0.5549348656964193\n",
      "GD iter. 93/499: loss=0.5546729472944338\n",
      "GD iter. 94/499: loss=0.5544032173533986\n",
      "GD iter. 95/499: loss=0.5541562686238038\n",
      "GD iter. 96/499: loss=0.553898038775063\n",
      "GD iter. 97/499: loss=0.553639684280843\n",
      "GD iter. 98/499: loss=0.5533857172086141\n",
      "GD iter. 99/499: loss=0.5531613674948639\n",
      "GD iter. 100/499: loss=0.5529374900005172\n",
      "GD iter. 101/499: loss=0.5526989144985681\n",
      "GD iter. 102/499: loss=0.5524784687354184\n",
      "GD iter. 103/499: loss=0.5522701983252037\n",
      "GD iter. 104/499: loss=0.5520670318866812\n",
      "GD iter. 105/499: loss=0.5518418864327004\n",
      "GD iter. 106/499: loss=0.5516194630415723\n",
      "GD iter. 107/499: loss=0.5513916354587961\n",
      "GD iter. 108/499: loss=0.551181154999584\n",
      "GD iter. 109/499: loss=0.5509638384523716\n",
      "GD iter. 110/499: loss=0.5507638692463315\n",
      "GD iter. 111/499: loss=0.5505719753999332\n",
      "GD iter. 112/499: loss=0.550366234349113\n",
      "GD iter. 113/499: loss=0.5501585059690989\n",
      "GD iter. 114/499: loss=0.5499750595024727\n",
      "GD iter. 115/499: loss=0.5497745823281597\n",
      "GD iter. 116/499: loss=0.5495843322024438\n",
      "GD iter. 117/499: loss=0.5493887603938782\n",
      "GD iter. 118/499: loss=0.5491998754683175\n",
      "GD iter. 119/499: loss=0.5490050201890134\n",
      "GD iter. 120/499: loss=0.5488212367043708\n",
      "GD iter. 121/499: loss=0.5486376036389421\n",
      "GD iter. 122/499: loss=0.5484458380060464\n",
      "GD iter. 123/499: loss=0.5482628041126393\n",
      "GD iter. 124/499: loss=0.5480863467256418\n",
      "GD iter. 125/499: loss=0.5479137440767257\n",
      "GD iter. 126/499: loss=0.5477177403493458\n",
      "GD iter. 127/499: loss=0.547557275406444\n",
      "GD iter. 128/499: loss=0.5473775577965465\n",
      "GD iter. 129/499: loss=0.5472063436710969\n",
      "GD iter. 130/499: loss=0.547028147042536\n",
      "GD iter. 131/499: loss=0.5468572760367263\n",
      "GD iter. 132/499: loss=0.5467181304607394\n",
      "GD iter. 133/499: loss=0.5465638319006022\n",
      "GD iter. 134/499: loss=0.5463687137529653\n",
      "GD iter. 135/499: loss=0.5461980956191321\n",
      "GD iter. 136/499: loss=0.5460451745472424\n",
      "GD iter. 137/499: loss=0.5459049374315244\n",
      "GD iter. 138/499: loss=0.5457296172522641\n",
      "GD iter. 139/499: loss=0.5455762054297401\n",
      "GD iter. 140/499: loss=0.5454016576081469\n",
      "GD iter. 141/499: loss=0.5452634431434676\n",
      "GD iter. 142/499: loss=0.5451280323686022\n",
      "GD iter. 143/499: loss=0.5449679221515698\n",
      "GD iter. 144/499: loss=0.5447951099152343\n",
      "GD iter. 145/499: loss=0.5446694571146936\n",
      "GD iter. 146/499: loss=0.5445078021954477\n",
      "GD iter. 147/499: loss=0.5443875845103122\n",
      "GD iter. 148/499: loss=0.5442257295796591\n",
      "GD iter. 149/499: loss=0.5440855265591594\n",
      "GD iter. 150/499: loss=0.5439464532155122\n",
      "GD iter. 151/499: loss=0.5438012440881385\n",
      "GD iter. 152/499: loss=0.5436633448480597\n",
      "GD iter. 153/499: loss=0.5435438315181966\n",
      "GD iter. 154/499: loss=0.5434187159288864\n",
      "GD iter. 155/499: loss=0.5432933317139798\n",
      "GD iter. 156/499: loss=0.5431656681996568\n",
      "GD iter. 157/499: loss=0.5430444629796743\n",
      "GD iter. 158/499: loss=0.542946852617154\n",
      "GD iter. 159/499: loss=0.5428334381548876\n",
      "GD iter. 160/499: loss=0.5427190456542268\n",
      "GD iter. 161/499: loss=0.5425971577754951\n",
      "GD iter. 162/499: loss=0.542490917659755\n",
      "GD iter. 163/499: loss=0.5424213140603835\n",
      "GD iter. 164/499: loss=0.5422867736196373\n",
      "GD iter. 165/499: loss=0.5421770022578684\n",
      "GD iter. 166/499: loss=0.5420526131967051\n",
      "GD iter. 167/499: loss=0.5419706329269443\n",
      "GD iter. 168/499: loss=0.5418436615620104\n",
      "GD iter. 169/499: loss=0.5417392633030572\n",
      "GD iter. 170/499: loss=0.5416410679880564\n",
      "GD iter. 171/499: loss=0.5415313082087913\n",
      "GD iter. 172/499: loss=0.5414188044034987\n",
      "GD iter. 173/499: loss=0.5413168794848532\n",
      "GD iter. 174/499: loss=0.5412322571943442\n",
      "GD iter. 175/499: loss=0.5411394386196277\n",
      "GD iter. 176/499: loss=0.5410145812300126\n",
      "GD iter. 177/499: loss=0.540925079212096\n",
      "GD iter. 178/499: loss=0.5408300182194291\n",
      "GD iter. 179/499: loss=0.5407168848039111\n",
      "GD iter. 180/499: loss=0.5406143722079614\n",
      "GD iter. 181/499: loss=0.5405212066644814\n",
      "GD iter. 182/499: loss=0.5404058102035857\n",
      "GD iter. 183/499: loss=0.5403044072560501\n",
      "GD iter. 184/499: loss=0.5402037348776209\n",
      "GD iter. 185/499: loss=0.5401088792445587\n",
      "GD iter. 186/499: loss=0.5399963363715652\n",
      "GD iter. 187/499: loss=0.5399316066088607\n",
      "GD iter. 188/499: loss=0.5398300042470181\n",
      "GD iter. 189/499: loss=0.5397398495337777\n",
      "GD iter. 190/499: loss=0.5396221836206958\n",
      "GD iter. 191/499: loss=0.5395236608216809\n",
      "GD iter. 192/499: loss=0.5394378549742008\n",
      "GD iter. 193/499: loss=0.5393652857772229\n",
      "GD iter. 194/499: loss=0.5392610291821025\n",
      "GD iter. 195/499: loss=0.5391671420384333\n",
      "GD iter. 196/499: loss=0.5390809196379961\n",
      "GD iter. 197/499: loss=0.5389941191362544\n",
      "GD iter. 198/499: loss=0.5389191532861444\n",
      "GD iter. 199/499: loss=0.5388283490651959\n",
      "GD iter. 200/499: loss=0.5387446017729445\n",
      "GD iter. 201/499: loss=0.5386550588408904\n",
      "GD iter. 202/499: loss=0.5385999933335726\n",
      "GD iter. 203/499: loss=0.5384891886106666\n",
      "GD iter. 204/499: loss=0.5384066425419504\n",
      "GD iter. 205/499: loss=0.5383240111372957\n",
      "GD iter. 206/499: loss=0.5382750909336147\n",
      "GD iter. 207/499: loss=0.538171839262441\n",
      "GD iter. 208/499: loss=0.5381089447819614\n",
      "GD iter. 209/499: loss=0.5380383466634004\n",
      "GD iter. 210/499: loss=0.5379732839855752\n",
      "GD iter. 211/499: loss=0.537905599441952\n",
      "GD iter. 212/499: loss=0.5378686252225631\n",
      "GD iter. 213/499: loss=0.5377531362200654\n",
      "GD iter. 214/499: loss=0.5376731682232827\n",
      "GD iter. 215/499: loss=0.5375966749043618\n",
      "GD iter. 216/499: loss=0.5375589513437822\n",
      "GD iter. 217/499: loss=0.5375008106172933\n",
      "GD iter. 218/499: loss=0.537425471401708\n",
      "GD iter. 219/499: loss=0.5373326000710692\n",
      "GD iter. 220/499: loss=0.5372730002057544\n",
      "GD iter. 221/499: loss=0.5372075632685372\n",
      "GD iter. 222/499: loss=0.5371566410960025\n",
      "GD iter. 223/499: loss=0.5371250615620298\n",
      "GD iter. 224/499: loss=0.5369994408720175\n",
      "GD iter. 225/499: loss=0.5369287490633415\n",
      "GD iter. 226/499: loss=0.5368590784734977\n",
      "GD iter. 227/499: loss=0.5368105546320496\n",
      "GD iter. 228/499: loss=0.5367428557854799\n",
      "GD iter. 229/499: loss=0.5366708384709519\n",
      "GD iter. 230/499: loss=0.5366065919512792\n",
      "GD iter. 231/499: loss=0.5365688768439744\n",
      "GD iter. 232/499: loss=0.5365138023218183\n",
      "GD iter. 233/499: loss=0.5364485703589723\n",
      "GD iter. 234/499: loss=0.5363785274676076\n",
      "GD iter. 235/499: loss=0.5363292421082079\n",
      "GD iter. 236/499: loss=0.5362633922215356\n",
      "GD iter. 237/499: loss=0.5361943707290134\n",
      "GD iter. 238/499: loss=0.5361300027484937\n",
      "GD iter. 239/499: loss=0.5360881540095217\n",
      "GD iter. 240/499: loss=0.5360074227187841\n",
      "GD iter. 241/499: loss=0.5359492225838103\n",
      "GD iter. 242/499: loss=0.5359066404200865\n",
      "GD iter. 243/499: loss=0.5358452125746552\n",
      "GD iter. 244/499: loss=0.5358009554473043\n",
      "GD iter. 245/499: loss=0.5357124023920556\n",
      "GD iter. 246/499: loss=0.5356569546840803\n",
      "GD iter. 247/499: loss=0.5356316728311504\n",
      "GD iter. 248/499: loss=0.5355484202077208\n",
      "GD iter. 249/499: loss=0.5355116240548343\n",
      "GD iter. 250/499: loss=0.5354114616223871\n",
      "GD iter. 251/499: loss=0.5353713930611805\n",
      "GD iter. 252/499: loss=0.5352942327634039\n",
      "GD iter. 253/499: loss=0.5352372785248596\n",
      "GD iter. 254/499: loss=0.5352039592674431\n",
      "GD iter. 255/499: loss=0.5351759134789945\n",
      "GD iter. 256/499: loss=0.535087859544994\n",
      "GD iter. 257/499: loss=0.5350111497995064\n",
      "GD iter. 258/499: loss=0.5349644715385473\n",
      "GD iter. 259/499: loss=0.5349407482811492\n",
      "GD iter. 260/499: loss=0.5348758824581642\n",
      "GD iter. 261/499: loss=0.5347862770838556\n",
      "GD iter. 262/499: loss=0.534746496280179\n",
      "GD iter. 263/499: loss=0.5346790895903797\n",
      "GD iter. 264/499: loss=0.5346303048769407\n",
      "GD iter. 265/499: loss=0.5345715154271244\n",
      "GD iter. 266/499: loss=0.5345408761505754\n",
      "GD iter. 267/499: loss=0.5345117088156041\n",
      "GD iter. 268/499: loss=0.5344163147098187\n",
      "GD iter. 269/499: loss=0.5343613456968025\n",
      "GD iter. 270/499: loss=0.5342989797818581\n",
      "GD iter. 271/499: loss=0.5342718036275411\n",
      "GD iter. 272/499: loss=0.5341973941636721\n",
      "GD iter. 273/499: loss=0.5341267699320464\n",
      "GD iter. 274/499: loss=0.534110181104337\n",
      "GD iter. 275/499: loss=0.5340671765894028\n",
      "GD iter. 276/499: loss=0.5340081303086988\n",
      "GD iter. 277/499: loss=0.5339490192754979\n",
      "GD iter. 278/499: loss=0.5338879841653903\n",
      "GD iter. 279/499: loss=0.533844646598669\n",
      "GD iter. 280/499: loss=0.5337733236896909\n",
      "GD iter. 281/499: loss=0.5337388128210039\n",
      "GD iter. 282/499: loss=0.5336999268852114\n",
      "GD iter. 283/499: loss=0.5336778521516834\n",
      "GD iter. 284/499: loss=0.533576516983252\n",
      "GD iter. 285/499: loss=0.5335228353733952\n",
      "GD iter. 286/499: loss=0.5335054624343818\n",
      "GD iter. 287/499: loss=0.5334930837712095\n",
      "GD iter. 288/499: loss=0.5333768950826131\n",
      "GD iter. 289/499: loss=0.5333291546038557\n",
      "GD iter. 290/499: loss=0.5332839026236363\n",
      "GD iter. 291/499: loss=0.5332448499320681\n",
      "GD iter. 292/499: loss=0.533224329625411\n",
      "GD iter. 293/499: loss=0.5331828528898824\n",
      "GD iter. 294/499: loss=0.5331174525143682\n",
      "GD iter. 295/499: loss=0.5330975824128262\n",
      "GD iter. 296/499: loss=0.533016250449595\n",
      "GD iter. 297/499: loss=0.5329498199521112\n",
      "GD iter. 298/499: loss=0.532914168021202\n",
      "GD iter. 299/499: loss=0.5328639833900575\n",
      "GD iter. 300/499: loss=0.5328147693466583\n",
      "GD iter. 301/499: loss=0.5327647798143481\n",
      "GD iter. 302/499: loss=0.5327291688326408\n",
      "GD iter. 303/499: loss=0.5327159500968969\n",
      "GD iter. 304/499: loss=0.532644163882111\n",
      "GD iter. 305/499: loss=0.5326184844459523\n",
      "GD iter. 306/499: loss=0.5326168194415977\n",
      "GD iter. 307/499: loss=0.5325357592902099\n",
      "GD iter. 308/499: loss=0.5325082982772773\n",
      "GD iter. 309/499: loss=0.5324470366507326\n",
      "GD iter. 310/499: loss=0.532398338138515\n",
      "GD iter. 311/499: loss=0.5323881908417114\n",
      "GD iter. 312/499: loss=0.5323552688711602\n",
      "GD iter. 313/499: loss=0.5322804164623963\n",
      "GD iter. 314/499: loss=0.5322422126433357\n",
      "GD iter. 315/499: loss=0.5321990094561334\n",
      "GD iter. 316/499: loss=0.5321976505288756\n",
      "GD iter. 317/499: loss=0.5321299678624158\n",
      "GD iter. 318/499: loss=0.5321486956319658\n",
      "GD iter. 319/499: loss=0.5320323342116592\n",
      "GD iter. 320/499: loss=0.5320166761443501\n",
      "GD iter. 321/499: loss=0.5319897255904424\n",
      "GD iter. 322/499: loss=0.5319473318424898\n",
      "GD iter. 323/499: loss=0.5318914764627932\n",
      "GD iter. 324/499: loss=0.5318760803080342\n",
      "GD iter. 325/499: loss=0.5318105909528503\n",
      "GD iter. 326/499: loss=0.5317860984983628\n",
      "GD iter. 327/499: loss=0.5317243626347457\n",
      "GD iter. 328/499: loss=0.53169546246317\n",
      "GD iter. 329/499: loss=0.5316813659882007\n",
      "GD iter. 330/499: loss=0.5316646349561411\n",
      "GD iter. 331/499: loss=0.5316117162094529\n",
      "GD iter. 332/499: loss=0.5316069673143909\n",
      "GD iter. 333/499: loss=0.531527121431919\n",
      "GD iter. 334/499: loss=0.531507538919595\n",
      "GD iter. 335/499: loss=0.5314677593638291\n",
      "GD iter. 336/499: loss=0.531442515162705\n",
      "GD iter. 337/499: loss=0.5314024120629302\n",
      "GD iter. 338/499: loss=0.5313790313645465\n",
      "GD iter. 339/499: loss=0.5313482505555018\n",
      "GD iter. 340/499: loss=0.531311119725179\n",
      "GD iter. 341/499: loss=0.5312800062330968\n",
      "GD iter. 342/499: loss=0.5312625418015298\n",
      "GD iter. 343/499: loss=0.5311817814284496\n",
      "GD iter. 344/499: loss=0.5311642229271623\n",
      "GD iter. 345/499: loss=0.5311514783518652\n",
      "GD iter. 346/499: loss=0.5311533496441712\n",
      "GD iter. 347/499: loss=0.5310724302769013\n",
      "GD iter. 348/499: loss=0.5310490675421982\n",
      "GD iter. 349/499: loss=0.5310020556542552\n",
      "GD iter. 350/499: loss=0.5309907240428082\n",
      "GD iter. 351/499: loss=0.5309436629960241\n",
      "GD iter. 352/499: loss=0.5309110253180112\n",
      "GD iter. 353/499: loss=0.5308838854032821\n",
      "GD iter. 354/499: loss=0.530834455639177\n",
      "GD iter. 355/499: loss=0.5308075845414947\n",
      "GD iter. 356/499: loss=0.5308334086273646\n",
      "GD iter. 357/499: loss=0.5307390483353027\n",
      "GD iter. 358/499: loss=0.5307174646641158\n",
      "GD iter. 359/499: loss=0.5307178664833544\n",
      "GD iter. 360/499: loss=0.530702388484343\n",
      "GD iter. 361/499: loss=0.5306186270405578\n",
      "GD iter. 362/499: loss=0.5306073818834194\n",
      "GD iter. 363/499: loss=0.5305681961154178\n",
      "GD iter. 364/499: loss=0.5305526310947438\n",
      "GD iter. 365/499: loss=0.5305456487902684\n",
      "GD iter. 366/499: loss=0.5304929481654049\n",
      "GD iter. 367/499: loss=0.530452971264591\n",
      "GD iter. 368/499: loss=0.5304083401423146\n",
      "GD iter. 369/499: loss=0.5303920841347401\n",
      "GD iter. 370/499: loss=0.5303915815236959\n",
      "GD iter. 371/499: loss=0.530362912914295\n",
      "GD iter. 372/499: loss=0.5302963148589843\n",
      "GD iter. 373/499: loss=0.5302632006709403\n",
      "GD iter. 374/499: loss=0.5302702554085369\n",
      "GD iter. 375/499: loss=0.5302746739709414\n",
      "GD iter. 376/499: loss=0.5301822300695799\n",
      "GD iter. 377/499: loss=0.5301619883991918\n",
      "GD iter. 378/499: loss=0.530123609234763\n",
      "GD iter. 379/499: loss=0.5300909306542765\n",
      "GD iter. 380/499: loss=0.5300754305889761\n",
      "GD iter. 381/499: loss=0.5300585944522718\n",
      "GD iter. 382/499: loss=0.530015850505713\n",
      "GD iter. 383/499: loss=0.529971682705009\n",
      "GD iter. 384/499: loss=0.5299427275918083\n",
      "GD iter. 385/499: loss=0.5299371679070288\n",
      "GD iter. 386/499: loss=0.5299057400760582\n",
      "GD iter. 387/499: loss=0.5299241549519756\n",
      "GD iter. 388/499: loss=0.5298268920047001\n",
      "GD iter. 389/499: loss=0.5297948860114757\n",
      "GD iter. 390/499: loss=0.5297785453322904\n",
      "GD iter. 391/499: loss=0.5297409214926362\n",
      "GD iter. 392/499: loss=0.52973723777299\n",
      "GD iter. 393/499: loss=0.5296985278402461\n",
      "GD iter. 394/499: loss=0.5297141824504722\n",
      "GD iter. 395/499: loss=0.5296606159851954\n",
      "GD iter. 396/499: loss=0.5296820599915055\n",
      "GD iter. 397/499: loss=0.5295910121870503\n",
      "GD iter. 398/499: loss=0.5295634807926656\n",
      "GD iter. 399/499: loss=0.5295501815049377\n",
      "GD iter. 400/499: loss=0.5294986959596015\n",
      "GD iter. 401/499: loss=0.5294876740539836\n",
      "GD iter. 402/499: loss=0.5294634025447817\n",
      "GD iter. 403/499: loss=0.5294484274083942\n",
      "GD iter. 404/499: loss=0.5294554038985448\n",
      "GD iter. 405/499: loss=0.5294243442600997\n",
      "GD iter. 406/499: loss=0.5293797469577167\n",
      "GD iter. 407/499: loss=0.5293442761934773\n",
      "GD iter. 408/499: loss=0.5293216644164093\n",
      "GD iter. 409/499: loss=0.5293332340382545\n",
      "GD iter. 410/499: loss=0.5292709444157897\n",
      "GD iter. 411/499: loss=0.5292525570955074\n",
      "GD iter. 412/499: loss=0.5292260012149611\n",
      "GD iter. 413/499: loss=0.5292135511940222\n",
      "GD iter. 414/499: loss=0.5291770411929534\n",
      "GD iter. 415/499: loss=0.5291515491316984\n",
      "GD iter. 416/499: loss=0.529132262290826\n",
      "GD iter. 417/499: loss=0.529096783367822\n",
      "GD iter. 418/499: loss=0.529070516177484\n",
      "GD iter. 419/499: loss=0.5290726454473478\n",
      "GD iter. 420/499: loss=0.5290398544580495\n",
      "GD iter. 421/499: loss=0.5290172699989639\n",
      "GD iter. 422/499: loss=0.5289858142409238\n",
      "GD iter. 423/499: loss=0.5289654665158781\n",
      "GD iter. 424/499: loss=0.528943088946488\n",
      "GD iter. 425/499: loss=0.5289362916100249\n",
      "GD iter. 426/499: loss=0.5289004434574586\n",
      "GD iter. 427/499: loss=0.5288739435889738\n",
      "GD iter. 428/499: loss=0.5288292776921689\n",
      "GD iter. 429/499: loss=0.5288047341617349\n",
      "GD iter. 430/499: loss=0.5287885343533967\n",
      "GD iter. 431/499: loss=0.528811057320689\n",
      "GD iter. 432/499: loss=0.5287499742718924\n",
      "GD iter. 433/499: loss=0.5287676571644604\n",
      "GD iter. 434/499: loss=0.5287302294520719\n",
      "GD iter. 435/499: loss=0.5287445473134762\n",
      "GD iter. 436/499: loss=0.528644122273789\n",
      "GD iter. 437/499: loss=0.5286377065552265\n",
      "GD iter. 438/499: loss=0.5286317798063115\n",
      "GD iter. 439/499: loss=0.5286286290463135\n",
      "GD iter. 440/499: loss=0.5285715366049709\n",
      "GD iter. 441/499: loss=0.5285960864940703\n",
      "GD iter. 442/499: loss=0.5285838806260821\n",
      "GD iter. 443/499: loss=0.5285939143890503\n",
      "GD iter. 444/499: loss=0.5284930811833636\n",
      "GD iter. 445/499: loss=0.5284758563215837\n",
      "GD iter. 446/499: loss=0.5284602820081712\n",
      "GD iter. 447/499: loss=0.5284903123038833\n",
      "GD iter. 448/499: loss=0.5284455131327039\n",
      "GD iter. 449/499: loss=0.528445319614621\n",
      "GD iter. 450/499: loss=0.5284358304213056\n",
      "GD iter. 451/499: loss=0.5284346594228458\n",
      "GD iter. 452/499: loss=0.5283612358847688\n",
      "GD iter. 453/499: loss=0.5283277077787134\n",
      "GD iter. 454/499: loss=0.5283109488019091\n",
      "GD iter. 455/499: loss=0.5283135169736692\n",
      "GD iter. 456/499: loss=0.5283693726221609\n",
      "GD iter. 457/499: loss=0.5282595153670414\n",
      "GD iter. 458/499: loss=0.5282546268020407\n",
      "GD iter. 459/499: loss=0.528216711787679\n",
      "GD iter. 460/499: loss=0.5282358654367166\n",
      "GD iter. 461/499: loss=0.5282148647937738\n",
      "GD iter. 462/499: loss=0.528225884317912\n",
      "GD iter. 463/499: loss=0.5281509982629209\n",
      "GD iter. 464/499: loss=0.5281397166493068\n",
      "GD iter. 465/499: loss=0.5281080268819228\n",
      "GD iter. 466/499: loss=0.5281246902339954\n",
      "GD iter. 467/499: loss=0.5280980250147084\n",
      "GD iter. 468/499: loss=0.5281272921049159\n",
      "GD iter. 469/499: loss=0.5280560093470281\n",
      "GD iter. 470/499: loss=0.528026563914141\n",
      "GD iter. 471/499: loss=0.5280134577452767\n",
      "GD iter. 472/499: loss=0.5280621397623945\n",
      "GD iter. 473/499: loss=0.5279722815042528\n",
      "GD iter. 474/499: loss=0.5279974002104613\n",
      "GD iter. 475/499: loss=0.5279386776294919\n",
      "GD iter. 476/499: loss=0.5279686173228779\n",
      "GD iter. 477/499: loss=0.5279041241074865\n",
      "GD iter. 478/499: loss=0.5279049347872278\n",
      "GD iter. 479/499: loss=0.5278810792268714\n",
      "GD iter. 480/499: loss=0.5279210404299232\n",
      "GD iter. 481/499: loss=0.5278711700137899\n",
      "GD iter. 482/499: loss=0.5278662087761745\n",
      "GD iter. 483/499: loss=0.5278223164687411\n",
      "GD iter. 484/499: loss=0.5278281703397458\n",
      "GD iter. 485/499: loss=0.5277704532088211\n",
      "GD iter. 486/499: loss=0.527799890041559\n",
      "GD iter. 487/499: loss=0.5277395628071108\n",
      "GD iter. 488/499: loss=0.5277914223283574\n",
      "GD iter. 489/499: loss=0.5277495966200779\n",
      "GD iter. 490/499: loss=0.5277754323446547\n",
      "GD iter. 491/499: loss=0.5276758451641373\n",
      "GD iter. 492/499: loss=0.527684036364331\n",
      "GD iter. 493/499: loss=0.5276533432100222\n",
      "GD iter. 494/499: loss=0.527649715145817\n",
      "GD iter. 495/499: loss=0.5276699291099192\n",
      "GD iter. 496/499: loss=0.5276637317739046\n",
      "GD iter. 497/499: loss=0.5276088043594409\n",
      "GD iter. 498/499: loss=0.5276199932027846\n",
      "GD iter. 499/499: loss=0.5275822003604096\n",
      "The Accuracy is: 0.6273\n",
      "The F1 score is: 0.2839\n",
      "The precision is: 0.1724\n",
      "The recall is: 0.8036\n",
      "GD iter. 0/499: loss=0.9727393862995509\n",
      "GD iter. 1/499: loss=0.890487161046471\n",
      "GD iter. 2/499: loss=0.8124736154886741\n",
      "GD iter. 3/499: loss=0.7517459275353915\n",
      "GD iter. 4/499: loss=0.714779011310937\n",
      "GD iter. 5/499: loss=0.6922474255715102\n",
      "GD iter. 6/499: loss=0.6756376530826029\n",
      "GD iter. 7/499: loss=0.6631784949425\n",
      "GD iter. 8/499: loss=0.6534368420261754\n",
      "GD iter. 9/499: loss=0.646262073865355\n",
      "GD iter. 10/499: loss=0.640040587746048\n",
      "GD iter. 11/499: loss=0.6347815857895651\n",
      "GD iter. 12/499: loss=0.630493216549665\n",
      "GD iter. 13/499: loss=0.6267341791745964\n",
      "GD iter. 14/499: loss=0.6233219187502869\n",
      "GD iter. 15/499: loss=0.6202340079146033\n",
      "GD iter. 16/499: loss=0.6175477262629068\n",
      "GD iter. 17/499: loss=0.6151956451448072\n",
      "GD iter. 18/499: loss=0.6130375018259772\n",
      "GD iter. 19/499: loss=0.6110457351376699\n",
      "GD iter. 20/499: loss=0.6091613306063247\n",
      "GD iter. 21/499: loss=0.6073653478650834\n",
      "GD iter. 22/499: loss=0.6056209191566654\n",
      "GD iter. 23/499: loss=0.6039904262778653\n",
      "GD iter. 24/499: loss=0.6024503891993392\n",
      "GD iter. 25/499: loss=0.6009490130532541\n",
      "GD iter. 26/499: loss=0.5994921072680408\n",
      "GD iter. 27/499: loss=0.5981236999440637\n",
      "GD iter. 28/499: loss=0.5967629017842604\n",
      "GD iter. 29/499: loss=0.5954532991690349\n",
      "GD iter. 30/499: loss=0.5942147115971309\n",
      "GD iter. 31/499: loss=0.5930370040493094\n",
      "GD iter. 32/499: loss=0.591879729803943\n",
      "GD iter. 33/499: loss=0.5907421370098254\n",
      "GD iter. 34/499: loss=0.5896429037043164\n",
      "GD iter. 35/499: loss=0.5885794197848002\n",
      "GD iter. 36/499: loss=0.5875380317455848\n",
      "GD iter. 37/499: loss=0.5864996778946168\n",
      "GD iter. 38/499: loss=0.5854685125736144\n",
      "GD iter. 39/499: loss=0.5844636491683887\n",
      "GD iter. 40/499: loss=0.5834994295326337\n",
      "GD iter. 41/499: loss=0.5825543612119457\n",
      "GD iter. 42/499: loss=0.5816327267798588\n",
      "GD iter. 43/499: loss=0.5807366623920762\n",
      "GD iter. 44/499: loss=0.5798931523641127\n",
      "GD iter. 45/499: loss=0.5791272175024543\n",
      "GD iter. 46/499: loss=0.5783879026120233\n",
      "GD iter. 47/499: loss=0.5776650693358066\n",
      "GD iter. 48/499: loss=0.5769602526075492\n",
      "GD iter. 49/499: loss=0.57626964326093\n",
      "GD iter. 50/499: loss=0.5755928045919398\n",
      "GD iter. 51/499: loss=0.5749258581029802\n",
      "GD iter. 52/499: loss=0.5743126428233796\n",
      "GD iter. 53/499: loss=0.5737203124865178\n",
      "GD iter. 54/499: loss=0.5731375999785231\n",
      "GD iter. 55/499: loss=0.5725717160442625\n",
      "GD iter. 56/499: loss=0.5720064650550067\n",
      "GD iter. 57/499: loss=0.5714735159891238\n",
      "GD iter. 58/499: loss=0.5709708587609442\n",
      "GD iter. 59/499: loss=0.5704936697605265\n",
      "GD iter. 60/499: loss=0.5700613724558815\n",
      "GD iter. 61/499: loss=0.5696248339643083\n",
      "GD iter. 62/499: loss=0.5692060716035439\n",
      "GD iter. 63/499: loss=0.5687914262133892\n",
      "GD iter. 64/499: loss=0.5683915566237983\n",
      "GD iter. 65/499: loss=0.5679997452357016\n",
      "GD iter. 66/499: loss=0.5676265328929788\n",
      "GD iter. 67/499: loss=0.5672398657878043\n",
      "GD iter. 68/499: loss=0.5668574581075697\n",
      "GD iter. 69/499: loss=0.5664828477441847\n",
      "GD iter. 70/499: loss=0.5661073737395901\n",
      "GD iter. 71/499: loss=0.5657273809873491\n",
      "GD iter. 72/499: loss=0.5653571011027079\n",
      "GD iter. 73/499: loss=0.5649896581648115\n",
      "GD iter. 74/499: loss=0.5646169262907015\n",
      "GD iter. 75/499: loss=0.5642604433896857\n",
      "GD iter. 76/499: loss=0.5639104225265191\n",
      "GD iter. 77/499: loss=0.5635498766615961\n",
      "GD iter. 78/499: loss=0.5632044384915152\n",
      "GD iter. 79/499: loss=0.5628720455202906\n",
      "GD iter. 80/499: loss=0.5625520128044007\n",
      "GD iter. 81/499: loss=0.5622396620971992\n",
      "GD iter. 82/499: loss=0.5619182706831061\n",
      "GD iter. 83/499: loss=0.5616083573582042\n",
      "GD iter. 84/499: loss=0.5612940878878043\n",
      "GD iter. 85/499: loss=0.560986736974857\n",
      "GD iter. 86/499: loss=0.5606891925084718\n",
      "GD iter. 87/499: loss=0.5604085017380479\n",
      "GD iter. 88/499: loss=0.5601134954375595\n",
      "GD iter. 89/499: loss=0.559832948146139\n",
      "GD iter. 90/499: loss=0.5595627116826011\n",
      "GD iter. 91/499: loss=0.5592983120402976\n",
      "GD iter. 92/499: loss=0.5590377129356218\n",
      "GD iter. 93/499: loss=0.558772995709957\n",
      "GD iter. 94/499: loss=0.5585089359643461\n",
      "GD iter. 95/499: loss=0.5582511406763488\n",
      "GD iter. 96/499: loss=0.5580283559822435\n",
      "GD iter. 97/499: loss=0.5578009160896986\n",
      "GD iter. 98/499: loss=0.5575826786416811\n",
      "GD iter. 99/499: loss=0.5573686953959843\n",
      "GD iter. 100/499: loss=0.5571505089908283\n",
      "GD iter. 101/499: loss=0.5569296524473768\n",
      "GD iter. 102/499: loss=0.5567190048465231\n",
      "GD iter. 103/499: loss=0.556495688558883\n",
      "GD iter. 104/499: loss=0.5562794339871107\n",
      "GD iter. 105/499: loss=0.5560690968308153\n",
      "GD iter. 106/499: loss=0.5558556829574214\n",
      "GD iter. 107/499: loss=0.5556386255322778\n",
      "GD iter. 108/499: loss=0.5554286803620135\n",
      "GD iter. 109/499: loss=0.5552180161165312\n",
      "GD iter. 110/499: loss=0.555004933357111\n",
      "GD iter. 111/499: loss=0.55480731339497\n",
      "GD iter. 112/499: loss=0.5546020265700238\n",
      "GD iter. 113/499: loss=0.5544018775961129\n",
      "GD iter. 114/499: loss=0.5541937534351586\n",
      "GD iter. 115/499: loss=0.5539868255522187\n",
      "GD iter. 116/499: loss=0.5537921947809188\n",
      "GD iter. 117/499: loss=0.5535881151662994\n",
      "GD iter. 118/499: loss=0.5533932193996127\n",
      "GD iter. 119/499: loss=0.5531911946380281\n",
      "GD iter. 120/499: loss=0.5529943954915734\n",
      "GD iter. 121/499: loss=0.5528100198114732\n",
      "GD iter. 122/499: loss=0.5526188751346967\n",
      "GD iter. 123/499: loss=0.5524301582474771\n",
      "GD iter. 124/499: loss=0.5522465210947386\n",
      "GD iter. 125/499: loss=0.5520598398795272\n",
      "GD iter. 126/499: loss=0.5518758660399599\n",
      "GD iter. 127/499: loss=0.5517013671353026\n",
      "GD iter. 128/499: loss=0.5515087705664552\n",
      "GD iter. 129/499: loss=0.5513354229837256\n",
      "GD iter. 130/499: loss=0.551166225107305\n",
      "GD iter. 131/499: loss=0.5509946530322197\n",
      "GD iter. 132/499: loss=0.5508224986914301\n",
      "GD iter. 133/499: loss=0.5506653262606762\n",
      "GD iter. 134/499: loss=0.550479413617894\n",
      "GD iter. 135/499: loss=0.5503134267323464\n",
      "GD iter. 136/499: loss=0.5501450336184494\n",
      "GD iter. 137/499: loss=0.5499837336401904\n",
      "GD iter. 138/499: loss=0.5498136017661996\n",
      "GD iter. 139/499: loss=0.5496540408631995\n",
      "GD iter. 140/499: loss=0.5495078869188419\n",
      "GD iter. 141/499: loss=0.5493556807209242\n",
      "GD iter. 142/499: loss=0.5491997031665923\n",
      "GD iter. 143/499: loss=0.5490562792578259\n",
      "GD iter. 144/499: loss=0.548902363013382\n",
      "GD iter. 145/499: loss=0.5487491461317969\n",
      "GD iter. 146/499: loss=0.5486228377179512\n",
      "GD iter. 147/499: loss=0.5484705192884747\n",
      "GD iter. 148/499: loss=0.5483300958529657\n",
      "GD iter. 149/499: loss=0.5481941490723127\n",
      "GD iter. 150/499: loss=0.5480515517063599\n",
      "GD iter. 151/499: loss=0.5479271969732921\n",
      "GD iter. 152/499: loss=0.5478103874812481\n",
      "GD iter. 153/499: loss=0.5476615607390591\n",
      "GD iter. 154/499: loss=0.5475189917653019\n",
      "GD iter. 155/499: loss=0.5473919829099981\n",
      "GD iter. 156/499: loss=0.5472660582924855\n",
      "GD iter. 157/499: loss=0.5471267646800881\n",
      "GD iter. 158/499: loss=0.5470065561984825\n",
      "GD iter. 159/499: loss=0.5468694467120434\n",
      "GD iter. 160/499: loss=0.5467369683767206\n",
      "GD iter. 161/499: loss=0.5466054312484621\n",
      "GD iter. 162/499: loss=0.5464885922972669\n",
      "GD iter. 163/499: loss=0.5463639299113189\n",
      "GD iter. 164/499: loss=0.5462337632001785\n",
      "GD iter. 165/499: loss=0.5461182089705281\n",
      "GD iter. 166/499: loss=0.5459931664910999\n",
      "GD iter. 167/499: loss=0.5458564295791165\n",
      "GD iter. 168/499: loss=0.5457356447027946\n",
      "GD iter. 169/499: loss=0.5456355142029673\n",
      "GD iter. 170/499: loss=0.5455401956347067\n",
      "GD iter. 171/499: loss=0.5454092877302746\n",
      "GD iter. 172/499: loss=0.5452802708040061\n",
      "GD iter. 173/499: loss=0.5451603173827198\n",
      "GD iter. 174/499: loss=0.5450446764594413\n",
      "GD iter. 175/499: loss=0.5449195098874091\n",
      "GD iter. 176/499: loss=0.5448162058829594\n",
      "GD iter. 177/499: loss=0.5447175078548595\n",
      "GD iter. 178/499: loss=0.5445978332855301\n",
      "GD iter. 179/499: loss=0.5444798507679747\n",
      "GD iter. 180/499: loss=0.544369964087148\n",
      "GD iter. 181/499: loss=0.5442658975218345\n",
      "GD iter. 182/499: loss=0.5441383909276888\n",
      "GD iter. 183/499: loss=0.5440316942630082\n",
      "GD iter. 184/499: loss=0.5439208993085615\n",
      "GD iter. 185/499: loss=0.5438106360321029\n",
      "GD iter. 186/499: loss=0.5436970662322567\n",
      "GD iter. 187/499: loss=0.5435951195034271\n",
      "GD iter. 188/499: loss=0.5435191140996203\n",
      "GD iter. 189/499: loss=0.5433814687785145\n",
      "GD iter. 190/499: loss=0.543269062284747\n",
      "GD iter. 191/499: loss=0.5431643826437728\n",
      "GD iter. 192/499: loss=0.5430540088786244\n",
      "GD iter. 193/499: loss=0.5429531845934167\n",
      "GD iter. 194/499: loss=0.5428584699148206\n",
      "GD iter. 195/499: loss=0.5427409170837068\n",
      "GD iter. 196/499: loss=0.5426226343084299\n",
      "GD iter. 197/499: loss=0.5425167463542918\n",
      "GD iter. 198/499: loss=0.5424616696595527\n",
      "GD iter. 199/499: loss=0.5423386382701909\n",
      "GD iter. 200/499: loss=0.5422179673278785\n",
      "GD iter. 201/499: loss=0.5421267366488306\n",
      "GD iter. 202/499: loss=0.5419984701368684\n",
      "GD iter. 203/499: loss=0.5418974137591377\n",
      "GD iter. 204/499: loss=0.5418461480837715\n",
      "GD iter. 205/499: loss=0.5417142695088684\n",
      "GD iter. 206/499: loss=0.5416150267576253\n",
      "GD iter. 207/499: loss=0.5415331223512869\n",
      "GD iter. 208/499: loss=0.5414525047157581\n",
      "GD iter. 209/499: loss=0.5413440793828082\n",
      "GD iter. 210/499: loss=0.5412432618464065\n",
      "GD iter. 211/499: loss=0.541178249441013\n",
      "GD iter. 212/499: loss=0.5410971294470771\n",
      "GD iter. 213/499: loss=0.5409948893816975\n",
      "GD iter. 214/499: loss=0.5408938949299162\n",
      "GD iter. 215/499: loss=0.5408149701211764\n",
      "GD iter. 216/499: loss=0.5407316865041127\n",
      "GD iter. 217/499: loss=0.5406428793316961\n",
      "GD iter. 218/499: loss=0.5405713255513469\n",
      "GD iter. 219/499: loss=0.5405002613178838\n",
      "GD iter. 220/499: loss=0.5404058746459782\n",
      "GD iter. 221/499: loss=0.5403043534946578\n",
      "GD iter. 222/499: loss=0.5402091483866766\n",
      "GD iter. 223/499: loss=0.540144168553506\n",
      "GD iter. 224/499: loss=0.5400485341544895\n",
      "GD iter. 225/499: loss=0.5399767518446699\n",
      "GD iter. 226/499: loss=0.5399037205865098\n",
      "GD iter. 227/499: loss=0.539804061328807\n",
      "GD iter. 228/499: loss=0.5397374542693154\n",
      "GD iter. 229/499: loss=0.5396813352962277\n",
      "GD iter. 230/499: loss=0.5395709194900674\n",
      "GD iter. 231/499: loss=0.5394867491044172\n",
      "GD iter. 232/499: loss=0.5394153866070709\n",
      "GD iter. 233/499: loss=0.5393713878519015\n",
      "GD iter. 234/499: loss=0.539253230321414\n",
      "GD iter. 235/499: loss=0.5391849694390723\n",
      "GD iter. 236/499: loss=0.5391067268724573\n",
      "GD iter. 237/499: loss=0.5390508656178548\n",
      "GD iter. 238/499: loss=0.5390036439744903\n",
      "GD iter. 239/499: loss=0.5389155780297655\n",
      "GD iter. 240/499: loss=0.5388429076878357\n",
      "GD iter. 241/499: loss=0.5387804124986371\n",
      "GD iter. 242/499: loss=0.5387354293320255\n",
      "GD iter. 243/499: loss=0.5386416333445031\n",
      "GD iter. 244/499: loss=0.5385674932977328\n",
      "GD iter. 245/499: loss=0.5385037988134198\n",
      "GD iter. 246/499: loss=0.5384422745882618\n",
      "GD iter. 247/499: loss=0.5383732747439363\n",
      "GD iter. 248/499: loss=0.5382935337944347\n",
      "GD iter. 249/499: loss=0.5382397466940743\n",
      "GD iter. 250/499: loss=0.5381891394863855\n",
      "GD iter. 251/499: loss=0.5381144305519913\n",
      "GD iter. 252/499: loss=0.5380456638459501\n",
      "GD iter. 253/499: loss=0.538004789027554\n",
      "GD iter. 254/499: loss=0.5379249405311106\n",
      "GD iter. 255/499: loss=0.5378579905889228\n",
      "GD iter. 256/499: loss=0.5378103241743829\n",
      "GD iter. 257/499: loss=0.5377501568326644\n",
      "GD iter. 258/499: loss=0.5376835884772009\n",
      "GD iter. 259/499: loss=0.5376524259645523\n",
      "GD iter. 260/499: loss=0.5376031018875612\n",
      "GD iter. 261/499: loss=0.5375526173149402\n",
      "GD iter. 262/499: loss=0.5374832287240042\n",
      "GD iter. 263/499: loss=0.5374206821313376\n",
      "GD iter. 264/499: loss=0.5373487887077073\n",
      "GD iter. 265/499: loss=0.537307256052302\n",
      "GD iter. 266/499: loss=0.5372602053768701\n",
      "GD iter. 267/499: loss=0.5371886373798772\n",
      "GD iter. 268/499: loss=0.537159427168988\n",
      "GD iter. 269/499: loss=0.5370921920636268\n",
      "GD iter. 270/499: loss=0.5370431880270601\n",
      "GD iter. 271/499: loss=0.5369903564035448\n",
      "GD iter. 272/499: loss=0.5369427051145401\n",
      "GD iter. 273/499: loss=0.5368951722544263\n",
      "GD iter. 274/499: loss=0.5368215670838683\n",
      "GD iter. 275/499: loss=0.5367992443154356\n",
      "GD iter. 276/499: loss=0.5367566693530019\n",
      "GD iter. 277/499: loss=0.5366731483181876\n",
      "GD iter. 278/499: loss=0.5366207665791802\n",
      "GD iter. 279/499: loss=0.5365569178908486\n",
      "GD iter. 280/499: loss=0.5365298907436997\n",
      "GD iter. 281/499: loss=0.5364737857437176\n",
      "GD iter. 282/499: loss=0.5364112156863661\n",
      "GD iter. 283/499: loss=0.5363783540024629\n",
      "GD iter. 284/499: loss=0.5363317454426619\n",
      "GD iter. 285/499: loss=0.5362745123928547\n",
      "GD iter. 286/499: loss=0.5362408553547562\n",
      "GD iter. 287/499: loss=0.5361688199861538\n",
      "GD iter. 288/499: loss=0.5361307932795392\n",
      "GD iter. 289/499: loss=0.5360711089100368\n",
      "GD iter. 290/499: loss=0.5360349688480539\n",
      "GD iter. 291/499: loss=0.5359629241334206\n",
      "GD iter. 292/499: loss=0.5359076393419893\n",
      "GD iter. 293/499: loss=0.5358708018330026\n",
      "GD iter. 294/499: loss=0.5358470369824438\n",
      "GD iter. 295/499: loss=0.5357872518137626\n",
      "GD iter. 296/499: loss=0.5357154217354383\n",
      "GD iter. 297/499: loss=0.535714193319592\n",
      "GD iter. 298/499: loss=0.5356814496402882\n",
      "GD iter. 299/499: loss=0.5355799638971682\n",
      "GD iter. 300/499: loss=0.5355218624513108\n",
      "GD iter. 301/499: loss=0.5354932373783878\n",
      "GD iter. 302/499: loss=0.535449768006472\n",
      "GD iter. 303/499: loss=0.5353957764579882\n",
      "GD iter. 304/499: loss=0.5353575575498788\n",
      "GD iter. 305/499: loss=0.5353134076408931\n",
      "GD iter. 306/499: loss=0.5353180195341902\n",
      "GD iter. 307/499: loss=0.5352248415741825\n",
      "GD iter. 308/499: loss=0.5351774878175469\n",
      "GD iter. 309/499: loss=0.5351344199597684\n",
      "GD iter. 310/499: loss=0.5350960343997413\n",
      "GD iter. 311/499: loss=0.5350362614842844\n",
      "GD iter. 312/499: loss=0.5349956161355999\n",
      "GD iter. 313/499: loss=0.5349438613610469\n",
      "GD iter. 314/499: loss=0.5349242748315655\n",
      "GD iter. 315/499: loss=0.534859114965284\n",
      "GD iter. 316/499: loss=0.5348080144206419\n",
      "GD iter. 317/499: loss=0.5347647576638369\n",
      "GD iter. 318/499: loss=0.5347380070054789\n",
      "GD iter. 319/499: loss=0.5346933582442335\n",
      "GD iter. 320/499: loss=0.5346438469784874\n",
      "GD iter. 321/499: loss=0.5346249828379103\n",
      "GD iter. 322/499: loss=0.534570437690307\n",
      "GD iter. 323/499: loss=0.5345172439738338\n",
      "GD iter. 324/499: loss=0.5344921696642048\n",
      "GD iter. 325/499: loss=0.5344665183026742\n",
      "GD iter. 326/499: loss=0.5343952511803141\n",
      "GD iter. 327/499: loss=0.534355062382072\n",
      "GD iter. 328/499: loss=0.5343090421843577\n",
      "GD iter. 329/499: loss=0.5342525461484282\n",
      "GD iter. 330/499: loss=0.5342270354791886\n",
      "GD iter. 331/499: loss=0.5342112883111161\n",
      "GD iter. 332/499: loss=0.5342072173149278\n",
      "GD iter. 333/499: loss=0.5341121087302563\n",
      "GD iter. 334/499: loss=0.5340815734087296\n",
      "GD iter. 335/499: loss=0.5340450439974338\n",
      "GD iter. 336/499: loss=0.5340084553336165\n",
      "GD iter. 337/499: loss=0.5339703532748913\n",
      "GD iter. 338/499: loss=0.5339241009709101\n",
      "GD iter. 339/499: loss=0.5338883355135751\n",
      "GD iter. 340/499: loss=0.5338861091021072\n",
      "GD iter. 341/499: loss=0.5338906391149819\n",
      "GD iter. 342/499: loss=0.5337639495147185\n",
      "GD iter. 343/499: loss=0.5337210398521945\n",
      "GD iter. 344/499: loss=0.5336991013612046\n",
      "GD iter. 345/499: loss=0.5336631295525176\n",
      "GD iter. 346/499: loss=0.533659690545625\n",
      "GD iter. 347/499: loss=0.5335925901335243\n",
      "GD iter. 348/499: loss=0.5335770804695008\n",
      "GD iter. 349/499: loss=0.5335770407663604\n",
      "GD iter. 350/499: loss=0.5335194094607829\n",
      "GD iter. 351/499: loss=0.5334698309009271\n",
      "GD iter. 352/499: loss=0.5334380213850914\n",
      "GD iter. 353/499: loss=0.5333945406342016\n",
      "GD iter. 354/499: loss=0.5333825196718757\n",
      "GD iter. 355/499: loss=0.533351126304277\n",
      "GD iter. 356/499: loss=0.5333106117267276\n",
      "GD iter. 357/499: loss=0.5332630106357213\n",
      "GD iter. 358/499: loss=0.5332345813965977\n",
      "GD iter. 359/499: loss=0.5332621395804764\n",
      "GD iter. 360/499: loss=0.533225854358418\n",
      "GD iter. 361/499: loss=0.5331606529849491\n",
      "GD iter. 362/499: loss=0.5331401043533172\n",
      "GD iter. 363/499: loss=0.533091119977176\n",
      "GD iter. 364/499: loss=0.5330836002672981\n",
      "GD iter. 365/499: loss=0.5330418191651407\n",
      "GD iter. 366/499: loss=0.5330247563310933\n",
      "GD iter. 367/499: loss=0.5329919355836849\n",
      "GD iter. 368/499: loss=0.5329994172181383\n",
      "GD iter. 369/499: loss=0.5329228449001887\n",
      "GD iter. 370/499: loss=0.5328899128673616\n",
      "GD iter. 371/499: loss=0.532885279516438\n",
      "GD iter. 372/499: loss=0.5328893265528051\n",
      "GD iter. 373/499: loss=0.5328300274278326\n",
      "GD iter. 374/499: loss=0.5327840132789968\n",
      "GD iter. 375/499: loss=0.5328115353330298\n",
      "GD iter. 376/499: loss=0.5327346534766506\n",
      "GD iter. 377/499: loss=0.5327061604709762\n",
      "GD iter. 378/499: loss=0.5326904390832111\n",
      "GD iter. 379/499: loss=0.5326591481714619\n",
      "GD iter. 380/499: loss=0.5326294265008117\n",
      "GD iter. 381/499: loss=0.532597720664445\n",
      "GD iter. 382/499: loss=0.5326174266822412\n",
      "GD iter. 383/499: loss=0.532570954137633\n",
      "GD iter. 384/499: loss=0.5325485670793088\n",
      "GD iter. 385/499: loss=0.5324962083700908\n",
      "GD iter. 386/499: loss=0.5324805802717424\n",
      "GD iter. 387/499: loss=0.532475495425124\n",
      "GD iter. 388/499: loss=0.5324680591280011\n",
      "GD iter. 389/499: loss=0.5323960853688204\n",
      "GD iter. 390/499: loss=0.5323705848219059\n",
      "GD iter. 391/499: loss=0.5323595784941701\n",
      "GD iter. 392/499: loss=0.532337309631539\n",
      "GD iter. 393/499: loss=0.5322961101675969\n",
      "GD iter. 394/499: loss=0.5322921876122044\n",
      "GD iter. 395/499: loss=0.532289741897787\n",
      "GD iter. 396/499: loss=0.5322134806998434\n",
      "GD iter. 397/499: loss=0.5321936941265052\n",
      "GD iter. 398/499: loss=0.5321690277680784\n",
      "GD iter. 399/499: loss=0.5321669365737711\n",
      "GD iter. 400/499: loss=0.532154145097883\n",
      "GD iter. 401/499: loss=0.5321092160319589\n",
      "GD iter. 402/499: loss=0.5320511601047717\n",
      "GD iter. 403/499: loss=0.532035114122717\n",
      "GD iter. 404/499: loss=0.5320136041676695\n",
      "GD iter. 405/499: loss=0.5319977819301572\n",
      "GD iter. 406/499: loss=0.5319717055699872\n",
      "GD iter. 407/499: loss=0.5319572943851534\n",
      "GD iter. 408/499: loss=0.5319423598192626\n",
      "GD iter. 409/499: loss=0.5319373828717672\n",
      "GD iter. 410/499: loss=0.5318836821035215\n",
      "GD iter. 411/499: loss=0.5318471656851331\n",
      "GD iter. 412/499: loss=0.5318297921289636\n",
      "GD iter. 413/499: loss=0.5318501603325058\n",
      "GD iter. 414/499: loss=0.5318008323257453\n",
      "GD iter. 415/499: loss=0.5317439379767062\n",
      "GD iter. 416/499: loss=0.5317259424318075\n",
      "GD iter. 417/499: loss=0.5317159617538613\n",
      "GD iter. 418/499: loss=0.5317002803127119\n",
      "GD iter. 419/499: loss=0.5316680914530781\n",
      "GD iter. 420/499: loss=0.5316348549969498\n",
      "GD iter. 421/499: loss=0.5316289264794855\n",
      "GD iter. 422/499: loss=0.5316282363099495\n",
      "GD iter. 423/499: loss=0.5315800965383997\n",
      "GD iter. 424/499: loss=0.5315442703254076\n",
      "GD iter. 425/499: loss=0.5315419965882912\n",
      "GD iter. 426/499: loss=0.5315076035788681\n",
      "GD iter. 427/499: loss=0.5314841032290923\n",
      "GD iter. 428/499: loss=0.5314946993041595\n",
      "GD iter. 429/499: loss=0.5314757147791598\n",
      "GD iter. 430/499: loss=0.5314050436383027\n",
      "GD iter. 431/499: loss=0.5314145122994579\n",
      "GD iter. 432/499: loss=0.5313749739633143\n",
      "GD iter. 433/499: loss=0.5313536058006646\n",
      "GD iter. 434/499: loss=0.5313371529995039\n",
      "GD iter. 435/499: loss=0.531332780932131\n",
      "GD iter. 436/499: loss=0.5313071216846218\n",
      "GD iter. 437/499: loss=0.531279173700622\n",
      "GD iter. 438/499: loss=0.5312562834513069\n",
      "GD iter. 439/499: loss=0.5312282326890084\n",
      "GD iter. 440/499: loss=0.5312142631785491\n",
      "GD iter. 441/499: loss=0.5311885465059673\n",
      "GD iter. 442/499: loss=0.5311754610767715\n",
      "GD iter. 443/499: loss=0.5311547702998142\n",
      "GD iter. 444/499: loss=0.5311396418885815\n",
      "GD iter. 445/499: loss=0.53111177407168\n",
      "GD iter. 446/499: loss=0.5311071637189658\n",
      "GD iter. 447/499: loss=0.5310824000353881\n",
      "GD iter. 448/499: loss=0.5310681100743296\n",
      "GD iter. 449/499: loss=0.5310114422291159\n",
      "GD iter. 450/499: loss=0.5310070605330308\n",
      "GD iter. 451/499: loss=0.5309682082110955\n",
      "GD iter. 452/499: loss=0.5309722337272563\n",
      "GD iter. 453/499: loss=0.5309650188297751\n",
      "GD iter. 454/499: loss=0.5309320683479505\n",
      "GD iter. 455/499: loss=0.5309134088413305\n",
      "GD iter. 456/499: loss=0.5309198319951722\n",
      "GD iter. 457/499: loss=0.5308822192838625\n",
      "GD iter. 458/499: loss=0.5308734580546692\n",
      "GD iter. 459/499: loss=0.5308155852348737\n",
      "GD iter. 460/499: loss=0.530795963085955\n",
      "GD iter. 461/499: loss=0.5308218435253582\n",
      "GD iter. 462/499: loss=0.5307752326813999\n",
      "GD iter. 463/499: loss=0.5307455135072725\n",
      "GD iter. 464/499: loss=0.5307295575005042\n",
      "GD iter. 465/499: loss=0.5307122278619315\n",
      "GD iter. 466/499: loss=0.530697531088202\n",
      "GD iter. 467/499: loss=0.5306714166539334\n",
      "GD iter. 468/499: loss=0.530657976329621\n",
      "GD iter. 469/499: loss=0.5306410888478593\n",
      "GD iter. 470/499: loss=0.5306419111976518\n",
      "GD iter. 471/499: loss=0.5306137157933728\n",
      "GD iter. 472/499: loss=0.5305891822359968\n",
      "GD iter. 473/499: loss=0.5305775630225449\n",
      "GD iter. 474/499: loss=0.530540364676166\n",
      "GD iter. 475/499: loss=0.530533852068469\n",
      "GD iter. 476/499: loss=0.5305310998985724\n",
      "GD iter. 477/499: loss=0.5305217856436434\n",
      "GD iter. 478/499: loss=0.5304754983419656\n",
      "GD iter. 479/499: loss=0.530474469928034\n",
      "GD iter. 480/499: loss=0.5304513396833822\n",
      "GD iter. 481/499: loss=0.5304351477267568\n",
      "GD iter. 482/499: loss=0.5303952578241137\n",
      "GD iter. 483/499: loss=0.5303920839216381\n",
      "GD iter. 484/499: loss=0.5303624605152207\n",
      "GD iter. 485/499: loss=0.5303683192458353\n",
      "GD iter. 486/499: loss=0.5303370378798626\n",
      "GD iter. 487/499: loss=0.5303145336119983\n",
      "GD iter. 488/499: loss=0.5302994492955929\n",
      "GD iter. 489/499: loss=0.5303037833201018\n",
      "GD iter. 490/499: loss=0.5303057889522437\n",
      "GD iter. 491/499: loss=0.5302798948804743\n",
      "GD iter. 492/499: loss=0.530241654962628\n",
      "GD iter. 493/499: loss=0.5302194320972107\n",
      "GD iter. 494/499: loss=0.5302144119368436\n",
      "GD iter. 495/499: loss=0.5302325284897729\n",
      "GD iter. 496/499: loss=0.530175866748304\n",
      "GD iter. 497/499: loss=0.5301747059595368\n",
      "GD iter. 498/499: loss=0.5301331904016944\n",
      "GD iter. 499/499: loss=0.5301319407660257\n",
      "The Accuracy is: 0.6273\n",
      "The F1 score is: 0.2458\n",
      "The precision is: 0.1412\n",
      "The recall is: 0.9487\n",
      "GD iter. 0/499: loss=0.9904926661191813\n",
      "GD iter. 1/499: loss=0.911409101410866\n",
      "GD iter. 2/499: loss=0.8346350968354639\n",
      "GD iter. 3/499: loss=0.7686598255592934\n",
      "GD iter. 4/499: loss=0.7230428440204556\n",
      "GD iter. 5/499: loss=0.6955185611430221\n",
      "GD iter. 6/499: loss=0.6760620322711199\n",
      "GD iter. 7/499: loss=0.6621990576317477\n",
      "GD iter. 8/499: loss=0.6516916587629763\n",
      "GD iter. 9/499: loss=0.6435940725572902\n",
      "GD iter. 10/499: loss=0.636996328447671\n",
      "GD iter. 11/499: loss=0.6315242254336175\n",
      "GD iter. 12/499: loss=0.6270487886801963\n",
      "GD iter. 13/499: loss=0.623114281236742\n",
      "GD iter. 14/499: loss=0.6196249435917195\n",
      "GD iter. 15/499: loss=0.6165017697410802\n",
      "GD iter. 16/499: loss=0.6137156429922139\n",
      "GD iter. 17/499: loss=0.6110247251231262\n",
      "GD iter. 18/499: loss=0.6084156766257014\n",
      "GD iter. 19/499: loss=0.6059457281787547\n",
      "GD iter. 20/499: loss=0.6036179998516633\n",
      "GD iter. 21/499: loss=0.6014348787225525\n",
      "GD iter. 22/499: loss=0.5994420495841223\n",
      "GD iter. 23/499: loss=0.5975557692057109\n",
      "GD iter. 24/499: loss=0.5958075003329273\n",
      "GD iter. 25/499: loss=0.5941043114822065\n",
      "GD iter. 26/499: loss=0.5924594812337666\n",
      "GD iter. 27/499: loss=0.5909550366625242\n",
      "GD iter. 28/499: loss=0.5895157611732414\n",
      "GD iter. 29/499: loss=0.5881024480202347\n",
      "GD iter. 30/499: loss=0.5867642650706759\n",
      "GD iter. 31/499: loss=0.5855323134725585\n",
      "GD iter. 32/499: loss=0.5843577281605197\n",
      "GD iter. 33/499: loss=0.5832279148846012\n",
      "GD iter. 34/499: loss=0.5821639509532802\n",
      "GD iter. 35/499: loss=0.5811607542780672\n",
      "GD iter. 36/499: loss=0.5801815132279671\n",
      "GD iter. 37/499: loss=0.5792267718492906\n",
      "GD iter. 38/499: loss=0.5782941222297183\n",
      "GD iter. 39/499: loss=0.5773925152710344\n",
      "GD iter. 40/499: loss=0.5765441984145263\n",
      "GD iter. 41/499: loss=0.5757627532252689\n",
      "GD iter. 42/499: loss=0.5750332501506532\n",
      "GD iter. 43/499: loss=0.5743244960974035\n",
      "GD iter. 44/499: loss=0.5736511596648586\n",
      "GD iter. 45/499: loss=0.5729950759436153\n",
      "GD iter. 46/499: loss=0.5723910909859931\n",
      "GD iter. 47/499: loss=0.571802242574418\n",
      "GD iter. 48/499: loss=0.5712467256715941\n",
      "GD iter. 49/499: loss=0.5707080265466543\n",
      "GD iter. 50/499: loss=0.5701911962441719\n",
      "GD iter. 51/499: loss=0.5696670816281152\n",
      "GD iter. 52/499: loss=0.5691664327294336\n",
      "GD iter. 53/499: loss=0.5686816886774843\n",
      "GD iter. 54/499: loss=0.5681924352161042\n",
      "GD iter. 55/499: loss=0.5677299771234061\n",
      "GD iter. 56/499: loss=0.5672899121374237\n",
      "GD iter. 57/499: loss=0.5668659931229874\n",
      "GD iter. 58/499: loss=0.5664503998914691\n",
      "GD iter. 59/499: loss=0.5660393073523224\n",
      "GD iter. 60/499: loss=0.5656354792864968\n",
      "GD iter. 61/499: loss=0.5652409686280757\n",
      "GD iter. 62/499: loss=0.5648423786917733\n",
      "GD iter. 63/499: loss=0.5644451494961846\n",
      "GD iter. 64/499: loss=0.5640427977976168\n",
      "GD iter. 65/499: loss=0.5636533007391192\n",
      "GD iter. 66/499: loss=0.5632748993020487\n",
      "GD iter. 67/499: loss=0.5628934907666743\n",
      "GD iter. 68/499: loss=0.562515225013609\n",
      "GD iter. 69/499: loss=0.5621665432115841\n",
      "GD iter. 70/499: loss=0.5618161310747823\n",
      "GD iter. 71/499: loss=0.5614853216409651\n",
      "GD iter. 72/499: loss=0.5611652522588272\n",
      "GD iter. 73/499: loss=0.5608460244530059\n",
      "GD iter. 74/499: loss=0.5605421033801921\n",
      "GD iter. 75/499: loss=0.560263147145629\n",
      "GD iter. 76/499: loss=0.5599838816865206\n",
      "GD iter. 77/499: loss=0.5597083814483865\n",
      "GD iter. 78/499: loss=0.5594329287008342\n",
      "GD iter. 79/499: loss=0.5591573827383408\n",
      "GD iter. 80/499: loss=0.5588966666996079\n",
      "GD iter. 81/499: loss=0.5586422512638928\n",
      "GD iter. 82/499: loss=0.558399333226834\n",
      "GD iter. 83/499: loss=0.5581547776764363\n",
      "GD iter. 84/499: loss=0.557919413768211\n",
      "GD iter. 85/499: loss=0.5576841430745666\n",
      "GD iter. 86/499: loss=0.5574426418350291\n",
      "GD iter. 87/499: loss=0.5572178309146959\n",
      "GD iter. 88/499: loss=0.556998041966013\n",
      "GD iter. 89/499: loss=0.5567606246116331\n",
      "GD iter. 90/499: loss=0.5565406721355943\n",
      "GD iter. 91/499: loss=0.5563227482585885\n",
      "GD iter. 92/499: loss=0.5561023433121021\n",
      "GD iter. 93/499: loss=0.5558960795157594\n",
      "GD iter. 94/499: loss=0.5556834606630658\n",
      "GD iter. 95/499: loss=0.5554797155046374\n",
      "GD iter. 96/499: loss=0.5552747391357614\n",
      "GD iter. 97/499: loss=0.5550691224661705\n",
      "GD iter. 98/499: loss=0.554887288689312\n",
      "GD iter. 99/499: loss=0.5546777081015959\n",
      "GD iter. 100/499: loss=0.5544823311776144\n",
      "GD iter. 101/499: loss=0.554290806781717\n",
      "GD iter. 102/499: loss=0.554113279349745\n",
      "GD iter. 103/499: loss=0.5539279000768165\n",
      "GD iter. 104/499: loss=0.5537529968589776\n",
      "GD iter. 105/499: loss=0.5535795519879239\n",
      "GD iter. 106/499: loss=0.5534081420598674\n",
      "GD iter. 107/499: loss=0.553225831303299\n",
      "GD iter. 108/499: loss=0.5530492804877516\n",
      "GD iter. 109/499: loss=0.5528966477339747\n",
      "GD iter. 110/499: loss=0.5527138392427648\n",
      "GD iter. 111/499: loss=0.552536272369645\n",
      "GD iter. 112/499: loss=0.5523596112779472\n",
      "GD iter. 113/499: loss=0.5521896589473768\n",
      "GD iter. 114/499: loss=0.5520267844750633\n",
      "GD iter. 115/499: loss=0.5518654674361386\n",
      "GD iter. 116/499: loss=0.5516831395772444\n",
      "GD iter. 117/499: loss=0.5515148062901568\n",
      "GD iter. 118/499: loss=0.5513469827978039\n",
      "GD iter. 119/499: loss=0.5511865398218946\n",
      "GD iter. 120/499: loss=0.5510189900422527\n",
      "GD iter. 121/499: loss=0.5508645764846346\n",
      "GD iter. 122/499: loss=0.5507155758657273\n",
      "GD iter. 123/499: loss=0.5505584915359333\n",
      "GD iter. 124/499: loss=0.5504021666608749\n",
      "GD iter. 125/499: loss=0.5502570616562648\n",
      "GD iter. 126/499: loss=0.5501245980498964\n",
      "GD iter. 127/499: loss=0.5500048657371936\n",
      "GD iter. 128/499: loss=0.5498469539891058\n",
      "GD iter. 129/499: loss=0.5497012232327791\n",
      "GD iter. 130/499: loss=0.5495733489113281\n",
      "GD iter. 131/499: loss=0.549437628287965\n",
      "GD iter. 132/499: loss=0.5493006678205231\n",
      "GD iter. 133/499: loss=0.5491707505977269\n",
      "GD iter. 134/499: loss=0.5490499293679143\n",
      "GD iter. 135/499: loss=0.5489081169810108\n",
      "GD iter. 136/499: loss=0.5487711440417619\n",
      "GD iter. 137/499: loss=0.5486333705386993\n",
      "GD iter. 138/499: loss=0.5485069366604357\n",
      "GD iter. 139/499: loss=0.5483658048302179\n",
      "GD iter. 140/499: loss=0.548250862552247\n",
      "GD iter. 141/499: loss=0.5481339308506309\n",
      "GD iter. 142/499: loss=0.5480230666142258\n",
      "GD iter. 143/499: loss=0.5478800588732644\n",
      "GD iter. 144/499: loss=0.5477684212267181\n",
      "GD iter. 145/499: loss=0.5476441543764422\n",
      "GD iter. 146/499: loss=0.5475320216181284\n",
      "GD iter. 147/499: loss=0.547400975932541\n",
      "GD iter. 148/499: loss=0.5472838589063177\n",
      "GD iter. 149/499: loss=0.5471570090405901\n",
      "GD iter. 150/499: loss=0.5470325651183449\n",
      "GD iter. 151/499: loss=0.546924686169172\n",
      "GD iter. 152/499: loss=0.5468000150069168\n",
      "GD iter. 153/499: loss=0.5466822324275152\n",
      "GD iter. 154/499: loss=0.5465858952085423\n",
      "GD iter. 155/499: loss=0.5464596504723487\n",
      "GD iter. 156/499: loss=0.546329149305054\n",
      "GD iter. 157/499: loss=0.5462353085066409\n",
      "GD iter. 158/499: loss=0.5461062297171466\n",
      "GD iter. 159/499: loss=0.5460021969121592\n",
      "GD iter. 160/499: loss=0.5458674646980705\n",
      "GD iter. 161/499: loss=0.5457733196013852\n",
      "GD iter. 162/499: loss=0.545656286789822\n",
      "GD iter. 163/499: loss=0.5455537244400311\n",
      "GD iter. 164/499: loss=0.5454405973694004\n",
      "GD iter. 165/499: loss=0.5453366946650627\n",
      "GD iter. 166/499: loss=0.5452098459667744\n",
      "GD iter. 167/499: loss=0.5451068028382927\n",
      "GD iter. 168/499: loss=0.5449741605945806\n",
      "GD iter. 169/499: loss=0.5448986046804213\n",
      "GD iter. 170/499: loss=0.544765937867721\n",
      "GD iter. 171/499: loss=0.5446600339488984\n",
      "GD iter. 172/499: loss=0.5445565982637154\n",
      "GD iter. 173/499: loss=0.5444660036060023\n",
      "GD iter. 174/499: loss=0.5443488039597695\n",
      "GD iter. 175/499: loss=0.5442414059894\n",
      "GD iter. 176/499: loss=0.5441463140590307\n",
      "GD iter. 177/499: loss=0.5440455096768126\n",
      "GD iter. 178/499: loss=0.5439467505238444\n",
      "GD iter. 179/499: loss=0.5438370778051791\n",
      "GD iter. 180/499: loss=0.5437415359094651\n",
      "GD iter. 181/499: loss=0.5436526168780685\n",
      "GD iter. 182/499: loss=0.54355097231988\n",
      "GD iter. 183/499: loss=0.5434373143829805\n",
      "GD iter. 184/499: loss=0.5433387511259904\n",
      "GD iter. 185/499: loss=0.5432596391862455\n",
      "GD iter. 186/499: loss=0.5431628294076375\n",
      "GD iter. 187/499: loss=0.5430541147846316\n",
      "GD iter. 188/499: loss=0.5429404090430092\n",
      "GD iter. 189/499: loss=0.5428795243374537\n",
      "GD iter. 190/499: loss=0.5427804220887212\n",
      "GD iter. 191/499: loss=0.5426889848789053\n",
      "GD iter. 192/499: loss=0.5425590899693402\n",
      "GD iter. 193/499: loss=0.5424776130458528\n",
      "GD iter. 194/499: loss=0.5423680269724753\n",
      "GD iter. 195/499: loss=0.542290797309923\n",
      "GD iter. 196/499: loss=0.5421916805478865\n",
      "GD iter. 197/499: loss=0.542132459243452\n",
      "GD iter. 198/499: loss=0.5420329850161053\n",
      "GD iter. 199/499: loss=0.5419373272426902\n",
      "GD iter. 200/499: loss=0.5418503191249592\n",
      "GD iter. 201/499: loss=0.5417744729237726\n",
      "GD iter. 202/499: loss=0.5416774444484269\n",
      "GD iter. 203/499: loss=0.5415812648597912\n",
      "GD iter. 204/499: loss=0.541480843002071\n",
      "GD iter. 205/499: loss=0.5413857323194796\n",
      "GD iter. 206/499: loss=0.541307031819462\n",
      "GD iter. 207/499: loss=0.541258521458617\n",
      "GD iter. 208/499: loss=0.5411685056774985\n",
      "GD iter. 209/499: loss=0.5410781195685695\n",
      "GD iter. 210/499: loss=0.540963885222076\n",
      "GD iter. 211/499: loss=0.5409018241613204\n",
      "GD iter. 212/499: loss=0.5408166198413115\n",
      "GD iter. 213/499: loss=0.540753349236627\n",
      "GD iter. 214/499: loss=0.540636291360381\n",
      "GD iter. 215/499: loss=0.5405689517861249\n",
      "GD iter. 216/499: loss=0.5405183522780217\n",
      "GD iter. 217/499: loss=0.5404242871523041\n",
      "GD iter. 218/499: loss=0.540358903773747\n",
      "GD iter. 219/499: loss=0.5402573289387661\n",
      "GD iter. 220/499: loss=0.5401775418431697\n",
      "GD iter. 221/499: loss=0.5401027504077258\n",
      "GD iter. 222/499: loss=0.5400405426800845\n",
      "GD iter. 223/499: loss=0.539966613283499\n",
      "GD iter. 224/499: loss=0.5399021795186018\n",
      "GD iter. 225/499: loss=0.5398284085147457\n",
      "GD iter. 226/499: loss=0.5397288495185125\n",
      "GD iter. 227/499: loss=0.5396596288436951\n",
      "GD iter. 228/499: loss=0.5395791845726817\n",
      "GD iter. 229/499: loss=0.5395119186241004\n",
      "GD iter. 230/499: loss=0.5394316095727929\n",
      "GD iter. 231/499: loss=0.5393524324058006\n",
      "GD iter. 232/499: loss=0.5392636900570784\n",
      "GD iter. 233/499: loss=0.5392175807341019\n",
      "GD iter. 234/499: loss=0.5391527036884252\n",
      "GD iter. 235/499: loss=0.5390886898276495\n",
      "GD iter. 236/499: loss=0.5389992518951098\n",
      "GD iter. 237/499: loss=0.538931286357905\n",
      "GD iter. 238/499: loss=0.538874720717026\n",
      "GD iter. 239/499: loss=0.538803983715919\n",
      "GD iter. 240/499: loss=0.5387132458352963\n",
      "GD iter. 241/499: loss=0.538650005496967\n",
      "GD iter. 242/499: loss=0.5386117230712183\n",
      "GD iter. 243/499: loss=0.5385640910258406\n",
      "GD iter. 244/499: loss=0.5384744261178916\n",
      "GD iter. 245/499: loss=0.5384132303555295\n",
      "GD iter. 246/499: loss=0.5383543054873945\n",
      "GD iter. 247/499: loss=0.538304208230737\n",
      "GD iter. 248/499: loss=0.5382118392600567\n",
      "GD iter. 249/499: loss=0.5381403185896104\n",
      "GD iter. 250/499: loss=0.5380811033093748\n",
      "GD iter. 251/499: loss=0.5380202884271095\n",
      "GD iter. 252/499: loss=0.5379966998122316\n",
      "GD iter. 253/499: loss=0.5379069356789434\n",
      "GD iter. 254/499: loss=0.5378554522764146\n",
      "GD iter. 255/499: loss=0.5377855731304467\n",
      "GD iter. 256/499: loss=0.5377148684392421\n",
      "GD iter. 257/499: loss=0.5376687168941316\n",
      "GD iter. 258/499: loss=0.5375888802878381\n",
      "GD iter. 259/499: loss=0.5375338658310822\n",
      "GD iter. 260/499: loss=0.5375267951017804\n",
      "GD iter. 261/499: loss=0.5374383123059202\n",
      "GD iter. 262/499: loss=0.5373822880205065\n",
      "GD iter. 263/499: loss=0.5373050486766052\n",
      "GD iter. 264/499: loss=0.537246662451691\n",
      "GD iter. 265/499: loss=0.5371886358368977\n",
      "GD iter. 266/499: loss=0.5371492668539859\n",
      "GD iter. 267/499: loss=0.5371067069157633\n",
      "GD iter. 268/499: loss=0.5370299659400534\n",
      "GD iter. 269/499: loss=0.5369668978861682\n",
      "GD iter. 270/499: loss=0.5369039249456787\n",
      "GD iter. 271/499: loss=0.5368560138426663\n",
      "GD iter. 272/499: loss=0.5368132212641251\n",
      "GD iter. 273/499: loss=0.5367573973156357\n",
      "GD iter. 274/499: loss=0.5366945002374257\n",
      "GD iter. 275/499: loss=0.5366504334478812\n",
      "GD iter. 276/499: loss=0.5366054155070271\n",
      "GD iter. 277/499: loss=0.536574119164215\n",
      "GD iter. 278/499: loss=0.5364945867304084\n",
      "GD iter. 279/499: loss=0.5364345370100383\n",
      "GD iter. 280/499: loss=0.5363969259128396\n",
      "GD iter. 281/499: loss=0.5363370533539781\n",
      "GD iter. 282/499: loss=0.5362476597698735\n",
      "GD iter. 283/499: loss=0.5362103689473235\n",
      "GD iter. 284/499: loss=0.5361388737719573\n",
      "GD iter. 285/499: loss=0.5361171707513923\n",
      "GD iter. 286/499: loss=0.5360754390168547\n",
      "GD iter. 287/499: loss=0.5360086317043449\n",
      "GD iter. 288/499: loss=0.5359509242123831\n",
      "GD iter. 289/499: loss=0.5359191601229553\n",
      "GD iter. 290/499: loss=0.5358656241814772\n",
      "GD iter. 291/499: loss=0.535802556869396\n",
      "GD iter. 292/499: loss=0.5357565691675189\n",
      "GD iter. 293/499: loss=0.5357061085323024\n",
      "GD iter. 294/499: loss=0.5356834637106092\n",
      "GD iter. 295/499: loss=0.5356418383701532\n",
      "GD iter. 296/499: loss=0.5355641538910543\n",
      "GD iter. 297/499: loss=0.5355033715899575\n",
      "GD iter. 298/499: loss=0.535481468026761\n",
      "GD iter. 299/499: loss=0.5354186091603713\n",
      "GD iter. 300/499: loss=0.5353557955846355\n",
      "GD iter. 301/499: loss=0.5353157330677408\n",
      "GD iter. 302/499: loss=0.5352799906079866\n",
      "GD iter. 303/499: loss=0.535246466205236\n",
      "GD iter. 304/499: loss=0.5352004718434389\n",
      "GD iter. 305/499: loss=0.5352001963380312\n",
      "GD iter. 306/499: loss=0.5350959035929217\n",
      "GD iter. 307/499: loss=0.5350510771858079\n",
      "GD iter. 308/499: loss=0.5350092148533275\n",
      "GD iter. 309/499: loss=0.5349823691503953\n",
      "GD iter. 310/499: loss=0.534948944968599\n",
      "GD iter. 311/499: loss=0.5348939931876355\n",
      "GD iter. 312/499: loss=0.5348676446923631\n",
      "GD iter. 313/499: loss=0.5348263144897082\n",
      "GD iter. 314/499: loss=0.5347708138102506\n",
      "GD iter. 315/499: loss=0.5347224431125023\n",
      "GD iter. 316/499: loss=0.5346996229078501\n",
      "GD iter. 317/499: loss=0.5346730792638537\n",
      "GD iter. 318/499: loss=0.5346105369722473\n",
      "GD iter. 319/499: loss=0.5345564043059772\n",
      "GD iter. 320/499: loss=0.5345325417238967\n",
      "GD iter. 321/499: loss=0.5345061838486003\n",
      "GD iter. 322/499: loss=0.5344943680267411\n",
      "GD iter. 323/499: loss=0.5344198586965418\n",
      "GD iter. 324/499: loss=0.5343849586662393\n",
      "GD iter. 325/499: loss=0.5343651548765762\n",
      "GD iter. 326/499: loss=0.5343451824479999\n",
      "GD iter. 327/499: loss=0.5342898099830168\n",
      "GD iter. 328/499: loss=0.5342582848909837\n",
      "GD iter. 329/499: loss=0.5342003817965012\n",
      "GD iter. 330/499: loss=0.5341619751543077\n",
      "GD iter. 331/499: loss=0.534145271609017\n",
      "GD iter. 332/499: loss=0.534122055602722\n",
      "GD iter. 333/499: loss=0.5340801409106462\n",
      "GD iter. 334/499: loss=0.5340581291839824\n",
      "GD iter. 335/499: loss=0.5339961185023505\n",
      "GD iter. 336/499: loss=0.5339733523156522\n",
      "GD iter. 337/499: loss=0.5339193896470894\n",
      "GD iter. 338/499: loss=0.5338799740908271\n",
      "GD iter. 339/499: loss=0.5338441030799478\n",
      "GD iter. 340/499: loss=0.5338433369100519\n",
      "GD iter. 341/499: loss=0.5337921817662895\n",
      "GD iter. 342/499: loss=0.5337614284078555\n",
      "GD iter. 343/499: loss=0.5337369837055125\n",
      "GD iter. 344/499: loss=0.533712242613446\n",
      "GD iter. 345/499: loss=0.5336504321751547\n",
      "GD iter. 346/499: loss=0.5336001921184655\n",
      "GD iter. 347/499: loss=0.5335730600407144\n",
      "GD iter. 348/499: loss=0.5335374826064249\n",
      "GD iter. 349/499: loss=0.5335221529657129\n",
      "GD iter. 350/499: loss=0.5334824712541995\n",
      "GD iter. 351/499: loss=0.5334499023174949\n",
      "GD iter. 352/499: loss=0.5333980982937611\n",
      "GD iter. 353/499: loss=0.5333761034748591\n",
      "GD iter. 354/499: loss=0.5333530765451086\n",
      "GD iter. 355/499: loss=0.5333682049556226\n",
      "GD iter. 356/499: loss=0.5332853964552549\n",
      "GD iter. 357/499: loss=0.5332611038658511\n",
      "GD iter. 358/499: loss=0.5332215579621751\n",
      "GD iter. 359/499: loss=0.5331767368522715\n",
      "GD iter. 360/499: loss=0.5331464759280115\n",
      "GD iter. 361/499: loss=0.5331555199003988\n",
      "GD iter. 362/499: loss=0.5331135588139397\n",
      "GD iter. 363/499: loss=0.5331170371827101\n",
      "GD iter. 364/499: loss=0.5330373002268324\n",
      "GD iter. 365/499: loss=0.5329925280726583\n",
      "GD iter. 366/499: loss=0.5329767439964704\n",
      "GD iter. 367/499: loss=0.53298430969681\n",
      "GD iter. 368/499: loss=0.5329359873363023\n",
      "GD iter. 369/499: loss=0.5329131084941007\n",
      "GD iter. 370/499: loss=0.5328539836492107\n",
      "GD iter. 371/499: loss=0.5328211172718698\n",
      "GD iter. 372/499: loss=0.5327778017896341\n",
      "GD iter. 373/499: loss=0.5327664311975407\n",
      "GD iter. 374/499: loss=0.5327428904055616\n",
      "GD iter. 375/499: loss=0.532692207845647\n",
      "GD iter. 376/499: loss=0.5326806912265478\n",
      "GD iter. 377/499: loss=0.5326780203556057\n",
      "GD iter. 378/499: loss=0.5326830226742533\n",
      "GD iter. 379/499: loss=0.532595748493239\n",
      "GD iter. 380/499: loss=0.5325826809347189\n",
      "GD iter. 381/499: loss=0.5325487954362238\n",
      "GD iter. 382/499: loss=0.5325100957286889\n",
      "GD iter. 383/499: loss=0.5325044101782471\n",
      "GD iter. 384/499: loss=0.532511709132894\n",
      "GD iter. 385/499: loss=0.532433183323086\n",
      "GD iter. 386/499: loss=0.5324102940017928\n",
      "GD iter. 387/499: loss=0.5323625125950338\n",
      "GD iter. 388/499: loss=0.5323391458308455\n",
      "GD iter. 389/499: loss=0.532337614309661\n",
      "GD iter. 390/499: loss=0.5323054013152004\n",
      "GD iter. 391/499: loss=0.5322659729076672\n",
      "GD iter. 392/499: loss=0.5322321651513093\n",
      "GD iter. 393/499: loss=0.532206057157161\n",
      "GD iter. 394/499: loss=0.5321657441595812\n",
      "GD iter. 395/499: loss=0.5321967261986774\n",
      "GD iter. 396/499: loss=0.5321783860240443\n",
      "GD iter. 397/499: loss=0.532093858049994\n",
      "GD iter. 398/499: loss=0.5320602917581364\n",
      "GD iter. 399/499: loss=0.5320374108494229\n",
      "GD iter. 400/499: loss=0.5320205604164715\n",
      "GD iter. 401/499: loss=0.5319967881050442\n",
      "GD iter. 402/499: loss=0.5319509126832491\n",
      "GD iter. 403/499: loss=0.5319477129987017\n",
      "GD iter. 404/499: loss=0.5319056169826234\n",
      "GD iter. 405/499: loss=0.5318805500067696\n",
      "GD iter. 406/499: loss=0.5318599908347629\n",
      "GD iter. 407/499: loss=0.531884936118844\n",
      "GD iter. 408/499: loss=0.5318769292749088\n",
      "GD iter. 409/499: loss=0.5317852736980025\n",
      "GD iter. 410/499: loss=0.5317582625733238\n",
      "GD iter. 411/499: loss=0.5317184444469981\n",
      "GD iter. 412/499: loss=0.5317023682746338\n",
      "GD iter. 413/499: loss=0.5316836884955103\n",
      "GD iter. 414/499: loss=0.5316595316933318\n",
      "GD iter. 415/499: loss=0.5316764326819613\n",
      "GD iter. 416/499: loss=0.5316557377698005\n",
      "GD iter. 417/499: loss=0.5315758096448612\n",
      "GD iter. 418/499: loss=0.5315457314410009\n",
      "GD iter. 419/499: loss=0.5315299036879825\n",
      "GD iter. 420/499: loss=0.531522476408776\n",
      "GD iter. 421/499: loss=0.5315089283810225\n",
      "GD iter. 422/499: loss=0.5314960657369917\n",
      "GD iter. 423/499: loss=0.5314462979578343\n",
      "GD iter. 424/499: loss=0.5313898076452206\n",
      "GD iter. 425/499: loss=0.5313773658302424\n",
      "GD iter. 426/499: loss=0.5313583717000968\n",
      "GD iter. 427/499: loss=0.5313833172152806\n",
      "GD iter. 428/499: loss=0.5313520390818757\n",
      "GD iter. 429/499: loss=0.5313289670207055\n",
      "GD iter. 430/499: loss=0.5312628373036359\n",
      "GD iter. 431/499: loss=0.5312421984745909\n",
      "GD iter. 432/499: loss=0.5312087905346997\n",
      "GD iter. 433/499: loss=0.5312050551489337\n",
      "GD iter. 434/499: loss=0.531232596604771\n",
      "GD iter. 435/499: loss=0.5311781372817369\n",
      "GD iter. 436/499: loss=0.5311449365021736\n",
      "GD iter. 437/499: loss=0.5311336658521211\n",
      "GD iter. 438/499: loss=0.5311090843664686\n",
      "GD iter. 439/499: loss=0.5310949268695393\n",
      "GD iter. 440/499: loss=0.5310490413217027\n",
      "GD iter. 441/499: loss=0.5310230899535624\n",
      "GD iter. 442/499: loss=0.5310116095344898\n",
      "GD iter. 443/499: loss=0.5310010924341684\n",
      "GD iter. 444/499: loss=0.5309849348345868\n",
      "GD iter. 445/499: loss=0.5310000639027267\n",
      "GD iter. 446/499: loss=0.530904062772985\n",
      "GD iter. 447/499: loss=0.5308783860477219\n",
      "GD iter. 448/499: loss=0.5308949347857699\n",
      "GD iter. 449/499: loss=0.5308827554593797\n",
      "GD iter. 450/499: loss=0.5308805395610832\n",
      "GD iter. 451/499: loss=0.5308151540953677\n",
      "GD iter. 452/499: loss=0.530808344231513\n",
      "GD iter. 453/499: loss=0.5307609663442099\n",
      "GD iter. 454/499: loss=0.5307492173918846\n",
      "GD iter. 455/499: loss=0.5307295768296424\n",
      "GD iter. 456/499: loss=0.5306924633009512\n",
      "GD iter. 457/499: loss=0.5307411954094626\n",
      "GD iter. 458/499: loss=0.5307421113622981\n",
      "GD iter. 459/499: loss=0.530642870050357\n",
      "GD iter. 460/499: loss=0.5306348240916523\n",
      "GD iter. 461/499: loss=0.5306369090577799\n",
      "GD iter. 462/499: loss=0.5306019141691433\n",
      "GD iter. 463/499: loss=0.5305807616293436\n",
      "GD iter. 464/499: loss=0.5305913965019466\n",
      "GD iter. 465/499: loss=0.5305390336056256\n",
      "GD iter. 466/499: loss=0.5305552122385748\n",
      "GD iter. 467/499: loss=0.5304914925845279\n",
      "GD iter. 468/499: loss=0.5304636312435612\n",
      "GD iter. 469/499: loss=0.5304534228440013\n",
      "GD iter. 470/499: loss=0.5304840152864105\n",
      "GD iter. 471/499: loss=0.5304307229094635\n",
      "GD iter. 472/499: loss=0.5304393333881057\n",
      "GD iter. 473/499: loss=0.5303899541708574\n",
      "GD iter. 474/499: loss=0.5303588530492351\n",
      "GD iter. 475/499: loss=0.5303221073133364\n",
      "GD iter. 476/499: loss=0.5303239930334632\n",
      "GD iter. 477/499: loss=0.5303356636348178\n",
      "GD iter. 478/499: loss=0.5302782202255705\n",
      "GD iter. 479/499: loss=0.5302725128497333\n",
      "GD iter. 480/499: loss=0.5303039422768395\n",
      "GD iter. 481/499: loss=0.5302375067509881\n",
      "GD iter. 482/499: loss=0.5302622060381661\n",
      "GD iter. 483/499: loss=0.5301996886814773\n",
      "GD iter. 484/499: loss=0.5302087577198534\n",
      "GD iter. 485/499: loss=0.5301903780514787\n",
      "GD iter. 486/499: loss=0.5301787281099841\n",
      "GD iter. 487/499: loss=0.5301295376850852\n",
      "GD iter. 488/499: loss=0.5301448638638412\n",
      "GD iter. 489/499: loss=0.530109153091555\n",
      "GD iter. 490/499: loss=0.5300928699884658\n",
      "GD iter. 491/499: loss=0.5300652367798515\n",
      "GD iter. 492/499: loss=0.5300587501306689\n",
      "GD iter. 493/499: loss=0.5300095543925544\n",
      "GD iter. 494/499: loss=0.5300036162933457\n",
      "GD iter. 495/499: loss=0.5300089398682428\n",
      "GD iter. 496/499: loss=0.5299728643260243\n",
      "GD iter. 497/499: loss=0.5299918979848697\n",
      "GD iter. 498/499: loss=0.5299910278469024\n",
      "GD iter. 499/499: loss=0.5299383955004505\n",
      "The Accuracy is: 0.6289\n",
      "The F1 score is: 0.3110\n",
      "The precision is: 0.1861\n",
      "The recall is: 0.9444\n",
      "GD iter. 0/499: loss=1.0179786200267622\n",
      "GD iter. 1/499: loss=0.9363867142323443\n",
      "GD iter. 2/499: loss=0.8571546421847507\n",
      "GD iter. 3/499: loss=0.787585275588012\n",
      "GD iter. 4/499: loss=0.7391581338188838\n",
      "GD iter. 5/499: loss=0.7062048629047956\n",
      "GD iter. 6/499: loss=0.6854111052309493\n",
      "GD iter. 7/499: loss=0.6699681043288882\n",
      "GD iter. 8/499: loss=0.6582382764267136\n",
      "GD iter. 9/499: loss=0.6502498452169309\n",
      "GD iter. 10/499: loss=0.6435484852479896\n",
      "GD iter. 11/499: loss=0.6378961143425235\n",
      "GD iter. 12/499: loss=0.6327925763010946\n",
      "GD iter. 13/499: loss=0.6281316434607701\n",
      "GD iter. 14/499: loss=0.6240159035762024\n",
      "GD iter. 15/499: loss=0.6204188352227495\n",
      "GD iter. 16/499: loss=0.6171637946332125\n",
      "GD iter. 17/499: loss=0.614190562305994\n",
      "GD iter. 18/499: loss=0.6114592578776971\n",
      "GD iter. 19/499: loss=0.6090917360196094\n",
      "GD iter. 20/499: loss=0.6069524870551536\n",
      "GD iter. 21/499: loss=0.6050375761073598\n",
      "GD iter. 22/499: loss=0.6032255244927825\n",
      "GD iter. 23/499: loss=0.6014587846616626\n",
      "GD iter. 24/499: loss=0.5997713491400664\n",
      "GD iter. 25/499: loss=0.5981446430142213\n",
      "GD iter. 26/499: loss=0.5966661698622618\n",
      "GD iter. 27/499: loss=0.5952691711145682\n",
      "GD iter. 28/499: loss=0.5939338705875351\n",
      "GD iter. 29/499: loss=0.5926496744897379\n",
      "GD iter. 30/499: loss=0.5914504474952272\n",
      "GD iter. 31/499: loss=0.5903097900834757\n",
      "GD iter. 32/499: loss=0.5892717150650705\n",
      "GD iter. 33/499: loss=0.5882906916931758\n",
      "GD iter. 34/499: loss=0.587359256478482\n",
      "GD iter. 35/499: loss=0.5864937206981556\n",
      "GD iter. 36/499: loss=0.5856318019345141\n",
      "GD iter. 37/499: loss=0.5847792007360355\n",
      "GD iter. 38/499: loss=0.5839642972499034\n",
      "GD iter. 39/499: loss=0.5831537586634961\n",
      "GD iter. 40/499: loss=0.5823536191268028\n",
      "GD iter. 41/499: loss=0.5815670529096203\n",
      "GD iter. 42/499: loss=0.5808222363001972\n",
      "GD iter. 43/499: loss=0.5800952741812826\n",
      "GD iter. 44/499: loss=0.5793695685127319\n",
      "GD iter. 45/499: loss=0.578656443217398\n",
      "GD iter. 46/499: loss=0.5779546018743417\n",
      "GD iter. 47/499: loss=0.5772963953017632\n",
      "GD iter. 48/499: loss=0.576646435550272\n",
      "GD iter. 49/499: loss=0.5760054154503149\n",
      "GD iter. 50/499: loss=0.5753708425523263\n",
      "GD iter. 51/499: loss=0.5747718607732681\n",
      "GD iter. 52/499: loss=0.5741750356046902\n",
      "GD iter. 53/499: loss=0.5735885845512657\n",
      "GD iter. 54/499: loss=0.5730098870068128\n",
      "GD iter. 55/499: loss=0.5724473836685807\n",
      "GD iter. 56/499: loss=0.5718872268965123\n",
      "GD iter. 57/499: loss=0.5713422673445767\n",
      "GD iter. 58/499: loss=0.5708197424401276\n",
      "GD iter. 59/499: loss=0.5703077177995945\n",
      "GD iter. 60/499: loss=0.5698157009424912\n",
      "GD iter. 61/499: loss=0.5693365032857592\n",
      "GD iter. 62/499: loss=0.5688607531323104\n",
      "GD iter. 63/499: loss=0.5684228706269209\n",
      "GD iter. 64/499: loss=0.5679803449246138\n",
      "GD iter. 65/499: loss=0.5675466856753335\n",
      "GD iter. 66/499: loss=0.5671060862085551\n",
      "GD iter. 67/499: loss=0.5666917731528166\n",
      "GD iter. 68/499: loss=0.5662815315467244\n",
      "GD iter. 69/499: loss=0.5658641947952855\n",
      "GD iter. 70/499: loss=0.5654529428965877\n",
      "GD iter. 71/499: loss=0.5650733796785021\n",
      "GD iter. 72/499: loss=0.5647005528464478\n",
      "GD iter. 73/499: loss=0.5643218203183651\n",
      "GD iter. 74/499: loss=0.5639425607416546\n",
      "GD iter. 75/499: loss=0.5635775040628763\n",
      "GD iter. 76/499: loss=0.5632141422265081\n",
      "GD iter. 77/499: loss=0.562863143624296\n",
      "GD iter. 78/499: loss=0.5625051437027127\n",
      "GD iter. 79/499: loss=0.5621619917867312\n",
      "GD iter. 80/499: loss=0.5618085215894246\n",
      "GD iter. 81/499: loss=0.5614652025643249\n",
      "GD iter. 82/499: loss=0.5611243207900211\n",
      "GD iter. 83/499: loss=0.5607875770185067\n",
      "GD iter. 84/499: loss=0.5604439738945871\n",
      "GD iter. 85/499: loss=0.5601045071692492\n",
      "GD iter. 86/499: loss=0.5597664144924874\n",
      "GD iter. 87/499: loss=0.5594406208973297\n",
      "GD iter. 88/499: loss=0.5591382916440352\n",
      "GD iter. 89/499: loss=0.5588124442018056\n",
      "GD iter. 90/499: loss=0.5584976545177815\n",
      "GD iter. 91/499: loss=0.5582064931600628\n",
      "GD iter. 92/499: loss=0.557951234522136\n",
      "GD iter. 93/499: loss=0.5577040737654569\n",
      "GD iter. 94/499: loss=0.557441124747174\n",
      "GD iter. 95/499: loss=0.5572194210494867\n",
      "GD iter. 96/499: loss=0.556983096590535\n",
      "GD iter. 97/499: loss=0.5567501643303711\n",
      "GD iter. 98/499: loss=0.5565129047639843\n",
      "GD iter. 99/499: loss=0.5563051175984419\n",
      "GD iter. 100/499: loss=0.5560744072875123\n",
      "GD iter. 101/499: loss=0.5558675140836074\n",
      "GD iter. 102/499: loss=0.5556666016564324\n",
      "GD iter. 103/499: loss=0.5554536660746454\n",
      "GD iter. 104/499: loss=0.5552504433403411\n",
      "GD iter. 105/499: loss=0.555054570643975\n",
      "GD iter. 106/499: loss=0.5548783498274771\n",
      "GD iter. 107/499: loss=0.5546973450226232\n",
      "GD iter. 108/499: loss=0.5545137590874822\n",
      "GD iter. 109/499: loss=0.5543167610007731\n",
      "GD iter. 110/499: loss=0.55412372560521\n",
      "GD iter. 111/499: loss=0.5539409802213071\n",
      "GD iter. 112/499: loss=0.5537644601602242\n",
      "GD iter. 113/499: loss=0.5535801917927595\n",
      "GD iter. 114/499: loss=0.5533978164335179\n",
      "GD iter. 115/499: loss=0.5532288310336871\n",
      "GD iter. 116/499: loss=0.5530536752467733\n",
      "GD iter. 117/499: loss=0.5528666583048206\n",
      "GD iter. 118/499: loss=0.5526943450035718\n",
      "GD iter. 119/499: loss=0.5525340917035007\n",
      "GD iter. 120/499: loss=0.5523448344857317\n",
      "GD iter. 121/499: loss=0.5521934713764715\n",
      "GD iter. 122/499: loss=0.552049543865489\n",
      "GD iter. 123/499: loss=0.5518610354119097\n",
      "GD iter. 124/499: loss=0.5517149756684018\n",
      "GD iter. 125/499: loss=0.5515635058938727\n",
      "GD iter. 126/499: loss=0.5514081480173713\n",
      "GD iter. 127/499: loss=0.5512643457977917\n",
      "GD iter. 128/499: loss=0.551125584626628\n",
      "GD iter. 129/499: loss=0.5509766730594824\n",
      "GD iter. 130/499: loss=0.5507967196531969\n",
      "GD iter. 131/499: loss=0.5506560981348165\n",
      "GD iter. 132/499: loss=0.5505170943118288\n",
      "GD iter. 133/499: loss=0.5503620188625514\n",
      "GD iter. 134/499: loss=0.5502155724787453\n",
      "GD iter. 135/499: loss=0.5500816480039846\n",
      "GD iter. 136/499: loss=0.5499339564009502\n",
      "GD iter. 137/499: loss=0.5497780799582168\n",
      "GD iter. 138/499: loss=0.5496321631467029\n",
      "GD iter. 139/499: loss=0.5494939473789933\n",
      "GD iter. 140/499: loss=0.5493523270519843\n",
      "GD iter. 141/499: loss=0.5492212767665825\n",
      "GD iter. 142/499: loss=0.5490765215323502\n",
      "GD iter. 143/499: loss=0.5489334256892824\n",
      "GD iter. 144/499: loss=0.5487835853581982\n",
      "GD iter. 145/499: loss=0.5486733912728724\n",
      "GD iter. 146/499: loss=0.5485128163851195\n",
      "GD iter. 147/499: loss=0.548391034944847\n",
      "GD iter. 148/499: loss=0.5482509353181665\n",
      "GD iter. 149/499: loss=0.5481157982594308\n",
      "GD iter. 150/499: loss=0.5480107641591251\n",
      "GD iter. 151/499: loss=0.5478599008489399\n",
      "GD iter. 152/499: loss=0.547743799171071\n",
      "GD iter. 153/499: loss=0.5476062385813375\n",
      "GD iter. 154/499: loss=0.5474871356056586\n",
      "GD iter. 155/499: loss=0.5473488001718224\n",
      "GD iter. 156/499: loss=0.5472439178436\n",
      "GD iter. 157/499: loss=0.547105518901329\n",
      "GD iter. 158/499: loss=0.5470044834223107\n",
      "GD iter. 159/499: loss=0.5468717967936016\n",
      "GD iter. 160/499: loss=0.5467749813655557\n",
      "GD iter. 161/499: loss=0.5466439481287805\n",
      "GD iter. 162/499: loss=0.5465216761413829\n",
      "GD iter. 163/499: loss=0.5464326633703851\n",
      "GD iter. 164/499: loss=0.546277980250971\n",
      "GD iter. 165/499: loss=0.5461832451615767\n",
      "GD iter. 166/499: loss=0.5460821642013152\n",
      "GD iter. 167/499: loss=0.545961035341387\n",
      "GD iter. 168/499: loss=0.5458421948541564\n",
      "GD iter. 169/499: loss=0.5457258822788841\n",
      "GD iter. 170/499: loss=0.5456061912915842\n",
      "GD iter. 171/499: loss=0.5454982832007945\n",
      "GD iter. 172/499: loss=0.5453908967294604\n",
      "GD iter. 173/499: loss=0.5452779351209608\n",
      "GD iter. 174/499: loss=0.5451832402588112\n",
      "GD iter. 175/499: loss=0.5450915475678968\n",
      "GD iter. 176/499: loss=0.5449412660761465\n",
      "GD iter. 177/499: loss=0.544856178218902\n",
      "GD iter. 178/499: loss=0.5447353273522494\n",
      "GD iter. 179/499: loss=0.5446407533102924\n",
      "GD iter. 180/499: loss=0.5445158672658614\n",
      "GD iter. 181/499: loss=0.5444190354812193\n",
      "GD iter. 182/499: loss=0.5443128219143608\n",
      "GD iter. 183/499: loss=0.54420710988413\n",
      "GD iter. 184/499: loss=0.54410690738043\n",
      "GD iter. 185/499: loss=0.5440013167164193\n",
      "GD iter. 186/499: loss=0.543895037853349\n",
      "GD iter. 187/499: loss=0.5438006144179744\n",
      "GD iter. 188/499: loss=0.5437026020992473\n",
      "GD iter. 189/499: loss=0.5435791572728247\n",
      "GD iter. 190/499: loss=0.543499962872907\n",
      "GD iter. 191/499: loss=0.5433952383727466\n",
      "GD iter. 192/499: loss=0.5433078098446146\n",
      "GD iter. 193/499: loss=0.5432163914942041\n",
      "GD iter. 194/499: loss=0.5431030967608744\n",
      "GD iter. 195/499: loss=0.542994690744023\n",
      "GD iter. 196/499: loss=0.5429109138167966\n",
      "GD iter. 197/499: loss=0.5428173806403187\n",
      "GD iter. 198/499: loss=0.5427104459558187\n",
      "GD iter. 199/499: loss=0.5426222420053202\n",
      "GD iter. 200/499: loss=0.5425405076075078\n",
      "GD iter. 201/499: loss=0.5424480040945562\n",
      "GD iter. 202/499: loss=0.5423511038375173\n",
      "GD iter. 203/499: loss=0.5422785983692379\n",
      "GD iter. 204/499: loss=0.5421532937309705\n",
      "GD iter. 205/499: loss=0.5420672845642763\n",
      "GD iter. 206/499: loss=0.5419728248049155\n",
      "GD iter. 207/499: loss=0.5418930514341733\n",
      "GD iter. 208/499: loss=0.5418082140213359\n",
      "GD iter. 209/499: loss=0.54174682754843\n",
      "GD iter. 210/499: loss=0.5416820297031809\n",
      "GD iter. 211/499: loss=0.5415747590013527\n",
      "GD iter. 212/499: loss=0.5414667781951606\n",
      "GD iter. 213/499: loss=0.5414006486294909\n",
      "GD iter. 214/499: loss=0.5413204849947909\n",
      "GD iter. 215/499: loss=0.5412240705999053\n",
      "GD iter. 216/499: loss=0.5411515444242215\n",
      "GD iter. 217/499: loss=0.5410468222084558\n",
      "GD iter. 218/499: loss=0.5409692495266233\n",
      "GD iter. 219/499: loss=0.5408988848759826\n",
      "GD iter. 220/499: loss=0.54081696840975\n",
      "GD iter. 221/499: loss=0.5407295207128783\n",
      "GD iter. 222/499: loss=0.5406514304201518\n",
      "GD iter. 223/499: loss=0.5405609346603981\n",
      "GD iter. 224/499: loss=0.5404796158071655\n",
      "GD iter. 225/499: loss=0.5404400792822869\n",
      "GD iter. 226/499: loss=0.5403432644070408\n",
      "GD iter. 227/499: loss=0.5402504349372564\n",
      "GD iter. 228/499: loss=0.540161521851114\n",
      "GD iter. 229/499: loss=0.5400821068759137\n",
      "GD iter. 230/499: loss=0.5400020726714635\n",
      "GD iter. 231/499: loss=0.5399162215730017\n",
      "GD iter. 232/499: loss=0.5398580430776788\n",
      "GD iter. 233/499: loss=0.5397897840924027\n",
      "GD iter. 234/499: loss=0.5396992648504969\n",
      "GD iter. 235/499: loss=0.5396275051926773\n",
      "GD iter. 236/499: loss=0.5395708317688916\n",
      "GD iter. 237/499: loss=0.5394925258330134\n",
      "GD iter. 238/499: loss=0.5394162301764802\n",
      "GD iter. 239/499: loss=0.5393369708167297\n",
      "GD iter. 240/499: loss=0.5392565424047251\n",
      "GD iter. 241/499: loss=0.5391491966416944\n",
      "GD iter. 242/499: loss=0.5390995078904464\n",
      "GD iter. 243/499: loss=0.5390113469418861\n",
      "GD iter. 244/499: loss=0.5389504239821197\n",
      "GD iter. 245/499: loss=0.5388814465480617\n",
      "GD iter. 246/499: loss=0.5388268067345164\n",
      "GD iter. 247/499: loss=0.5387513548779194\n",
      "GD iter. 248/499: loss=0.5387004045110549\n",
      "GD iter. 249/499: loss=0.5386289900288622\n",
      "GD iter. 250/499: loss=0.5385889732043648\n",
      "GD iter. 251/499: loss=0.5385174321667399\n",
      "GD iter. 252/499: loss=0.5384314805690938\n",
      "GD iter. 253/499: loss=0.5383763933828909\n",
      "GD iter. 254/499: loss=0.538315440790552\n",
      "GD iter. 255/499: loss=0.5382508812369655\n",
      "GD iter. 256/499: loss=0.5381744547532613\n",
      "GD iter. 257/499: loss=0.5381170315723649\n",
      "GD iter. 258/499: loss=0.5380379374362746\n",
      "GD iter. 259/499: loss=0.53798438968722\n",
      "GD iter. 260/499: loss=0.5379777969569891\n",
      "GD iter. 261/499: loss=0.5379240816814799\n",
      "GD iter. 262/499: loss=0.5378167716502383\n",
      "GD iter. 263/499: loss=0.537762507326637\n",
      "GD iter. 264/499: loss=0.5377009478880007\n",
      "GD iter. 265/499: loss=0.537664861256311\n",
      "GD iter. 266/499: loss=0.537599518663404\n",
      "GD iter. 267/499: loss=0.5375626305312289\n",
      "GD iter. 268/499: loss=0.53749056303006\n",
      "GD iter. 269/499: loss=0.5374403786847816\n",
      "GD iter. 270/499: loss=0.5373749437797078\n",
      "GD iter. 271/499: loss=0.5372984559740774\n",
      "GD iter. 272/499: loss=0.5372377527752926\n",
      "GD iter. 273/499: loss=0.5371937024966067\n",
      "GD iter. 274/499: loss=0.5371466608884035\n",
      "GD iter. 275/499: loss=0.5371112955436098\n",
      "GD iter. 276/499: loss=0.5370614669669211\n",
      "GD iter. 277/499: loss=0.5369988475679407\n",
      "GD iter. 278/499: loss=0.536957818459607\n",
      "GD iter. 279/499: loss=0.5369158746183508\n",
      "GD iter. 280/499: loss=0.5368721199524558\n",
      "GD iter. 281/499: loss=0.5368169063823648\n",
      "GD iter. 282/499: loss=0.5367513158104414\n",
      "GD iter. 283/499: loss=0.536687511594105\n",
      "GD iter. 284/499: loss=0.5366562566651131\n",
      "GD iter. 285/499: loss=0.5366044689143145\n",
      "GD iter. 286/499: loss=0.536564034702285\n",
      "GD iter. 287/499: loss=0.5365057445086387\n",
      "GD iter. 288/499: loss=0.536500794588767\n",
      "GD iter. 289/499: loss=0.5364540277287223\n",
      "GD iter. 290/499: loss=0.5363591421396817\n",
      "GD iter. 291/499: loss=0.5363154252218022\n",
      "GD iter. 292/499: loss=0.5362749105532364\n",
      "GD iter. 293/499: loss=0.5362496390015106\n",
      "GD iter. 294/499: loss=0.5361701704671238\n",
      "GD iter. 295/499: loss=0.5361153183218902\n",
      "GD iter. 296/499: loss=0.5360725972116727\n",
      "GD iter. 297/499: loss=0.5360397548915765\n",
      "GD iter. 298/499: loss=0.5360023255721162\n",
      "GD iter. 299/499: loss=0.5359569195772599\n",
      "GD iter. 300/499: loss=0.5359190521544687\n",
      "GD iter. 301/499: loss=0.5358547479042862\n",
      "GD iter. 302/499: loss=0.5358375348273758\n",
      "GD iter. 303/499: loss=0.5357675815849492\n",
      "GD iter. 304/499: loss=0.5357424928665444\n",
      "GD iter. 305/499: loss=0.535680407465951\n",
      "GD iter. 306/499: loss=0.535646227379313\n",
      "GD iter. 307/499: loss=0.5356190322879152\n",
      "GD iter. 308/499: loss=0.5355621038658169\n",
      "GD iter. 309/499: loss=0.5355198163078166\n",
      "GD iter. 310/499: loss=0.5354608021363038\n",
      "GD iter. 311/499: loss=0.5354238744366266\n",
      "GD iter. 312/499: loss=0.5353984241968427\n",
      "GD iter. 313/499: loss=0.5353860916149473\n",
      "GD iter. 314/499: loss=0.5353052878849105\n",
      "GD iter. 315/499: loss=0.5352502693509562\n",
      "GD iter. 316/499: loss=0.5352093050904037\n",
      "GD iter. 317/499: loss=0.5351581539056737\n",
      "GD iter. 318/499: loss=0.5351067502172009\n",
      "GD iter. 319/499: loss=0.5351137242552835\n",
      "GD iter. 320/499: loss=0.5350459332520953\n",
      "GD iter. 321/499: loss=0.535004987491917\n",
      "GD iter. 322/499: loss=0.5349809264974134\n",
      "GD iter. 323/499: loss=0.5349428689330102\n",
      "GD iter. 324/499: loss=0.534895650007094\n",
      "GD iter. 325/499: loss=0.5348274129582515\n",
      "GD iter. 326/499: loss=0.5347954165798731\n",
      "GD iter. 327/499: loss=0.5347332792093465\n",
      "GD iter. 328/499: loss=0.5347139305515176\n",
      "GD iter. 329/499: loss=0.5346760650620881\n",
      "GD iter. 330/499: loss=0.5346325941890062\n",
      "GD iter. 331/499: loss=0.5345794640002997\n",
      "GD iter. 332/499: loss=0.5345577493180336\n",
      "GD iter. 333/499: loss=0.5345220320150903\n",
      "GD iter. 334/499: loss=0.5344855516340535\n",
      "GD iter. 335/499: loss=0.5344552224937922\n",
      "GD iter. 336/499: loss=0.5343961261681482\n",
      "GD iter. 337/499: loss=0.5343714549235569\n",
      "GD iter. 338/499: loss=0.5343056051157985\n",
      "GD iter. 339/499: loss=0.5342573078944796\n",
      "GD iter. 340/499: loss=0.5342254979572932\n",
      "GD iter. 341/499: loss=0.5341822335752586\n",
      "GD iter. 342/499: loss=0.5341431093103589\n",
      "GD iter. 343/499: loss=0.5341602845126191\n",
      "GD iter. 344/499: loss=0.5340744969769119\n",
      "GD iter. 345/499: loss=0.5340552461993567\n",
      "GD iter. 346/499: loss=0.5339863848187202\n",
      "GD iter. 347/499: loss=0.5339471699243744\n",
      "GD iter. 348/499: loss=0.5339316417502248\n",
      "GD iter. 349/499: loss=0.5338869364459453\n",
      "GD iter. 350/499: loss=0.53383866297791\n",
      "GD iter. 351/499: loss=0.5338263672915634\n",
      "GD iter. 352/499: loss=0.5337660376589237\n",
      "GD iter. 353/499: loss=0.5337585005305452\n",
      "GD iter. 354/499: loss=0.533686827346249\n",
      "GD iter. 355/499: loss=0.5336431401160113\n",
      "GD iter. 356/499: loss=0.5336010652408679\n",
      "GD iter. 357/499: loss=0.5335803702126177\n",
      "GD iter. 358/499: loss=0.5335549787595895\n",
      "GD iter. 359/499: loss=0.5334956483043988\n",
      "GD iter. 360/499: loss=0.5334706307353212\n",
      "GD iter. 361/499: loss=0.533424971710256\n",
      "GD iter. 362/499: loss=0.5333721258122345\n",
      "GD iter. 363/499: loss=0.5333480019495269\n",
      "GD iter. 364/499: loss=0.5333089258773195\n",
      "GD iter. 365/499: loss=0.5332872334076808\n",
      "GD iter. 366/499: loss=0.533250455547416\n",
      "GD iter. 367/499: loss=0.5332033021468728\n",
      "GD iter. 368/499: loss=0.5331666475664393\n",
      "GD iter. 369/499: loss=0.5331408037341634\n",
      "GD iter. 370/499: loss=0.533125504105058\n",
      "GD iter. 371/499: loss=0.5330845199725416\n",
      "GD iter. 372/499: loss=0.5330561309742494\n",
      "GD iter. 373/499: loss=0.5330077209867913\n",
      "GD iter. 374/499: loss=0.5329628589021211\n",
      "GD iter. 375/499: loss=0.5329433956217662\n",
      "GD iter. 376/499: loss=0.5329178552320166\n",
      "GD iter. 377/499: loss=0.5329213964714027\n",
      "GD iter. 378/499: loss=0.5328635611637348\n",
      "GD iter. 379/499: loss=0.5328083080845194\n",
      "GD iter. 380/499: loss=0.5327827948146048\n",
      "GD iter. 381/499: loss=0.5327686134745441\n",
      "GD iter. 382/499: loss=0.5327481303505678\n",
      "GD iter. 383/499: loss=0.5326790120051669\n",
      "GD iter. 384/499: loss=0.5326477299436573\n",
      "GD iter. 385/499: loss=0.5326497935594903\n",
      "GD iter. 386/499: loss=0.5326184638225046\n",
      "GD iter. 387/499: loss=0.5325618207833589\n",
      "GD iter. 388/499: loss=0.5325665808279432\n",
      "GD iter. 389/499: loss=0.5325132611766878\n",
      "GD iter. 390/499: loss=0.5324639833116501\n",
      "GD iter. 391/499: loss=0.5324265156175425\n",
      "GD iter. 392/499: loss=0.5324111311712997\n",
      "GD iter. 393/499: loss=0.5324021675516055\n",
      "GD iter. 394/499: loss=0.5323485720747225\n",
      "GD iter. 395/499: loss=0.5323512712910343\n",
      "GD iter. 396/499: loss=0.5322875223786592\n",
      "GD iter. 397/499: loss=0.5322978788091962\n",
      "GD iter. 398/499: loss=0.5322443410141603\n",
      "GD iter. 399/499: loss=0.5322298236969808\n",
      "GD iter. 400/499: loss=0.5322059020278924\n",
      "GD iter. 401/499: loss=0.5321469472717535\n",
      "GD iter. 402/499: loss=0.5321299814483887\n",
      "GD iter. 403/499: loss=0.5321477084189832\n",
      "GD iter. 404/499: loss=0.532061034920922\n",
      "GD iter. 405/499: loss=0.5320462110593653\n",
      "GD iter. 406/499: loss=0.5320416619442149\n",
      "GD iter. 407/499: loss=0.5320095337223438\n",
      "GD iter. 408/499: loss=0.5319690865460609\n",
      "GD iter. 409/499: loss=0.5319473447847268\n",
      "GD iter. 410/499: loss=0.5319100934432206\n",
      "GD iter. 411/499: loss=0.5319092223267665\n",
      "GD iter. 412/499: loss=0.5319006287333461\n",
      "GD iter. 413/499: loss=0.5318529718143118\n",
      "GD iter. 414/499: loss=0.531799300927367\n",
      "GD iter. 415/499: loss=0.5317763791214326\n",
      "GD iter. 416/499: loss=0.5317445594962714\n",
      "GD iter. 417/499: loss=0.5317291613520013\n",
      "GD iter. 418/499: loss=0.5317126998459714\n",
      "GD iter. 419/499: loss=0.5317037054348012\n",
      "GD iter. 420/499: loss=0.5316575407990954\n",
      "GD iter. 421/499: loss=0.5316376261486615\n",
      "GD iter. 422/499: loss=0.5316171639109174\n",
      "GD iter. 423/499: loss=0.5315912003798045\n",
      "GD iter. 424/499: loss=0.5315459653953909\n",
      "GD iter. 425/499: loss=0.5315298991096209\n",
      "GD iter. 426/499: loss=0.5314893319823917\n",
      "GD iter. 427/499: loss=0.5314928835876972\n",
      "GD iter. 428/499: loss=0.5314847895043497\n",
      "GD iter. 429/499: loss=0.5314222977127301\n",
      "GD iter. 430/499: loss=0.5313858236237625\n",
      "GD iter. 431/499: loss=0.5313995144621043\n",
      "GD iter. 432/499: loss=0.531369497299295\n",
      "GD iter. 433/499: loss=0.5313346178755378\n",
      "GD iter. 434/499: loss=0.5313383929559767\n",
      "GD iter. 435/499: loss=0.5313030066522331\n",
      "GD iter. 436/499: loss=0.5312818925446227\n",
      "GD iter. 437/499: loss=0.5312643895506513\n",
      "GD iter. 438/499: loss=0.5312167959958651\n",
      "GD iter. 439/499: loss=0.5311916969486971\n",
      "GD iter. 440/499: loss=0.5311794287306639\n",
      "GD iter. 441/499: loss=0.5311752936211963\n",
      "GD iter. 442/499: loss=0.5311286729797399\n",
      "GD iter. 443/499: loss=0.5310795243995111\n",
      "GD iter. 444/499: loss=0.5310887801623491\n",
      "GD iter. 445/499: loss=0.5310820811470867\n",
      "GD iter. 446/499: loss=0.5310702049851791\n",
      "GD iter. 447/499: loss=0.5310217637724618\n",
      "GD iter. 448/499: loss=0.530987844520191\n",
      "GD iter. 449/499: loss=0.5309740042712264\n",
      "GD iter. 450/499: loss=0.530985459640411\n",
      "GD iter. 451/499: loss=0.53096480035236\n",
      "GD iter. 452/499: loss=0.5309304975985901\n",
      "GD iter. 453/499: loss=0.5309065972274167\n",
      "GD iter. 454/499: loss=0.5308646914730727\n",
      "GD iter. 455/499: loss=0.5308595069316183\n",
      "GD iter. 456/499: loss=0.5308477999513959\n",
      "GD iter. 457/499: loss=0.5308454916354458\n",
      "GD iter. 458/499: loss=0.5307737659305327\n",
      "GD iter. 459/499: loss=0.5307747859061692\n",
      "GD iter. 460/499: loss=0.5307593554880784\n",
      "GD iter. 461/499: loss=0.5307675767152954\n",
      "GD iter. 462/499: loss=0.5306958405147314\n",
      "GD iter. 463/499: loss=0.530680905717808\n",
      "GD iter. 464/499: loss=0.5306476740260553\n",
      "GD iter. 465/499: loss=0.5306358914440616\n",
      "GD iter. 466/499: loss=0.5306122522803389\n",
      "GD iter. 467/499: loss=0.5306198001145821\n",
      "GD iter. 468/499: loss=0.5305951594011392\n",
      "GD iter. 469/499: loss=0.5306090832392032\n",
      "GD iter. 470/499: loss=0.5305615786738899\n",
      "GD iter. 471/499: loss=0.5305513475065764\n",
      "GD iter. 472/499: loss=0.5305132536755911\n",
      "GD iter. 473/499: loss=0.5304853363540616\n",
      "GD iter. 474/499: loss=0.5304662798423037\n",
      "GD iter. 475/499: loss=0.5304980338202938\n",
      "GD iter. 476/499: loss=0.5304496628250911\n",
      "GD iter. 477/499: loss=0.5304283227339133\n",
      "GD iter. 478/499: loss=0.5303839675570411\n",
      "GD iter. 479/499: loss=0.5303729291497292\n",
      "GD iter. 480/499: loss=0.5303623578867162\n",
      "GD iter. 481/499: loss=0.5303755864552622\n",
      "GD iter. 482/499: loss=0.5303210422472702\n",
      "GD iter. 483/499: loss=0.5303273803172714\n",
      "GD iter. 484/499: loss=0.5302768393950503\n",
      "GD iter. 485/499: loss=0.5302615220980322\n",
      "GD iter. 486/499: loss=0.5302522902633972\n",
      "GD iter. 487/499: loss=0.5302611436624423\n",
      "GD iter. 488/499: loss=0.5302108879909404\n",
      "GD iter. 489/499: loss=0.5301623827768469\n",
      "GD iter. 490/499: loss=0.5301594708665701\n",
      "GD iter. 491/499: loss=0.5301453333026087\n",
      "GD iter. 492/499: loss=0.5301186423230231\n",
      "GD iter. 493/499: loss=0.5301161841346033\n",
      "GD iter. 494/499: loss=0.5301163819105218\n",
      "GD iter. 495/499: loss=0.530079210232834\n",
      "GD iter. 496/499: loss=0.5300464581497698\n",
      "GD iter. 497/499: loss=0.530046976813473\n",
      "GD iter. 498/499: loss=0.5300525499745748\n",
      "GD iter. 499/499: loss=0.530014710809891\n",
      "The Accuracy is: 0.6158\n",
      "The F1 score is: 0.2595\n",
      "The precision is: 0.1530\n",
      "The recall is: 0.8542\n",
      "GD iter. 0/499: loss=0.9797130794191966\n",
      "GD iter. 1/499: loss=0.8971303895035022\n",
      "GD iter. 2/499: loss=0.819808794848825\n",
      "GD iter. 3/499: loss=0.7566431929997764\n",
      "GD iter. 4/499: loss=0.7178597248825863\n",
      "GD iter. 5/499: loss=0.6938975969569928\n",
      "GD iter. 6/499: loss=0.6765943711479814\n",
      "GD iter. 7/499: loss=0.6636973861317603\n",
      "GD iter. 8/499: loss=0.6536812780074454\n",
      "GD iter. 9/499: loss=0.6452800484666193\n",
      "GD iter. 10/499: loss=0.63836727255156\n",
      "GD iter. 11/499: loss=0.6328963522476109\n",
      "GD iter. 12/499: loss=0.6282597595993477\n",
      "GD iter. 13/499: loss=0.6243546892285206\n",
      "GD iter. 14/499: loss=0.6207176548171714\n",
      "GD iter. 15/499: loss=0.6175120134145143\n",
      "GD iter. 16/499: loss=0.614734335065541\n",
      "GD iter. 17/499: loss=0.612257798610576\n",
      "GD iter. 18/499: loss=0.6100507359937479\n",
      "GD iter. 19/499: loss=0.6080598969985995\n",
      "GD iter. 20/499: loss=0.606139077991356\n",
      "GD iter. 21/499: loss=0.6044148063245582\n",
      "GD iter. 22/499: loss=0.6028683078565694\n",
      "GD iter. 23/499: loss=0.6014270734720132\n",
      "GD iter. 24/499: loss=0.6000740207463587\n",
      "GD iter. 25/499: loss=0.5988051535148486\n",
      "GD iter. 26/499: loss=0.5975407010664436\n",
      "GD iter. 27/499: loss=0.5963226520766027\n",
      "GD iter. 28/499: loss=0.5952028445215025\n",
      "GD iter. 29/499: loss=0.594123161762497\n",
      "GD iter. 30/499: loss=0.5931193963178981\n",
      "GD iter. 31/499: loss=0.5921396798395877\n",
      "GD iter. 32/499: loss=0.5911838325651185\n",
      "GD iter. 33/499: loss=0.5902662342483774\n",
      "GD iter. 34/499: loss=0.5893673010664223\n",
      "GD iter. 35/499: loss=0.5884765785933788\n",
      "GD iter. 36/499: loss=0.5876083348418808\n",
      "GD iter. 37/499: loss=0.5867589394529558\n",
      "GD iter. 38/499: loss=0.5859646684891695\n",
      "GD iter. 39/499: loss=0.5851753142575805\n",
      "GD iter. 40/499: loss=0.5843985273587462\n",
      "GD iter. 41/499: loss=0.5836337688138732\n",
      "GD iter. 42/499: loss=0.5828843489280655\n",
      "GD iter. 43/499: loss=0.5821478795845757\n",
      "GD iter. 44/499: loss=0.5814364947945692\n",
      "GD iter. 45/499: loss=0.5807354490902318\n",
      "GD iter. 46/499: loss=0.580043063749663\n",
      "GD iter. 47/499: loss=0.5793785162281921\n",
      "GD iter. 48/499: loss=0.5787264974768588\n",
      "GD iter. 49/499: loss=0.5780895846030877\n",
      "GD iter. 50/499: loss=0.5774649618649138\n",
      "GD iter. 51/499: loss=0.576846742747089\n",
      "GD iter. 52/499: loss=0.576262545599407\n",
      "GD iter. 53/499: loss=0.5757315993022721\n",
      "GD iter. 54/499: loss=0.5752295814801052\n",
      "GD iter. 55/499: loss=0.5747320256698671\n",
      "GD iter. 56/499: loss=0.5742428719420783\n",
      "GD iter. 57/499: loss=0.57376693167244\n",
      "GD iter. 58/499: loss=0.5732888588939107\n",
      "GD iter. 59/499: loss=0.5728201345408581\n",
      "GD iter. 60/499: loss=0.572362621494698\n",
      "GD iter. 61/499: loss=0.5719014833594875\n",
      "GD iter. 62/499: loss=0.5714471144303872\n",
      "GD iter. 63/499: loss=0.57100742583093\n",
      "GD iter. 64/499: loss=0.5705883499944921\n",
      "GD iter. 65/499: loss=0.5701749873424218\n",
      "GD iter. 66/499: loss=0.5697738316891242\n",
      "GD iter. 67/499: loss=0.5693918213592073\n",
      "GD iter. 68/499: loss=0.5690133049667844\n",
      "GD iter. 69/499: loss=0.5686338478463273\n",
      "GD iter. 70/499: loss=0.5682673758472526\n",
      "GD iter. 71/499: loss=0.5678991231032049\n",
      "GD iter. 72/499: loss=0.567539563767531\n",
      "GD iter. 73/499: loss=0.5671728506824012\n",
      "GD iter. 74/499: loss=0.5668172135598128\n",
      "GD iter. 75/499: loss=0.5664520534213925\n",
      "GD iter. 76/499: loss=0.566094270911655\n",
      "GD iter. 77/499: loss=0.5657401208704137\n",
      "GD iter. 78/499: loss=0.5653949707137236\n",
      "GD iter. 79/499: loss=0.5650531531415803\n",
      "GD iter. 80/499: loss=0.5647114336366372\n",
      "GD iter. 81/499: loss=0.5643866141199592\n",
      "GD iter. 82/499: loss=0.5640438901556688\n",
      "GD iter. 83/499: loss=0.5637228251913604\n",
      "GD iter. 84/499: loss=0.5634058990681134\n",
      "GD iter. 85/499: loss=0.5630899662424129\n",
      "GD iter. 86/499: loss=0.5627797798077571\n",
      "GD iter. 87/499: loss=0.5624952568601002\n",
      "GD iter. 88/499: loss=0.5621886389656698\n",
      "GD iter. 89/499: loss=0.5618924110010455\n",
      "GD iter. 90/499: loss=0.5616111579913655\n",
      "GD iter. 91/499: loss=0.5613298687466669\n",
      "GD iter. 92/499: loss=0.5610484054915562\n",
      "GD iter. 93/499: loss=0.5607767740614504\n",
      "GD iter. 94/499: loss=0.5605086016279494\n",
      "GD iter. 95/499: loss=0.5602417364279119\n",
      "GD iter. 96/499: loss=0.5599919471334759\n",
      "GD iter. 97/499: loss=0.559708458962203\n",
      "GD iter. 98/499: loss=0.5594534463230426\n",
      "GD iter. 99/499: loss=0.5591874613968829\n",
      "GD iter. 100/499: loss=0.5589301840435269\n",
      "GD iter. 101/499: loss=0.558665772966161\n",
      "GD iter. 102/499: loss=0.5584087237158853\n",
      "GD iter. 103/499: loss=0.5581602972980018\n",
      "GD iter. 104/499: loss=0.5579151973382265\n",
      "GD iter. 105/499: loss=0.5576754389825883\n",
      "GD iter. 106/499: loss=0.5574590725892993\n",
      "GD iter. 107/499: loss=0.5572237825676617\n",
      "GD iter. 108/499: loss=0.5570031611188154\n",
      "GD iter. 109/499: loss=0.5567718020106531\n",
      "GD iter. 110/499: loss=0.5565521942454644\n",
      "GD iter. 111/499: loss=0.556322207082084\n",
      "GD iter. 112/499: loss=0.5561105803285521\n",
      "GD iter. 113/499: loss=0.5558962791580951\n",
      "GD iter. 114/499: loss=0.555675081695814\n",
      "GD iter. 115/499: loss=0.5554687815523808\n",
      "GD iter. 116/499: loss=0.5552471954805673\n",
      "GD iter. 117/499: loss=0.5550500797103257\n",
      "GD iter. 118/499: loss=0.5548292666545515\n",
      "GD iter. 119/499: loss=0.5546239232183611\n",
      "GD iter. 120/499: loss=0.5544163780779432\n",
      "GD iter. 121/499: loss=0.5542298020377036\n",
      "GD iter. 122/499: loss=0.5540199407479792\n",
      "GD iter. 123/499: loss=0.5538283922209609\n",
      "GD iter. 124/499: loss=0.5535923907374174\n",
      "GD iter. 125/499: loss=0.5533991916107526\n",
      "GD iter. 126/499: loss=0.5532262337637908\n",
      "GD iter. 127/499: loss=0.5529990922082231\n",
      "GD iter. 128/499: loss=0.552802094565868\n",
      "GD iter. 129/499: loss=0.5525933813843102\n",
      "GD iter. 130/499: loss=0.5524319652612725\n",
      "GD iter. 131/499: loss=0.5522315984758245\n",
      "GD iter. 132/499: loss=0.5520398512292085\n",
      "GD iter. 133/499: loss=0.5518626504178125\n",
      "GD iter. 134/499: loss=0.5516857545664977\n",
      "GD iter. 135/499: loss=0.5515056106242131\n",
      "GD iter. 136/499: loss=0.5513597416482968\n",
      "GD iter. 137/499: loss=0.5511769050634574\n",
      "GD iter. 138/499: loss=0.5510067773859002\n",
      "GD iter. 139/499: loss=0.5508303078660272\n",
      "GD iter. 140/499: loss=0.5506725396868442\n",
      "GD iter. 141/499: loss=0.5505049698194648\n",
      "GD iter. 142/499: loss=0.550333880108199\n",
      "GD iter. 143/499: loss=0.5501684273896064\n",
      "GD iter. 144/499: loss=0.5500063334350292\n",
      "GD iter. 145/499: loss=0.5498619774402388\n",
      "GD iter. 146/499: loss=0.549717191619256\n",
      "GD iter. 147/499: loss=0.5495694063447599\n",
      "GD iter. 148/499: loss=0.549427642192964\n",
      "GD iter. 149/499: loss=0.5492887150216662\n",
      "GD iter. 150/499: loss=0.5491611237195856\n",
      "GD iter. 151/499: loss=0.5490182470820995\n",
      "GD iter. 152/499: loss=0.5489061552551195\n",
      "GD iter. 153/499: loss=0.5487578221926819\n",
      "GD iter. 154/499: loss=0.5486463216574595\n",
      "GD iter. 155/499: loss=0.5485282284203271\n",
      "GD iter. 156/499: loss=0.5483746389168639\n",
      "GD iter. 157/499: loss=0.5482455339168383\n",
      "GD iter. 158/499: loss=0.5481292650421756\n",
      "GD iter. 159/499: loss=0.5479881254349704\n",
      "GD iter. 160/499: loss=0.5478725376944996\n",
      "GD iter. 161/499: loss=0.5477317254593909\n",
      "GD iter. 162/499: loss=0.5476285190804865\n",
      "GD iter. 163/499: loss=0.5475027921947291\n",
      "GD iter. 164/499: loss=0.5473758231532351\n",
      "GD iter. 165/499: loss=0.5472656744170884\n",
      "GD iter. 166/499: loss=0.5471421754801482\n",
      "GD iter. 167/499: loss=0.5470297791127384\n",
      "GD iter. 168/499: loss=0.5469118500594026\n",
      "GD iter. 169/499: loss=0.5468110513971858\n",
      "GD iter. 170/499: loss=0.5467050456339383\n",
      "GD iter. 171/499: loss=0.5465815165260675\n",
      "GD iter. 172/499: loss=0.5464780611914967\n",
      "GD iter. 173/499: loss=0.5463593256887674\n",
      "GD iter. 174/499: loss=0.5462478566483895\n",
      "GD iter. 175/499: loss=0.5461346812034437\n",
      "GD iter. 176/499: loss=0.546033158079531\n",
      "GD iter. 177/499: loss=0.5459370448748335\n",
      "GD iter. 178/499: loss=0.5458074543196986\n",
      "GD iter. 179/499: loss=0.5456904239486384\n",
      "GD iter. 180/499: loss=0.5455931617406045\n",
      "GD iter. 181/499: loss=0.5454993132003805\n",
      "GD iter. 182/499: loss=0.5453911795602597\n",
      "GD iter. 183/499: loss=0.545275447718076\n",
      "GD iter. 184/499: loss=0.5451647076650784\n",
      "GD iter. 185/499: loss=0.5450507787820098\n",
      "GD iter. 186/499: loss=0.5449740462032322\n",
      "GD iter. 187/499: loss=0.544843427850853\n",
      "GD iter. 188/499: loss=0.5447502905756617\n",
      "GD iter. 189/499: loss=0.5446348580570122\n",
      "GD iter. 190/499: loss=0.5445315570089836\n",
      "GD iter. 191/499: loss=0.5444142621772453\n",
      "GD iter. 192/499: loss=0.5443158224275548\n",
      "GD iter. 193/499: loss=0.5442257991078581\n",
      "GD iter. 194/499: loss=0.5441093519890996\n",
      "GD iter. 195/499: loss=0.5440025682387983\n",
      "GD iter. 196/499: loss=0.5439011253729003\n",
      "GD iter. 197/499: loss=0.5437973081474108\n",
      "GD iter. 198/499: loss=0.5437124313018284\n",
      "GD iter. 199/499: loss=0.5435836349508003\n",
      "GD iter. 200/499: loss=0.5435081995006913\n",
      "GD iter. 201/499: loss=0.5433850939501201\n",
      "GD iter. 202/499: loss=0.5432934488394635\n",
      "GD iter. 203/499: loss=0.5432049467434902\n",
      "GD iter. 204/499: loss=0.5430918223879215\n",
      "GD iter. 205/499: loss=0.543010264051432\n",
      "GD iter. 206/499: loss=0.5429033989732993\n",
      "GD iter. 207/499: loss=0.5428108385031694\n",
      "GD iter. 208/499: loss=0.5426849318004995\n",
      "GD iter. 209/499: loss=0.5425874309344968\n",
      "GD iter. 210/499: loss=0.5424877829728982\n",
      "GD iter. 211/499: loss=0.5423790377438691\n",
      "GD iter. 212/499: loss=0.5423090683883357\n",
      "GD iter. 213/499: loss=0.5422204728161878\n",
      "GD iter. 214/499: loss=0.5421289902940228\n",
      "GD iter. 215/499: loss=0.5420221220526109\n",
      "GD iter. 216/499: loss=0.5419128171327143\n",
      "GD iter. 217/499: loss=0.5418308883614887\n",
      "GD iter. 218/499: loss=0.5417380280298603\n",
      "GD iter. 219/499: loss=0.5416336002041223\n",
      "GD iter. 220/499: loss=0.5415710438543456\n",
      "GD iter. 221/499: loss=0.5414725477938764\n",
      "GD iter. 222/499: loss=0.5413635594248367\n",
      "GD iter. 223/499: loss=0.5412825115074553\n",
      "GD iter. 224/499: loss=0.5412011926748308\n",
      "GD iter. 225/499: loss=0.5411169394616228\n",
      "GD iter. 226/499: loss=0.5410353743182149\n",
      "GD iter. 227/499: loss=0.5409586652185133\n",
      "GD iter. 228/499: loss=0.5408533480070109\n",
      "GD iter. 229/499: loss=0.5407537293937648\n",
      "GD iter. 230/499: loss=0.5406859306173591\n",
      "GD iter. 231/499: loss=0.5406077726578741\n",
      "GD iter. 232/499: loss=0.5405179012254203\n",
      "GD iter. 233/499: loss=0.5404569798067858\n",
      "GD iter. 234/499: loss=0.5403454934963496\n",
      "GD iter. 235/499: loss=0.5402511723723327\n",
      "GD iter. 236/499: loss=0.5401835844164791\n",
      "GD iter. 237/499: loss=0.5401228809697671\n",
      "GD iter. 238/499: loss=0.5400406227022779\n",
      "GD iter. 239/499: loss=0.5399497664046277\n",
      "GD iter. 240/499: loss=0.539864841304214\n",
      "GD iter. 241/499: loss=0.5397769677558303\n",
      "GD iter. 242/499: loss=0.5397106440398663\n",
      "GD iter. 243/499: loss=0.5396502840598375\n",
      "GD iter. 244/499: loss=0.5395866829209587\n",
      "GD iter. 245/499: loss=0.5395270857344863\n",
      "GD iter. 246/499: loss=0.539452570054405\n",
      "GD iter. 247/499: loss=0.5393459886238555\n",
      "GD iter. 248/499: loss=0.5393008541638435\n",
      "GD iter. 249/499: loss=0.5392155730693095\n",
      "GD iter. 250/499: loss=0.5391278603862809\n",
      "GD iter. 251/499: loss=0.5390892220823206\n",
      "GD iter. 252/499: loss=0.5390290272912651\n",
      "GD iter. 253/499: loss=0.5389115191545223\n",
      "GD iter. 254/499: loss=0.5388690888853801\n",
      "GD iter. 255/499: loss=0.5388290795105567\n",
      "GD iter. 256/499: loss=0.5387134730404015\n",
      "GD iter. 257/499: loss=0.5386281734685108\n",
      "GD iter. 258/499: loss=0.5386049571839894\n",
      "GD iter. 259/499: loss=0.5385573198159745\n",
      "GD iter. 260/499: loss=0.5384403040040566\n",
      "GD iter. 261/499: loss=0.5383580250940411\n",
      "GD iter. 262/499: loss=0.5382918468879717\n",
      "GD iter. 263/499: loss=0.5382424008195039\n",
      "GD iter. 264/499: loss=0.5381821938766461\n",
      "GD iter. 265/499: loss=0.5381202147945159\n",
      "GD iter. 266/499: loss=0.5380581387276214\n",
      "GD iter. 267/499: loss=0.5379906477024538\n",
      "GD iter. 268/499: loss=0.537927892070417\n",
      "GD iter. 269/499: loss=0.5378659421140279\n",
      "GD iter. 270/499: loss=0.5377790318749187\n",
      "GD iter. 271/499: loss=0.5377260918020768\n",
      "GD iter. 272/499: loss=0.5376968053586084\n",
      "GD iter. 273/499: loss=0.5376725211320347\n",
      "GD iter. 274/499: loss=0.5375533699407332\n",
      "GD iter. 275/499: loss=0.5374822733705718\n",
      "GD iter. 276/499: loss=0.5374295451343578\n",
      "GD iter. 277/499: loss=0.5373966836505395\n",
      "GD iter. 278/499: loss=0.5373383954043418\n",
      "GD iter. 279/499: loss=0.5373273988426673\n",
      "GD iter. 280/499: loss=0.5372094664685071\n",
      "GD iter. 281/499: loss=0.5371296656521805\n",
      "GD iter. 282/499: loss=0.5370827923904128\n",
      "GD iter. 283/499: loss=0.5370320344146212\n",
      "GD iter. 284/499: loss=0.5369667330873643\n",
      "GD iter. 285/499: loss=0.5369192336929983\n",
      "GD iter. 286/499: loss=0.5369154710637715\n",
      "GD iter. 287/499: loss=0.5368170340680276\n",
      "GD iter. 288/499: loss=0.5367846751058706\n",
      "GD iter. 289/499: loss=0.536710479212107\n",
      "GD iter. 290/499: loss=0.5366750076668757\n",
      "GD iter. 291/499: loss=0.536583363663847\n",
      "GD iter. 292/499: loss=0.536530885275727\n",
      "GD iter. 293/499: loss=0.5364921170074798\n",
      "GD iter. 294/499: loss=0.5364273981613001\n",
      "GD iter. 295/499: loss=0.5363906880620896\n",
      "GD iter. 296/499: loss=0.5363398431659673\n",
      "GD iter. 297/499: loss=0.5362878965036814\n",
      "GD iter. 298/499: loss=0.5362091298336854\n",
      "GD iter. 299/499: loss=0.5361793276337616\n",
      "GD iter. 300/499: loss=0.5361292733787046\n",
      "GD iter. 301/499: loss=0.5360582876425963\n",
      "GD iter. 302/499: loss=0.5360338498577277\n",
      "GD iter. 303/499: loss=0.5360217705724954\n",
      "GD iter. 304/499: loss=0.5359375761170324\n",
      "GD iter. 305/499: loss=0.5358846193153626\n",
      "GD iter. 306/499: loss=0.5358598606452372\n",
      "GD iter. 307/499: loss=0.5358159503892674\n",
      "GD iter. 308/499: loss=0.5357829100370097\n",
      "GD iter. 309/499: loss=0.5357273584244415\n",
      "GD iter. 310/499: loss=0.5357107795103598\n",
      "GD iter. 311/499: loss=0.5356799008162744\n",
      "GD iter. 312/499: loss=0.5356272082834439\n",
      "GD iter. 313/499: loss=0.5355531511828776\n",
      "GD iter. 314/499: loss=0.5355131532508709\n",
      "GD iter. 315/499: loss=0.5355231842200926\n",
      "GD iter. 316/499: loss=0.535505101582207\n",
      "GD iter. 317/499: loss=0.535415145439583\n",
      "GD iter. 318/499: loss=0.5353895450965289\n",
      "GD iter. 319/499: loss=0.535351591804771\n",
      "GD iter. 320/499: loss=0.535327781251971\n",
      "GD iter. 321/499: loss=0.5352805401652327\n",
      "GD iter. 322/499: loss=0.5352148265633744\n",
      "GD iter. 323/499: loss=0.5352418712130741\n",
      "GD iter. 324/499: loss=0.5352072462895168\n",
      "GD iter. 325/499: loss=0.5351498063709563\n",
      "GD iter. 326/499: loss=0.5350941767484442\n",
      "GD iter. 327/499: loss=0.5350891922765888\n",
      "GD iter. 328/499: loss=0.5350427083870534\n",
      "GD iter. 329/499: loss=0.5349886183169312\n",
      "GD iter. 330/499: loss=0.5349531699588794\n",
      "GD iter. 331/499: loss=0.5349535008477115\n",
      "GD iter. 332/499: loss=0.5349181590373744\n",
      "GD iter. 333/499: loss=0.5349178895444907\n",
      "GD iter. 334/499: loss=0.5348428073508146\n",
      "GD iter. 335/499: loss=0.5348169497697329\n",
      "GD iter. 336/499: loss=0.5348053961265513\n",
      "GD iter. 337/499: loss=0.5347561183594308\n",
      "GD iter. 338/499: loss=0.5347107102757812\n",
      "GD iter. 339/499: loss=0.5347286456365064\n",
      "GD iter. 340/499: loss=0.5347019052250049\n",
      "GD iter. 341/499: loss=0.5347115447130388\n",
      "GD iter. 342/499: loss=0.5345990175447278\n",
      "GD iter. 343/499: loss=0.5345714579812624\n",
      "GD iter. 344/499: loss=0.5345324165886853\n",
      "GD iter. 345/499: loss=0.5345161796377983\n",
      "GD iter. 346/499: loss=0.5344847690751757\n",
      "GD iter. 347/499: loss=0.5344522554828592\n",
      "GD iter. 348/499: loss=0.5344610975285242\n",
      "GD iter. 349/499: loss=0.5344671954523068\n",
      "GD iter. 350/499: loss=0.5343730755346013\n",
      "GD iter. 351/499: loss=0.5343634474232027\n",
      "GD iter. 352/499: loss=0.5343280249944113\n",
      "GD iter. 353/499: loss=0.5343300166796086\n",
      "GD iter. 354/499: loss=0.534316190071426\n",
      "GD iter. 355/499: loss=0.5342652742572986\n",
      "GD iter. 356/499: loss=0.5342506552818654\n",
      "GD iter. 357/499: loss=0.5342377246483202\n",
      "GD iter. 358/499: loss=0.5341569446588684\n",
      "GD iter. 359/499: loss=0.5341666418676747\n",
      "GD iter. 360/499: loss=0.5341381233231757\n",
      "GD iter. 361/499: loss=0.5341313497060884\n",
      "GD iter. 362/499: loss=0.534073942929094\n",
      "GD iter. 363/499: loss=0.5340588540089489\n",
      "GD iter. 364/499: loss=0.5340064525439948\n",
      "GD iter. 365/499: loss=0.5340040318040258\n",
      "GD iter. 366/499: loss=0.5339671913021513\n",
      "GD iter. 367/499: loss=0.5339734229255473\n",
      "GD iter. 368/499: loss=0.533982396906341\n",
      "GD iter. 369/499: loss=0.5338774086620715\n",
      "GD iter. 370/499: loss=0.5338833981321989\n",
      "GD iter. 371/499: loss=0.5338506850041428\n",
      "GD iter. 372/499: loss=0.5337994734878103\n",
      "GD iter. 373/499: loss=0.5337661601959561\n",
      "GD iter. 374/499: loss=0.5337799886345386\n",
      "GD iter. 375/499: loss=0.5338079853789901\n",
      "GD iter. 376/499: loss=0.5337222376995876\n",
      "GD iter. 377/499: loss=0.5336796305352486\n",
      "GD iter. 378/499: loss=0.5336665612243743\n",
      "GD iter. 379/499: loss=0.5336981741724611\n",
      "GD iter. 380/499: loss=0.533641323307784\n",
      "GD iter. 381/499: loss=0.5336060821203278\n",
      "GD iter. 382/499: loss=0.5335877714778846\n",
      "GD iter. 383/499: loss=0.5335646077419796\n",
      "GD iter. 384/499: loss=0.5335423068212998\n",
      "GD iter. 385/499: loss=0.5335441207674315\n",
      "GD iter. 386/499: loss=0.5334836428080266\n",
      "GD iter. 387/499: loss=0.5334743169058807\n",
      "GD iter. 388/499: loss=0.5334290794189653\n",
      "GD iter. 389/499: loss=0.5334136153144384\n",
      "GD iter. 390/499: loss=0.5333869686012847\n",
      "GD iter. 391/499: loss=0.5333746701956434\n",
      "GD iter. 392/499: loss=0.5333303349760672\n",
      "GD iter. 393/499: loss=0.5333140710074622\n",
      "GD iter. 394/499: loss=0.5332797643155507\n",
      "GD iter. 395/499: loss=0.5332566401314525\n",
      "GD iter. 396/499: loss=0.5332331119469257\n",
      "GD iter. 397/499: loss=0.5332055378718834\n",
      "GD iter. 398/499: loss=0.5332350590246067\n",
      "GD iter. 399/499: loss=0.5331813770352286\n",
      "GD iter. 400/499: loss=0.5331632808675034\n",
      "GD iter. 401/499: loss=0.5331327577337144\n",
      "GD iter. 402/499: loss=0.5331580330441478\n",
      "GD iter. 403/499: loss=0.5330784896143268\n",
      "GD iter. 404/499: loss=0.533076140997485\n",
      "GD iter. 405/499: loss=0.5330306313460865\n",
      "GD iter. 406/499: loss=0.5330457524198735\n",
      "GD iter. 407/499: loss=0.5330412921127297\n",
      "GD iter. 408/499: loss=0.5330011124891385\n",
      "GD iter. 409/499: loss=0.5329868489577901\n",
      "GD iter. 410/499: loss=0.5329537855419221\n",
      "GD iter. 411/499: loss=0.5329259427839002\n",
      "GD iter. 412/499: loss=0.5329553909519875\n",
      "GD iter. 413/499: loss=0.5329110191396689\n",
      "GD iter. 414/499: loss=0.5328908403288745\n",
      "GD iter. 415/499: loss=0.5328390652635341\n",
      "GD iter. 416/499: loss=0.5328076704507501\n",
      "GD iter. 417/499: loss=0.5327942943252091\n",
      "GD iter. 418/499: loss=0.5328231638688187\n",
      "GD iter. 419/499: loss=0.5327627205513245\n",
      "GD iter. 420/499: loss=0.5327332241488303\n",
      "GD iter. 421/499: loss=0.5327787800950711\n",
      "GD iter. 422/499: loss=0.5327660994380071\n",
      "GD iter. 423/499: loss=0.5326711096994331\n",
      "GD iter. 424/499: loss=0.5326581165680726\n",
      "GD iter. 425/499: loss=0.5326754180903185\n",
      "GD iter. 426/499: loss=0.5326366380881489\n",
      "GD iter. 427/499: loss=0.5325964542002977\n",
      "GD iter. 428/499: loss=0.5325772766037007\n",
      "GD iter. 429/499: loss=0.5325696018875428\n",
      "GD iter. 430/499: loss=0.5325661015096642\n",
      "GD iter. 431/499: loss=0.532559744709011\n",
      "GD iter. 432/499: loss=0.5325873897716158\n",
      "GD iter. 433/499: loss=0.53249354628003\n",
      "GD iter. 434/499: loss=0.5324463057329726\n",
      "GD iter. 435/499: loss=0.5324687216526048\n",
      "GD iter. 436/499: loss=0.532444166863298\n",
      "GD iter. 437/499: loss=0.5323916137758575\n",
      "GD iter. 438/499: loss=0.5323729439653515\n",
      "GD iter. 439/499: loss=0.532423512121618\n",
      "GD iter. 440/499: loss=0.5323981158433881\n",
      "GD iter. 441/499: loss=0.5323363599858894\n",
      "GD iter. 442/499: loss=0.5323680754325474\n",
      "GD iter. 443/499: loss=0.5323074822844709\n",
      "GD iter. 444/499: loss=0.5322701865572392\n",
      "GD iter. 445/499: loss=0.5322515548317094\n",
      "GD iter. 446/499: loss=0.532271334903042\n",
      "GD iter. 447/499: loss=0.5322218802460236\n",
      "GD iter. 448/499: loss=0.5322162252938715\n",
      "GD iter. 449/499: loss=0.5321818590896619\n",
      "GD iter. 450/499: loss=0.5321834573332749\n",
      "GD iter. 451/499: loss=0.5321601966121686\n",
      "GD iter. 452/499: loss=0.5321532979999578\n",
      "GD iter. 453/499: loss=0.5321158365196972\n",
      "GD iter. 454/499: loss=0.5320887081729864\n",
      "GD iter. 455/499: loss=0.5320722457906031\n",
      "GD iter. 456/499: loss=0.5320700916017574\n",
      "GD iter. 457/499: loss=0.5320329806867065\n",
      "GD iter. 458/499: loss=0.5320718444970959\n",
      "GD iter. 459/499: loss=0.5319936115531113\n",
      "GD iter. 460/499: loss=0.5319726925791624\n",
      "GD iter. 461/499: loss=0.5319848134937842\n",
      "GD iter. 462/499: loss=0.5319523628312088\n",
      "GD iter. 463/499: loss=0.5319350776530662\n",
      "GD iter. 464/499: loss=0.5319864934230549\n",
      "GD iter. 465/499: loss=0.5318927248290436\n",
      "GD iter. 466/499: loss=0.5318789959391259\n",
      "GD iter. 467/499: loss=0.5318889384587396\n",
      "GD iter. 468/499: loss=0.531907798729137\n",
      "GD iter. 469/499: loss=0.5318248948689261\n",
      "GD iter. 470/499: loss=0.5318349686425029\n",
      "GD iter. 471/499: loss=0.5317988037823114\n",
      "GD iter. 472/499: loss=0.5317792728763943\n",
      "GD iter. 473/499: loss=0.5317745730359185\n",
      "GD iter. 474/499: loss=0.5317528363326828\n",
      "GD iter. 475/499: loss=0.5317479421353833\n",
      "GD iter. 476/499: loss=0.5318099298721874\n",
      "GD iter. 477/499: loss=0.5316911679702959\n",
      "GD iter. 478/499: loss=0.5316809823063171\n",
      "GD iter. 479/499: loss=0.5316703502142232\n",
      "GD iter. 480/499: loss=0.5316785238071324\n",
      "GD iter. 481/499: loss=0.5316360098474816\n",
      "GD iter. 482/499: loss=0.531631542564544\n",
      "GD iter. 483/499: loss=0.5316059998038678\n",
      "GD iter. 484/499: loss=0.5316578913544329\n",
      "GD iter. 485/499: loss=0.5315819302042584\n",
      "GD iter. 486/499: loss=0.5315606387948987\n",
      "GD iter. 487/499: loss=0.5315431349999494\n",
      "GD iter. 488/499: loss=0.5315126184377986\n",
      "GD iter. 489/499: loss=0.5315126701402982\n",
      "GD iter. 490/499: loss=0.5315035375689479\n",
      "GD iter. 491/499: loss=0.5314862524290409\n",
      "GD iter. 492/499: loss=0.5314397516321243\n",
      "GD iter. 493/499: loss=0.5314327383767543\n",
      "GD iter. 494/499: loss=0.5314319924834241\n",
      "GD iter. 495/499: loss=0.5314934460776446\n",
      "GD iter. 496/499: loss=0.5313904449675864\n",
      "GD iter. 497/499: loss=0.5313926398285985\n",
      "GD iter. 498/499: loss=0.5313844116653844\n",
      "GD iter. 499/499: loss=0.5313807639792791\n",
      "The Accuracy is: 0.6404\n",
      "The F1 score is: 0.3135\n",
      "The precision is: 0.1880\n",
      "The recall is: 0.9434\n",
      "GD iter. 0/499: loss=1.0481092829713041\n",
      "GD iter. 1/499: loss=0.9669383696244886\n",
      "GD iter. 2/499: loss=0.8866238848135795\n",
      "GD iter. 3/499: loss=0.8145583165834519\n",
      "GD iter. 4/499: loss=0.7645113574131652\n",
      "GD iter. 5/499: loss=0.7331174938826318\n",
      "GD iter. 6/499: loss=0.7098342576083568\n",
      "GD iter. 7/499: loss=0.6930301076170359\n",
      "GD iter. 8/499: loss=0.6801536519783916\n",
      "GD iter. 9/499: loss=0.6685606453381305\n",
      "GD iter. 10/499: loss=0.6582506940479702\n",
      "GD iter. 11/499: loss=0.6498517495986468\n",
      "GD iter. 12/499: loss=0.642895845167134\n",
      "GD iter. 13/499: loss=0.6371744521622128\n",
      "GD iter. 14/499: loss=0.6320538309084489\n",
      "GD iter. 15/499: loss=0.6274978063029742\n",
      "GD iter. 16/499: loss=0.6234048610991656\n",
      "GD iter. 17/499: loss=0.6199861126406758\n",
      "GD iter. 18/499: loss=0.6167974118832376\n",
      "GD iter. 19/499: loss=0.6140480742323867\n",
      "GD iter. 20/499: loss=0.6118670286012109\n",
      "GD iter. 21/499: loss=0.6099573830318197\n",
      "GD iter. 22/499: loss=0.6081439674509744\n",
      "GD iter. 23/499: loss=0.6064387050811925\n",
      "GD iter. 24/499: loss=0.6048214140485935\n",
      "GD iter. 25/499: loss=0.6033076259267045\n",
      "GD iter. 26/499: loss=0.601825882371414\n",
      "GD iter. 27/499: loss=0.600430975639469\n",
      "GD iter. 28/499: loss=0.5991265849963906\n",
      "GD iter. 29/499: loss=0.5978585621090275\n",
      "GD iter. 30/499: loss=0.5966414471330604\n",
      "GD iter. 31/499: loss=0.5954465472501232\n",
      "GD iter. 32/499: loss=0.5942670683728496\n",
      "GD iter. 33/499: loss=0.5931418118261313\n",
      "GD iter. 34/499: loss=0.5920143180972306\n",
      "GD iter. 35/499: loss=0.5909143293778999\n",
      "GD iter. 36/499: loss=0.5898337081755322\n",
      "GD iter. 37/499: loss=0.5887703000543004\n",
      "GD iter. 38/499: loss=0.5877650730116339\n",
      "GD iter. 39/499: loss=0.5867614734358205\n",
      "GD iter. 40/499: loss=0.5857877274592722\n",
      "GD iter. 41/499: loss=0.5848320791655179\n",
      "GD iter. 42/499: loss=0.5839632900208084\n",
      "GD iter. 43/499: loss=0.5831020132281572\n",
      "GD iter. 44/499: loss=0.5822710345268817\n",
      "GD iter. 45/499: loss=0.5814699542952408\n",
      "GD iter. 46/499: loss=0.5806713336609617\n",
      "GD iter. 47/499: loss=0.5798789572364514\n",
      "GD iter. 48/499: loss=0.5790953990883544\n",
      "GD iter. 49/499: loss=0.5783125399107303\n",
      "GD iter. 50/499: loss=0.5775554095468025\n",
      "GD iter. 51/499: loss=0.5768290825292957\n",
      "GD iter. 52/499: loss=0.5761118646727228\n",
      "GD iter. 53/499: loss=0.5753914763202427\n",
      "GD iter. 54/499: loss=0.5746839857649136\n",
      "GD iter. 55/499: loss=0.5740215262523409\n",
      "GD iter. 56/499: loss=0.573391724875831\n",
      "GD iter. 57/499: loss=0.5727917627539149\n",
      "GD iter. 58/499: loss=0.5722459127861037\n",
      "GD iter. 59/499: loss=0.5717124390138978\n",
      "GD iter. 60/499: loss=0.5711751516924647\n",
      "GD iter. 61/499: loss=0.5706516106092704\n",
      "GD iter. 62/499: loss=0.5701322127276407\n",
      "GD iter. 63/499: loss=0.5696324106029732\n",
      "GD iter. 64/499: loss=0.5691470876912316\n",
      "GD iter. 65/499: loss=0.5686681397986635\n",
      "GD iter. 66/499: loss=0.5682040456320671\n",
      "GD iter. 67/499: loss=0.5677353680256025\n",
      "GD iter. 68/499: loss=0.5672879908605631\n",
      "GD iter. 69/499: loss=0.5668404868715915\n",
      "GD iter. 70/499: loss=0.5664006663066659\n",
      "GD iter. 71/499: loss=0.5660038913605168\n",
      "GD iter. 72/499: loss=0.5655898583414619\n",
      "GD iter. 73/499: loss=0.5651892553398198\n",
      "GD iter. 74/499: loss=0.5647825227131478\n",
      "GD iter. 75/499: loss=0.5644100442398379\n",
      "GD iter. 76/499: loss=0.5640484368310992\n",
      "GD iter. 77/499: loss=0.5636787048472574\n",
      "GD iter. 78/499: loss=0.5633211801520585\n",
      "GD iter. 79/499: loss=0.5629586377153045\n",
      "GD iter. 80/499: loss=0.5626297655653675\n",
      "GD iter. 81/499: loss=0.5622960648647142\n",
      "GD iter. 82/499: loss=0.561978866987015\n",
      "GD iter. 83/499: loss=0.56164586464821\n",
      "GD iter. 84/499: loss=0.5613188403425791\n",
      "GD iter. 85/499: loss=0.5610094097836869\n",
      "GD iter. 86/499: loss=0.5606895743722616\n",
      "GD iter. 87/499: loss=0.5603788617816237\n",
      "GD iter. 88/499: loss=0.5600710982442783\n",
      "GD iter. 89/499: loss=0.5597740739192558\n",
      "GD iter. 90/499: loss=0.5594802479069293\n",
      "GD iter. 91/499: loss=0.559188828290622\n",
      "GD iter. 92/499: loss=0.5589050205651593\n",
      "GD iter. 93/499: loss=0.5586317707036248\n",
      "GD iter. 94/499: loss=0.5583380493365515\n",
      "GD iter. 95/499: loss=0.5580504452128218\n",
      "GD iter. 96/499: loss=0.5577745836586943\n",
      "GD iter. 97/499: loss=0.5575136296219129\n",
      "GD iter. 98/499: loss=0.5572569902068533\n",
      "GD iter. 99/499: loss=0.5569996217302942\n",
      "GD iter. 100/499: loss=0.5567631237060658\n",
      "GD iter. 101/499: loss=0.5565084386505222\n",
      "GD iter. 102/499: loss=0.5562673615583179\n",
      "GD iter. 103/499: loss=0.5560377892360981\n",
      "GD iter. 104/499: loss=0.5558055611467457\n",
      "GD iter. 105/499: loss=0.5555615618117157\n",
      "GD iter. 106/499: loss=0.5553320398642786\n",
      "GD iter. 107/499: loss=0.5551067312869732\n",
      "GD iter. 108/499: loss=0.554880481052544\n",
      "GD iter. 109/499: loss=0.5546417563904302\n",
      "GD iter. 110/499: loss=0.5544127263651244\n",
      "GD iter. 111/499: loss=0.5541874962240854\n",
      "GD iter. 112/499: loss=0.5539606778640694\n",
      "GD iter. 113/499: loss=0.5537289916637287\n",
      "GD iter. 114/499: loss=0.5535113180551234\n",
      "GD iter. 115/499: loss=0.5532944150672133\n",
      "GD iter. 116/499: loss=0.5530821470396013\n",
      "GD iter. 117/499: loss=0.5528763544315022\n",
      "GD iter. 118/499: loss=0.5526676055509565\n",
      "GD iter. 119/499: loss=0.5524541942150426\n",
      "GD iter. 120/499: loss=0.5522364148329538\n",
      "GD iter. 121/499: loss=0.5520297950139749\n",
      "GD iter. 122/499: loss=0.5518317945559799\n",
      "GD iter. 123/499: loss=0.5516335954285841\n",
      "GD iter. 124/499: loss=0.5514261006984725\n",
      "GD iter. 125/499: loss=0.5512286011784804\n",
      "GD iter. 126/499: loss=0.5510491169304027\n",
      "GD iter. 127/499: loss=0.5508386192527214\n",
      "GD iter. 128/499: loss=0.5506479055659842\n",
      "GD iter. 129/499: loss=0.5504757914221147\n",
      "GD iter. 130/499: loss=0.5502747165826638\n",
      "GD iter. 131/499: loss=0.5500870048544894\n",
      "GD iter. 132/499: loss=0.5498894733614642\n",
      "GD iter. 133/499: loss=0.5497044640863757\n",
      "GD iter. 134/499: loss=0.5495376840078278\n",
      "GD iter. 135/499: loss=0.5493515595337745\n",
      "GD iter. 136/499: loss=0.5491795237602193\n",
      "GD iter. 137/499: loss=0.5490340088220872\n",
      "GD iter. 138/499: loss=0.5488646145556119\n",
      "GD iter. 139/499: loss=0.5486757666019003\n",
      "GD iter. 140/499: loss=0.5485364147497112\n",
      "GD iter. 141/499: loss=0.5483687845744858\n",
      "GD iter. 142/499: loss=0.5482272812925385\n",
      "GD iter. 143/499: loss=0.5480854281652799\n",
      "GD iter. 144/499: loss=0.5479417283690419\n",
      "GD iter. 145/499: loss=0.5478107261654783\n",
      "GD iter. 146/499: loss=0.5476590086169473\n",
      "GD iter. 147/499: loss=0.5475008553127727\n",
      "GD iter. 148/499: loss=0.5473788440425321\n",
      "GD iter. 149/499: loss=0.5472389609558214\n",
      "GD iter. 150/499: loss=0.5470927028887775\n",
      "GD iter. 151/499: loss=0.5469607928588138\n",
      "GD iter. 152/499: loss=0.5467903129307365\n",
      "GD iter. 153/499: loss=0.5466676892359272\n",
      "GD iter. 154/499: loss=0.546522834005817\n",
      "GD iter. 155/499: loss=0.5463689873972305\n",
      "GD iter. 156/499: loss=0.5462312481801832\n",
      "GD iter. 157/499: loss=0.546102216132293\n",
      "GD iter. 158/499: loss=0.5459554472685968\n",
      "GD iter. 159/499: loss=0.5458389699913969\n",
      "GD iter. 160/499: loss=0.5457020851849356\n",
      "GD iter. 161/499: loss=0.5455563418909807\n",
      "GD iter. 162/499: loss=0.5454185324250234\n",
      "GD iter. 163/499: loss=0.5452785963484952\n",
      "GD iter. 164/499: loss=0.5451542680139375\n",
      "GD iter. 165/499: loss=0.5450199226927244\n",
      "GD iter. 166/499: loss=0.5448829004200395\n",
      "GD iter. 167/499: loss=0.5447409822706002\n",
      "GD iter. 168/499: loss=0.5446185851136459\n",
      "GD iter. 169/499: loss=0.544501852821672\n",
      "GD iter. 170/499: loss=0.5443612349991728\n",
      "GD iter. 171/499: loss=0.5442454397468619\n",
      "GD iter. 172/499: loss=0.5441092652286214\n",
      "GD iter. 173/499: loss=0.5439785794727764\n",
      "GD iter. 174/499: loss=0.5438662455059857\n",
      "GD iter. 175/499: loss=0.5437362063112514\n",
      "GD iter. 176/499: loss=0.5436093482421555\n",
      "GD iter. 177/499: loss=0.5434985311933427\n",
      "GD iter. 178/499: loss=0.5433672736553998\n",
      "GD iter. 179/499: loss=0.543244746123224\n",
      "GD iter. 180/499: loss=0.5431231380269559\n",
      "GD iter. 181/499: loss=0.5430065207495506\n",
      "GD iter. 182/499: loss=0.5428768156112789\n",
      "GD iter. 183/499: loss=0.5427685297569855\n",
      "GD iter. 184/499: loss=0.5426512838581901\n",
      "GD iter. 185/499: loss=0.5425354220122162\n",
      "GD iter. 186/499: loss=0.54242440914944\n",
      "GD iter. 187/499: loss=0.5422922718690694\n",
      "GD iter. 188/499: loss=0.5421909856369604\n",
      "GD iter. 189/499: loss=0.5420895170648141\n",
      "GD iter. 190/499: loss=0.5419647389239178\n",
      "GD iter. 191/499: loss=0.5418596588842229\n",
      "GD iter. 192/499: loss=0.5417572587311115\n",
      "GD iter. 193/499: loss=0.5416624860714618\n",
      "GD iter. 194/499: loss=0.5415231636219117\n",
      "GD iter. 195/499: loss=0.5414201613398444\n",
      "GD iter. 196/499: loss=0.5413121849755801\n",
      "GD iter. 197/499: loss=0.5412062063212068\n",
      "GD iter. 198/499: loss=0.5410775302987122\n",
      "GD iter. 199/499: loss=0.5409716990735176\n",
      "GD iter. 200/499: loss=0.5408805135260201\n",
      "GD iter. 201/499: loss=0.540760505382821\n",
      "GD iter. 202/499: loss=0.5406428008707083\n",
      "GD iter. 203/499: loss=0.5405307143868272\n",
      "GD iter. 204/499: loss=0.5404348678871683\n",
      "GD iter. 205/499: loss=0.5403275996041752\n",
      "GD iter. 206/499: loss=0.5402332283020493\n",
      "GD iter. 207/499: loss=0.5401327065953524\n",
      "GD iter. 208/499: loss=0.5400302362476272\n",
      "GD iter. 209/499: loss=0.53992551229993\n",
      "GD iter. 210/499: loss=0.5398424126711402\n",
      "GD iter. 211/499: loss=0.5397563296146333\n",
      "GD iter. 212/499: loss=0.5396637090633664\n",
      "GD iter. 213/499: loss=0.5395750833539794\n",
      "GD iter. 214/499: loss=0.5394883605235129\n",
      "GD iter. 215/499: loss=0.5394007293175205\n",
      "GD iter. 216/499: loss=0.5393179739279089\n",
      "GD iter. 217/499: loss=0.5392333926329677\n",
      "GD iter. 218/499: loss=0.5391490717178407\n",
      "GD iter. 219/499: loss=0.5390585979012451\n",
      "GD iter. 220/499: loss=0.5389842037510922\n",
      "GD iter. 221/499: loss=0.5388995365672086\n",
      "GD iter. 222/499: loss=0.5388126744652698\n",
      "GD iter. 223/499: loss=0.538737638736419\n",
      "GD iter. 224/499: loss=0.5386500644444667\n",
      "GD iter. 225/499: loss=0.5385816998920795\n",
      "GD iter. 226/499: loss=0.5384951716813442\n",
      "GD iter. 227/499: loss=0.5384525488514001\n",
      "GD iter. 228/499: loss=0.5383274017343426\n",
      "GD iter. 229/499: loss=0.5382542522999496\n",
      "GD iter. 230/499: loss=0.5381761844343724\n",
      "GD iter. 231/499: loss=0.5381239632685588\n",
      "GD iter. 232/499: loss=0.5380645205947678\n",
      "GD iter. 233/499: loss=0.5380089592205831\n",
      "GD iter. 234/499: loss=0.5378814204560983\n",
      "GD iter. 235/499: loss=0.5378137891192335\n",
      "GD iter. 236/499: loss=0.5377422543676835\n",
      "GD iter. 237/499: loss=0.5376620686704966\n",
      "GD iter. 238/499: loss=0.5375946635570004\n",
      "GD iter. 239/499: loss=0.5375071122804707\n",
      "GD iter. 240/499: loss=0.5374395647011717\n",
      "GD iter. 241/499: loss=0.5373622503946203\n",
      "GD iter. 242/499: loss=0.5372930975658978\n",
      "GD iter. 243/499: loss=0.5372176523901074\n",
      "GD iter. 244/499: loss=0.5371496775530493\n",
      "GD iter. 245/499: loss=0.5370638570991233\n",
      "GD iter. 246/499: loss=0.5370248810892759\n",
      "GD iter. 247/499: loss=0.5369360483691211\n",
      "GD iter. 248/499: loss=0.5368657449953839\n",
      "GD iter. 249/499: loss=0.5368082078529847\n",
      "GD iter. 250/499: loss=0.5367357438211391\n",
      "GD iter. 251/499: loss=0.5366785183811926\n",
      "GD iter. 252/499: loss=0.5366398540687721\n",
      "GD iter. 253/499: loss=0.5365404885485168\n",
      "GD iter. 254/499: loss=0.5364723837458127\n",
      "GD iter. 255/499: loss=0.5364124851491677\n",
      "GD iter. 256/499: loss=0.5363549652504346\n",
      "GD iter. 257/499: loss=0.5362923729436877\n",
      "GD iter. 258/499: loss=0.536228357330461\n",
      "GD iter. 259/499: loss=0.5361526594680938\n",
      "GD iter. 260/499: loss=0.5361138807378755\n",
      "GD iter. 261/499: loss=0.5360325831790045\n",
      "GD iter. 262/499: loss=0.5359933373645924\n",
      "GD iter. 263/499: loss=0.535954119864829\n",
      "GD iter. 264/499: loss=0.535883683436396\n",
      "GD iter. 265/499: loss=0.5358057533000097\n",
      "GD iter. 266/499: loss=0.535735717241101\n",
      "GD iter. 267/499: loss=0.5356884929321917\n",
      "GD iter. 268/499: loss=0.5356282055746135\n",
      "GD iter. 269/499: loss=0.5355818186029049\n",
      "GD iter. 270/499: loss=0.53552438137047\n",
      "GD iter. 271/499: loss=0.5354536167073579\n",
      "GD iter. 272/499: loss=0.5353941156932709\n",
      "GD iter. 273/499: loss=0.535336914383514\n",
      "GD iter. 274/499: loss=0.5352965562732381\n",
      "GD iter. 275/499: loss=0.5352416709840323\n",
      "GD iter. 276/499: loss=0.5351826359848825\n",
      "GD iter. 277/499: loss=0.5351146693413155\n",
      "GD iter. 278/499: loss=0.5350435597039511\n",
      "GD iter. 279/499: loss=0.5349896314860862\n",
      "GD iter. 280/499: loss=0.5349530447973261\n",
      "GD iter. 281/499: loss=0.5349180447379502\n",
      "GD iter. 282/499: loss=0.5348537453997622\n",
      "GD iter. 283/499: loss=0.5347768642698286\n",
      "GD iter. 284/499: loss=0.534713296776959\n",
      "GD iter. 285/499: loss=0.5346631330906957\n",
      "GD iter. 286/499: loss=0.5346273096104702\n",
      "GD iter. 287/499: loss=0.5345543642735833\n",
      "GD iter. 288/499: loss=0.5345135463370199\n",
      "GD iter. 289/499: loss=0.5344446702203571\n",
      "GD iter. 290/499: loss=0.5344104204291228\n",
      "GD iter. 291/499: loss=0.5343348933978561\n",
      "GD iter. 292/499: loss=0.5342900701494568\n",
      "GD iter. 293/499: loss=0.5342371537083332\n",
      "GD iter. 294/499: loss=0.5342050733564254\n",
      "GD iter. 295/499: loss=0.534144383666697\n",
      "GD iter. 296/499: loss=0.5340908678376367\n",
      "GD iter. 297/499: loss=0.5340286516945689\n",
      "GD iter. 298/499: loss=0.5340011275813102\n",
      "GD iter. 299/499: loss=0.533940042338074\n",
      "GD iter. 300/499: loss=0.5338892627008471\n",
      "GD iter. 301/499: loss=0.5338311520388106\n",
      "GD iter. 302/499: loss=0.5338014902903157\n",
      "GD iter. 303/499: loss=0.5337554142167171\n",
      "GD iter. 304/499: loss=0.5336916824685616\n",
      "GD iter. 305/499: loss=0.5336339514566542\n",
      "GD iter. 306/499: loss=0.5336010146231428\n",
      "GD iter. 307/499: loss=0.5335423378166008\n",
      "GD iter. 308/499: loss=0.5334863945955552\n",
      "GD iter. 309/499: loss=0.5334252597803103\n",
      "GD iter. 310/499: loss=0.5333736732398403\n",
      "GD iter. 311/499: loss=0.5333352168153924\n",
      "GD iter. 312/499: loss=0.5332918060617184\n",
      "GD iter. 313/499: loss=0.5332500419520682\n",
      "GD iter. 314/499: loss=0.5331954754794632\n",
      "GD iter. 315/499: loss=0.5331377509072076\n",
      "GD iter. 316/499: loss=0.5331098085097008\n",
      "GD iter. 317/499: loss=0.5330411181558238\n",
      "GD iter. 318/499: loss=0.5329892084591877\n",
      "GD iter. 319/499: loss=0.5329564877393682\n",
      "GD iter. 320/499: loss=0.5329309633881193\n",
      "GD iter. 321/499: loss=0.532879744217497\n",
      "GD iter. 322/499: loss=0.5328201349298844\n",
      "GD iter. 323/499: loss=0.5327668955473946\n",
      "GD iter. 324/499: loss=0.532729686190504\n",
      "GD iter. 325/499: loss=0.5326980561737221\n",
      "GD iter. 326/499: loss=0.5326760200299306\n",
      "GD iter. 327/499: loss=0.5326227986749674\n",
      "GD iter. 328/499: loss=0.5325706875040688\n",
      "GD iter. 329/499: loss=0.5325135408837846\n",
      "GD iter. 330/499: loss=0.5324897447691123\n",
      "GD iter. 331/499: loss=0.5324774403890483\n",
      "GD iter. 332/499: loss=0.532438193330357\n",
      "GD iter. 333/499: loss=0.5323834963869398\n",
      "GD iter. 334/499: loss=0.5323470800489728\n",
      "GD iter. 335/499: loss=0.5323034698446095\n",
      "GD iter. 336/499: loss=0.5322660820797918\n",
      "GD iter. 337/499: loss=0.5322170633134126\n",
      "GD iter. 338/499: loss=0.5321724098975806\n",
      "GD iter. 339/499: loss=0.532135070165049\n",
      "GD iter. 340/499: loss=0.5321365878326987\n",
      "GD iter. 341/499: loss=0.5320760883137188\n",
      "GD iter. 342/499: loss=0.5320514611943373\n",
      "GD iter. 343/499: loss=0.531990254663697\n",
      "GD iter. 344/499: loss=0.5319589233504093\n",
      "GD iter. 345/499: loss=0.5319317765905532\n",
      "GD iter. 346/499: loss=0.5319090261559739\n",
      "GD iter. 347/499: loss=0.5318623953173643\n",
      "GD iter. 348/499: loss=0.531818666460204\n",
      "GD iter. 349/499: loss=0.5318010013343225\n",
      "GD iter. 350/499: loss=0.5317849937700188\n",
      "GD iter. 351/499: loss=0.5317317825645856\n",
      "GD iter. 352/499: loss=0.5316917274044062\n",
      "GD iter. 353/499: loss=0.5316560549061264\n",
      "GD iter. 354/499: loss=0.5316498049139886\n",
      "GD iter. 355/499: loss=0.5316163597711994\n",
      "GD iter. 356/499: loss=0.5315623512980383\n",
      "GD iter. 357/499: loss=0.5315236598087932\n",
      "GD iter. 358/499: loss=0.5315026680019829\n",
      "GD iter. 359/499: loss=0.5314900738778843\n",
      "GD iter. 360/499: loss=0.5314806791486864\n",
      "GD iter. 361/499: loss=0.5314235587080208\n",
      "GD iter. 362/499: loss=0.5313807775633447\n",
      "GD iter. 363/499: loss=0.5313459998932619\n",
      "GD iter. 364/499: loss=0.5313085486733966\n",
      "GD iter. 365/499: loss=0.5312723899965675\n",
      "GD iter. 366/499: loss=0.5312466160893246\n",
      "GD iter. 367/499: loss=0.5312396891987963\n",
      "GD iter. 368/499: loss=0.5312061311792301\n",
      "GD iter. 369/499: loss=0.531218469684299\n",
      "GD iter. 370/499: loss=0.5311494816921415\n",
      "GD iter. 371/499: loss=0.5310967867787187\n",
      "GD iter. 372/499: loss=0.5310898640834933\n",
      "GD iter. 373/499: loss=0.5310901204059297\n",
      "GD iter. 374/499: loss=0.5310299062690266\n",
      "GD iter. 375/499: loss=0.5310151216777758\n",
      "GD iter. 376/499: loss=0.5310019073764218\n",
      "GD iter. 377/499: loss=0.5309782682321005\n",
      "GD iter. 378/499: loss=0.5309347041358254\n",
      "GD iter. 379/499: loss=0.5309004576360802\n",
      "GD iter. 380/499: loss=0.530881210187581\n",
      "GD iter. 381/499: loss=0.530868142635596\n",
      "GD iter. 382/499: loss=0.5308560059167473\n",
      "GD iter. 383/499: loss=0.5308437354910291\n",
      "GD iter. 384/499: loss=0.530774584869067\n",
      "GD iter. 385/499: loss=0.5307440681644919\n",
      "GD iter. 386/499: loss=0.5307287044841292\n",
      "GD iter. 387/499: loss=0.5307341445045235\n",
      "GD iter. 388/499: loss=0.5307016805847373\n",
      "GD iter. 389/499: loss=0.5306923156444904\n",
      "GD iter. 390/499: loss=0.5306561449514876\n",
      "GD iter. 391/499: loss=0.5306244288221688\n",
      "GD iter. 392/499: loss=0.530617341048137\n",
      "GD iter. 393/499: loss=0.5305734047243157\n",
      "GD iter. 394/499: loss=0.5305749455620807\n",
      "GD iter. 395/499: loss=0.5305551270827391\n",
      "GD iter. 396/499: loss=0.5305027581664085\n",
      "GD iter. 397/499: loss=0.5304942483678627\n",
      "GD iter. 398/499: loss=0.5304722184759372\n",
      "GD iter. 399/499: loss=0.5304734274017585\n",
      "GD iter. 400/499: loss=0.5304498354432228\n",
      "GD iter. 401/499: loss=0.5304066485376607\n",
      "GD iter. 402/499: loss=0.5303832342243461\n",
      "GD iter. 403/499: loss=0.5303694086279815\n",
      "GD iter. 404/499: loss=0.5303563164916869\n",
      "GD iter. 405/499: loss=0.5303529898955527\n",
      "GD iter. 406/499: loss=0.5302967399054224\n",
      "GD iter. 407/499: loss=0.5302802358820008\n",
      "GD iter. 408/499: loss=0.5302619504040397\n",
      "GD iter. 409/499: loss=0.5302251805641649\n",
      "GD iter. 410/499: loss=0.5302160225397937\n",
      "GD iter. 411/499: loss=0.5302064329408348\n",
      "GD iter. 412/499: loss=0.5302465148548411\n",
      "GD iter. 413/499: loss=0.5301779823418526\n",
      "GD iter. 414/499: loss=0.5301742735127485\n",
      "GD iter. 415/499: loss=0.5301277731077128\n",
      "GD iter. 416/499: loss=0.5301405566165225\n",
      "GD iter. 417/499: loss=0.5301101563044198\n",
      "GD iter. 418/499: loss=0.5301075480072865\n",
      "GD iter. 419/499: loss=0.5300363682804459\n",
      "GD iter. 420/499: loss=0.5300352596721695\n",
      "GD iter. 421/499: loss=0.5300341124092871\n",
      "GD iter. 422/499: loss=0.5300247204788204\n",
      "GD iter. 423/499: loss=0.5299748171228233\n",
      "GD iter. 424/499: loss=0.5299598156874147\n",
      "GD iter. 425/499: loss=0.529953845426932\n",
      "GD iter. 426/499: loss=0.5299781589368604\n",
      "GD iter. 427/499: loss=0.529897501633037\n",
      "GD iter. 428/499: loss=0.529891211359446\n",
      "GD iter. 429/499: loss=0.5298849801724705\n",
      "GD iter. 430/499: loss=0.5298483272783611\n",
      "GD iter. 431/499: loss=0.5298277865370082\n",
      "GD iter. 432/499: loss=0.5298175988897883\n",
      "GD iter. 433/499: loss=0.5298205644918732\n",
      "GD iter. 434/499: loss=0.5297697234713553\n",
      "GD iter. 435/499: loss=0.5297485273297402\n",
      "GD iter. 436/499: loss=0.5297225860781003\n",
      "GD iter. 437/499: loss=0.5297298617927269\n",
      "GD iter. 438/499: loss=0.5296896615208075\n",
      "GD iter. 439/499: loss=0.5297024422369812\n",
      "GD iter. 440/499: loss=0.5297518689436309\n",
      "GD iter. 441/499: loss=0.5296957663446973\n",
      "GD iter. 442/499: loss=0.5296434725793651\n",
      "GD iter. 443/499: loss=0.5295977794811184\n",
      "GD iter. 444/499: loss=0.529586522965594\n",
      "GD iter. 445/499: loss=0.5295869732090795\n",
      "GD iter. 446/499: loss=0.5295792979434468\n",
      "GD iter. 447/499: loss=0.5295828241983895\n",
      "GD iter. 448/499: loss=0.5295416471944939\n",
      "GD iter. 449/499: loss=0.5295371400562805\n",
      "GD iter. 450/499: loss=0.5294793947361522\n",
      "GD iter. 451/499: loss=0.5294740198097152\n",
      "GD iter. 452/499: loss=0.529490303378939\n",
      "GD iter. 453/499: loss=0.5294617863770992\n",
      "GD iter. 454/499: loss=0.529436538224489\n",
      "GD iter. 455/499: loss=0.5294177591333634\n",
      "GD iter. 456/499: loss=0.529380490865761\n",
      "GD iter. 457/499: loss=0.5293811902217438\n",
      "GD iter. 458/499: loss=0.5293767004463852\n",
      "GD iter. 459/499: loss=0.5293511655580388\n",
      "GD iter. 460/499: loss=0.5293449282984147\n",
      "GD iter. 461/499: loss=0.529307266735205\n",
      "GD iter. 462/499: loss=0.5293043456364519\n",
      "GD iter. 463/499: loss=0.5293232081687436\n",
      "GD iter. 464/499: loss=0.5292622295666456\n",
      "GD iter. 465/499: loss=0.529250939148108\n",
      "GD iter. 466/499: loss=0.529228890716774\n",
      "GD iter. 467/499: loss=0.5292497085121075\n",
      "GD iter. 468/499: loss=0.5291973439453658\n",
      "GD iter. 469/499: loss=0.5291872144986965\n",
      "GD iter. 470/499: loss=0.5291828736480332\n",
      "GD iter. 471/499: loss=0.5292001380152368\n",
      "GD iter. 472/499: loss=0.5291405445331614\n",
      "GD iter. 473/499: loss=0.5291554920280895\n",
      "GD iter. 474/499: loss=0.5291216867570677\n",
      "GD iter. 475/499: loss=0.5290899673676802\n",
      "GD iter. 476/499: loss=0.5290756047588078\n",
      "GD iter. 477/499: loss=0.5290937648252052\n",
      "GD iter. 478/499: loss=0.5290256506614943\n",
      "GD iter. 479/499: loss=0.5290014940928862\n",
      "GD iter. 480/499: loss=0.5290402760748643\n",
      "GD iter. 481/499: loss=0.5289995568303211\n",
      "GD iter. 482/499: loss=0.5290158239331604\n",
      "GD iter. 483/499: loss=0.528996294592489\n",
      "GD iter. 484/499: loss=0.529005541321711\n",
      "GD iter. 485/499: loss=0.5289360619313638\n",
      "GD iter. 486/499: loss=0.5289437649371775\n",
      "GD iter. 487/499: loss=0.5289225860896274\n",
      "GD iter. 488/499: loss=0.5289078757840532\n",
      "GD iter. 489/499: loss=0.5288736625402414\n",
      "GD iter. 490/499: loss=0.5288491214956011\n",
      "GD iter. 491/499: loss=0.5288512087581166\n",
      "GD iter. 492/499: loss=0.5288268579053781\n",
      "GD iter. 493/499: loss=0.5288257994494279\n",
      "GD iter. 494/499: loss=0.52884459848613\n",
      "GD iter. 495/499: loss=0.5288120940155369\n",
      "GD iter. 496/499: loss=0.5288205657470836\n",
      "GD iter. 497/499: loss=0.5287687605474634\n",
      "GD iter. 498/499: loss=0.5287571786710189\n",
      "GD iter. 499/499: loss=0.5287571888032737\n",
      "The Accuracy is: 0.5993\n",
      "The F1 score is: 0.2469\n",
      "The precision is: 0.1444\n",
      "The recall is: 0.8511\n",
      "GD iter. 0/499: loss=1.041420752969521\n",
      "GD iter. 1/499: loss=0.9575645375594771\n",
      "GD iter. 2/499: loss=0.8748669029905133\n",
      "GD iter. 3/499: loss=0.7981609169365198\n",
      "GD iter. 4/499: loss=0.7443222942267844\n",
      "GD iter. 5/499: loss=0.7127744715679704\n",
      "GD iter. 6/499: loss=0.6909753175432678\n",
      "GD iter. 7/499: loss=0.6744583353347403\n",
      "GD iter. 8/499: loss=0.6623289394471092\n",
      "GD iter. 9/499: loss=0.6522742393264102\n",
      "GD iter. 10/499: loss=0.6444352861467069\n",
      "GD iter. 11/499: loss=0.6383741179813452\n",
      "GD iter. 12/499: loss=0.6333471162511902\n",
      "GD iter. 13/499: loss=0.6291950532961568\n",
      "GD iter. 14/499: loss=0.6256466605171189\n",
      "GD iter. 15/499: loss=0.6225329066787855\n",
      "GD iter. 16/499: loss=0.6197248517888677\n",
      "GD iter. 17/499: loss=0.6170425020181994\n",
      "GD iter. 18/499: loss=0.6145634707905665\n",
      "GD iter. 19/499: loss=0.6122999596978467\n",
      "GD iter. 20/499: loss=0.6102916814681975\n",
      "GD iter. 21/499: loss=0.6084401014041649\n",
      "GD iter. 22/499: loss=0.6066958215365152\n",
      "GD iter. 23/499: loss=0.605097065150708\n",
      "GD iter. 24/499: loss=0.603566186246046\n",
      "GD iter. 25/499: loss=0.6020869784335865\n",
      "GD iter. 26/499: loss=0.6006582180203384\n",
      "GD iter. 27/499: loss=0.5992811401871404\n",
      "GD iter. 28/499: loss=0.5979395268754301\n",
      "GD iter. 29/499: loss=0.5966477265632373\n",
      "GD iter. 30/499: loss=0.5954398752003621\n",
      "GD iter. 31/499: loss=0.5942702690873588\n",
      "GD iter. 32/499: loss=0.5931251876269292\n",
      "GD iter. 33/499: loss=0.5919981797798477\n",
      "GD iter. 34/499: loss=0.5908904766362287\n",
      "GD iter. 35/499: loss=0.5897903481962743\n",
      "GD iter. 36/499: loss=0.5887084373718307\n",
      "GD iter. 37/499: loss=0.5876484687786478\n",
      "GD iter. 38/499: loss=0.5866344583151316\n",
      "GD iter. 39/499: loss=0.5856525405923393\n",
      "GD iter. 40/499: loss=0.5847526318859747\n",
      "GD iter. 41/499: loss=0.5838762522000335\n",
      "GD iter. 42/499: loss=0.5830015512591337\n",
      "GD iter. 43/499: loss=0.5821445025659523\n",
      "GD iter. 44/499: loss=0.5812904965453465\n",
      "GD iter. 45/499: loss=0.5804463115505247\n",
      "GD iter. 46/499: loss=0.5796114714866284\n",
      "GD iter. 47/499: loss=0.5787852844434136\n",
      "GD iter. 48/499: loss=0.5779934730804273\n",
      "GD iter. 49/499: loss=0.5772259651519934\n",
      "GD iter. 50/499: loss=0.5764784596143413\n",
      "GD iter. 51/499: loss=0.5757497882889576\n",
      "GD iter. 52/499: loss=0.575027926293259\n",
      "GD iter. 53/499: loss=0.5743169086207317\n",
      "GD iter. 54/499: loss=0.5736605730501866\n",
      "GD iter. 55/499: loss=0.5730377814536253\n",
      "GD iter. 56/499: loss=0.5724383463938719\n",
      "GD iter. 57/499: loss=0.571877308874333\n",
      "GD iter. 58/499: loss=0.5713401903485316\n",
      "GD iter. 59/499: loss=0.5708187799332756\n",
      "GD iter. 60/499: loss=0.5703126667812481\n",
      "GD iter. 61/499: loss=0.5698105565921613\n",
      "GD iter. 62/499: loss=0.5693425022818824\n",
      "GD iter. 63/499: loss=0.5688499953717105\n",
      "GD iter. 64/499: loss=0.5683769685421403\n",
      "GD iter. 65/499: loss=0.5679313100758101\n",
      "GD iter. 66/499: loss=0.5674804177404998\n",
      "GD iter. 67/499: loss=0.567052260111732\n",
      "GD iter. 68/499: loss=0.5666220625898988\n",
      "GD iter. 69/499: loss=0.5662081545237163\n",
      "GD iter. 70/499: loss=0.565812092257162\n",
      "GD iter. 71/499: loss=0.5654231652965992\n",
      "GD iter. 72/499: loss=0.5650714451307344\n",
      "GD iter. 73/499: loss=0.5646867922064073\n",
      "GD iter. 74/499: loss=0.5643096829396097\n",
      "GD iter. 75/499: loss=0.5639508161850018\n",
      "GD iter. 76/499: loss=0.563580423003888\n",
      "GD iter. 77/499: loss=0.5632373794655603\n",
      "GD iter. 78/499: loss=0.5628984042442197\n",
      "GD iter. 79/499: loss=0.5625649448329892\n",
      "GD iter. 80/499: loss=0.5622212255966622\n",
      "GD iter. 81/499: loss=0.5618905732184647\n",
      "GD iter. 82/499: loss=0.5615690416963037\n",
      "GD iter. 83/499: loss=0.5612389536257383\n",
      "GD iter. 84/499: loss=0.5609233539722612\n",
      "GD iter. 85/499: loss=0.560623611410344\n",
      "GD iter. 86/499: loss=0.5603296126043491\n",
      "GD iter. 87/499: loss=0.5600090520970369\n",
      "GD iter. 88/499: loss=0.5597015253227497\n",
      "GD iter. 89/499: loss=0.5594193271629139\n",
      "GD iter. 90/499: loss=0.5591070604427933\n",
      "GD iter. 91/499: loss=0.5588075645780696\n",
      "GD iter. 92/499: loss=0.5585071045966166\n",
      "GD iter. 93/499: loss=0.5582375683032746\n",
      "GD iter. 94/499: loss=0.557966915668447\n",
      "GD iter. 95/499: loss=0.5576808759897779\n",
      "GD iter. 96/499: loss=0.5574196417301321\n",
      "GD iter. 97/499: loss=0.5571434288189282\n",
      "GD iter. 98/499: loss=0.5568639787409365\n",
      "GD iter. 99/499: loss=0.5566002782837249\n",
      "GD iter. 100/499: loss=0.556334549769814\n",
      "GD iter. 101/499: loss=0.5560842482817105\n",
      "GD iter. 102/499: loss=0.5558510161763717\n",
      "GD iter. 103/499: loss=0.5556002930327305\n",
      "GD iter. 104/499: loss=0.5553537595430448\n",
      "GD iter. 105/499: loss=0.5551276566410441\n",
      "GD iter. 106/499: loss=0.5548837267146172\n",
      "GD iter. 107/499: loss=0.5546390581634372\n",
      "GD iter. 108/499: loss=0.5544062798330286\n",
      "GD iter. 109/499: loss=0.5541722674126013\n",
      "GD iter. 110/499: loss=0.5539706752505467\n",
      "GD iter. 111/499: loss=0.5537620546377685\n",
      "GD iter. 112/499: loss=0.5535435551786179\n",
      "GD iter. 113/499: loss=0.5533372360930996\n",
      "GD iter. 114/499: loss=0.5531488326044539\n",
      "GD iter. 115/499: loss=0.5529410385199585\n",
      "GD iter. 116/499: loss=0.5527472318932628\n",
      "GD iter. 117/499: loss=0.5525458873365341\n",
      "GD iter. 118/499: loss=0.5523505821595528\n",
      "GD iter. 119/499: loss=0.5521463735633153\n",
      "GD iter. 120/499: loss=0.5519639486757851\n",
      "GD iter. 121/499: loss=0.5517745393199732\n",
      "GD iter. 122/499: loss=0.5515750956649367\n",
      "GD iter. 123/499: loss=0.5513741880610679\n",
      "GD iter. 124/499: loss=0.5511780642532249\n",
      "GD iter. 125/499: loss=0.5510101065184162\n",
      "GD iter. 126/499: loss=0.5508198998508019\n",
      "GD iter. 127/499: loss=0.5506074497780322\n",
      "GD iter. 128/499: loss=0.5504262744987304\n",
      "GD iter. 129/499: loss=0.5502271863213206\n",
      "GD iter. 130/499: loss=0.5500651248192316\n",
      "GD iter. 131/499: loss=0.5498558369996263\n",
      "GD iter. 132/499: loss=0.5496903048319915\n",
      "GD iter. 133/499: loss=0.5494839612898813\n",
      "GD iter. 134/499: loss=0.5493277479561953\n",
      "GD iter. 135/499: loss=0.549174477578228\n",
      "GD iter. 136/499: loss=0.5489885548951523\n",
      "GD iter. 137/499: loss=0.5488279805679901\n",
      "GD iter. 138/499: loss=0.5486826578867244\n",
      "GD iter. 139/499: loss=0.548508647155611\n",
      "GD iter. 140/499: loss=0.5483610334959486\n",
      "GD iter. 141/499: loss=0.5482080418108326\n",
      "GD iter. 142/499: loss=0.548045002105253\n",
      "GD iter. 143/499: loss=0.5478991779805991\n",
      "GD iter. 144/499: loss=0.5477237538516275\n",
      "GD iter. 145/499: loss=0.547587415554431\n",
      "GD iter. 146/499: loss=0.5474108360332114\n",
      "GD iter. 147/499: loss=0.5472731129902346\n",
      "GD iter. 148/499: loss=0.5471159077289691\n",
      "GD iter. 149/499: loss=0.546972946566217\n",
      "GD iter. 150/499: loss=0.546804107857031\n",
      "GD iter. 151/499: loss=0.5466674223662308\n",
      "GD iter. 152/499: loss=0.5464910191070317\n",
      "GD iter. 153/499: loss=0.5463666722666399\n",
      "GD iter. 154/499: loss=0.5461975132089468\n",
      "GD iter. 155/499: loss=0.5460414002227598\n",
      "GD iter. 156/499: loss=0.5459237887994615\n",
      "GD iter. 157/499: loss=0.5457418568263293\n",
      "GD iter. 158/499: loss=0.5456094413632521\n",
      "GD iter. 159/499: loss=0.5454560058573693\n",
      "GD iter. 160/499: loss=0.5453204246441176\n",
      "GD iter. 161/499: loss=0.5451680203643755\n",
      "GD iter. 162/499: loss=0.545040535467882\n",
      "GD iter. 163/499: loss=0.5449023137042113\n",
      "GD iter. 164/499: loss=0.5447508442219686\n",
      "GD iter. 165/499: loss=0.5446402066520857\n",
      "GD iter. 166/499: loss=0.5445183572905995\n",
      "GD iter. 167/499: loss=0.5444003915766168\n",
      "GD iter. 168/499: loss=0.544288628658582\n",
      "GD iter. 169/499: loss=0.5441586456783416\n",
      "GD iter. 170/499: loss=0.544039944881449\n",
      "GD iter. 171/499: loss=0.5439388600238745\n",
      "GD iter. 172/499: loss=0.5438258738272056\n",
      "GD iter. 173/499: loss=0.5436931330403331\n",
      "GD iter. 174/499: loss=0.543581708290613\n",
      "GD iter. 175/499: loss=0.5434602463279091\n",
      "GD iter. 176/499: loss=0.5433610324609109\n",
      "GD iter. 177/499: loss=0.5432358593198273\n",
      "GD iter. 178/499: loss=0.5431138922181855\n",
      "GD iter. 179/499: loss=0.5430051159153771\n",
      "GD iter. 180/499: loss=0.5428999840974024\n",
      "GD iter. 181/499: loss=0.5427857262367137\n",
      "GD iter. 182/499: loss=0.5426774323577689\n",
      "GD iter. 183/499: loss=0.5425570707901498\n",
      "GD iter. 184/499: loss=0.5424408184577283\n",
      "GD iter. 185/499: loss=0.5423242188783157\n",
      "GD iter. 186/499: loss=0.5422196194775492\n",
      "GD iter. 187/499: loss=0.5421105931344775\n",
      "GD iter. 188/499: loss=0.5420017384185639\n",
      "GD iter. 189/499: loss=0.5418971206619562\n",
      "GD iter. 190/499: loss=0.5417839376113567\n",
      "GD iter. 191/499: loss=0.5416761285625248\n",
      "GD iter. 192/499: loss=0.5415610367810914\n",
      "GD iter. 193/499: loss=0.5414760136608122\n",
      "GD iter. 194/499: loss=0.5413534981733893\n",
      "GD iter. 195/499: loss=0.5412422332165906\n",
      "GD iter. 196/499: loss=0.5411381617553629\n",
      "GD iter. 197/499: loss=0.5410284369650767\n",
      "GD iter. 198/499: loss=0.5409143925007301\n",
      "GD iter. 199/499: loss=0.5408339952949212\n",
      "GD iter. 200/499: loss=0.5407050835272285\n",
      "GD iter. 201/499: loss=0.5405982342510112\n",
      "GD iter. 202/499: loss=0.5405003933632537\n",
      "GD iter. 203/499: loss=0.5403842507062284\n",
      "GD iter. 204/499: loss=0.5402889720997167\n",
      "GD iter. 205/499: loss=0.5401838279947108\n",
      "GD iter. 206/499: loss=0.5400881502708299\n",
      "GD iter. 207/499: loss=0.5399825262681337\n",
      "GD iter. 208/499: loss=0.5398833003925481\n",
      "GD iter. 209/499: loss=0.539784865088041\n",
      "GD iter. 210/499: loss=0.5396957893788527\n",
      "GD iter. 211/499: loss=0.5396030050534649\n",
      "GD iter. 212/499: loss=0.5395308446289665\n",
      "GD iter. 213/499: loss=0.539413604765838\n",
      "GD iter. 214/499: loss=0.5393233739620438\n",
      "GD iter. 215/499: loss=0.539232284209611\n",
      "GD iter. 216/499: loss=0.5391525606881922\n",
      "GD iter. 217/499: loss=0.5390572285556031\n",
      "GD iter. 218/499: loss=0.5389873982850897\n",
      "GD iter. 219/499: loss=0.5388772705813081\n",
      "GD iter. 220/499: loss=0.5388024882321752\n",
      "GD iter. 221/499: loss=0.538706678828739\n",
      "GD iter. 222/499: loss=0.5386169138008042\n",
      "GD iter. 223/499: loss=0.5385380143290533\n",
      "GD iter. 224/499: loss=0.5384454774914922\n",
      "GD iter. 225/499: loss=0.5383700040265088\n",
      "GD iter. 226/499: loss=0.5382725997877237\n",
      "GD iter. 227/499: loss=0.5382012233338911\n",
      "GD iter. 228/499: loss=0.5380995411979215\n",
      "GD iter. 229/499: loss=0.5380277563116048\n",
      "GD iter. 230/499: loss=0.537947416376103\n",
      "GD iter. 231/499: loss=0.5378584988613493\n",
      "GD iter. 232/499: loss=0.5377640323155781\n",
      "GD iter. 233/499: loss=0.5377046261010242\n",
      "GD iter. 234/499: loss=0.5376292629983059\n",
      "GD iter. 235/499: loss=0.5375337236146897\n",
      "GD iter. 236/499: loss=0.5374661031000421\n",
      "GD iter. 237/499: loss=0.5373892139911425\n",
      "GD iter. 238/499: loss=0.5373054704077476\n",
      "GD iter. 239/499: loss=0.5372286891028099\n",
      "GD iter. 240/499: loss=0.5371620289564911\n",
      "GD iter. 241/499: loss=0.5370745612190165\n",
      "GD iter. 242/499: loss=0.5370018400977999\n",
      "GD iter. 243/499: loss=0.5369413359403851\n",
      "GD iter. 244/499: loss=0.536860155608333\n",
      "GD iter. 245/499: loss=0.5367814369015101\n",
      "GD iter. 246/499: loss=0.5366942529733426\n",
      "GD iter. 247/499: loss=0.5366332502630259\n",
      "GD iter. 248/499: loss=0.5365492736560612\n",
      "GD iter. 249/499: loss=0.5364710809666882\n",
      "GD iter. 250/499: loss=0.5364075525114088\n",
      "GD iter. 251/499: loss=0.5363324886632281\n",
      "GD iter. 252/499: loss=0.5362728727339967\n",
      "GD iter. 253/499: loss=0.5362132971919246\n",
      "GD iter. 254/499: loss=0.5361220720907266\n",
      "GD iter. 255/499: loss=0.5360815655203588\n",
      "GD iter. 256/499: loss=0.5360030655918867\n",
      "GD iter. 257/499: loss=0.5359405361888345\n",
      "GD iter. 258/499: loss=0.535849299538782\n",
      "GD iter. 259/499: loss=0.5358099537493187\n",
      "GD iter. 260/499: loss=0.5357304008845808\n",
      "GD iter. 261/499: loss=0.5356747892887963\n",
      "GD iter. 262/499: loss=0.5356240965873428\n",
      "GD iter. 263/499: loss=0.5355521212231568\n",
      "GD iter. 264/499: loss=0.5354909253973571\n",
      "GD iter. 265/499: loss=0.5354290409804816\n",
      "GD iter. 266/499: loss=0.5353658014476529\n",
      "GD iter. 267/499: loss=0.5353157398191313\n",
      "GD iter. 268/499: loss=0.5352453933478971\n",
      "GD iter. 269/499: loss=0.5351836387345599\n",
      "GD iter. 270/499: loss=0.5351077115521533\n",
      "GD iter. 271/499: loss=0.5350662024255181\n",
      "GD iter. 272/499: loss=0.5349824834270553\n",
      "GD iter. 273/499: loss=0.5349369692586021\n",
      "GD iter. 274/499: loss=0.5349061702880419\n",
      "GD iter. 275/499: loss=0.5348262283149915\n",
      "GD iter. 276/499: loss=0.5347470247034259\n",
      "GD iter. 277/499: loss=0.5347089321255166\n",
      "GD iter. 278/499: loss=0.5346367348235671\n",
      "GD iter. 279/499: loss=0.5345763251562158\n",
      "GD iter. 280/499: loss=0.534552575095301\n",
      "GD iter. 281/499: loss=0.5344881346939585\n",
      "GD iter. 282/499: loss=0.5344103385614948\n",
      "GD iter. 283/499: loss=0.5343686625001829\n",
      "GD iter. 284/499: loss=0.5343097203794499\n",
      "GD iter. 285/499: loss=0.5342316964932585\n",
      "GD iter. 286/499: loss=0.5342155682094872\n",
      "GD iter. 287/499: loss=0.5341548387358084\n",
      "GD iter. 288/499: loss=0.534087949846237\n",
      "GD iter. 289/499: loss=0.5340283578550067\n",
      "GD iter. 290/499: loss=0.5339786312252203\n",
      "GD iter. 291/499: loss=0.5339335565124893\n",
      "GD iter. 292/499: loss=0.5338887656408292\n",
      "GD iter. 293/499: loss=0.5337978985809514\n",
      "GD iter. 294/499: loss=0.5337657317105687\n",
      "GD iter. 295/499: loss=0.5337412989603686\n",
      "GD iter. 296/499: loss=0.5336735143226853\n",
      "GD iter. 297/499: loss=0.5336096859763048\n",
      "GD iter. 298/499: loss=0.5335530025740808\n",
      "GD iter. 299/499: loss=0.5335310563285389\n",
      "GD iter. 300/499: loss=0.5335231212580291\n",
      "GD iter. 301/499: loss=0.5334238285292785\n",
      "GD iter. 302/499: loss=0.5333686794577468\n",
      "GD iter. 303/499: loss=0.5333381190730672\n",
      "GD iter. 304/499: loss=0.5332945928151057\n",
      "GD iter. 305/499: loss=0.5332348746557938\n",
      "GD iter. 306/499: loss=0.5331979452832819\n",
      "GD iter. 307/499: loss=0.5331533876255088\n",
      "GD iter. 308/499: loss=0.5331349775606196\n",
      "GD iter. 309/499: loss=0.5330666381667117\n",
      "GD iter. 310/499: loss=0.5330019685594733\n",
      "GD iter. 311/499: loss=0.5329628038772515\n",
      "GD iter. 312/499: loss=0.5329194525557641\n",
      "GD iter. 313/499: loss=0.5328964154871291\n",
      "GD iter. 314/499: loss=0.532834353484895\n",
      "GD iter. 315/499: loss=0.5328158375873562\n",
      "GD iter. 316/499: loss=0.5327735725040634\n",
      "GD iter. 317/499: loss=0.5327361538316184\n",
      "GD iter. 318/499: loss=0.5326799978264619\n",
      "GD iter. 319/499: loss=0.5326386328894478\n",
      "GD iter. 320/499: loss=0.5326118702960863\n",
      "GD iter. 321/499: loss=0.5325562864637013\n",
      "GD iter. 322/499: loss=0.5325229040471694\n",
      "GD iter. 323/499: loss=0.5324624549113942\n",
      "GD iter. 324/499: loss=0.5324274895140251\n",
      "GD iter. 325/499: loss=0.5323849356727707\n",
      "GD iter. 326/499: loss=0.5323833625766723\n",
      "GD iter. 327/499: loss=0.5323200630011936\n",
      "GD iter. 328/499: loss=0.5322589347555675\n",
      "GD iter. 329/499: loss=0.5322256051090853\n",
      "GD iter. 330/499: loss=0.532209428618559\n",
      "GD iter. 331/499: loss=0.5321968893730629\n",
      "GD iter. 332/499: loss=0.5320929589532494\n",
      "GD iter. 333/499: loss=0.532043575616278\n",
      "GD iter. 334/499: loss=0.5320322354852166\n",
      "GD iter. 335/499: loss=0.5319764402014563\n",
      "GD iter. 336/499: loss=0.5319443637307439\n",
      "GD iter. 337/499: loss=0.5319490978805926\n",
      "GD iter. 338/499: loss=0.5318874200078889\n",
      "GD iter. 339/499: loss=0.5318349020926334\n",
      "GD iter. 340/499: loss=0.5318071341581273\n",
      "GD iter. 341/499: loss=0.5318174536581618\n",
      "GD iter. 342/499: loss=0.5317214690537724\n",
      "GD iter. 343/499: loss=0.5317046018719755\n",
      "GD iter. 344/499: loss=0.5317006477377277\n",
      "GD iter. 345/499: loss=0.5316434600378804\n",
      "GD iter. 346/499: loss=0.5315837306474742\n",
      "GD iter. 347/499: loss=0.5315611734847772\n",
      "GD iter. 348/499: loss=0.5315139499767756\n",
      "GD iter. 349/499: loss=0.5314893103829801\n",
      "GD iter. 350/499: loss=0.5314925540996208\n",
      "GD iter. 351/499: loss=0.5314210488059342\n",
      "GD iter. 352/499: loss=0.5313744276088596\n",
      "GD iter. 353/499: loss=0.531354570713777\n",
      "GD iter. 354/499: loss=0.5313393032307823\n",
      "GD iter. 355/499: loss=0.5312779979931059\n",
      "GD iter. 356/499: loss=0.5312494020743082\n",
      "GD iter. 357/499: loss=0.5312175501630886\n",
      "GD iter. 358/499: loss=0.531181306332101\n",
      "GD iter. 359/499: loss=0.5311427737642936\n",
      "GD iter. 360/499: loss=0.5311146367158164\n",
      "GD iter. 361/499: loss=0.5310791218302752\n",
      "GD iter. 362/499: loss=0.5310377359238936\n",
      "GD iter. 363/499: loss=0.5310308015282742\n",
      "GD iter. 364/499: loss=0.531007195098169\n",
      "GD iter. 365/499: loss=0.5309482818984671\n",
      "GD iter. 366/499: loss=0.5309252709284326\n",
      "GD iter. 367/499: loss=0.5309127642319542\n",
      "GD iter. 368/499: loss=0.530862755991166\n",
      "GD iter. 369/499: loss=0.5308093559013903\n",
      "GD iter. 370/499: loss=0.5307715425180861\n",
      "GD iter. 371/499: loss=0.5307486253583831\n",
      "GD iter. 372/499: loss=0.5307277159640255\n",
      "GD iter. 373/499: loss=0.5307221194949134\n",
      "GD iter. 374/499: loss=0.5306887868119472\n",
      "GD iter. 375/499: loss=0.5306327962106531\n",
      "GD iter. 376/499: loss=0.5306202080107134\n",
      "GD iter. 377/499: loss=0.5305941480220442\n",
      "GD iter. 378/499: loss=0.5305501289934481\n",
      "GD iter. 379/499: loss=0.5305491254944923\n",
      "GD iter. 380/499: loss=0.5304899823364028\n",
      "GD iter. 381/499: loss=0.5304725730344478\n",
      "GD iter. 382/499: loss=0.5304349204872453\n",
      "GD iter. 383/499: loss=0.5303971297973804\n",
      "GD iter. 384/499: loss=0.5303800591153695\n",
      "GD iter. 385/499: loss=0.5303784704418819\n",
      "GD iter. 386/499: loss=0.5303049710652497\n",
      "GD iter. 387/499: loss=0.5302802653345942\n",
      "GD iter. 388/499: loss=0.5302794730446274\n",
      "GD iter. 389/499: loss=0.5302602847800333\n",
      "GD iter. 390/499: loss=0.530228768979967\n",
      "GD iter. 391/499: loss=0.5301734832395637\n",
      "GD iter. 392/499: loss=0.5301653435736121\n",
      "GD iter. 393/499: loss=0.5301204905823654\n",
      "GD iter. 394/499: loss=0.5300886120524807\n",
      "GD iter. 395/499: loss=0.5300761881309698\n",
      "GD iter. 396/499: loss=0.5300390880606073\n",
      "GD iter. 397/499: loss=0.5300151299800969\n",
      "GD iter. 398/499: loss=0.529986133195866\n",
      "GD iter. 399/499: loss=0.5299954415702318\n",
      "GD iter. 400/499: loss=0.5299895400711456\n",
      "GD iter. 401/499: loss=0.529932361634402\n",
      "GD iter. 402/499: loss=0.5299029500861708\n",
      "GD iter. 403/499: loss=0.5298442347471088\n",
      "GD iter. 404/499: loss=0.5298010182666264\n",
      "GD iter. 405/499: loss=0.5298337703536145\n",
      "GD iter. 406/499: loss=0.5297914652412641\n",
      "GD iter. 407/499: loss=0.5297640630009621\n",
      "GD iter. 408/499: loss=0.5297498127243053\n",
      "GD iter. 409/499: loss=0.5297049394595217\n",
      "GD iter. 410/499: loss=0.5296569964665604\n",
      "GD iter. 411/499: loss=0.5296443141540645\n",
      "GD iter. 412/499: loss=0.5296227902830261\n",
      "GD iter. 413/499: loss=0.5296088059414105\n",
      "GD iter. 414/499: loss=0.5295936760229965\n",
      "GD iter. 415/499: loss=0.529523792519938\n",
      "GD iter. 416/499: loss=0.5294930324718155\n",
      "GD iter. 417/499: loss=0.5295144307014089\n",
      "GD iter. 418/499: loss=0.5294928815867715\n",
      "GD iter. 419/499: loss=0.5294569518923717\n",
      "GD iter. 420/499: loss=0.5294112249074794\n",
      "GD iter. 421/499: loss=0.5293856130734519\n",
      "GD iter. 422/499: loss=0.529353491229187\n",
      "GD iter. 423/499: loss=0.5293601897933315\n",
      "GD iter. 424/499: loss=0.5293339404019457\n",
      "GD iter. 425/499: loss=0.5293114681928641\n",
      "GD iter. 426/499: loss=0.5292997596089106\n",
      "GD iter. 427/499: loss=0.52927768433779\n",
      "GD iter. 428/499: loss=0.5292181461938597\n",
      "GD iter. 429/499: loss=0.5291915343492821\n",
      "GD iter. 430/499: loss=0.5291653691397882\n",
      "GD iter. 431/499: loss=0.5291193544127895\n",
      "GD iter. 432/499: loss=0.5291217776397231\n",
      "GD iter. 433/499: loss=0.5291504671933274\n",
      "GD iter. 434/499: loss=0.5290849739982969\n",
      "GD iter. 435/499: loss=0.5290424461471964\n",
      "GD iter. 436/499: loss=0.5290353098018418\n",
      "GD iter. 437/499: loss=0.5290205142366278\n",
      "GD iter. 438/499: loss=0.5289907618298444\n",
      "GD iter. 439/499: loss=0.528961460458331\n",
      "GD iter. 440/499: loss=0.5289530506170232\n",
      "GD iter. 441/499: loss=0.5289718285215238\n",
      "GD iter. 442/499: loss=0.5289073810929751\n",
      "GD iter. 443/499: loss=0.5288537492732683\n",
      "GD iter. 444/499: loss=0.5288335455537309\n",
      "GD iter. 445/499: loss=0.5288269681964579\n",
      "GD iter. 446/499: loss=0.5288453810749943\n",
      "GD iter. 447/499: loss=0.5287862279513821\n",
      "GD iter. 448/499: loss=0.5287644352455356\n",
      "GD iter. 449/499: loss=0.5287374596129795\n",
      "GD iter. 450/499: loss=0.5287391991538282\n",
      "GD iter. 451/499: loss=0.5286841586995397\n",
      "GD iter. 452/499: loss=0.5286879122099164\n",
      "GD iter. 453/499: loss=0.5286645784925779\n",
      "GD iter. 454/499: loss=0.5286416176726116\n",
      "GD iter. 455/499: loss=0.5286454088880191\n",
      "GD iter. 456/499: loss=0.5286221722132234\n",
      "GD iter. 457/499: loss=0.5285861554511492\n",
      "GD iter. 458/499: loss=0.5285376612022527\n",
      "GD iter. 459/499: loss=0.5285315926344997\n",
      "GD iter. 460/499: loss=0.5285594492044591\n",
      "GD iter. 461/499: loss=0.528547322387876\n",
      "GD iter. 462/499: loss=0.5284762505752131\n",
      "GD iter. 463/499: loss=0.5284329518158676\n",
      "GD iter. 464/499: loss=0.528417779246527\n",
      "GD iter. 465/499: loss=0.5284512638149423\n",
      "GD iter. 466/499: loss=0.528419439674212\n",
      "GD iter. 467/499: loss=0.5283549848525432\n",
      "GD iter. 468/499: loss=0.5283601280383818\n",
      "GD iter. 469/499: loss=0.528335481678128\n",
      "GD iter. 470/499: loss=0.5283158158841946\n",
      "GD iter. 471/499: loss=0.5283132642765209\n",
      "GD iter. 472/499: loss=0.5282688615203015\n",
      "GD iter. 473/499: loss=0.5282442140044695\n",
      "GD iter. 474/499: loss=0.5282257794073024\n",
      "GD iter. 475/499: loss=0.5282323559268816\n",
      "GD iter. 476/499: loss=0.5282068637963786\n",
      "GD iter. 477/499: loss=0.5282083484801009\n",
      "GD iter. 478/499: loss=0.528164200091342\n",
      "GD iter. 479/499: loss=0.5281201359002915\n",
      "GD iter. 480/499: loss=0.5281079794982432\n",
      "GD iter. 481/499: loss=0.5280867312049934\n",
      "GD iter. 482/499: loss=0.5280860479693549\n",
      "GD iter. 483/499: loss=0.5280779163151108\n",
      "GD iter. 484/499: loss=0.5280817389325267\n",
      "GD iter. 485/499: loss=0.5280419742711076\n",
      "GD iter. 486/499: loss=0.5279729294030425\n",
      "GD iter. 487/499: loss=0.5279686994400791\n",
      "GD iter. 488/499: loss=0.5279692611795326\n",
      "GD iter. 489/499: loss=0.5279506299781145\n",
      "GD iter. 490/499: loss=0.5279003232384971\n",
      "GD iter. 491/499: loss=0.5278980954342369\n",
      "GD iter. 492/499: loss=0.5279032791552984\n",
      "GD iter. 493/499: loss=0.5279047754348619\n",
      "GD iter. 494/499: loss=0.5278539545444652\n",
      "GD iter. 495/499: loss=0.5278381632269965\n",
      "GD iter. 496/499: loss=0.5278334676175288\n",
      "GD iter. 497/499: loss=0.5277840881964805\n",
      "GD iter. 498/499: loss=0.5277845739468076\n",
      "GD iter. 499/499: loss=0.527744409108239\n",
      "The Accuracy is: 0.6289\n",
      "The F1 score is: 0.3652\n",
      "The precision is: 0.2289\n",
      "The recall is: 0.9028\n",
      "GD iter. 0/499: loss=1.028851509013942\n",
      "GD iter. 1/499: loss=0.947184066882047\n",
      "GD iter. 2/499: loss=0.8663415837301506\n",
      "GD iter. 3/499: loss=0.7931478655319878\n",
      "GD iter. 4/499: loss=0.7422380072603375\n",
      "GD iter. 5/499: loss=0.7108123443814706\n",
      "GD iter. 6/499: loss=0.6887940490220441\n",
      "GD iter. 7/499: loss=0.6737860875589048\n",
      "GD iter. 8/499: loss=0.6629248627085079\n",
      "GD iter. 9/499: loss=0.653914288491201\n",
      "GD iter. 10/499: loss=0.6464307639462405\n",
      "GD iter. 11/499: loss=0.6402293624106637\n",
      "GD iter. 12/499: loss=0.6348322102743821\n",
      "GD iter. 13/499: loss=0.6303490131509351\n",
      "GD iter. 14/499: loss=0.6266549797169109\n",
      "GD iter. 15/499: loss=0.6235863992229473\n",
      "GD iter. 16/499: loss=0.6208489382128048\n",
      "GD iter. 17/499: loss=0.6183088133026978\n",
      "GD iter. 18/499: loss=0.6159191728103229\n",
      "GD iter. 19/499: loss=0.6136798366293309\n",
      "GD iter. 20/499: loss=0.6115858627557513\n",
      "GD iter. 21/499: loss=0.6095783536380236\n",
      "GD iter. 22/499: loss=0.6076876436593054\n",
      "GD iter. 23/499: loss=0.6059645575327292\n",
      "GD iter. 24/499: loss=0.6044417992339225\n",
      "GD iter. 25/499: loss=0.6029472863536047\n",
      "GD iter. 26/499: loss=0.6014856195944533\n",
      "GD iter. 27/499: loss=0.6001400592276873\n",
      "GD iter. 28/499: loss=0.5988574405503481\n",
      "GD iter. 29/499: loss=0.5976308681858579\n",
      "GD iter. 30/499: loss=0.5964809442208195\n",
      "GD iter. 31/499: loss=0.5953552798083408\n",
      "GD iter. 32/499: loss=0.594245057223741\n",
      "GD iter. 33/499: loss=0.5931474505052903\n",
      "GD iter. 34/499: loss=0.5920754356252914\n",
      "GD iter. 35/499: loss=0.5910704675050343\n",
      "GD iter. 36/499: loss=0.590093519245868\n",
      "GD iter. 37/499: loss=0.5891422298913238\n",
      "GD iter. 38/499: loss=0.5882190893065751\n",
      "GD iter. 39/499: loss=0.5873120024855678\n",
      "GD iter. 40/499: loss=0.5864506934195775\n",
      "GD iter. 41/499: loss=0.5856709211053576\n",
      "GD iter. 42/499: loss=0.5849353887959682\n",
      "GD iter. 43/499: loss=0.584210645573372\n",
      "GD iter. 44/499: loss=0.5835108389859006\n",
      "GD iter. 45/499: loss=0.5828344211581594\n",
      "GD iter. 46/499: loss=0.5821456477905244\n",
      "GD iter. 47/499: loss=0.5814865911523442\n",
      "GD iter. 48/499: loss=0.5808485791354512\n",
      "GD iter. 49/499: loss=0.5802163431124016\n",
      "GD iter. 50/499: loss=0.579596761380009\n",
      "GD iter. 51/499: loss=0.5789721248387564\n",
      "GD iter. 52/499: loss=0.5783579681363678\n",
      "GD iter. 53/499: loss=0.5777617727071281\n",
      "GD iter. 54/499: loss=0.577169810804012\n",
      "GD iter. 55/499: loss=0.5765972714729439\n",
      "GD iter. 56/499: loss=0.5760291257024921\n",
      "GD iter. 57/499: loss=0.5754709572164574\n",
      "GD iter. 58/499: loss=0.574926265260237\n",
      "GD iter. 59/499: loss=0.5744161898425293\n",
      "GD iter. 60/499: loss=0.5739275813241544\n",
      "GD iter. 61/499: loss=0.5734438312019671\n",
      "GD iter. 62/499: loss=0.5729692425113546\n",
      "GD iter. 63/499: loss=0.5725404771236073\n",
      "GD iter. 64/499: loss=0.5721060444308802\n",
      "GD iter. 65/499: loss=0.5716827368929548\n",
      "GD iter. 66/499: loss=0.5712856684748179\n",
      "GD iter. 67/499: loss=0.5708890735177186\n",
      "GD iter. 68/499: loss=0.5705137253781642\n",
      "GD iter. 69/499: loss=0.5701512786577866\n",
      "GD iter. 70/499: loss=0.5697880943442767\n",
      "GD iter. 71/499: loss=0.5694260698217088\n",
      "GD iter. 72/499: loss=0.5690688170095785\n",
      "GD iter. 73/499: loss=0.5687200692918246\n",
      "GD iter. 74/499: loss=0.5683671152954717\n",
      "GD iter. 75/499: loss=0.5680365581325229\n",
      "GD iter. 76/499: loss=0.5676876976611701\n",
      "GD iter. 77/499: loss=0.5673529564414606\n",
      "GD iter. 78/499: loss=0.5670167191735289\n",
      "GD iter. 79/499: loss=0.5666910904502513\n",
      "GD iter. 80/499: loss=0.5663689304115297\n",
      "GD iter. 81/499: loss=0.5660495075215785\n",
      "GD iter. 82/499: loss=0.5657440653436309\n",
      "GD iter. 83/499: loss=0.5654409552489528\n",
      "GD iter. 84/499: loss=0.5651510666101509\n",
      "GD iter. 85/499: loss=0.5648718898450917\n",
      "GD iter. 86/499: loss=0.5645608938415773\n",
      "GD iter. 87/499: loss=0.5642826283462348\n",
      "GD iter. 88/499: loss=0.5640100111530508\n",
      "GD iter. 89/499: loss=0.5637283683322253\n",
      "GD iter. 90/499: loss=0.5634521615641033\n",
      "GD iter. 91/499: loss=0.5631888419312319\n",
      "GD iter. 92/499: loss=0.5629123708676766\n",
      "GD iter. 93/499: loss=0.5626515961043085\n",
      "GD iter. 94/499: loss=0.5624090709989992\n",
      "GD iter. 95/499: loss=0.5621083844072303\n",
      "GD iter. 96/499: loss=0.5618404972331651\n",
      "GD iter. 97/499: loss=0.5615817638646814\n",
      "GD iter. 98/499: loss=0.5613450351849363\n",
      "GD iter. 99/499: loss=0.5611027884910137\n",
      "GD iter. 100/499: loss=0.5608529412089895\n",
      "GD iter. 101/499: loss=0.5606068663879192\n",
      "GD iter. 102/499: loss=0.5604044178859788\n",
      "GD iter. 103/499: loss=0.560170176992402\n",
      "GD iter. 104/499: loss=0.5599480121470592\n",
      "GD iter. 105/499: loss=0.5597419022929129\n",
      "GD iter. 106/499: loss=0.5595286526220151\n",
      "GD iter. 107/499: loss=0.5593076816249734\n",
      "GD iter. 108/499: loss=0.5590858617749593\n",
      "GD iter. 109/499: loss=0.5588787076005814\n",
      "GD iter. 110/499: loss=0.5587002632154267\n",
      "GD iter. 111/499: loss=0.5584650612375588\n",
      "GD iter. 112/499: loss=0.5582725361463751\n",
      "GD iter. 113/499: loss=0.5580529422300443\n",
      "GD iter. 114/499: loss=0.5578362035557285\n",
      "GD iter. 115/499: loss=0.5576418071600702\n",
      "GD iter. 116/499: loss=0.5574391045123377\n",
      "GD iter. 117/499: loss=0.557271913751159\n",
      "GD iter. 118/499: loss=0.5570545628626442\n",
      "GD iter. 119/499: loss=0.5568769622882996\n",
      "GD iter. 120/499: loss=0.5566855009368249\n",
      "GD iter. 121/499: loss=0.5564773920097605\n",
      "GD iter. 122/499: loss=0.5563156197929726\n",
      "GD iter. 123/499: loss=0.556103048535134\n",
      "GD iter. 124/499: loss=0.5559338368301538\n",
      "GD iter. 125/499: loss=0.5557409856646072\n",
      "GD iter. 126/499: loss=0.5555516076249161\n",
      "GD iter. 127/499: loss=0.5553665506413403\n",
      "GD iter. 128/499: loss=0.5551763010601409\n",
      "GD iter. 129/499: loss=0.5550009021825727\n",
      "GD iter. 130/499: loss=0.5548575407467782\n",
      "GD iter. 131/499: loss=0.5546764962185929\n",
      "GD iter. 132/499: loss=0.5544927440572884\n",
      "GD iter. 133/499: loss=0.5543122826339354\n",
      "GD iter. 134/499: loss=0.5541404168037487\n",
      "GD iter. 135/499: loss=0.5539765618062639\n",
      "GD iter. 136/499: loss=0.5538048324623964\n",
      "GD iter. 137/499: loss=0.5536444611056857\n",
      "GD iter. 138/499: loss=0.5534694383743266\n",
      "GD iter. 139/499: loss=0.553302236993323\n",
      "GD iter. 140/499: loss=0.553145061236788\n",
      "GD iter. 141/499: loss=0.5530090199293611\n",
      "GD iter. 142/499: loss=0.5528299073210707\n",
      "GD iter. 143/499: loss=0.5526575596298982\n",
      "GD iter. 144/499: loss=0.5524918684258818\n",
      "GD iter. 145/499: loss=0.5523351941527967\n",
      "GD iter. 146/499: loss=0.5521910043922189\n",
      "GD iter. 147/499: loss=0.5520140049811542\n",
      "GD iter. 148/499: loss=0.5518558926161661\n",
      "GD iter. 149/499: loss=0.5516932952511432\n",
      "GD iter. 150/499: loss=0.5515629616542619\n",
      "GD iter. 151/499: loss=0.5513838005526406\n",
      "GD iter. 152/499: loss=0.5512403339373485\n",
      "GD iter. 153/499: loss=0.5510765906309197\n",
      "GD iter. 154/499: loss=0.5509069740008042\n",
      "GD iter. 155/499: loss=0.5507855990743286\n",
      "GD iter. 156/499: loss=0.550652016678792\n",
      "GD iter. 157/499: loss=0.5504945788738055\n",
      "GD iter. 158/499: loss=0.5503616495128641\n",
      "GD iter. 159/499: loss=0.5502561353565608\n",
      "GD iter. 160/499: loss=0.5501300329215386\n",
      "GD iter. 161/499: loss=0.5499438192055793\n",
      "GD iter. 162/499: loss=0.549836645930301\n",
      "GD iter. 163/499: loss=0.5496964382464506\n",
      "GD iter. 164/499: loss=0.5495571783028504\n",
      "GD iter. 165/499: loss=0.5494386789853741\n",
      "GD iter. 166/499: loss=0.5492766376202208\n",
      "GD iter. 167/499: loss=0.549164127539396\n",
      "GD iter. 168/499: loss=0.549053140950794\n",
      "GD iter. 169/499: loss=0.5489066547487966\n",
      "GD iter. 170/499: loss=0.5487872240195641\n",
      "GD iter. 171/499: loss=0.5486634860876343\n",
      "GD iter. 172/499: loss=0.5485275972919496\n",
      "GD iter. 173/499: loss=0.548383499392184\n",
      "GD iter. 174/499: loss=0.5482848779895473\n",
      "GD iter. 175/499: loss=0.5481431303438996\n",
      "GD iter. 176/499: loss=0.5480157414539101\n",
      "GD iter. 177/499: loss=0.547920973510603\n",
      "GD iter. 178/499: loss=0.5477710746144072\n",
      "GD iter. 179/499: loss=0.5476443014194861\n",
      "GD iter. 180/499: loss=0.5475347112881365\n",
      "GD iter. 181/499: loss=0.5474279953115436\n",
      "GD iter. 182/499: loss=0.5472843230480295\n",
      "GD iter. 183/499: loss=0.5471818141967104\n",
      "GD iter. 184/499: loss=0.5470415715727016\n",
      "GD iter. 185/499: loss=0.5469209362978573\n",
      "GD iter. 186/499: loss=0.5468351496119486\n",
      "GD iter. 187/499: loss=0.5466926688014505\n",
      "GD iter. 188/499: loss=0.5465891006393748\n",
      "GD iter. 189/499: loss=0.5464935048393375\n",
      "GD iter. 190/499: loss=0.5463517933540837\n",
      "GD iter. 191/499: loss=0.5462579777307265\n",
      "GD iter. 192/499: loss=0.5461233860156753\n",
      "GD iter. 193/499: loss=0.546018024204915\n",
      "GD iter. 194/499: loss=0.5459293817691646\n",
      "GD iter. 195/499: loss=0.5457853520630057\n",
      "GD iter. 196/499: loss=0.5456737195556283\n",
      "GD iter. 197/499: loss=0.54557678801726\n",
      "GD iter. 198/499: loss=0.545468462621715\n",
      "GD iter. 199/499: loss=0.5453547282691955\n",
      "GD iter. 200/499: loss=0.5452481756940604\n",
      "GD iter. 201/499: loss=0.5451138972374032\n",
      "GD iter. 202/499: loss=0.5450196653895174\n",
      "GD iter. 203/499: loss=0.5449229748834234\n",
      "GD iter. 204/499: loss=0.5448072694487498\n",
      "GD iter. 205/499: loss=0.5447234056891748\n",
      "GD iter. 206/499: loss=0.5446113527750658\n",
      "GD iter. 207/499: loss=0.5445323374095794\n",
      "GD iter. 208/499: loss=0.5444145846456536\n",
      "GD iter. 209/499: loss=0.5443302911653757\n",
      "GD iter. 210/499: loss=0.5442036960745512\n",
      "GD iter. 211/499: loss=0.5441680226652619\n",
      "GD iter. 212/499: loss=0.5440332938418001\n",
      "GD iter. 213/499: loss=0.5439232974607611\n",
      "GD iter. 214/499: loss=0.5438269021500727\n",
      "GD iter. 215/499: loss=0.5437426855901684\n",
      "GD iter. 216/499: loss=0.5436572605804624\n",
      "GD iter. 217/499: loss=0.5435618751170265\n",
      "GD iter. 218/499: loss=0.543500583112861\n",
      "GD iter. 219/499: loss=0.5433649432365346\n",
      "GD iter. 220/499: loss=0.5432975602183425\n",
      "GD iter. 221/499: loss=0.5431770830863699\n",
      "GD iter. 222/499: loss=0.5431205323506515\n",
      "GD iter. 223/499: loss=0.5430540042825057\n",
      "GD iter. 224/499: loss=0.5429373822576544\n",
      "GD iter. 225/499: loss=0.5428445661655644\n",
      "GD iter. 226/499: loss=0.5427329656412171\n",
      "GD iter. 227/499: loss=0.5426835614365205\n",
      "GD iter. 228/499: loss=0.542585635961281\n",
      "GD iter. 229/499: loss=0.5424758373372496\n",
      "GD iter. 230/499: loss=0.5423982422386988\n",
      "GD iter. 231/499: loss=0.5423299706770528\n",
      "GD iter. 232/499: loss=0.5422228974840253\n",
      "GD iter. 233/499: loss=0.5421357728852668\n",
      "GD iter. 234/499: loss=0.5420367922105873\n",
      "GD iter. 235/499: loss=0.5419784118830945\n",
      "GD iter. 236/499: loss=0.5418804081785885\n",
      "GD iter. 237/499: loss=0.541806927377468\n",
      "GD iter. 238/499: loss=0.5417171484423569\n",
      "GD iter. 239/499: loss=0.5416384757247558\n",
      "GD iter. 240/499: loss=0.5415613343495638\n",
      "GD iter. 241/499: loss=0.5414747943251932\n",
      "GD iter. 242/499: loss=0.5414090692892314\n",
      "GD iter. 243/499: loss=0.5413050533208325\n",
      "GD iter. 244/499: loss=0.5412446468052016\n",
      "GD iter. 245/499: loss=0.5411653859727267\n",
      "GD iter. 246/499: loss=0.5410646082537468\n",
      "GD iter. 247/499: loss=0.5409897766302241\n",
      "GD iter. 248/499: loss=0.5409154298109642\n",
      "GD iter. 249/499: loss=0.5408740550101266\n",
      "GD iter. 250/499: loss=0.5407774385840461\n",
      "GD iter. 251/499: loss=0.5406871801275703\n",
      "GD iter. 252/499: loss=0.5406109706526918\n",
      "GD iter. 253/499: loss=0.5405414177358928\n",
      "GD iter. 254/499: loss=0.5404895441552987\n",
      "GD iter. 255/499: loss=0.5404175791883623\n",
      "GD iter. 256/499: loss=0.5403214695254335\n",
      "GD iter. 257/499: loss=0.54023835101034\n",
      "GD iter. 258/499: loss=0.5401564609300288\n",
      "GD iter. 259/499: loss=0.540109555680732\n",
      "GD iter. 260/499: loss=0.5400534246044495\n",
      "GD iter. 261/499: loss=0.5399991286670095\n",
      "GD iter. 262/499: loss=0.5398871261058953\n",
      "GD iter. 263/499: loss=0.5398036606738643\n",
      "GD iter. 264/499: loss=0.5397718770298264\n",
      "GD iter. 265/499: loss=0.5397332638405532\n",
      "GD iter. 266/499: loss=0.5396185798344005\n",
      "GD iter. 267/499: loss=0.5395392626335294\n",
      "GD iter. 268/499: loss=0.5394548351237957\n",
      "GD iter. 269/499: loss=0.5393936926980156\n",
      "GD iter. 270/499: loss=0.5393449764927053\n",
      "GD iter. 271/499: loss=0.539274402129756\n",
      "GD iter. 272/499: loss=0.5392070902126908\n",
      "GD iter. 273/499: loss=0.5391907944455193\n",
      "GD iter. 274/499: loss=0.5390911290275986\n",
      "GD iter. 275/499: loss=0.5390266244440817\n",
      "GD iter. 276/499: loss=0.5389695769275799\n",
      "GD iter. 277/499: loss=0.5388787873373873\n",
      "GD iter. 278/499: loss=0.5388652917185461\n",
      "GD iter. 279/499: loss=0.5387958955897403\n",
      "GD iter. 280/499: loss=0.5386982483637837\n",
      "GD iter. 281/499: loss=0.5386312938143861\n",
      "GD iter. 282/499: loss=0.5385605353845496\n",
      "GD iter. 283/499: loss=0.5385205128464149\n",
      "GD iter. 284/499: loss=0.5384735130744042\n",
      "GD iter. 285/499: loss=0.5384664350128855\n",
      "GD iter. 286/499: loss=0.5383236756614616\n",
      "GD iter. 287/499: loss=0.5382921578294932\n",
      "GD iter. 288/499: loss=0.5382229216750415\n",
      "GD iter. 289/499: loss=0.5381736042091512\n",
      "GD iter. 290/499: loss=0.5381232365862875\n",
      "GD iter. 291/499: loss=0.5380570146472287\n",
      "GD iter. 292/499: loss=0.5380008585444118\n",
      "GD iter. 293/499: loss=0.5379373114726324\n",
      "GD iter. 294/499: loss=0.5378922994731717\n",
      "GD iter. 295/499: loss=0.5378315151055757\n",
      "GD iter. 296/499: loss=0.5377732554631103\n",
      "GD iter. 297/499: loss=0.5376953198983797\n",
      "GD iter. 298/499: loss=0.5376785790615142\n",
      "GD iter. 299/499: loss=0.5376058712459902\n",
      "GD iter. 300/499: loss=0.5375749302609241\n",
      "GD iter. 301/499: loss=0.5375531647880387\n",
      "GD iter. 302/499: loss=0.5374547489778063\n",
      "GD iter. 303/499: loss=0.5373757014726009\n",
      "GD iter. 304/499: loss=0.537314817362701\n",
      "GD iter. 305/499: loss=0.5372799782857244\n",
      "GD iter. 306/499: loss=0.5372510686033221\n",
      "GD iter. 307/499: loss=0.5371840644356852\n",
      "GD iter. 308/499: loss=0.5371509052839333\n",
      "GD iter. 309/499: loss=0.5370944321919506\n",
      "GD iter. 310/499: loss=0.5370257270974101\n",
      "GD iter. 311/499: loss=0.5369755162692474\n",
      "GD iter. 312/499: loss=0.5369137478851959\n",
      "GD iter. 313/499: loss=0.5368819352277213\n",
      "GD iter. 314/499: loss=0.536862254338511\n",
      "GD iter. 315/499: loss=0.5368194443106884\n",
      "GD iter. 316/499: loss=0.5367720388312904\n",
      "GD iter. 317/499: loss=0.5366860799313724\n",
      "GD iter. 318/499: loss=0.5366166787978485\n",
      "GD iter. 319/499: loss=0.5365801353049563\n",
      "GD iter. 320/499: loss=0.5365636868593565\n",
      "GD iter. 321/499: loss=0.5365287707796184\n",
      "GD iter. 322/499: loss=0.536473199082353\n",
      "GD iter. 323/499: loss=0.5364262833422697\n",
      "GD iter. 324/499: loss=0.5363773805193728\n",
      "GD iter. 325/499: loss=0.536311167283681\n",
      "GD iter. 326/499: loss=0.53629986295802\n",
      "GD iter. 327/499: loss=0.5362253394036905\n",
      "GD iter. 328/499: loss=0.536159201461332\n",
      "GD iter. 329/499: loss=0.5361455388233711\n",
      "GD iter. 330/499: loss=0.5361667592972665\n",
      "GD iter. 331/499: loss=0.5360573437973217\n",
      "GD iter. 332/499: loss=0.5360088059310834\n",
      "GD iter. 333/499: loss=0.5359869814277707\n",
      "GD iter. 334/499: loss=0.5359316989624221\n",
      "GD iter. 335/499: loss=0.5359096930639408\n",
      "GD iter. 336/499: loss=0.5358855600651647\n",
      "GD iter. 337/499: loss=0.5358068642287674\n",
      "GD iter. 338/499: loss=0.5357719346027164\n",
      "GD iter. 339/499: loss=0.5357255362645238\n",
      "GD iter. 340/499: loss=0.5356536578571175\n",
      "GD iter. 341/499: loss=0.5356472075308913\n",
      "GD iter. 342/499: loss=0.5356219741834193\n",
      "GD iter. 343/499: loss=0.5355908263396089\n",
      "GD iter. 344/499: loss=0.5355088635067271\n",
      "GD iter. 345/499: loss=0.5354547432907406\n",
      "GD iter. 346/499: loss=0.5354148301474669\n",
      "GD iter. 347/499: loss=0.5353577844630704\n",
      "GD iter. 348/499: loss=0.5353179445029362\n",
      "GD iter. 349/499: loss=0.5352729518523272\n",
      "GD iter. 350/499: loss=0.5352713365459087\n",
      "GD iter. 351/499: loss=0.5352111261792987\n",
      "GD iter. 352/499: loss=0.5351550588811064\n",
      "GD iter. 353/499: loss=0.5351397449816268\n",
      "GD iter. 354/499: loss=0.5351033109054192\n",
      "GD iter. 355/499: loss=0.5350556092681664\n",
      "GD iter. 356/499: loss=0.5350432362568363\n",
      "GD iter. 357/499: loss=0.5349886266958624\n",
      "GD iter. 358/499: loss=0.5349864247929632\n",
      "GD iter. 359/499: loss=0.5349469724166849\n",
      "GD iter. 360/499: loss=0.534933695379352\n",
      "GD iter. 361/499: loss=0.5348401506005745\n",
      "GD iter. 362/499: loss=0.534807446813911\n",
      "GD iter. 363/499: loss=0.5348019196504408\n",
      "GD iter. 364/499: loss=0.5347861868167609\n",
      "GD iter. 365/499: loss=0.5347105556265489\n",
      "GD iter. 366/499: loss=0.5346870550279508\n",
      "GD iter. 367/499: loss=0.5345813478026294\n",
      "GD iter. 368/499: loss=0.5345745230139046\n",
      "GD iter. 369/499: loss=0.5345401113587354\n",
      "GD iter. 370/499: loss=0.53446673094278\n",
      "GD iter. 371/499: loss=0.5344644713797455\n",
      "GD iter. 372/499: loss=0.5344565407204446\n",
      "GD iter. 373/499: loss=0.534481434123111\n",
      "GD iter. 374/499: loss=0.5343748856784706\n",
      "GD iter. 375/499: loss=0.5343934443474097\n",
      "GD iter. 376/499: loss=0.5343209005444549\n",
      "GD iter. 377/499: loss=0.5342635760201837\n",
      "GD iter. 378/499: loss=0.5342695009410429\n",
      "GD iter. 379/499: loss=0.5342101399168422\n",
      "GD iter. 380/499: loss=0.534150755754026\n",
      "GD iter. 381/499: loss=0.5341315967968474\n",
      "GD iter. 382/499: loss=0.5341080099921653\n",
      "GD iter. 383/499: loss=0.5340726376396349\n",
      "GD iter. 384/499: loss=0.5340541903619613\n",
      "GD iter. 385/499: loss=0.5340222290651335\n",
      "GD iter. 386/499: loss=0.5339846426582853\n",
      "GD iter. 387/499: loss=0.5339357466838505\n",
      "GD iter. 388/499: loss=0.5339302361825821\n",
      "GD iter. 389/499: loss=0.5338782942808825\n",
      "GD iter. 390/499: loss=0.5338562065576038\n",
      "GD iter. 391/499: loss=0.5337956593116538\n",
      "GD iter. 392/499: loss=0.5338125350646203\n",
      "GD iter. 393/499: loss=0.5337376814886714\n",
      "GD iter. 394/499: loss=0.5336803717426367\n",
      "GD iter. 395/499: loss=0.5336563008637208\n",
      "GD iter. 396/499: loss=0.5336289322563679\n",
      "GD iter. 397/499: loss=0.5336075957514281\n",
      "GD iter. 398/499: loss=0.5336179061874352\n",
      "GD iter. 399/499: loss=0.5335635796250294\n",
      "GD iter. 400/499: loss=0.5335490883861203\n",
      "GD iter. 401/499: loss=0.533522765447312\n",
      "GD iter. 402/499: loss=0.533532738228311\n",
      "GD iter. 403/499: loss=0.5334650448917087\n",
      "GD iter. 404/499: loss=0.5333941068769513\n",
      "GD iter. 405/499: loss=0.5333634966193044\n",
      "GD iter. 406/499: loss=0.5333610966531966\n",
      "GD iter. 407/499: loss=0.5333828699609183\n",
      "GD iter. 408/499: loss=0.5333399465340836\n",
      "GD iter. 409/499: loss=0.5332786374412665\n",
      "GD iter. 410/499: loss=0.5332524907543047\n",
      "GD iter. 411/499: loss=0.5332358636249331\n",
      "GD iter. 412/499: loss=0.5332174318096886\n",
      "GD iter. 413/499: loss=0.5332084677179503\n",
      "GD iter. 414/499: loss=0.5331740728797826\n",
      "GD iter. 415/499: loss=0.5331397167488945\n",
      "GD iter. 416/499: loss=0.5331159686486453\n",
      "GD iter. 417/499: loss=0.5330610305708455\n",
      "GD iter. 418/499: loss=0.5330281596721202\n",
      "GD iter. 419/499: loss=0.533003256389006\n",
      "GD iter. 420/499: loss=0.5329992784381561\n",
      "GD iter. 421/499: loss=0.5330059138008817\n",
      "GD iter. 422/499: loss=0.5329688747923955\n",
      "GD iter. 423/499: loss=0.5329146827511345\n",
      "GD iter. 424/499: loss=0.5329027516375894\n",
      "GD iter. 425/499: loss=0.5328894945117102\n",
      "GD iter. 426/499: loss=0.5328510491466628\n",
      "GD iter. 427/499: loss=0.5328271040615936\n",
      "GD iter. 428/499: loss=0.5328100431831815\n",
      "GD iter. 429/499: loss=0.5327859415591245\n",
      "GD iter. 430/499: loss=0.532755292844574\n",
      "GD iter. 431/499: loss=0.5326983114435788\n",
      "GD iter. 432/499: loss=0.532678475773533\n",
      "GD iter. 433/499: loss=0.5326771589924136\n",
      "GD iter. 434/499: loss=0.5326328204132872\n",
      "GD iter. 435/499: loss=0.5326278532060184\n",
      "GD iter. 436/499: loss=0.5325653231510373\n",
      "GD iter. 437/499: loss=0.5326244087471801\n",
      "GD iter. 438/499: loss=0.5325982017903442\n",
      "GD iter. 439/499: loss=0.5325378354234721\n",
      "GD iter. 440/499: loss=0.53247654503135\n",
      "GD iter. 441/499: loss=0.5324701321323507\n",
      "GD iter. 442/499: loss=0.5324477073867298\n",
      "GD iter. 443/499: loss=0.5323967085530134\n",
      "GD iter. 444/499: loss=0.5324222098052751\n",
      "GD iter. 445/499: loss=0.5324352894711741\n",
      "GD iter. 446/499: loss=0.532384690605322\n",
      "GD iter. 447/499: loss=0.5323418960902281\n",
      "GD iter. 448/499: loss=0.5322757147763469\n",
      "GD iter. 449/499: loss=0.5322538462461177\n",
      "GD iter. 450/499: loss=0.5322406602078998\n",
      "GD iter. 451/499: loss=0.5322233368223406\n",
      "GD iter. 452/499: loss=0.5321800841042941\n",
      "GD iter. 453/499: loss=0.5322340579237279\n",
      "GD iter. 454/499: loss=0.5322082268335219\n",
      "GD iter. 455/499: loss=0.5321841529839169\n",
      "GD iter. 456/499: loss=0.5321936501625202\n",
      "GD iter. 457/499: loss=0.5321046875852369\n",
      "GD iter. 458/499: loss=0.5320629119347845\n",
      "GD iter. 459/499: loss=0.5320556893154504\n",
      "GD iter. 460/499: loss=0.5320287991011978\n",
      "GD iter. 461/499: loss=0.5320132622829471\n",
      "GD iter. 462/499: loss=0.5320625472920203\n",
      "GD iter. 463/499: loss=0.5320073005626079\n",
      "GD iter. 464/499: loss=0.5319702729975965\n",
      "GD iter. 465/499: loss=0.531975199303561\n",
      "GD iter. 466/499: loss=0.5320095924016914\n",
      "GD iter. 467/499: loss=0.5319138598992232\n",
      "GD iter. 468/499: loss=0.5318638636304234\n",
      "GD iter. 469/499: loss=0.5318583152481717\n",
      "GD iter. 470/499: loss=0.5318685709193912\n",
      "GD iter. 471/499: loss=0.5319192168390261\n",
      "GD iter. 472/499: loss=0.5318498714385317\n",
      "GD iter. 473/499: loss=0.531763274316971\n",
      "GD iter. 474/499: loss=0.5317811840936328\n",
      "GD iter. 475/499: loss=0.5317797001870596\n",
      "GD iter. 476/499: loss=0.5318292799226376\n",
      "GD iter. 477/499: loss=0.531715197340372\n",
      "GD iter. 478/499: loss=0.5317023050442283\n",
      "GD iter. 479/499: loss=0.531710997588624\n",
      "GD iter. 480/499: loss=0.5316771537909866\n",
      "GD iter. 481/499: loss=0.5316509601586246\n",
      "GD iter. 482/499: loss=0.5316724686480285\n",
      "GD iter. 483/499: loss=0.5316330762997038\n",
      "GD iter. 484/499: loss=0.5316467835488999\n",
      "GD iter. 485/499: loss=0.5315963564970908\n",
      "GD iter. 486/499: loss=0.531607447641408\n",
      "GD iter. 487/499: loss=0.5315388203195661\n",
      "GD iter. 488/499: loss=0.5315275406646063\n",
      "GD iter. 489/499: loss=0.5314932462835887\n",
      "GD iter. 490/499: loss=0.5314704052670868\n",
      "GD iter. 491/499: loss=0.5314703377068409\n",
      "GD iter. 492/499: loss=0.5314900737679779\n",
      "GD iter. 493/499: loss=0.5314369812230421\n",
      "GD iter. 494/499: loss=0.5314556328180499\n",
      "GD iter. 495/499: loss=0.5314085160922949\n",
      "GD iter. 496/499: loss=0.5314445739867862\n",
      "GD iter. 497/499: loss=0.5313482540859075\n",
      "GD iter. 498/499: loss=0.5313291707285541\n",
      "GD iter. 499/499: loss=0.531347973729753\n",
      "The Accuracy is: 0.6273\n",
      "The F1 score is: 0.3058\n",
      "The precision is: 0.1852\n",
      "The recall is: 0.8772\n",
      "GD iter. 0/499: loss=1.033608864065813\n",
      "GD iter. 1/499: loss=0.9529437708629152\n",
      "GD iter. 2/499: loss=0.8735291726782103\n",
      "GD iter. 3/499: loss=0.7988989886559552\n",
      "GD iter. 4/499: loss=0.7442309424403303\n",
      "GD iter. 5/499: loss=0.7082108962545873\n",
      "GD iter. 6/499: loss=0.6841369051243693\n",
      "GD iter. 7/499: loss=0.6680107655378644\n",
      "GD iter. 8/499: loss=0.6557490006035505\n",
      "GD iter. 9/499: loss=0.646581116148859\n",
      "GD iter. 10/499: loss=0.6392700215598496\n",
      "GD iter. 11/499: loss=0.6336577516133161\n",
      "GD iter. 12/499: loss=0.6288448031880196\n",
      "GD iter. 13/499: loss=0.6244125609036179\n",
      "GD iter. 14/499: loss=0.62045971429522\n",
      "GD iter. 15/499: loss=0.6169169120823874\n",
      "GD iter. 16/499: loss=0.6137766130072646\n",
      "GD iter. 17/499: loss=0.6110387212688159\n",
      "GD iter. 18/499: loss=0.608540260358064\n",
      "GD iter. 19/499: loss=0.6061934456394789\n",
      "GD iter. 20/499: loss=0.6039846397877353\n",
      "GD iter. 21/499: loss=0.6020712104727082\n",
      "GD iter. 22/499: loss=0.6002852561250697\n",
      "GD iter. 23/499: loss=0.5985906132082913\n",
      "GD iter. 24/499: loss=0.5970870199283688\n",
      "GD iter. 25/499: loss=0.5957357390003648\n",
      "GD iter. 26/499: loss=0.5944374193032499\n",
      "GD iter. 27/499: loss=0.5931944603206646\n",
      "GD iter. 28/499: loss=0.5919984428221415\n",
      "GD iter. 29/499: loss=0.5908255036055063\n",
      "GD iter. 30/499: loss=0.5897058569963853\n",
      "GD iter. 31/499: loss=0.5885972977471049\n",
      "GD iter. 32/499: loss=0.5874984299586237\n",
      "GD iter. 33/499: loss=0.5864433661872002\n",
      "GD iter. 34/499: loss=0.5854346946293835\n",
      "GD iter. 35/499: loss=0.5844315500985726\n",
      "GD iter. 36/499: loss=0.5834358921318364\n",
      "GD iter. 37/499: loss=0.5824639532440861\n",
      "GD iter. 38/499: loss=0.5815369457854882\n",
      "GD iter. 39/499: loss=0.5806366608102589\n",
      "GD iter. 40/499: loss=0.5797567161286922\n",
      "GD iter. 41/499: loss=0.5789076679227226\n",
      "GD iter. 42/499: loss=0.5781346725795239\n",
      "GD iter. 43/499: loss=0.577385017481341\n",
      "GD iter. 44/499: loss=0.5766660796055592\n",
      "GD iter. 45/499: loss=0.5759690517783567\n",
      "GD iter. 46/499: loss=0.5752996073254968\n",
      "GD iter. 47/499: loss=0.5746423875856067\n",
      "GD iter. 48/499: loss=0.574024837503289\n",
      "GD iter. 49/499: loss=0.573413440213944\n",
      "GD iter. 50/499: loss=0.5728182675490423\n",
      "GD iter. 51/499: loss=0.5722223169037655\n",
      "GD iter. 52/499: loss=0.5716247626854344\n",
      "GD iter. 53/499: loss=0.5710453493652395\n",
      "GD iter. 54/499: loss=0.5705053455621908\n",
      "GD iter. 55/499: loss=0.56997312923231\n",
      "GD iter. 56/499: loss=0.5694342225468474\n",
      "GD iter. 57/499: loss=0.5689161699869063\n",
      "GD iter. 58/499: loss=0.5684035906864043\n",
      "GD iter. 59/499: loss=0.567898945653402\n",
      "GD iter. 60/499: loss=0.567422163422258\n",
      "GD iter. 61/499: loss=0.5669682982053812\n",
      "GD iter. 62/499: loss=0.5665282818590611\n",
      "GD iter. 63/499: loss=0.5660855806241628\n",
      "GD iter. 64/499: loss=0.565645839334372\n",
      "GD iter. 65/499: loss=0.5652087643352948\n",
      "GD iter. 66/499: loss=0.5647741347265072\n",
      "GD iter. 67/499: loss=0.5643456542374834\n",
      "GD iter. 68/499: loss=0.5639107916486341\n",
      "GD iter. 69/499: loss=0.5634897285811529\n",
      "GD iter. 70/499: loss=0.5630766407000165\n",
      "GD iter. 71/499: loss=0.56267382976783\n",
      "GD iter. 72/499: loss=0.5622690053002446\n",
      "GD iter. 73/499: loss=0.5618795861259328\n",
      "GD iter. 74/499: loss=0.5615040020123219\n",
      "GD iter. 75/499: loss=0.5611359023838834\n",
      "GD iter. 76/499: loss=0.5607668065622722\n",
      "GD iter. 77/499: loss=0.5604194858933148\n",
      "GD iter. 78/499: loss=0.560064855549748\n",
      "GD iter. 79/499: loss=0.5597157670093175\n",
      "GD iter. 80/499: loss=0.5593780139849559\n",
      "GD iter. 81/499: loss=0.5590406248380084\n",
      "GD iter. 82/499: loss=0.5587113317372375\n",
      "GD iter. 83/499: loss=0.5583790066044799\n",
      "GD iter. 84/499: loss=0.5580429681352376\n",
      "GD iter. 85/499: loss=0.5577214538451043\n",
      "GD iter. 86/499: loss=0.5573961958296844\n",
      "GD iter. 87/499: loss=0.5570923853371484\n",
      "GD iter. 88/499: loss=0.5567941600534757\n",
      "GD iter. 89/499: loss=0.5565035535758498\n",
      "GD iter. 90/499: loss=0.5562256994531162\n",
      "GD iter. 91/499: loss=0.555942157410039\n",
      "GD iter. 92/499: loss=0.5556830732230782\n",
      "GD iter. 93/499: loss=0.5554358752947335\n",
      "GD iter. 94/499: loss=0.5551857326987308\n",
      "GD iter. 95/499: loss=0.5549362673119167\n",
      "GD iter. 96/499: loss=0.5547015258826479\n",
      "GD iter. 97/499: loss=0.5544571250071886\n",
      "GD iter. 98/499: loss=0.5542281741575389\n",
      "GD iter. 99/499: loss=0.5539905840756274\n",
      "GD iter. 100/499: loss=0.5537801170911104\n",
      "GD iter. 101/499: loss=0.5535454570598415\n",
      "GD iter. 102/499: loss=0.553332882114188\n",
      "GD iter. 103/499: loss=0.5531104622496225\n",
      "GD iter. 104/499: loss=0.5529179656952402\n",
      "GD iter. 105/499: loss=0.5527234537331539\n",
      "GD iter. 106/499: loss=0.5525379864234056\n",
      "GD iter. 107/499: loss=0.5523297389522285\n",
      "GD iter. 108/499: loss=0.5521477687202403\n",
      "GD iter. 109/499: loss=0.5519440538621327\n",
      "GD iter. 110/499: loss=0.5517627667971527\n",
      "GD iter. 111/499: loss=0.5515651895785382\n",
      "GD iter. 112/499: loss=0.5513703262112875\n",
      "GD iter. 113/499: loss=0.551205065511536\n",
      "GD iter. 114/499: loss=0.5510182326635442\n",
      "GD iter. 115/499: loss=0.550832431692881\n",
      "GD iter. 116/499: loss=0.5506530362581202\n",
      "GD iter. 117/499: loss=0.5504695564957685\n",
      "GD iter. 118/499: loss=0.5502965284578675\n",
      "GD iter. 119/499: loss=0.5501258301068022\n",
      "GD iter. 120/499: loss=0.54996708612416\n",
      "GD iter. 121/499: loss=0.5497891489132563\n",
      "GD iter. 122/499: loss=0.549613461243427\n",
      "GD iter. 123/499: loss=0.5494379803978103\n",
      "GD iter. 124/499: loss=0.5492890774595487\n",
      "GD iter. 125/499: loss=0.549107570150366\n",
      "GD iter. 126/499: loss=0.5489510131452375\n",
      "GD iter. 127/499: loss=0.5488263919739262\n",
      "GD iter. 128/499: loss=0.5486735034718115\n",
      "GD iter. 129/499: loss=0.5485102507850241\n",
      "GD iter. 130/499: loss=0.5483422571333822\n",
      "GD iter. 131/499: loss=0.5481788657060048\n",
      "GD iter. 132/499: loss=0.5480500485956892\n",
      "GD iter. 133/499: loss=0.5479045762415936\n",
      "GD iter. 134/499: loss=0.5477367368464034\n",
      "GD iter. 135/499: loss=0.5475787151324921\n",
      "GD iter. 136/499: loss=0.547419307080315\n",
      "GD iter. 137/499: loss=0.5472905411630284\n",
      "GD iter. 138/499: loss=0.5471642918058804\n",
      "GD iter. 139/499: loss=0.5470254657881632\n",
      "GD iter. 140/499: loss=0.5468646996921043\n",
      "GD iter. 141/499: loss=0.5467267650013262\n",
      "GD iter. 142/499: loss=0.5466086242383595\n",
      "GD iter. 143/499: loss=0.5464934862415082\n",
      "GD iter. 144/499: loss=0.5463549381840532\n",
      "GD iter. 145/499: loss=0.5461965710833336\n",
      "GD iter. 146/499: loss=0.5460624124106864\n",
      "GD iter. 147/499: loss=0.5459374707293994\n",
      "GD iter. 148/499: loss=0.5458157452944759\n",
      "GD iter. 149/499: loss=0.5457132384686627\n",
      "GD iter. 150/499: loss=0.5455607978156978\n",
      "GD iter. 151/499: loss=0.545429205220761\n",
      "GD iter. 152/499: loss=0.5453069269573337\n",
      "GD iter. 153/499: loss=0.5451751606027244\n",
      "GD iter. 154/499: loss=0.5450697392912616\n",
      "GD iter. 155/499: loss=0.5449460784882658\n",
      "GD iter. 156/499: loss=0.5448096709514483\n",
      "GD iter. 157/499: loss=0.5446706640925838\n",
      "GD iter. 158/499: loss=0.5445542102403693\n",
      "GD iter. 159/499: loss=0.5444258278187613\n",
      "GD iter. 160/499: loss=0.5443315550999853\n",
      "GD iter. 161/499: loss=0.544206004612419\n",
      "GD iter. 162/499: loss=0.544068714909092\n",
      "GD iter. 163/499: loss=0.5439463012889204\n",
      "GD iter. 164/499: loss=0.5438406922963223\n",
      "GD iter. 165/499: loss=0.5437265461728166\n",
      "GD iter. 166/499: loss=0.5436022472055397\n",
      "GD iter. 167/499: loss=0.5434679523799573\n",
      "GD iter. 168/499: loss=0.543353661808901\n",
      "GD iter. 169/499: loss=0.5432288872768912\n",
      "GD iter. 170/499: loss=0.5431133795134435\n",
      "GD iter. 171/499: loss=0.5430214037656329\n",
      "GD iter. 172/499: loss=0.5428893997367213\n",
      "GD iter. 173/499: loss=0.5427659488756472\n",
      "GD iter. 174/499: loss=0.54263626242905\n",
      "GD iter. 175/499: loss=0.5425490740942661\n",
      "GD iter. 176/499: loss=0.5424265445828341\n",
      "GD iter. 177/499: loss=0.5423431432701511\n",
      "GD iter. 178/499: loss=0.5422106703981542\n",
      "GD iter. 179/499: loss=0.5420895730291689\n",
      "GD iter. 180/499: loss=0.5419986811965629\n",
      "GD iter. 181/499: loss=0.5418608082384345\n",
      "GD iter. 182/499: loss=0.5417798526366168\n",
      "GD iter. 183/499: loss=0.5416694071924709\n",
      "GD iter. 184/499: loss=0.541566781255561\n",
      "GD iter. 185/499: loss=0.5414722907578352\n",
      "GD iter. 186/499: loss=0.5413776537985764\n",
      "GD iter. 187/499: loss=0.5412532136513378\n",
      "GD iter. 188/499: loss=0.5411537650172781\n",
      "GD iter. 189/499: loss=0.541055009249574\n",
      "GD iter. 190/499: loss=0.5409681466245763\n",
      "GD iter. 191/499: loss=0.5408602744467729\n",
      "GD iter. 192/499: loss=0.5407663877909591\n",
      "GD iter. 193/499: loss=0.5406633322356851\n",
      "GD iter. 194/499: loss=0.5405626847531438\n",
      "GD iter. 195/499: loss=0.5404674192632304\n",
      "GD iter. 196/499: loss=0.5403817192717929\n",
      "GD iter. 197/499: loss=0.5402886837000258\n",
      "GD iter. 198/499: loss=0.5401990342762202\n",
      "GD iter. 199/499: loss=0.5401128315828615\n",
      "GD iter. 200/499: loss=0.540059532931781\n",
      "GD iter. 201/499: loss=0.5399726066041367\n",
      "GD iter. 202/499: loss=0.5398885576265513\n",
      "GD iter. 203/499: loss=0.5397906813121327\n",
      "GD iter. 204/499: loss=0.5397086184158958\n",
      "GD iter. 205/499: loss=0.539628256344932\n",
      "GD iter. 206/499: loss=0.5395621116164216\n",
      "GD iter. 207/499: loss=0.5394930511144257\n",
      "GD iter. 208/499: loss=0.5393936190922474\n",
      "GD iter. 209/499: loss=0.5393132683596487\n",
      "GD iter. 210/499: loss=0.5392369933897911\n",
      "GD iter. 211/499: loss=0.5391673764967568\n",
      "GD iter. 212/499: loss=0.5391055570286352\n",
      "GD iter. 213/499: loss=0.5390636097788876\n",
      "GD iter. 214/499: loss=0.5389497188354245\n",
      "GD iter. 215/499: loss=0.5388663660225503\n",
      "GD iter. 216/499: loss=0.5388001384167039\n",
      "GD iter. 217/499: loss=0.5387294569153872\n",
      "GD iter. 218/499: loss=0.5386647032014336\n",
      "GD iter. 219/499: loss=0.5385725357296514\n",
      "GD iter. 220/499: loss=0.5385069636135762\n",
      "GD iter. 221/499: loss=0.5384311644670114\n",
      "GD iter. 222/499: loss=0.538394550380631\n",
      "GD iter. 223/499: loss=0.5383010915944157\n",
      "GD iter. 224/499: loss=0.5382657850203413\n",
      "GD iter. 225/499: loss=0.5382118436246046\n",
      "GD iter. 226/499: loss=0.5381185208950805\n",
      "GD iter. 227/499: loss=0.5380533675147152\n",
      "GD iter. 228/499: loss=0.5379906343943925\n",
      "GD iter. 229/499: loss=0.5379228917426895\n",
      "GD iter. 230/499: loss=0.5378390975063692\n",
      "GD iter. 231/499: loss=0.5377942099259027\n",
      "GD iter. 232/499: loss=0.537737302095838\n",
      "GD iter. 233/499: loss=0.5376676586468726\n",
      "GD iter. 234/499: loss=0.537597957537677\n",
      "GD iter. 235/499: loss=0.5375292732210298\n",
      "GD iter. 236/499: loss=0.5374902976871976\n",
      "GD iter. 237/499: loss=0.5374219036726747\n",
      "GD iter. 238/499: loss=0.5373550187639975\n",
      "GD iter. 239/499: loss=0.5373070725074108\n",
      "GD iter. 240/499: loss=0.5372236486021171\n",
      "GD iter. 241/499: loss=0.5371613890360161\n",
      "GD iter. 242/499: loss=0.5371065617459323\n",
      "GD iter. 243/499: loss=0.5370472534601921\n",
      "GD iter. 244/499: loss=0.5369808963331052\n",
      "GD iter. 245/499: loss=0.5369264513914275\n",
      "GD iter. 246/499: loss=0.5368701114406206\n",
      "GD iter. 247/499: loss=0.5367979852480448\n",
      "GD iter. 248/499: loss=0.5367310461841257\n",
      "GD iter. 249/499: loss=0.5366862842574884\n",
      "GD iter. 250/499: loss=0.5366622735545196\n",
      "GD iter. 251/499: loss=0.5365750101100509\n",
      "GD iter. 252/499: loss=0.536523154131212\n",
      "GD iter. 253/499: loss=0.5364434155469492\n",
      "GD iter. 254/499: loss=0.536381877687296\n",
      "GD iter. 255/499: loss=0.5363199634780562\n",
      "GD iter. 256/499: loss=0.5362933677366818\n",
      "GD iter. 257/499: loss=0.5362256126985606\n",
      "GD iter. 258/499: loss=0.5361555915557691\n",
      "GD iter. 259/499: loss=0.536083451266993\n",
      "GD iter. 260/499: loss=0.5360190403882026\n",
      "GD iter. 261/499: loss=0.5359817395430054\n",
      "GD iter. 262/499: loss=0.5359200981071092\n",
      "GD iter. 263/499: loss=0.5358655709317902\n",
      "GD iter. 264/499: loss=0.5358099436884154\n",
      "GD iter. 265/499: loss=0.5357735466738002\n",
      "GD iter. 266/499: loss=0.5357236622265653\n",
      "GD iter. 267/499: loss=0.5356564900440467\n",
      "GD iter. 268/499: loss=0.5356023284893805\n",
      "GD iter. 269/499: loss=0.5355364610370349\n",
      "GD iter. 270/499: loss=0.5354827232593984\n",
      "GD iter. 271/499: loss=0.5354466446399578\n",
      "GD iter. 272/499: loss=0.5354036868312102\n",
      "GD iter. 273/499: loss=0.5353202944090177\n",
      "GD iter. 274/499: loss=0.5352771799023025\n",
      "GD iter. 275/499: loss=0.5352425547134266\n",
      "GD iter. 276/499: loss=0.5351751588740508\n",
      "GD iter. 277/499: loss=0.5351314503905618\n",
      "GD iter. 278/499: loss=0.5350886573624712\n",
      "GD iter. 279/499: loss=0.5350488951732255\n",
      "GD iter. 280/499: loss=0.5349727756967996\n",
      "GD iter. 281/499: loss=0.534923995608277\n",
      "GD iter. 282/499: loss=0.5348936443333211\n",
      "GD iter. 283/499: loss=0.5348385137449616\n",
      "GD iter. 284/499: loss=0.5347761761421287\n",
      "GD iter. 285/499: loss=0.5347300435047374\n",
      "GD iter. 286/499: loss=0.5346769760411052\n",
      "GD iter. 287/499: loss=0.5346304408119407\n",
      "GD iter. 288/499: loss=0.5345852881037731\n",
      "GD iter. 289/499: loss=0.5345343646461791\n",
      "GD iter. 290/499: loss=0.5344805546906592\n",
      "GD iter. 291/499: loss=0.5344226843481655\n",
      "GD iter. 292/499: loss=0.5343797466885212\n",
      "GD iter. 293/499: loss=0.5343374546623626\n",
      "GD iter. 294/499: loss=0.5342876256017624\n",
      "GD iter. 295/499: loss=0.5342464737147042\n",
      "GD iter. 296/499: loss=0.5342137356127424\n",
      "GD iter. 297/499: loss=0.5341334170755745\n",
      "GD iter. 298/499: loss=0.5340914510797585\n",
      "GD iter. 299/499: loss=0.5340384254543851\n",
      "GD iter. 300/499: loss=0.5339788371975444\n",
      "GD iter. 301/499: loss=0.533955145069476\n",
      "GD iter. 302/499: loss=0.5339095195959502\n",
      "GD iter. 303/499: loss=0.5338542010561426\n",
      "GD iter. 304/499: loss=0.5337952903833271\n",
      "GD iter. 305/499: loss=0.5337453658427372\n",
      "GD iter. 306/499: loss=0.5337115781638117\n",
      "GD iter. 307/499: loss=0.5337009111560735\n",
      "GD iter. 308/499: loss=0.5336106388160194\n",
      "GD iter. 309/499: loss=0.5335625187821775\n",
      "GD iter. 310/499: loss=0.5335199649513166\n",
      "GD iter. 311/499: loss=0.5334615865436164\n",
      "GD iter. 312/499: loss=0.5334326270254113\n",
      "GD iter. 313/499: loss=0.5333886988632333\n",
      "GD iter. 314/499: loss=0.533342334063819\n",
      "GD iter. 315/499: loss=0.533302003614981\n",
      "GD iter. 316/499: loss=0.5332495434744257\n",
      "GD iter. 317/499: loss=0.5332197869767409\n",
      "GD iter. 318/499: loss=0.5331556516811874\n",
      "GD iter. 319/499: loss=0.5331093341041244\n",
      "GD iter. 320/499: loss=0.5331091039050405\n",
      "GD iter. 321/499: loss=0.5330665105179485\n",
      "GD iter. 322/499: loss=0.5330043958054559\n",
      "GD iter. 323/499: loss=0.5329532908205864\n",
      "GD iter. 324/499: loss=0.5329012700243891\n",
      "GD iter. 325/499: loss=0.5328695329519082\n",
      "GD iter. 326/499: loss=0.5328544496445381\n",
      "GD iter. 327/499: loss=0.5327808903749582\n",
      "GD iter. 328/499: loss=0.5327496838421073\n",
      "GD iter. 329/499: loss=0.5326978076529759\n",
      "GD iter. 330/499: loss=0.5326580242910134\n",
      "GD iter. 331/499: loss=0.5326293119406238\n",
      "GD iter. 332/499: loss=0.5325960744411395\n",
      "GD iter. 333/499: loss=0.5325696103317384\n",
      "GD iter. 334/499: loss=0.5324889426469066\n",
      "GD iter. 335/499: loss=0.5324520431989264\n",
      "GD iter. 336/499: loss=0.5324248943094666\n",
      "GD iter. 337/499: loss=0.5323856867392014\n",
      "GD iter. 338/499: loss=0.532339908048338\n",
      "GD iter. 339/499: loss=0.5322828792281408\n",
      "GD iter. 340/499: loss=0.5323014043291321\n",
      "GD iter. 341/499: loss=0.5322372884166852\n",
      "GD iter. 342/499: loss=0.5321985200303478\n",
      "GD iter. 343/499: loss=0.5321587898884138\n",
      "GD iter. 344/499: loss=0.5320970064561255\n",
      "GD iter. 345/499: loss=0.5320932350609519\n",
      "GD iter. 346/499: loss=0.5320734205279097\n",
      "GD iter. 347/499: loss=0.5320080181310188\n",
      "GD iter. 348/499: loss=0.5319708074455897\n",
      "GD iter. 349/499: loss=0.5319190641259464\n",
      "GD iter. 350/499: loss=0.5318763950021573\n",
      "GD iter. 351/499: loss=0.5318552245591697\n",
      "GD iter. 352/499: loss=0.5318258745879867\n",
      "GD iter. 353/499: loss=0.5318041556251256\n",
      "GD iter. 354/499: loss=0.5317461323347004\n",
      "GD iter. 355/499: loss=0.5317133115671722\n",
      "GD iter. 356/499: loss=0.5317013293292027\n",
      "GD iter. 357/499: loss=0.5316688950323569\n",
      "GD iter. 358/499: loss=0.5316237973577745\n",
      "GD iter. 359/499: loss=0.53159021202375\n",
      "GD iter. 360/499: loss=0.5315428934642216\n",
      "GD iter. 361/499: loss=0.5315112171363947\n",
      "GD iter. 362/499: loss=0.5315042866222041\n",
      "GD iter. 363/499: loss=0.5314902607768848\n",
      "GD iter. 364/499: loss=0.5314313301501469\n",
      "GD iter. 365/499: loss=0.531406205145244\n",
      "GD iter. 366/499: loss=0.5313659293948432\n",
      "GD iter. 367/499: loss=0.5313273664806978\n",
      "GD iter. 368/499: loss=0.5312867554558042\n",
      "GD iter. 369/499: loss=0.5312502811719167\n",
      "GD iter. 370/499: loss=0.5312162504906308\n",
      "GD iter. 371/499: loss=0.531205680765524\n",
      "GD iter. 372/499: loss=0.5311400813763981\n",
      "GD iter. 373/499: loss=0.5311126198945122\n",
      "GD iter. 374/499: loss=0.5311123749034568\n",
      "GD iter. 375/499: loss=0.5310993388256187\n",
      "GD iter. 376/499: loss=0.5310415390215164\n",
      "GD iter. 377/499: loss=0.5310022001660373\n",
      "GD iter. 378/499: loss=0.5309544360169716\n",
      "GD iter. 379/499: loss=0.5309176221869577\n",
      "GD iter. 380/499: loss=0.5308950760698941\n",
      "GD iter. 381/499: loss=0.5308908735066675\n",
      "GD iter. 382/499: loss=0.5308473256362105\n",
      "GD iter. 383/499: loss=0.5308330164160158\n",
      "GD iter. 384/499: loss=0.5308142167555983\n",
      "GD iter. 385/499: loss=0.5307610449936224\n",
      "GD iter. 386/499: loss=0.5307349379011099\n",
      "GD iter. 387/499: loss=0.5307092744681037\n",
      "GD iter. 388/499: loss=0.5306795856723896\n",
      "GD iter. 389/499: loss=0.5306436817164446\n",
      "GD iter. 390/499: loss=0.530633082271536\n",
      "GD iter. 391/499: loss=0.5306133798279036\n",
      "GD iter. 392/499: loss=0.5306212268910285\n",
      "GD iter. 393/499: loss=0.5305552246500653\n",
      "GD iter. 394/499: loss=0.5305228025526476\n",
      "GD iter. 395/499: loss=0.530486398888504\n",
      "GD iter. 396/499: loss=0.5304641102551501\n",
      "GD iter. 397/499: loss=0.5304507919301961\n",
      "GD iter. 398/499: loss=0.5304271589121553\n",
      "GD iter. 399/499: loss=0.5304347748591873\n",
      "GD iter. 400/499: loss=0.5303891044971131\n",
      "GD iter. 401/499: loss=0.5303687371577578\n",
      "GD iter. 402/499: loss=0.5303260386993779\n",
      "GD iter. 403/499: loss=0.5303066987127902\n",
      "GD iter. 404/499: loss=0.5302647525892286\n",
      "GD iter. 405/499: loss=0.5302856157209006\n",
      "GD iter. 406/499: loss=0.5302423161531769\n",
      "GD iter. 407/499: loss=0.5302179987374475\n",
      "GD iter. 408/499: loss=0.5301890959001062\n",
      "GD iter. 409/499: loss=0.5301625992980018\n",
      "GD iter. 410/499: loss=0.5301600374220163\n",
      "GD iter. 411/499: loss=0.5301128430983991\n",
      "GD iter. 412/499: loss=0.5300885549170614\n",
      "GD iter. 413/499: loss=0.530052695664403\n",
      "GD iter. 414/499: loss=0.5300672660839291\n",
      "GD iter. 415/499: loss=0.5300378617212064\n",
      "GD iter. 416/499: loss=0.5300443645174108\n",
      "GD iter. 417/499: loss=0.5300161279742701\n",
      "GD iter. 418/499: loss=0.5299590371999486\n",
      "GD iter. 419/499: loss=0.5299352599352837\n",
      "GD iter. 420/499: loss=0.529919786684026\n",
      "GD iter. 421/499: loss=0.5299018419395545\n",
      "GD iter. 422/499: loss=0.5299020676048174\n",
      "GD iter. 423/499: loss=0.5298615244093725\n",
      "GD iter. 424/499: loss=0.5298392071849894\n",
      "GD iter. 425/499: loss=0.5298388346332668\n",
      "GD iter. 426/499: loss=0.529788167944904\n",
      "GD iter. 427/499: loss=0.5297787529669625\n",
      "GD iter. 428/499: loss=0.5297561778693309\n",
      "GD iter. 429/499: loss=0.5297400340483774\n",
      "GD iter. 430/499: loss=0.5296913320401933\n",
      "GD iter. 431/499: loss=0.5296712850342848\n",
      "GD iter. 432/499: loss=0.5296493091344251\n",
      "GD iter. 433/499: loss=0.5296498285852493\n",
      "GD iter. 434/499: loss=0.5296475300090362\n",
      "GD iter. 435/499: loss=0.5295833569080612\n",
      "GD iter. 436/499: loss=0.5295968148785783\n",
      "GD iter. 437/499: loss=0.5296188950592439\n",
      "GD iter. 438/499: loss=0.5295519152600078\n",
      "GD iter. 439/499: loss=0.5295315633106793\n",
      "GD iter. 440/499: loss=0.5294846351928746\n",
      "GD iter. 441/499: loss=0.5294800104147703\n",
      "GD iter. 442/499: loss=0.5294614137331892\n",
      "GD iter. 443/499: loss=0.5294769617544047\n",
      "GD iter. 444/499: loss=0.5294099536052633\n",
      "GD iter. 445/499: loss=0.5293849121718784\n",
      "GD iter. 446/499: loss=0.5293604442958434\n",
      "GD iter. 447/499: loss=0.5293641403012787\n",
      "GD iter. 448/499: loss=0.5293463451778846\n",
      "GD iter. 449/499: loss=0.5293374770876054\n",
      "GD iter. 450/499: loss=0.5292877969076736\n",
      "GD iter. 451/499: loss=0.5292776829626102\n",
      "GD iter. 452/499: loss=0.5292386146104592\n",
      "GD iter. 453/499: loss=0.5292480911019312\n",
      "GD iter. 454/499: loss=0.5292075307509769\n",
      "GD iter. 455/499: loss=0.5292603304031049\n",
      "GD iter. 456/499: loss=0.5291779618957221\n",
      "GD iter. 457/499: loss=0.5291462218037646\n",
      "GD iter. 458/499: loss=0.5291282064243295\n",
      "GD iter. 459/499: loss=0.5291206903161761\n",
      "GD iter. 460/499: loss=0.5290927760020535\n",
      "GD iter. 461/499: loss=0.5290946774126419\n",
      "GD iter. 462/499: loss=0.5290783171945694\n",
      "GD iter. 463/499: loss=0.5290253472959154\n",
      "GD iter. 464/499: loss=0.5290335872156404\n",
      "GD iter. 465/499: loss=0.5290171637074034\n",
      "GD iter. 466/499: loss=0.5290136165282987\n",
      "GD iter. 467/499: loss=0.5289833010857069\n",
      "GD iter. 468/499: loss=0.5289734090783402\n",
      "GD iter. 469/499: loss=0.528956245187377\n",
      "GD iter. 470/499: loss=0.5289875967761903\n",
      "GD iter. 471/499: loss=0.5289051787477103\n",
      "GD iter. 472/499: loss=0.5289180130775958\n",
      "GD iter. 473/499: loss=0.5288821127713262\n",
      "GD iter. 474/499: loss=0.5288879238667867\n",
      "GD iter. 475/499: loss=0.5288433678569691\n",
      "GD iter. 476/499: loss=0.5288220511549443\n",
      "GD iter. 477/499: loss=0.5288434055147616\n",
      "GD iter. 478/499: loss=0.5288136852623364\n",
      "GD iter. 479/499: loss=0.5287767994477099\n",
      "GD iter. 480/499: loss=0.528776311486623\n",
      "GD iter. 481/499: loss=0.5287736297547686\n",
      "GD iter. 482/499: loss=0.5287451538805046\n",
      "GD iter. 483/499: loss=0.5287554053268284\n",
      "GD iter. 484/499: loss=0.5286923010025003\n",
      "GD iter. 485/499: loss=0.5287011113361092\n",
      "GD iter. 486/499: loss=0.5286762696141342\n",
      "GD iter. 487/499: loss=0.5286891197748468\n",
      "GD iter. 488/499: loss=0.5286456230262093\n",
      "GD iter. 489/499: loss=0.528616272945819\n",
      "GD iter. 490/499: loss=0.5286311974872056\n",
      "GD iter. 491/499: loss=0.5286074013308826\n",
      "GD iter. 492/499: loss=0.5285959165022677\n",
      "GD iter. 493/499: loss=0.5285684222217263\n",
      "GD iter. 494/499: loss=0.5285463013432002\n",
      "GD iter. 495/499: loss=0.5285646807234273\n",
      "GD iter. 496/499: loss=0.5285247565042275\n",
      "GD iter. 497/499: loss=0.5285211245447664\n",
      "GD iter. 498/499: loss=0.5284932008675279\n",
      "GD iter. 499/499: loss=0.5284663236541791\n",
      "The Accuracy is: 0.6420\n",
      "The F1 score is: 0.2733\n",
      "The precision is: 0.1627\n",
      "The recall is: 0.8542\n",
      "GD iter. 0/499: loss=0.9826369295239642\n",
      "GD iter. 1/499: loss=0.9066005605678787\n",
      "GD iter. 2/499: loss=0.8329742308586924\n",
      "GD iter. 3/499: loss=0.7713029367284064\n",
      "GD iter. 4/499: loss=0.7272580640474714\n",
      "GD iter. 5/499: loss=0.6977709276013917\n",
      "GD iter. 6/499: loss=0.677965362195266\n",
      "GD iter. 7/499: loss=0.6625721002770668\n",
      "GD iter. 8/499: loss=0.6511897396049\n",
      "GD iter. 9/499: loss=0.6423671138803915\n",
      "GD iter. 10/499: loss=0.6349457561918768\n",
      "GD iter. 11/499: loss=0.6288401962241332\n",
      "GD iter. 12/499: loss=0.6240225712950113\n",
      "GD iter. 13/499: loss=0.6198723597392761\n",
      "GD iter. 14/499: loss=0.6162071145290567\n",
      "GD iter. 15/499: loss=0.6131349126352834\n",
      "GD iter. 16/499: loss=0.6102564816297733\n",
      "GD iter. 17/499: loss=0.6076073077270211\n",
      "GD iter. 18/499: loss=0.6052703987726292\n",
      "GD iter. 19/499: loss=0.6030343060536533\n",
      "GD iter. 20/499: loss=0.6009592743646027\n",
      "GD iter. 21/499: loss=0.5990534567296113\n",
      "GD iter. 22/499: loss=0.597287095006404\n",
      "GD iter. 23/499: loss=0.5957134366431819\n",
      "GD iter. 24/499: loss=0.5942077882423263\n",
      "GD iter. 25/499: loss=0.5928118158697678\n",
      "GD iter. 26/499: loss=0.5914798020699632\n",
      "GD iter. 27/499: loss=0.5901513816001238\n",
      "GD iter. 28/499: loss=0.5888376518649753\n",
      "GD iter. 29/499: loss=0.5875425612640797\n",
      "GD iter. 30/499: loss=0.5862615941277997\n",
      "GD iter. 31/499: loss=0.5850043917449195\n",
      "GD iter. 32/499: loss=0.5837632854556443\n",
      "GD iter. 33/499: loss=0.5825603540462911\n",
      "GD iter. 34/499: loss=0.5813890426590023\n",
      "GD iter. 35/499: loss=0.5802989590857471\n",
      "GD iter. 36/499: loss=0.5792667616606548\n",
      "GD iter. 37/499: loss=0.5782665434114206\n",
      "GD iter. 38/499: loss=0.5772652327093837\n",
      "GD iter. 39/499: loss=0.5763255296584301\n",
      "GD iter. 40/499: loss=0.5754887808602769\n",
      "GD iter. 41/499: loss=0.5746722884589944\n",
      "GD iter. 42/499: loss=0.5738764762803579\n",
      "GD iter. 43/499: loss=0.5730905054434247\n",
      "GD iter. 44/499: loss=0.5723269212324575\n",
      "GD iter. 45/499: loss=0.5715879082219403\n",
      "GD iter. 46/499: loss=0.5708969895298197\n",
      "GD iter. 47/499: loss=0.5702127051769669\n",
      "GD iter. 48/499: loss=0.5695491326225219\n",
      "GD iter. 49/499: loss=0.5689075433662758\n",
      "GD iter. 50/499: loss=0.5683067178362265\n",
      "GD iter. 51/499: loss=0.5677308697096767\n",
      "GD iter. 52/499: loss=0.567158933025859\n",
      "GD iter. 53/499: loss=0.5666002239400517\n",
      "GD iter. 54/499: loss=0.5660496300176223\n",
      "GD iter. 55/499: loss=0.565515552454218\n",
      "GD iter. 56/499: loss=0.5649830066902445\n",
      "GD iter. 57/499: loss=0.5644632357940462\n",
      "GD iter. 58/499: loss=0.5639407847785222\n",
      "GD iter. 59/499: loss=0.563426641023346\n",
      "GD iter. 60/499: loss=0.5629274866185205\n",
      "GD iter. 61/499: loss=0.5624420435446288\n",
      "GD iter. 62/499: loss=0.5619739244382652\n",
      "GD iter. 63/499: loss=0.5615311707537483\n",
      "GD iter. 64/499: loss=0.5610933204010857\n",
      "GD iter. 65/499: loss=0.5606680693132395\n",
      "GD iter. 66/499: loss=0.5602467037418879\n",
      "GD iter. 67/499: loss=0.5598406018145662\n",
      "GD iter. 68/499: loss=0.5594426604893745\n",
      "GD iter. 69/499: loss=0.5590562061438193\n",
      "GD iter. 70/499: loss=0.5586977107874672\n",
      "GD iter. 71/499: loss=0.5583499741230996\n",
      "GD iter. 72/499: loss=0.5579931194079145\n",
      "GD iter. 73/499: loss=0.5576648216643051\n",
      "GD iter. 74/499: loss=0.5573418520146554\n",
      "GD iter. 75/499: loss=0.5570048249595323\n",
      "GD iter. 76/499: loss=0.5566787123186893\n",
      "GD iter. 77/499: loss=0.556371571282188\n",
      "GD iter. 78/499: loss=0.5560389019807082\n",
      "GD iter. 79/499: loss=0.5557253326105285\n",
      "GD iter. 80/499: loss=0.555421396425558\n",
      "GD iter. 81/499: loss=0.5551315403104505\n",
      "GD iter. 82/499: loss=0.5548222630894292\n",
      "GD iter. 83/499: loss=0.5545378448468795\n",
      "GD iter. 84/499: loss=0.5542554682757314\n",
      "GD iter. 85/499: loss=0.5539652433899102\n",
      "GD iter. 86/499: loss=0.5536732757700387\n",
      "GD iter. 87/499: loss=0.5534075618147023\n",
      "GD iter. 88/499: loss=0.5531297661364681\n",
      "GD iter. 89/499: loss=0.5528483208330679\n",
      "GD iter. 90/499: loss=0.5525917706809685\n",
      "GD iter. 91/499: loss=0.5523249470608096\n",
      "GD iter. 92/499: loss=0.5520518265909241\n",
      "GD iter. 93/499: loss=0.5517813118379707\n",
      "GD iter. 94/499: loss=0.5515294136851417\n",
      "GD iter. 95/499: loss=0.5512855109442327\n",
      "GD iter. 96/499: loss=0.5510204286424527\n",
      "GD iter. 97/499: loss=0.5507956943517246\n",
      "GD iter. 98/499: loss=0.5505861317125578\n",
      "GD iter. 99/499: loss=0.5503232619983085\n",
      "GD iter. 100/499: loss=0.5500917734631651\n",
      "GD iter. 101/499: loss=0.5498825699274761\n",
      "GD iter. 102/499: loss=0.5496874278644012\n",
      "GD iter. 103/499: loss=0.5494626897837553\n",
      "GD iter. 104/499: loss=0.5492652419763554\n",
      "GD iter. 105/499: loss=0.5490692528660337\n",
      "GD iter. 106/499: loss=0.5488817131423133\n",
      "GD iter. 107/499: loss=0.5486869427155555\n",
      "GD iter. 108/499: loss=0.5485077762193166\n",
      "GD iter. 109/499: loss=0.5483159381754472\n",
      "GD iter. 110/499: loss=0.548150607209779\n",
      "GD iter. 111/499: loss=0.5479712281440806\n",
      "GD iter. 112/499: loss=0.5477988865952464\n",
      "GD iter. 113/499: loss=0.5476018324226479\n",
      "GD iter. 114/499: loss=0.5474350961689378\n",
      "GD iter. 115/499: loss=0.5472771651980549\n",
      "GD iter. 116/499: loss=0.5471019620939036\n",
      "GD iter. 117/499: loss=0.5469276002466288\n",
      "GD iter. 118/499: loss=0.5467758401131465\n",
      "GD iter. 119/499: loss=0.5466113330217008\n",
      "GD iter. 120/499: loss=0.5464795362365743\n",
      "GD iter. 121/499: loss=0.5463351108710974\n",
      "GD iter. 122/499: loss=0.5461651256587254\n",
      "GD iter. 123/499: loss=0.5460075703938245\n",
      "GD iter. 124/499: loss=0.5458581972468438\n",
      "GD iter. 125/499: loss=0.5457159641817643\n",
      "GD iter. 126/499: loss=0.5455877033372823\n",
      "GD iter. 127/499: loss=0.5454277669113831\n",
      "GD iter. 128/499: loss=0.5452779091310115\n",
      "GD iter. 129/499: loss=0.5451348466551855\n",
      "GD iter. 130/499: loss=0.5450220072691051\n",
      "GD iter. 131/499: loss=0.5448549755926848\n",
      "GD iter. 132/499: loss=0.5447244644590213\n",
      "GD iter. 133/499: loss=0.5445952931638097\n",
      "GD iter. 134/499: loss=0.5444683801023839\n",
      "GD iter. 135/499: loss=0.5443292576287047\n",
      "GD iter. 136/499: loss=0.5441822215201297\n",
      "GD iter. 137/499: loss=0.5440582174781753\n",
      "GD iter. 138/499: loss=0.5439381638981073\n",
      "GD iter. 139/499: loss=0.5437815255321247\n",
      "GD iter. 140/499: loss=0.5436736953562497\n",
      "GD iter. 141/499: loss=0.5435671710724503\n",
      "GD iter. 142/499: loss=0.5434087493952787\n",
      "GD iter. 143/499: loss=0.5432818437196268\n",
      "GD iter. 144/499: loss=0.5431525415001164\n",
      "GD iter. 145/499: loss=0.5430245234040119\n",
      "GD iter. 146/499: loss=0.5429038744132375\n",
      "GD iter. 147/499: loss=0.5427588578759555\n",
      "GD iter. 148/499: loss=0.5426355781883605\n",
      "GD iter. 149/499: loss=0.5425229085039909\n",
      "GD iter. 150/499: loss=0.5423879300490716\n",
      "GD iter. 151/499: loss=0.5422743177116913\n",
      "GD iter. 152/499: loss=0.5421402768783602\n",
      "GD iter. 153/499: loss=0.5420121422438327\n",
      "GD iter. 154/499: loss=0.5418675773290188\n",
      "GD iter. 155/499: loss=0.5417398533443238\n",
      "GD iter. 156/499: loss=0.541624173028754\n",
      "GD iter. 157/499: loss=0.5415475567748043\n",
      "GD iter. 158/499: loss=0.5413881882427758\n",
      "GD iter. 159/499: loss=0.5412660532186919\n",
      "GD iter. 160/499: loss=0.5411435763088138\n",
      "GD iter. 161/499: loss=0.5410445452760829\n",
      "GD iter. 162/499: loss=0.5409117248504638\n",
      "GD iter. 163/499: loss=0.5407961012582668\n",
      "GD iter. 164/499: loss=0.5406910071258346\n",
      "GD iter. 165/499: loss=0.5405725309236923\n",
      "GD iter. 166/499: loss=0.5404549796254867\n",
      "GD iter. 167/499: loss=0.5403563188943791\n",
      "GD iter. 168/499: loss=0.5402387990564175\n",
      "GD iter. 169/499: loss=0.5401548540171839\n",
      "GD iter. 170/499: loss=0.5400083796657686\n",
      "GD iter. 171/499: loss=0.5399184996938103\n",
      "GD iter. 172/499: loss=0.5398116513412304\n",
      "GD iter. 173/499: loss=0.5397187029249686\n",
      "GD iter. 174/499: loss=0.5396082634905917\n",
      "GD iter. 175/499: loss=0.5395084201111155\n",
      "GD iter. 176/499: loss=0.5393878870266624\n",
      "GD iter. 177/499: loss=0.5392774859719075\n",
      "GD iter. 178/499: loss=0.5391975941348555\n",
      "GD iter. 179/499: loss=0.5391210278454972\n",
      "GD iter. 180/499: loss=0.5389935085768859\n",
      "GD iter. 181/499: loss=0.538900264764475\n",
      "GD iter. 182/499: loss=0.5387761970551703\n",
      "GD iter. 183/499: loss=0.5386856987215649\n",
      "GD iter. 184/499: loss=0.5385897150838803\n",
      "GD iter. 185/499: loss=0.5385520529422394\n",
      "GD iter. 186/499: loss=0.5383819207093281\n",
      "GD iter. 187/499: loss=0.538282269494228\n",
      "GD iter. 188/499: loss=0.5382039033455576\n",
      "GD iter. 189/499: loss=0.5381039950869613\n",
      "GD iter. 190/499: loss=0.5380242382280636\n",
      "GD iter. 191/499: loss=0.537919559953457\n",
      "GD iter. 192/499: loss=0.5378207986900764\n",
      "GD iter. 193/499: loss=0.5377440114005253\n",
      "GD iter. 194/499: loss=0.5376545975000956\n",
      "GD iter. 195/499: loss=0.5375329002754076\n",
      "GD iter. 196/499: loss=0.5374586497235652\n",
      "GD iter. 197/499: loss=0.5373834829719017\n",
      "GD iter. 198/499: loss=0.5372789956915054\n",
      "GD iter. 199/499: loss=0.5372113081328994\n",
      "GD iter. 200/499: loss=0.5371414773194578\n",
      "GD iter. 201/499: loss=0.5370352351187014\n",
      "GD iter. 202/499: loss=0.5369981570291064\n",
      "GD iter. 203/499: loss=0.5369063351689742\n",
      "GD iter. 204/499: loss=0.5367973702928749\n",
      "GD iter. 205/499: loss=0.5367189084137434\n",
      "GD iter. 206/499: loss=0.536645498550686\n",
      "GD iter. 207/499: loss=0.5365822233577211\n",
      "GD iter. 208/499: loss=0.5365231303591713\n",
      "GD iter. 209/499: loss=0.5364441209716413\n",
      "GD iter. 210/499: loss=0.536341624243651\n",
      "GD iter. 211/499: loss=0.5362681408566368\n",
      "GD iter. 212/499: loss=0.5361976994900118\n",
      "GD iter. 213/499: loss=0.536181213855695\n",
      "GD iter. 214/499: loss=0.5360591829953344\n",
      "GD iter. 215/499: loss=0.5359808741556048\n",
      "GD iter. 216/499: loss=0.5359190228091382\n",
      "GD iter. 217/499: loss=0.5358265988576397\n",
      "GD iter. 218/499: loss=0.5357722841708059\n",
      "GD iter. 219/499: loss=0.5357002940359589\n",
      "GD iter. 220/499: loss=0.5356177015907811\n",
      "GD iter. 221/499: loss=0.5355730401612149\n",
      "GD iter. 222/499: loss=0.5354734678226\n",
      "GD iter. 223/499: loss=0.5354150358768474\n",
      "GD iter. 224/499: loss=0.5353265597995811\n",
      "GD iter. 225/499: loss=0.5352639743422873\n",
      "GD iter. 226/499: loss=0.5351844316604129\n",
      "GD iter. 227/499: loss=0.5351323507451049\n",
      "GD iter. 228/499: loss=0.5350525031522678\n",
      "GD iter. 229/499: loss=0.5349727083832821\n",
      "GD iter. 230/499: loss=0.5349065605679767\n",
      "GD iter. 231/499: loss=0.534837500137738\n",
      "GD iter. 232/499: loss=0.5347585014525608\n",
      "GD iter. 233/499: loss=0.5346761677030297\n",
      "GD iter. 234/499: loss=0.5346082265561493\n",
      "GD iter. 235/499: loss=0.5345718514461497\n",
      "GD iter. 236/499: loss=0.5345294899888537\n",
      "GD iter. 237/499: loss=0.5344162161836399\n",
      "GD iter. 238/499: loss=0.5343396268333458\n",
      "GD iter. 239/499: loss=0.5342723304434358\n",
      "GD iter. 240/499: loss=0.534210953816641\n",
      "GD iter. 241/499: loss=0.5341429929269303\n",
      "GD iter. 242/499: loss=0.5340845757708663\n",
      "GD iter. 243/499: loss=0.534018620407787\n",
      "GD iter. 244/499: loss=0.533958174908459\n",
      "GD iter. 245/499: loss=0.5338881528983386\n",
      "GD iter. 246/499: loss=0.5338131499353724\n",
      "GD iter. 247/499: loss=0.5337647719331096\n",
      "GD iter. 248/499: loss=0.5337216856401107\n",
      "GD iter. 249/499: loss=0.5336633173029778\n",
      "GD iter. 250/499: loss=0.5335897505701219\n",
      "GD iter. 251/499: loss=0.5335605137623958\n",
      "GD iter. 252/499: loss=0.5334633065839475\n",
      "GD iter. 253/499: loss=0.5334061689988245\n",
      "GD iter. 254/499: loss=0.5333466196524851\n",
      "GD iter. 255/499: loss=0.5332757452256187\n",
      "GD iter. 256/499: loss=0.5332336569426108\n",
      "GD iter. 257/499: loss=0.5331772624305803\n",
      "GD iter. 258/499: loss=0.53309534780085\n",
      "GD iter. 259/499: loss=0.5330516850108794\n",
      "GD iter. 260/499: loss=0.5330286778322187\n",
      "GD iter. 261/499: loss=0.5329538525663375\n",
      "GD iter. 262/499: loss=0.5328883142131668\n",
      "GD iter. 263/499: loss=0.5328306647046952\n",
      "GD iter. 264/499: loss=0.5327509551156089\n",
      "GD iter. 265/499: loss=0.5327241498134297\n",
      "GD iter. 266/499: loss=0.5326856279061033\n",
      "GD iter. 267/499: loss=0.5326066355111633\n",
      "GD iter. 268/499: loss=0.5325400731477902\n",
      "GD iter. 269/499: loss=0.5325032198326365\n",
      "GD iter. 270/499: loss=0.5324617686350239\n",
      "GD iter. 271/499: loss=0.5324253953501525\n",
      "GD iter. 272/499: loss=0.532393094849659\n",
      "GD iter. 273/499: loss=0.5322835297705255\n",
      "GD iter. 274/499: loss=0.5322245231579465\n",
      "GD iter. 275/499: loss=0.5321966740620118\n",
      "GD iter. 276/499: loss=0.5321345092476109\n",
      "GD iter. 277/499: loss=0.5321146723972562\n",
      "GD iter. 278/499: loss=0.5320430827101628\n",
      "GD iter. 279/499: loss=0.531975228448741\n",
      "GD iter. 280/499: loss=0.5319494761228175\n",
      "GD iter. 281/499: loss=0.5318844378127576\n",
      "GD iter. 282/499: loss=0.5318779636124076\n",
      "GD iter. 283/499: loss=0.5318039514821251\n",
      "GD iter. 284/499: loss=0.5317411880126713\n",
      "GD iter. 285/499: loss=0.5316745771280875\n",
      "GD iter. 286/499: loss=0.5316308153655701\n",
      "GD iter. 287/499: loss=0.5315760748568524\n",
      "GD iter. 288/499: loss=0.5315915606774323\n",
      "GD iter. 289/499: loss=0.531496290249144\n",
      "GD iter. 290/499: loss=0.5314481266040731\n",
      "GD iter. 291/499: loss=0.5314043335976009\n",
      "GD iter. 292/499: loss=0.5313604075032081\n",
      "GD iter. 293/499: loss=0.5313563330243847\n",
      "GD iter. 294/499: loss=0.5312874304953835\n",
      "GD iter. 295/499: loss=0.531241040973939\n",
      "GD iter. 296/499: loss=0.5311805877755684\n",
      "GD iter. 297/499: loss=0.5311824824635374\n",
      "GD iter. 298/499: loss=0.5310992897034628\n",
      "GD iter. 299/499: loss=0.5310639995280632\n",
      "GD iter. 300/499: loss=0.5310228340956503\n",
      "GD iter. 301/499: loss=0.5309911760107777\n",
      "GD iter. 302/499: loss=0.5309618086899831\n",
      "GD iter. 303/499: loss=0.5309490754312265\n",
      "GD iter. 304/499: loss=0.530851079084659\n",
      "GD iter. 305/499: loss=0.5308177069593852\n",
      "GD iter. 306/499: loss=0.5307831074425033\n",
      "GD iter. 307/499: loss=0.5307610466087485\n",
      "GD iter. 308/499: loss=0.5307110698825707\n",
      "GD iter. 309/499: loss=0.5306691735300496\n",
      "GD iter. 310/499: loss=0.5306223021392547\n",
      "GD iter. 311/499: loss=0.5305897025903084\n",
      "GD iter. 312/499: loss=0.5305343480645618\n",
      "GD iter. 313/499: loss=0.5305123586701636\n",
      "GD iter. 314/499: loss=0.5304677189515784\n",
      "GD iter. 315/499: loss=0.530433279977555\n",
      "GD iter. 316/499: loss=0.5304086608836279\n",
      "GD iter. 317/499: loss=0.5303467189586426\n",
      "GD iter. 318/499: loss=0.5303289103602498\n",
      "GD iter. 319/499: loss=0.5303001525424029\n",
      "GD iter. 320/499: loss=0.5302956798691103\n",
      "GD iter. 321/499: loss=0.5302244109443265\n",
      "GD iter. 322/499: loss=0.5301660461618347\n",
      "GD iter. 323/499: loss=0.5301624597793303\n",
      "GD iter. 324/499: loss=0.5300683012155849\n",
      "GD iter. 325/499: loss=0.5300639958129832\n",
      "GD iter. 326/499: loss=0.530035205015329\n",
      "GD iter. 327/499: loss=0.5299917203840386\n",
      "GD iter. 328/499: loss=0.5299372534731844\n",
      "GD iter. 329/499: loss=0.5299312454016807\n",
      "GD iter. 330/499: loss=0.5298695806644307\n",
      "GD iter. 331/499: loss=0.5298546090179475\n",
      "GD iter. 332/499: loss=0.5298280879649124\n",
      "GD iter. 333/499: loss=0.5297682104283861\n",
      "GD iter. 334/499: loss=0.5297199858749853\n",
      "GD iter. 335/499: loss=0.5296964790838075\n",
      "GD iter. 336/499: loss=0.5296633682105629\n",
      "GD iter. 337/499: loss=0.5296493323643781\n",
      "GD iter. 338/499: loss=0.5295704509071726\n",
      "GD iter. 339/499: loss=0.5295697223575786\n",
      "GD iter. 340/499: loss=0.5295052384180117\n",
      "GD iter. 341/499: loss=0.5294845845841518\n",
      "GD iter. 342/499: loss=0.5294415601493988\n",
      "GD iter. 343/499: loss=0.5294322558847018\n",
      "GD iter. 344/499: loss=0.5293963997755563\n",
      "GD iter. 345/499: loss=0.5293933056358522\n",
      "GD iter. 346/499: loss=0.5293168938742646\n",
      "GD iter. 347/499: loss=0.5292972857772724\n",
      "GD iter. 348/499: loss=0.5292608272191798\n",
      "GD iter. 349/499: loss=0.5292638000268718\n",
      "GD iter. 350/499: loss=0.5291888940108663\n",
      "GD iter. 351/499: loss=0.5291895670844095\n",
      "GD iter. 352/499: loss=0.5291604059183174\n",
      "GD iter. 353/499: loss=0.5291297080190751\n",
      "GD iter. 354/499: loss=0.5290967976284408\n",
      "GD iter. 355/499: loss=0.5290497985413021\n",
      "GD iter. 356/499: loss=0.5290146517332301\n",
      "GD iter. 357/499: loss=0.5290027415143089\n",
      "GD iter. 358/499: loss=0.5289745834357004\n",
      "GD iter. 359/499: loss=0.5290047655380233\n",
      "GD iter. 360/499: loss=0.5289317419105737\n",
      "GD iter. 361/499: loss=0.5288650620231388\n",
      "GD iter. 362/499: loss=0.5288548126173703\n",
      "GD iter. 363/499: loss=0.5288142168523149\n",
      "GD iter. 364/499: loss=0.5287849939085334\n",
      "GD iter. 365/499: loss=0.5287665866809684\n",
      "GD iter. 366/499: loss=0.5287674193211327\n",
      "GD iter. 367/499: loss=0.52873810365714\n",
      "GD iter. 368/499: loss=0.5287622324277036\n",
      "GD iter. 369/499: loss=0.5286551714982061\n",
      "GD iter. 370/499: loss=0.5286581921662963\n",
      "GD iter. 371/499: loss=0.5286102045960457\n",
      "GD iter. 372/499: loss=0.5285950165122966\n",
      "GD iter. 373/499: loss=0.5285926339542073\n",
      "GD iter. 374/499: loss=0.5286073423778734\n",
      "GD iter. 375/499: loss=0.5285198919884234\n",
      "GD iter. 376/499: loss=0.528477782491032\n",
      "GD iter. 377/499: loss=0.5284481775747683\n",
      "GD iter. 378/499: loss=0.5284437364574436\n",
      "GD iter. 379/499: loss=0.5284066938612467\n",
      "GD iter. 380/499: loss=0.5284072356519735\n",
      "GD iter. 381/499: loss=0.5283982429328108\n",
      "GD iter. 382/499: loss=0.5283711739925984\n",
      "GD iter. 383/499: loss=0.5283057236691004\n",
      "GD iter. 384/499: loss=0.5283220214290281\n",
      "GD iter. 385/499: loss=0.5283138295029948\n",
      "GD iter. 386/499: loss=0.5282990718459857\n",
      "GD iter. 387/499: loss=0.5282043852966003\n",
      "GD iter. 388/499: loss=0.5281724989193745\n",
      "GD iter. 389/499: loss=0.5281468888475342\n",
      "GD iter. 390/499: loss=0.5281456794373764\n",
      "GD iter. 391/499: loss=0.528105042924204\n",
      "GD iter. 392/499: loss=0.5281094412419279\n",
      "GD iter. 393/499: loss=0.528062313635232\n",
      "GD iter. 394/499: loss=0.5280699713395249\n",
      "GD iter. 395/499: loss=0.5280454521613696\n",
      "GD iter. 396/499: loss=0.5280707540762415\n",
      "GD iter. 397/499: loss=0.527969130039827\n",
      "GD iter. 398/499: loss=0.5279654786370706\n",
      "GD iter. 399/499: loss=0.5279148021300498\n",
      "GD iter. 400/499: loss=0.527911771581269\n",
      "GD iter. 401/499: loss=0.5278896438191099\n",
      "GD iter. 402/499: loss=0.5278701890088681\n",
      "GD iter. 403/499: loss=0.5278551981585305\n",
      "GD iter. 404/499: loss=0.5278752197867166\n",
      "GD iter. 405/499: loss=0.5278079645734786\n",
      "GD iter. 406/499: loss=0.5277795378209196\n",
      "GD iter. 407/499: loss=0.5277512157378206\n",
      "GD iter. 408/499: loss=0.5277362069982593\n",
      "GD iter. 409/499: loss=0.5277100213015521\n",
      "GD iter. 410/499: loss=0.5277454134732221\n",
      "GD iter. 411/499: loss=0.5276731848813704\n",
      "GD iter. 412/499: loss=0.5276673597051573\n",
      "GD iter. 413/499: loss=0.5276336856527969\n",
      "GD iter. 414/499: loss=0.5275927209631429\n",
      "GD iter. 415/499: loss=0.5275725589127335\n",
      "GD iter. 416/499: loss=0.5275745098411521\n",
      "GD iter. 417/499: loss=0.5275424182358873\n",
      "GD iter. 418/499: loss=0.5275537065116598\n",
      "GD iter. 419/499: loss=0.5274912691125964\n",
      "GD iter. 420/499: loss=0.5275046376684006\n",
      "GD iter. 421/499: loss=0.5274586289580246\n",
      "GD iter. 422/499: loss=0.527417618612044\n",
      "GD iter. 423/499: loss=0.5273939122010489\n",
      "GD iter. 424/499: loss=0.5274126358699222\n",
      "GD iter. 425/499: loss=0.5273894165526188\n",
      "GD iter. 426/499: loss=0.5274001066482633\n",
      "GD iter. 427/499: loss=0.5273270674166226\n",
      "GD iter. 428/499: loss=0.5273288516520795\n",
      "GD iter. 429/499: loss=0.5272666064403009\n",
      "GD iter. 430/499: loss=0.5272521597833673\n",
      "GD iter. 431/499: loss=0.527214555138998\n",
      "GD iter. 432/499: loss=0.5272249730727455\n",
      "GD iter. 433/499: loss=0.5271940264356405\n",
      "GD iter. 434/499: loss=0.5272015173167741\n",
      "GD iter. 435/499: loss=0.5271546384976491\n",
      "GD iter. 436/499: loss=0.5271826021225683\n",
      "GD iter. 437/499: loss=0.5271289761014908\n",
      "GD iter. 438/499: loss=0.5271355595143479\n",
      "GD iter. 439/499: loss=0.5270335962330556\n",
      "GD iter. 440/499: loss=0.5270658207215196\n",
      "GD iter. 441/499: loss=0.5270582647328927\n",
      "GD iter. 442/499: loss=0.5269985414687741\n",
      "GD iter. 443/499: loss=0.526998382946659\n",
      "GD iter. 444/499: loss=0.5269814598784147\n",
      "GD iter. 445/499: loss=0.5269856908485808\n",
      "GD iter. 446/499: loss=0.5269493611706073\n",
      "GD iter. 447/499: loss=0.5269750926246181\n",
      "GD iter. 448/499: loss=0.5269030869032097\n",
      "GD iter. 449/499: loss=0.5268786464060371\n",
      "GD iter. 450/499: loss=0.526845554413464\n",
      "GD iter. 451/499: loss=0.5268234476874826\n",
      "GD iter. 452/499: loss=0.526831729650139\n",
      "GD iter. 453/499: loss=0.526853647353658\n",
      "GD iter. 454/499: loss=0.5267775204046239\n",
      "GD iter. 455/499: loss=0.5267904496344178\n",
      "GD iter. 456/499: loss=0.5267448021478495\n",
      "GD iter. 457/499: loss=0.5267337880218615\n",
      "GD iter. 458/499: loss=0.5267058499764095\n",
      "GD iter. 459/499: loss=0.5267688723288211\n",
      "GD iter. 460/499: loss=0.526680903940653\n",
      "GD iter. 461/499: loss=0.526645365386447\n",
      "GD iter. 462/499: loss=0.5266444109903\n",
      "GD iter. 463/499: loss=0.5266351686745746\n",
      "GD iter. 464/499: loss=0.5266254149036891\n",
      "GD iter. 465/499: loss=0.5266060435896903\n",
      "GD iter. 466/499: loss=0.5265881047262273\n",
      "GD iter. 467/499: loss=0.5265535961111053\n",
      "GD iter. 468/499: loss=0.5265058573720625\n",
      "GD iter. 469/499: loss=0.5264845478278326\n",
      "GD iter. 470/499: loss=0.5264776933008762\n",
      "GD iter. 471/499: loss=0.5264719926786052\n",
      "GD iter. 472/499: loss=0.5265336705625504\n",
      "GD iter. 473/499: loss=0.5265434034006468\n",
      "GD iter. 474/499: loss=0.5263962420310857\n",
      "GD iter. 475/499: loss=0.5263908572202958\n",
      "GD iter. 476/499: loss=0.5263861242484655\n",
      "GD iter. 477/499: loss=0.526357519211929\n",
      "GD iter. 478/499: loss=0.5263352352376244\n",
      "GD iter. 479/499: loss=0.5263894878714528\n",
      "GD iter. 480/499: loss=0.5263945806126675\n",
      "GD iter. 481/499: loss=0.526279094633883\n",
      "GD iter. 482/499: loss=0.5262550304586247\n",
      "GD iter. 483/499: loss=0.5263155001387809\n",
      "GD iter. 484/499: loss=0.5263085183455657\n",
      "GD iter. 485/499: loss=0.52624373836118\n",
      "GD iter. 486/499: loss=0.5262391752361676\n",
      "GD iter. 487/499: loss=0.5261902663307759\n",
      "GD iter. 488/499: loss=0.5261912631216104\n",
      "GD iter. 489/499: loss=0.5261403604031146\n",
      "GD iter. 490/499: loss=0.5261282418374528\n",
      "GD iter. 491/499: loss=0.5261413735077964\n",
      "GD iter. 492/499: loss=0.5261031653261135\n",
      "GD iter. 493/499: loss=0.526122349807951\n",
      "GD iter. 494/499: loss=0.5261236905429254\n",
      "GD iter. 495/499: loss=0.5261068029984747\n",
      "GD iter. 496/499: loss=0.5261304970302013\n",
      "GD iter. 497/499: loss=0.5260447220376594\n",
      "GD iter. 498/499: loss=0.5260257299460207\n",
      "GD iter. 499/499: loss=0.5259911674816844\n",
      "The Accuracy is: 0.6386\n",
      "The F1 score is: 0.3096\n",
      "The precision is: 0.1916\n",
      "The recall is: 0.8065\n",
      "Average accuracy score is:  0.6275683760342565\n",
      "Average f1 score is:  0.291453112615007\n"
     ]
    }
   ],
   "source": [
    "sub_x, sub_y = split_cross_validation(add_bias(x_train_processed), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(10):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = hinge_regression(y_t, x_t, initial_w, lambda_=0.1, max_iters=500, gamma=0.01)\n",
    "    y_pred = ((x_v @ w) > 0.5).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.svm as svm\n",
    "# cls = svm.SVC(C=50, kernel='rbf')\n",
    "# x_t, y_t, x_v, y_v = split_data(x_train_processed_hinge, y_train_processed_hinge, 0.9)\n",
    "# cls.fit(x_t, y_t)\n",
    "# y_pred = cls.predict(x_v)\n",
    "# predict_acc_pure(y_pred, y_v)\n",
    "# predict_f1_pure(y_pred, y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/199: loss=(0.27515635707163844+0j)\n",
      "GD iter. 10/199: loss=(0.08984273199245048+0j)\n",
      "GD iter. 20/199: loss=(0.08060750266357689+0j)\n",
      "GD iter. 30/199: loss=(0.07886529366687861+0j)\n",
      "GD iter. 40/199: loss=(0.07847559600273021+0j)\n",
      "GD iter. 50/199: loss=(0.0783783302151632+0j)\n",
      "GD iter. 60/199: loss=(0.07835091425607206+0j)\n",
      "GD iter. 70/199: loss=(0.07834218765984834+0j)\n",
      "GD iter. 80/199: loss=(0.07833910366872215+0j)\n",
      "GD iter. 90/199: loss=(0.07833792276927705+0j)\n",
      "GD iter. 100/199: loss=(0.0783374434632982+0j)\n",
      "GD iter. 110/199: loss=(0.07833724047591689+0j)\n",
      "GD iter. 120/199: loss=(0.0783371516904745+0j)\n",
      "GD iter. 130/199: loss=(0.07833711184407235+0j)\n",
      "GD iter. 140/199: loss=(0.07833709357592061+0j)\n",
      "GD iter. 150/199: loss=(0.07833708504789638+0j)\n",
      "GD iter. 160/199: loss=(0.07833708100472225+0j)\n",
      "GD iter. 170/199: loss=(0.07833707906222277+0j)\n",
      "GD iter. 180/199: loss=(0.07833707811831084+0j)\n",
      "GD iter. 190/199: loss=(0.07833707765517979+0j)\n",
      "The Accuracy is: 0.6069\n",
      "The F1 score is: 0.2936\n",
      "The precision is: 0.1743\n",
      "The recall is: 0.9283\n",
      "GD iter. 0/199: loss=(0.2846161611244507+0j)\n",
      "GD iter. 10/199: loss=(0.09014140647473523+0j)\n",
      "GD iter. 20/199: loss=(0.08069392168323945+0j)\n",
      "GD iter. 30/199: loss=(0.07890733234487343+0j)\n",
      "GD iter. 40/199: loss=(0.07851609393044172+0j)\n",
      "GD iter. 50/199: loss=(0.07842155841451023+0j)\n",
      "GD iter. 60/199: loss=(0.07839591039009966+0j)\n",
      "GD iter. 70/199: loss=(0.07838799385930861+0j)\n",
      "GD iter. 80/199: loss=(0.07838521543558337+0j)\n",
      "GD iter. 90/199: loss=(0.07838412046638024+0j)\n",
      "GD iter. 100/199: loss=(0.07838364441136755+0j)\n",
      "GD iter. 110/199: loss=(0.07838342010628076+0j)\n",
      "GD iter. 120/199: loss=(0.07838330740337833+0j)\n",
      "GD iter. 130/199: loss=(0.07838324787174156+0j)\n",
      "GD iter. 140/199: loss=(0.07838321521652837+0j)\n",
      "GD iter. 150/199: loss=(0.07838319680143119+0j)\n",
      "GD iter. 160/199: loss=(0.07838318620909235+0j)\n",
      "GD iter. 170/199: loss=(0.07838318003110276+0j)\n",
      "GD iter. 180/199: loss=(0.0783831763928772+0j)\n",
      "GD iter. 190/199: loss=(0.07838317423606121+0j)\n",
      "The Accuracy is: 0.6088\n",
      "The F1 score is: 0.2968\n",
      "The precision is: 0.1766\n",
      "The recall is: 0.9290\n",
      "GD iter. 0/199: loss=(0.23230137670596837+0j)\n",
      "GD iter. 10/199: loss=(0.08940770951464859+0j)\n",
      "GD iter. 20/199: loss=(0.08056627498094943+0j)\n",
      "GD iter. 30/199: loss=(0.07888153237255731+0j)\n",
      "GD iter. 40/199: loss=(0.0785203883675157+0j)\n",
      "GD iter. 50/199: loss=(0.0784358674413288+0j)\n",
      "GD iter. 60/199: loss=(0.07841398079697176+0j)\n",
      "GD iter. 70/199: loss=(0.07840766495330338+0j)\n",
      "GD iter. 80/199: loss=(0.07840564776843828+0j)\n",
      "GD iter. 90/199: loss=(0.07840494725773377+0j)\n",
      "GD iter. 100/199: loss=(0.07840468809829372+0j)\n",
      "GD iter. 110/199: loss=(0.07840458768459518+0j)\n",
      "GD iter. 120/199: loss=(0.07840454742205481+0j)\n",
      "GD iter. 130/199: loss=(0.07840453084214044+0j)\n",
      "GD iter. 140/199: loss=(0.07840452386292898+0j)\n",
      "GD iter. 150/199: loss=(0.078404520868478+0j)\n",
      "GD iter. 160/199: loss=(0.0784045195614292+0j)\n",
      "GD iter. 170/199: loss=(0.07840451898182292+0j)\n",
      "GD iter. 180/199: loss=(0.07840451872099662+0j)\n",
      "GD iter. 190/199: loss=(0.07840451860200925+0j)\n",
      "The Accuracy is: 0.6057\n",
      "The F1 score is: 0.2982\n",
      "The precision is: 0.1775\n",
      "The recall is: 0.9317\n",
      "GD iter. 0/199: loss=(0.2930305353559513+0j)\n",
      "GD iter. 10/199: loss=(0.08883365472866561+0j)\n",
      "GD iter. 20/199: loss=(0.08017022775306351+0j)\n",
      "GD iter. 30/199: loss=(0.0786305364494257+0j)\n",
      "GD iter. 40/199: loss=(0.07830053686458358+0j)\n",
      "GD iter. 50/199: loss=(0.07822263043457+0j)\n",
      "GD iter. 60/199: loss=(0.07820217650509315+0j)\n",
      "GD iter. 70/199: loss=(0.07819616718851223+0j)\n",
      "GD iter. 80/199: loss=(0.07819420731584838+0j)\n",
      "GD iter. 90/199: loss=(0.07819351079907666+0j)\n",
      "GD iter. 100/199: loss=(0.07819324651164275+0j)\n",
      "GD iter. 110/199: loss=(0.07819314119737848+0j)\n",
      "GD iter. 120/199: loss=(0.07819309762517387+0j)\n",
      "GD iter. 130/199: loss=(0.0781930790453939+0j)\n",
      "GD iter. 140/199: loss=(0.07819307091922614+0j)\n",
      "GD iter. 150/199: loss=(0.0781930672861911+0j)\n",
      "GD iter. 160/199: loss=(0.07819306563027878+0j)\n",
      "GD iter. 170/199: loss=(0.07819306486256472+0j)\n",
      "GD iter. 180/199: loss=(0.07819306450126971+0j)\n",
      "GD iter. 190/199: loss=(0.07819306432900246+0j)\n",
      "The Accuracy is: 0.6094\n",
      "The F1 score is: 0.3058\n",
      "The precision is: 0.1831\n",
      "The recall is: 0.9271\n",
      "GD iter. 0/199: loss=(0.2383604914309017+0j)\n",
      "GD iter. 10/199: loss=(0.08948082451978777+0j)\n",
      "GD iter. 20/199: loss=(0.08066960464393926+0j)\n",
      "GD iter. 30/199: loss=(0.07903034485231582+0j)\n",
      "GD iter. 40/199: loss=(0.07867338584861172+0j)\n",
      "GD iter. 50/199: loss=(0.07858776373460157+0j)\n",
      "GD iter. 60/199: loss=(0.07856494224374455+0j)\n",
      "GD iter. 70/199: loss=(0.07855813289590702+0j)\n",
      "GD iter. 80/199: loss=(0.07855586742633729+0j)\n",
      "GD iter. 90/199: loss=(0.0785550388017252+0j)\n",
      "GD iter. 100/199: loss=(0.0785547116127108+0j)\n",
      "GD iter. 110/199: loss=(0.0785545744916742+0j)\n",
      "GD iter. 120/199: loss=(0.0785545143272344+0j)\n",
      "GD iter. 130/199: loss=(0.07855448697504984+0j)\n",
      "GD iter. 140/199: loss=(0.07855447419169254+0j)\n",
      "GD iter. 150/199: loss=(0.07855446808679135+0j)\n",
      "GD iter. 160/199: loss=(0.07855446512157656+0j)\n",
      "GD iter. 170/199: loss=(0.07855446366214731+0j)\n",
      "GD iter. 180/199: loss=(0.07855446293635811+0j)\n",
      "GD iter. 190/199: loss=(0.07855446257247434+0j)\n",
      "The Accuracy is: 0.6097\n",
      "The F1 score is: 0.3021\n",
      "The precision is: 0.1802\n",
      "The recall is: 0.9339\n",
      "GD iter. 0/199: loss=(0.21765003979816455+0j)\n",
      "GD iter. 10/199: loss=(0.08858419533204454+0j)\n",
      "GD iter. 20/199: loss=(0.08020779303754724+0j)\n",
      "GD iter. 30/199: loss=(0.07860343974807799+0j)\n",
      "GD iter. 40/199: loss=(0.07825274288417783+0j)\n",
      "GD iter. 50/199: loss=(0.07816852028749141+0j)\n",
      "GD iter. 60/199: loss=(0.07814600713530162+0j)\n",
      "GD iter. 70/199: loss=(0.07813924616390117+0j)\n",
      "GD iter. 80/199: loss=(0.07813696835934174+0j)\n",
      "GD iter. 90/199: loss=(0.07813611541365499+0j)\n",
      "GD iter. 100/199: loss=(0.07813576446818675+0j)\n",
      "GD iter. 110/199: loss=(0.07813560754817171+0j)\n",
      "GD iter. 120/199: loss=(0.0781355321430868+0j)\n",
      "GD iter. 130/199: loss=(0.07813549367048882+0j)\n",
      "GD iter. 140/199: loss=(0.07813547309046455+0j)\n",
      "GD iter. 150/199: loss=(0.07813546168462286+0j)\n",
      "GD iter. 160/199: loss=(0.07813545520061436+0j)\n",
      "GD iter. 170/199: loss=(0.07813545144892335+0j)\n",
      "GD iter. 180/199: loss=(0.07813544925192019+0j)\n",
      "GD iter. 190/199: loss=(0.07813544795489287+0j)\n",
      "The Accuracy is: 0.6089\n",
      "The F1 score is: 0.2956\n",
      "The precision is: 0.1760\n",
      "The recall is: 0.9242\n",
      "GD iter. 0/199: loss=(0.24773993005471298+0j)\n",
      "GD iter. 10/199: loss=(0.08947187022257233+0j)\n",
      "GD iter. 20/199: loss=(0.08053179287011329+0j)\n",
      "GD iter. 30/199: loss=(0.07883804145093276+0j)\n",
      "GD iter. 40/199: loss=(0.07847104228005773+0j)\n",
      "GD iter. 50/199: loss=(0.07838363654534751+0j)\n",
      "GD iter. 60/199: loss=(0.07836039551502097+0j)\n",
      "GD iter. 70/199: loss=(0.07835342525774026+0j)\n",
      "GD iter. 80/199: loss=(0.07835108243361551+0j)\n",
      "GD iter. 90/199: loss=(0.0783502175814759+0j)\n",
      "GD iter. 100/199: loss=(0.07834987528422718+0j)\n",
      "GD iter. 110/199: loss=(0.07834973296579544+0j)\n",
      "GD iter. 120/199: loss=(0.07834967170928782+0j)\n",
      "GD iter. 130/199: loss=(0.07834964468097293+0j)\n",
      "GD iter. 140/199: loss=(0.07834963253471312+0j)\n",
      "GD iter. 150/199: loss=(0.07834962699974894+0j)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# x_t, y_t, x_v, y_v = split_data(x_pca_t, y_train_processed, 0.9)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# x_t, y_t = data_augmentation(x_t, y_t)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m initial_w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(x_t\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39m*\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m w, loss \u001b[39m=\u001b[39m mean_square_error_gd(y_t, x_t, initial_w, max_iters \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#Y110sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m y_pred \u001b[39m=\u001b[39m x_v \u001b[39m@\u001b[39m w\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zewzhang/Course/ML/Project/ML_proj/project1/data_glance.ipynb#Y110sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m y_pred_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(y_pred)\n",
      "File \u001b[0;32m~/Course/ML/Project/ML_proj/project1/implementations.py:61\u001b[0m, in \u001b[0;36mmean_square_error_gd\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m n_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[1;32m     60\u001b[0m     gradient \u001b[39m=\u001b[39m compute_gradient(y, tx, w)\n\u001b[0;32m---> 61\u001b[0m     loss \u001b[39m=\u001b[39m compute_mse(y, tx, w)\n\u001b[1;32m     62\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39m gamma \u001b[39m*\u001b[39m gradient\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m n_iter \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Course/ML/Project/ML_proj/project1/implementations.py:5\u001b[0m, in \u001b[0;36mcompute_mse\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"a function used to compute the mean squared error.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_mse\u001b[39m(y, tx, w):\n\u001b[1;32m      6\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculate the loss using MSE.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m        the value of the loss (a scalar), corresponding to the input parameters w.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     MSE \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((y \u001b[39m-\u001b[39m tx\u001b[39m.\u001b[39mdot(w))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(y))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## linear regression using PCA feature selection ##\n",
    "x_pca_t = add_bias(x_pca)\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_pca_t), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(10):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    # x_t, y_t, x_v, y_v = split_data(x_pca_t, y_train_processed, 0.9)\n",
    "    # x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = mean_square_error_gd(y_t, x_t, initial_w, max_iters = 200, gamma=0.05)\n",
    "    y_pred = x_v @ w\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    y_pred = (y_pred >= y_pred_mean).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/199: loss=(0.7051782385232673-0j)\n",
      "GD iter. 1/199: loss=(0.5404015004541018-0j)\n",
      "GD iter. 2/199: loss=(0.5237815040444291-0j)\n",
      "GD iter. 3/199: loss=(0.5125213644680006-0j)\n",
      "GD iter. 4/199: loss=(0.5037396832180407-0j)\n",
      "GD iter. 5/199: loss=(0.4964323775304104-0j)\n",
      "GD iter. 6/199: loss=(0.49014541073232765-0j)\n",
      "GD iter. 7/199: loss=(0.4846201627099105-0j)\n",
      "GD iter. 8/199: loss=(0.47969142164938333-0j)\n",
      "GD iter. 9/199: loss=(0.4752466002781531-0j)\n",
      "GD iter. 10/199: loss=(0.4712052029308098-0j)\n",
      "GD iter. 11/199: loss=(0.46750730913320426-0j)\n",
      "GD iter. 12/199: loss=(0.4641067384413045-0j)\n",
      "GD iter. 13/199: loss=(0.4609668312389616-0j)\n",
      "GD iter. 14/199: loss=(0.4580577559632113-0j)\n",
      "GD iter. 15/199: loss=(0.4553547361313669-0j)\n",
      "GD iter. 16/199: loss=(0.45283684730878704-0j)\n",
      "GD iter. 17/199: loss=(0.45048617659717805-0j)\n",
      "GD iter. 18/199: loss=(0.4482872186612688-0j)\n",
      "GD iter. 19/199: loss=(0.44622643002943596-0j)\n",
      "GD iter. 20/199: loss=(0.4442918919764712-0j)\n",
      "GD iter. 21/199: loss=(0.4424730497523351-0j)\n",
      "GD iter. 22/199: loss=(0.440760506790805-0j)\n",
      "GD iter. 23/199: loss=(0.43914585942828877-0j)\n",
      "GD iter. 24/199: loss=(0.43762156211994024-0j)\n",
      "GD iter. 25/199: loss=(0.4361808160743833-0j)\n",
      "GD iter. 26/199: loss=(0.4348174761966901-0j)\n",
      "GD iter. 27/199: loss=(0.43352597257503545-0j)\n",
      "GD iter. 28/199: loss=(0.4323012436844881-0j)\n",
      "GD iter. 29/199: loss=(0.4311386791478963-0j)\n",
      "GD iter. 30/199: loss=(0.4300340703764356-0j)\n",
      "GD iter. 31/199: loss=(0.4289835677682441-0j)\n",
      "GD iter. 32/199: loss=(0.42798364341050243-0j)\n",
      "GD iter. 33/199: loss=(0.4270310584337316-0j)\n",
      "GD iter. 34/199: loss=(0.42612283432432774-0j)\n",
      "GD iter. 35/199: loss=(0.425256227624483-0j)\n",
      "GD iter. 36/199: loss=(0.42442870754616946-0j)\n",
      "GD iter. 37/199: loss=(0.42363793610389366-0j)\n",
      "GD iter. 38/199: loss=(0.4228817504339453-0j)\n",
      "GD iter. 39/199: loss=(0.4221581470191715-0j)\n",
      "GD iter. 40/199: loss=(0.4214652675803913-0j)\n",
      "GD iter. 41/199: loss=(0.420801386430344-0j)\n",
      "GD iter. 42/199: loss=(0.42016489911495825-0j)\n",
      "GD iter. 43/199: loss=(0.4195543121909198-0j)\n",
      "GD iter. 44/199: loss=(0.41896823400883615-0j)\n",
      "GD iter. 45/199: loss=(0.4184053663884977-0j)\n",
      "GD iter. 46/199: loss=(0.41786449708733114-0j)\n",
      "GD iter. 47/199: loss=(0.4173444929756059-0j)\n",
      "GD iter. 48/199: loss=(0.4168442938426308-0j)\n",
      "GD iter. 49/199: loss=(0.4163629067673642-0j)\n",
      "GD iter. 50/199: loss=(0.4158994009947864-0j)\n",
      "GD iter. 51/199: loss=(0.4154529032662527-0j)\n",
      "GD iter. 52/199: loss=(0.41502259355800825-0j)\n",
      "GD iter. 53/199: loss=(0.41460770118724183-0j)\n",
      "GD iter. 54/199: loss=(0.41420750124959627-0j)\n",
      "GD iter. 55/199: loss=(0.4138213113560219-0j)\n",
      "GD iter. 56/199: loss=(0.4134484886403525-0j)\n",
      "GD iter. 57/199: loss=(0.41308842701204446-0j)\n",
      "GD iter. 58/199: loss=(0.41274055463122283-0j)\n",
      "GD iter. 59/199: loss=(0.4124043315855656-0j)\n",
      "GD iter. 60/199: loss=(0.41207924775066596-0j)\n",
      "GD iter. 61/199: loss=(0.41176482081737836-0j)\n",
      "GD iter. 62/199: loss=(0.41146059447131894-0j)\n",
      "GD iter. 63/199: loss=(0.41116613671115937-0j)\n",
      "GD iter. 64/199: loss=(0.41088103829367034-0j)\n",
      "GD iter. 65/199: loss=(0.41060491129463605-0j)\n",
      "GD iter. 66/199: loss=(0.4103373877758095-0j)\n",
      "GD iter. 67/199: loss=(0.4100781185490097-0j)\n",
      "GD iter. 68/199: loss=(0.4098267720292981-0j)\n",
      "GD iter. 69/199: loss=(0.40958303316992145-0j)\n",
      "GD iter. 70/199: loss=(0.409346602472379-0j)\n",
      "GD iter. 71/199: loss=(0.4091171950655762-0j)\n",
      "GD iter. 72/199: loss=(0.40889453984857227-0j)\n",
      "GD iter. 73/199: loss=(0.40867837869191476-0j)\n",
      "GD iter. 74/199: loss=(0.40846846569299883-0j)\n",
      "GD iter. 75/199: loss=(0.4082645664812845-0j)\n",
      "GD iter. 76/199: loss=(0.4080664575695671-0j)\n",
      "GD iter. 77/199: loss=(0.4078739257478194-0j)\n",
      "GD iter. 78/199: loss=(0.4076867675164205-0j)\n",
      "GD iter. 79/199: loss=(0.40750478855585265-0j)\n",
      "GD iter. 80/199: loss=(0.40732780323018986-0j)\n",
      "GD iter. 81/199: loss=(0.4071556341219213-0j)\n",
      "GD iter. 82/199: loss=(0.40698811159585535-0j)\n",
      "GD iter. 83/199: loss=(0.4068250733900283-0j)\n",
      "GD iter. 84/199: loss=(0.40666636423171054-0j)\n",
      "GD iter. 85/199: loss=(0.4065118354767531-0j)\n",
      "GD iter. 86/199: loss=(0.4063613447706554-0j)\n",
      "GD iter. 87/199: loss=(0.4062147557298612-0j)\n",
      "GD iter. 88/199: loss=(0.4060719376419046-0j)\n",
      "GD iter. 89/199: loss=(0.40593276518313254-0j)\n",
      "GD iter. 90/199: loss=(0.40579711815282915-0j)\n",
      "GD iter. 91/199: loss=(0.4056648812226523-0j)\n",
      "GD iter. 92/199: loss=(0.40553594370037604-0j)\n",
      "GD iter. 93/199: loss=(0.40541019930700545-0j)\n",
      "GD iter. 94/199: loss=(0.40528754596640043-0j)\n",
      "GD iter. 95/199: loss=(0.4051678856066057-0j)\n",
      "GD iter. 96/199: loss=(0.405051123972143-0j)\n",
      "GD iter. 97/199: loss=(0.40493717044657473-0j)\n",
      "GD iter. 98/199: loss=(0.4048259378846962-0j)\n",
      "GD iter. 99/199: loss=(0.4047173424537608-0j)\n",
      "GD iter. 100/199: loss=(0.40461130348318025-0j)\n",
      "GD iter. 101/199: loss=(0.40450774332218564-0j)\n",
      "GD iter. 102/199: loss=(0.4044065872049641-0j)\n",
      "GD iter. 103/199: loss=(0.4043077631228255-0j)\n",
      "GD iter. 104/199: loss=(0.40421120170297786-0j)\n",
      "GD iter. 105/199: loss=(0.4041168360935227-0j)\n",
      "GD iter. 106/199: loss=(0.40402460185430333-0j)\n",
      "GD iter. 107/199: loss=(0.40393443685326785-0j)\n",
      "GD iter. 108/199: loss=(0.40384628116802634-0j)\n",
      "GD iter. 109/199: loss=(0.40376007699230565-0j)\n",
      "GD iter. 110/199: loss=(0.403675768547023-0j)\n",
      "GD iter. 111/199: loss=(0.40359330199571647-0j)\n",
      "GD iter. 112/199: loss=(0.40351262536409055-0j)\n",
      "GD iter. 113/199: loss=(0.4034336884634449-0j)\n",
      "GD iter. 114/199: loss=(0.4033564428177741-0j)\n",
      "GD iter. 115/199: loss=(0.40328084159433614-0j)\n",
      "GD iter. 116/199: loss=(0.4032068395375011-0j)\n",
      "GD iter. 117/199: loss=(0.40313439290570247-0j)\n",
      "GD iter. 118/199: loss=(0.40306345941132493-0j)\n",
      "GD iter. 119/199: loss=(0.4029939981633723-0j)\n",
      "GD iter. 120/199: loss=(0.40292596961276783-0j)\n",
      "GD iter. 121/199: loss=(0.4028593355001494-0j)\n",
      "GD iter. 122/199: loss=(0.40279405880602914-0j)\n",
      "GD iter. 123/199: loss=(0.40273010370319456-0j)\n",
      "GD iter. 124/199: loss=(0.4026674355112366-0j)\n",
      "GD iter. 125/199: loss=(0.4026060206530955-0j)\n",
      "GD iter. 126/199: loss=(0.4025458266135218-0j)\n",
      "GD iter. 127/199: loss=(0.4024868218993565-0j)\n",
      "GD iter. 128/199: loss=(0.4024289760015387-0j)\n",
      "GD iter. 129/199: loss=(0.4023722593587558-0j)\n",
      "GD iter. 130/199: loss=(0.4023166433226529-0j)\n",
      "GD iter. 131/199: loss=(0.40226210012452757-0j)\n",
      "GD iter. 132/199: loss=(0.4022086028434359-0j)\n",
      "GD iter. 133/199: loss=(0.40215612537564116-0j)\n",
      "GD iter. 134/199: loss=(0.402104642405342-0j)\n",
      "GD iter. 135/199: loss=(0.40205412937661694-0j)\n",
      "GD iter. 136/199: loss=(0.4020045624665292-0j)\n",
      "GD iter. 137/199: loss=(0.40195591855933605-0j)\n",
      "GD iter. 138/199: loss=(0.4019081752217504-0j)\n",
      "GD iter. 139/199: loss=(0.4018613106792068-0j)\n",
      "GD iter. 140/199: loss=(0.40181530379308417-0j)\n",
      "GD iter. 141/199: loss=(0.40177013403884243-0j)\n",
      "GD iter. 142/199: loss=(0.4017257814850286-0j)\n",
      "GD iter. 143/199: loss=(0.40168222677311677-0j)\n",
      "GD iter. 144/199: loss=(0.40163945109813987-0j)\n",
      "GD iter. 145/199: loss=(0.40159743619008137-0j)\n",
      "GD iter. 146/199: loss=(0.4015561642959902-0j)\n",
      "GD iter. 147/199: loss=(0.40151561816278813-0j)\n",
      "GD iter. 148/199: loss=(0.4014757810207395-0j)\n",
      "GD iter. 149/199: loss=(0.4014366365675517-0j)\n",
      "GD iter. 150/199: loss=(0.4013981689530822-0j)\n",
      "GD iter. 151/199: loss=(0.4013603627646235-0j)\n",
      "GD iter. 152/199: loss=(0.4013232030127417-0j)\n",
      "GD iter. 153/199: loss=(0.401286675117645-0j)\n",
      "GD iter. 154/199: loss=(0.40125076489605926-0j)\n",
      "GD iter. 155/199: loss=(0.40121545854858964-0j)\n",
      "GD iter. 156/199: loss=(0.40118074264754744-0j)\n",
      "GD iter. 157/199: loss=(0.40114660412522213-0j)\n",
      "GD iter. 158/199: loss=(0.401113030262581-0j)\n",
      "GD iter. 159/199: loss=(0.4010800086783782-0j)\n",
      "GD iter. 160/199: loss=(0.4010475273186553-0j)\n",
      "GD iter. 161/199: loss=(0.40101557444661995-0j)\n",
      "GD iter. 162/199: loss=(0.40098413863288385-0j)\n",
      "GD iter. 163/199: loss=(0.40095320874604734-0j)\n",
      "GD iter. 164/199: loss=(0.4009227739436178-0j)\n",
      "GD iter. 165/199: loss=(0.40089282366324447-0j)\n",
      "GD iter. 166/199: loss=(0.4008633476142621-0j)\n",
      "GD iter. 167/199: loss=(0.4008343357695266-0j)\n",
      "GD iter. 168/199: loss=(0.4008057783575341-0j)\n",
      "GD iter. 169/199: loss=(0.4007776658548112-0j)\n",
      "GD iter. 170/199: loss=(0.40074998897856556-0j)\n",
      "GD iter. 171/199: loss=(0.4007227386795875-0j)\n",
      "GD iter. 172/199: loss=(0.4006959061353919-0j)\n",
      "GD iter. 173/199: loss=(0.4006694827435914-0j)\n",
      "GD iter. 174/199: loss=(0.40064346011549357-0j)\n",
      "GD iter. 175/199: loss=(0.40061783006991025-0j)\n",
      "GD iter. 176/199: loss=(0.40059258462717456-0j)\n",
      "GD iter. 177/199: loss=(0.40056771600335495-0j)\n",
      "GD iter. 178/199: loss=(0.40054321660466075-0j)\n",
      "GD iter. 179/199: loss=(0.40051907902203093-0j)\n",
      "GD iter. 180/199: loss=(0.40049529602589956-0j)\n",
      "GD iter. 181/199: loss=(0.4004718605611319-0j)\n",
      "GD iter. 182/199: loss=(0.40044876574212385-0j)\n",
      "GD iter. 183/199: loss=(0.40042600484805974-0j)\n",
      "GD iter. 184/199: loss=(0.4004035713183217-0j)\n",
      "GD iter. 185/199: loss=(0.4003814587480468-0j)\n",
      "GD iter. 186/199: loss=(0.40035966088382363-0j)\n",
      "GD iter. 187/199: loss=(0.4003381716195269-0j)\n",
      "GD iter. 188/199: loss=(0.4003169849922819-0j)\n",
      "GD iter. 189/199: loss=(0.40029609517855674-0j)\n",
      "GD iter. 190/199: loss=(0.4002754964903758-0j)\n",
      "GD iter. 191/199: loss=(0.4002551833716513-0j)\n",
      "GD iter. 192/199: loss=(0.40023515039462865-0j)\n",
      "GD iter. 193/199: loss=(0.4002153922564412-0j)\n",
      "GD iter. 194/199: loss=(0.40019590377577097-0j)\n",
      "GD iter. 195/199: loss=(0.4001766798896111-0j)\n",
      "GD iter. 196/199: loss=(0.4001577156501278-0j)\n",
      "GD iter. 197/199: loss=(0.4001390062216164-0j)\n",
      "GD iter. 198/199: loss=(0.40012054687755033-0j)\n",
      "GD iter. 199/199: loss=(0.4001023329977179-0j)\n",
      "The Accuracy is: 0.8719\n",
      "The F1 score is: 0.3500\n",
      "The precision is: 0.3182\n",
      "The recall is: 0.3889\n",
      "GD iter. 0/199: loss=(0.7015732688617091-0j)\n",
      "GD iter. 1/199: loss=(0.539829557931797-0j)\n",
      "GD iter. 2/199: loss=(0.5239441256994992-0j)\n",
      "GD iter. 3/199: loss=(0.5128333901031201-0j)\n",
      "GD iter. 4/199: loss=(0.504117895882217-0j)\n",
      "GD iter. 5/199: loss=(0.4968717264965824-0j)\n",
      "GD iter. 6/199: loss=(0.4906452212503425-0j)\n",
      "GD iter. 7/199: loss=(0.4851780705095003-0j)\n",
      "GD iter. 8/199: loss=(0.4803041059477057-0j)\n",
      "GD iter. 9/199: loss=(0.4759105100916613-0j)\n",
      "GD iter. 10/199: loss=(0.47191692974539473-0j)\n",
      "GD iter. 11/199: loss=(0.4682637447856973-0j)\n",
      "GD iter. 12/199: loss=(0.4649051219366952-0j)\n",
      "GD iter. 13/199: loss=(0.46180474164722307-0j)\n",
      "GD iter. 14/199: loss=(0.4589330812273271-0j)\n",
      "GD iter. 15/199: loss=(0.4562656330617522-0j)\n",
      "GD iter. 16/199: loss=(0.45378170042246285-0j)\n",
      "GD iter. 17/199: loss=(0.45146355947895184-0j)\n",
      "GD iter. 18/199: loss=(0.44929585944525546-0j)\n",
      "GD iter. 19/199: loss=(0.44726518152452205-0j)\n",
      "GD iter. 20/199: loss=(0.4453597064175671-0j)\n",
      "GD iter. 21/199: loss=(0.44356895789811135-0j)\n",
      "GD iter. 22/199: loss=(0.44188360097144-0j)\n",
      "GD iter. 23/199: loss=(0.4402952801007524-0j)\n",
      "GD iter. 24/199: loss=(0.4387964874766861-0j)\n",
      "GD iter. 25/199: loss=(0.4373804542573131-0j)\n",
      "GD iter. 26/199: loss=(0.43604105968536183-0j)\n",
      "GD iter. 27/199: loss=(0.43477275434419366-0j)\n",
      "GD iter. 28/199: loss=(0.4335704947599591-0j)\n",
      "GD iter. 29/199: loss=(0.43242968723022845-0j)\n",
      "GD iter. 30/199: loss=(0.43134613924615456-0j)\n",
      "GD iter. 31/199: loss=(0.4303160172326687-0j)\n",
      "GD iter. 32/199: loss=(0.42933580959716144-0j)\n",
      "GD iter. 33/199: loss=(0.4284022942774445-0j)\n",
      "GD iter. 34/199: loss=(0.4275125101324667-0j)\n",
      "GD iter. 35/199: loss=(0.4266637316370198-0j)\n",
      "GD iter. 36/199: loss=(0.42585344643359-0j)\n",
      "GD iter. 37/199: loss=(0.42507933536717174-0j)\n",
      "GD iter. 38/199: loss=(0.4243392546870081-0j)\n",
      "GD iter. 39/199: loss=(0.4236312201463308-0j)\n",
      "GD iter. 40/199: loss=(0.4229533927697845-0j)\n",
      "GD iter. 41/199: loss=(0.4223040660902039-0j)\n",
      "GD iter. 42/199: loss=(0.4216816546831553-0j)\n",
      "GD iter. 43/199: loss=(0.4210846838502145-0j)\n",
      "GD iter. 44/199: loss=(0.42051178032109643-0j)\n",
      "GD iter. 45/199: loss=(0.4199616638611294-0j)\n",
      "GD iter. 46/199: loss=(0.4194331396846059-0j)\n",
      "GD iter. 47/199: loss=(0.4189250915866676-0j)\n",
      "GD iter. 48/199: loss=(0.4184364757168635-0j)\n",
      "GD iter. 49/199: loss=(0.4179663149266226-0j)\n",
      "GD iter. 50/199: loss=(0.4175136936308046-0j)\n",
      "GD iter. 51/199: loss=(0.4170777531303946-0j)\n",
      "GD iter. 52/199: loss=(0.4166576873494541-0j)\n",
      "GD iter. 53/199: loss=(0.4162527389447171-0j)\n",
      "GD iter. 54/199: loss=(0.41586219575086475-0j)\n",
      "GD iter. 55/199: loss=(0.4154853875285772-0j)\n",
      "GD iter. 56/199: loss=(0.4151216829860524-0j)\n",
      "GD iter. 57/199: loss=(0.4147704870478299-0j)\n",
      "GD iter. 58/199: loss=(0.414431238347552-0j)\n",
      "GD iter. 59/199: loss=(0.4141034069237469-0j)\n",
      "GD iter. 60/199: loss=(0.4137864920999035-0j)\n",
      "GD iter. 61/199: loss=(0.41348002053203003-0j)\n",
      "GD iter. 62/199: loss=(0.4131835444086069-0j)\n",
      "GD iter. 63/199: loss=(0.4128966397893541-0j)\n",
      "GD iter. 64/199: loss=(0.4126189050705977-0j)\n",
      "GD iter. 65/199: loss=(0.4123499595662108-0j)\n",
      "GD iter. 66/199: loss=(0.41208944219419025-0j)\n",
      "GD iter. 67/199: loss=(0.4118370102598769-0j)\n",
      "GD iter. 68/199: loss=(0.41159233832769265-0j)\n",
      "GD iter. 69/199: loss=(0.41135511717402934-0j)\n",
      "GD iter. 70/199: loss=(0.411125052814613-0j)\n",
      "GD iter. 71/199: loss=(0.4109018656002825-0j)\n",
      "GD iter. 72/199: loss=(0.41068528937567544-0j)\n",
      "GD iter. 73/199: loss=(0.4104750706958101-0j)\n",
      "GD iter. 74/199: loss=(0.41027096809600444-0j)\n",
      "GD iter. 75/199: loss=(0.41007275141097094-0j)\n",
      "GD iter. 76/199: loss=(0.40988020113929324-0j)\n",
      "GD iter. 77/199: loss=(0.4096931078498215-0j)\n",
      "GD iter. 78/199: loss=(0.40951127162681594-0j)\n",
      "GD iter. 79/199: loss=(0.40933450155094103-0j)\n",
      "GD iter. 80/199: loss=(0.4091626152134538-0j)\n",
      "GD iter. 81/199: loss=(0.40899543826115486-0j)\n",
      "GD iter. 82/199: loss=(0.4088328039698646-0j)\n",
      "GD iter. 83/199: loss=(0.4086745528443783-0j)\n",
      "GD iter. 84/199: loss=(0.4085205322430112-0j)\n",
      "GD iter. 85/199: loss=(0.40837059602500164-0j)\n",
      "GD iter. 86/199: loss=(0.408224604219175-0j)\n",
      "GD iter. 87/199: loss=(0.4080824227123974-0j)\n",
      "GD iter. 88/199: loss=(0.4079439229564635-0j)\n",
      "GD iter. 89/199: loss=(0.40780898169216534-0j)\n",
      "GD iter. 90/199: loss=(0.40767748068938847-0j)\n",
      "GD iter. 91/199: loss=(0.4075493065021639-0j)\n",
      "GD iter. 92/199: loss=(0.40742435023769086-0j)\n",
      "GD iter. 93/199: loss=(0.4073025073384135-0j)\n",
      "GD iter. 94/199: loss=(0.40718367737630623-0j)\n",
      "GD iter. 95/199: loss=(0.40706776385858073-0j)\n",
      "GD iter. 96/199: loss=(0.40695467404408775-0j)\n",
      "GD iter. 97/199: loss=(0.40684431876973803-0j)\n",
      "GD iter. 98/199: loss=(0.40673661228631314-0j)\n",
      "GD iter. 99/199: loss=(0.40663147210308487-0j)\n",
      "GD iter. 100/199: loss=(0.4065288188406992-0j)\n",
      "GD iter. 101/199: loss=(0.4064285760918205-0j)\n",
      "GD iter. 102/199: loss=(0.4063306702890669-0j)\n",
      "GD iter. 103/199: loss=(0.40623503057979776-0j)\n",
      "GD iter. 104/199: loss=(0.40614158870734574-0j)\n",
      "GD iter. 105/199: loss=(0.4060502788983119-0j)\n",
      "GD iter. 106/199: loss=(0.4059610377555709-0j)\n",
      "GD iter. 107/199: loss=(0.4058738041566509-0j)\n",
      "GD iter. 108/199: loss=(0.4057885191571814-0j)\n",
      "GD iter. 109/199: loss=(0.4057051258991171-0j)\n",
      "GD iter. 110/199: loss=(0.40562356952346756-0j)\n",
      "GD iter. 111/199: loss=(0.40554379708727906-0j)\n",
      "GD iter. 112/199: loss=(0.40546575748463004-0j)\n",
      "GD iter. 113/199: loss=(0.40538940137141904-0j)\n",
      "GD iter. 114/199: loss=(0.4053146810937351-0j)\n",
      "GD iter. 115/199: loss=(0.40524155061961564-0j)\n",
      "GD iter. 116/199: loss=(0.4051699654740085-0j)\n",
      "GD iter. 117/199: loss=(0.40509988267676444-0j)\n",
      "GD iter. 118/199: loss=(0.40503126068350026-0j)\n",
      "GD iter. 119/199: loss=(0.40496405932917806-0j)\n",
      "GD iter. 120/199: loss=(0.4048982397742598-0j)\n",
      "GD iter. 121/199: loss=(0.4048337644533013-0j)\n",
      "GD iter. 122/199: loss=(0.4047705970258593-0j)\n",
      "GD iter. 123/199: loss=(0.40470870232959283-0j)\n",
      "GD iter. 124/199: loss=(0.4046480463354461-0j)\n",
      "GD iter. 125/199: loss=(0.4045885961048084-0j)\n",
      "GD iter. 126/199: loss=(0.4045303197485489-0j)\n",
      "GD iter. 127/199: loss=(0.4044731863878354-0j)\n",
      "GD iter. 128/199: loss=(0.4044171661166455-0j)\n",
      "GD iter. 129/199: loss=(0.4043622299658888-0j)\n",
      "GD iter. 130/199: loss=(0.40430834986905956-0j)\n",
      "GD iter. 131/199: loss=(0.4042554986293458-0j)\n",
      "GD iter. 132/199: loss=(0.4042036498881239-0j)\n",
      "GD iter. 133/199: loss=(0.40415277809477307-0j)\n",
      "GD iter. 134/199: loss=(0.40410285847774446-0j)\n",
      "GD iter. 135/199: loss=(0.4040538670168285-0j)\n",
      "GD iter. 136/199: loss=(0.4040057804165602-0j)\n",
      "GD iter. 137/199: loss=(0.4039585760807119-0j)\n",
      "GD iter. 138/199: loss=(0.40391223208782184-0j)\n",
      "GD iter. 139/199: loss=(0.40386672716770994-0j)\n",
      "GD iter. 140/199: loss=(0.4038220406789366-0j)\n",
      "GD iter. 141/199: loss=(0.40377815258715977-0j)\n",
      "GD iter. 142/199: loss=(0.4037350434443515-0j)\n",
      "GD iter. 143/199: loss=(0.403692694368833-0j)\n",
      "GD iter. 144/199: loss=(0.40365108702609287-0j)\n",
      "GD iter. 145/199: loss=(0.4036102036103534-0j)\n",
      "GD iter. 146/199: loss=(0.40357002682685045-0j)\n",
      "GD iter. 147/199: loss=(0.40353053987479753-0j)\n",
      "GD iter. 148/199: loss=(0.4034917264310021-0j)\n",
      "GD iter. 149/199: loss=(0.4034535706341068-0j)\n",
      "GD iter. 150/199: loss=(0.4034160570694279-0j)\n",
      "GD iter. 151/199: loss=(0.40337917075436586-0j)\n",
      "GD iter. 152/199: loss=(0.40334289712436255-0j)\n",
      "GD iter. 153/199: loss=(0.4033072220193834-0j)\n",
      "GD iter. 154/199: loss=(0.40327213167089915-0j)\n",
      "GD iter. 155/199: loss=(0.40323761268935043-0j)\n",
      "GD iter. 156/199: loss=(0.4032036520520714-0j)\n",
      "GD iter. 157/199: loss=(0.4031702370916546-0j)\n",
      "GD iter. 158/199: loss=(0.4031373554847388-0j)\n",
      "GD iter. 159/199: loss=(0.40310499524120225-0j)\n",
      "GD iter. 160/199: loss=(0.4030731446937448-0j)\n",
      "GD iter. 161/199: loss=(0.40304179248784283-0j)\n",
      "GD iter. 162/199: loss=(0.40301092757206186-0j)\n",
      "GD iter. 163/199: loss=(0.4029805391887122-0j)\n",
      "GD iter. 164/199: loss=(0.4029506168648347-0j)\n",
      "GD iter. 165/199: loss=(0.4029211504035025-0j)\n",
      "GD iter. 166/199: loss=(0.4028921298754263-0j)\n",
      "GD iter. 167/199: loss=(0.4028635456108517-0j)\n",
      "GD iter. 168/199: loss=(0.40283538819173664-0j)\n",
      "GD iter. 169/199: loss=(0.4028076484441983-0j)\n",
      "GD iter. 170/199: loss=(0.40278031743121817-0j)\n",
      "GD iter. 171/199: loss=(0.4027533864455963-0j)\n",
      "GD iter. 172/199: loss=(0.4027268470031451-0j)\n",
      "GD iter. 173/199: loss=(0.40270069083611193-0j)\n",
      "GD iter. 174/199: loss=(0.40267490988682403-0j)\n",
      "GD iter. 175/199: loss=(0.40264949630154506-0j)\n",
      "GD iter. 176/199: loss=(0.40262444242453765-0j)\n",
      "GD iter. 177/199: loss=(0.40259974079232147-0j)\n",
      "GD iter. 178/199: loss=(0.4025753841281221-0j)\n",
      "GD iter. 179/199: loss=(0.4025513653365015-0j)\n",
      "GD iter. 180/199: loss=(0.40252767749816487-0j)\n",
      "GD iter. 181/199: loss=(0.4025043138649364-0j)\n",
      "GD iter. 182/199: loss=(0.40248126785489796-0j)\n",
      "GD iter. 183/199: loss=(0.40245853304768525-0j)\n",
      "GD iter. 184/199: loss=(0.4024361031799343-0j)\n",
      "GD iter. 185/199: loss=(0.4024139721408751-0j)\n",
      "GD iter. 186/199: loss=(0.40239213396806467-0j)\n",
      "GD iter. 187/199: loss=(0.4023705828432562-0j)\n",
      "GD iter. 188/199: loss=(0.40234931308839833-0j)\n",
      "GD iter. 189/199: loss=(0.40232831916176065-0j)\n",
      "GD iter. 190/199: loss=(0.4023075956541814-0j)\n",
      "GD iter. 191/199: loss=(0.40228713728543036-0j)\n",
      "GD iter. 192/199: loss=(0.40226693890068754-0j)\n",
      "GD iter. 193/199: loss=(0.4022469954671284-0j)\n",
      "GD iter. 194/199: loss=(0.402227302070616-0j)\n",
      "GD iter. 195/199: loss=(0.4022078539124943-0j)\n",
      "GD iter. 196/199: loss=(0.40218864630647916-0j)\n",
      "GD iter. 197/199: loss=(0.4021696746756448-0j)\n",
      "GD iter. 198/199: loss=(0.4021509345495014-0j)\n",
      "GD iter. 199/199: loss=(0.4021324215611606-0j)\n",
      "The Accuracy is: 0.8637\n",
      "The F1 score is: 0.3664\n",
      "The precision is: 0.3038\n",
      "The recall is: 0.4615\n",
      "GD iter. 0/199: loss=(0.7053537893434695-0j)\n",
      "GD iter. 1/199: loss=(0.5372297887289191-0j)\n",
      "GD iter. 2/199: loss=(0.5218944171783777-0j)\n",
      "GD iter. 3/199: loss=(0.5110232120428475-0j)\n",
      "GD iter. 4/199: loss=(0.502388346157611-0j)\n",
      "GD iter. 5/199: loss=(0.4951377340494223-0j)\n",
      "GD iter. 6/199: loss=(0.48885722385836683-0j)\n",
      "GD iter. 7/199: loss=(0.48330526610288316-0j)\n",
      "GD iter. 8/199: loss=(0.47832708567744875-0j)\n",
      "GD iter. 9/199: loss=(0.4738173795370052-0j)\n",
      "GD iter. 10/199: loss=(0.46970093758511394-0j)\n",
      "GD iter. 11/199: loss=(0.46592171385212106-0j)\n",
      "GD iter. 12/199: loss=(0.4624363698927091-0j)\n",
      "GD iter. 13/199: loss=(0.4592103222826158-0j)\n",
      "GD iter. 14/199: loss=(0.456215243770466-0j)\n",
      "GD iter. 15/199: loss=(0.4534274332924623-0j)\n",
      "GD iter. 16/199: loss=(0.45082671934433716-0j)\n",
      "GD iter. 17/199: loss=(0.4483956992856554-0j)\n",
      "GD iter. 18/199: loss=(0.44611919567937486-0j)\n",
      "GD iter. 19/199: loss=(0.44398385646170924-0j)\n",
      "GD iter. 20/199: loss=(0.44197785288674624-0j)\n",
      "GD iter. 21/199: loss=(0.44009064564473316-0j)\n",
      "GD iter. 22/199: loss=(0.43831279971795106-0j)\n",
      "GD iter. 23/199: loss=(0.4366358349363561-0j)\n",
      "GD iter. 24/199: loss=(0.43505210329772936-0j)\n",
      "GD iter. 25/199: loss=(0.4335546867965256-0j)\n",
      "GD iter. 26/199: loss=(0.43213731128799904-0j)\n",
      "GD iter. 27/199: loss=(0.43079427312164953-0j)\n",
      "GD iter. 28/199: loss=(0.4295203761109897-0j)\n",
      "GD iter. 29/199: loss=(0.4283108769916494-0j)\n",
      "GD iter. 30/199: loss=(0.4271614379381204-0j)\n",
      "GD iter. 31/199: loss=(0.42606808501386-0j)\n",
      "GD iter. 32/199: loss=(0.4250271716549377-0j)\n",
      "GD iter. 33/199: loss=(0.4240353464573259-0j)\n",
      "GD iter. 34/199: loss=(0.4230895246681538-0j)\n",
      "GD iter. 35/199: loss=(0.4221868628826919-0j)\n",
      "GD iter. 36/199: loss=(0.4213247365290743-0j)\n",
      "GD iter. 37/199: loss=(0.42050071978715686-0j)\n",
      "GD iter. 38/199: loss=(0.41971256764024417-0j)\n",
      "GD iter. 39/199: loss=(0.4189581998014562-0j)\n",
      "GD iter. 40/199: loss=(0.41823568629225605-0j)\n",
      "GD iter. 41/199: loss=(0.41754323448062036-0j)\n",
      "GD iter. 42/199: loss=(0.41687917741163283-0j)\n",
      "GD iter. 43/199: loss=(0.4162419632847792-0j)\n",
      "GD iter. 44/199: loss=(0.4156301459505958-0j)\n",
      "GD iter. 45/199: loss=(0.41504237631509233-0j)\n",
      "GD iter. 46/199: loss=(0.414477394553967-0j)\n",
      "GD iter. 47/199: loss=(0.4139340230503838-0j)\n",
      "GD iter. 48/199: loss=(0.4134111599802861-0j)\n",
      "GD iter. 49/199: loss=(0.4129077734780879-0j)\n",
      "GD iter. 50/199: loss=(0.4124228963233162-0j)\n",
      "GD iter. 51/199: loss=(0.4119556210955392-0j)\n",
      "GD iter. 52/199: loss=(0.411505095750824-0j)\n",
      "GD iter. 53/199: loss=(0.41107051957815816-0j)\n",
      "GD iter. 54/199: loss=(0.4106511394988251-0j)\n",
      "GD iter. 55/199: loss=(0.41024624667573323-0j)\n",
      "GD iter. 56/199: loss=(0.4098551734032326-0j)\n",
      "GD iter. 57/199: loss=(0.40947729025107144-0j)\n",
      "GD iter. 58/199: loss=(0.40911200343890497-0j)\n",
      "GD iter. 59/199: loss=(0.40875875242020493-0j)\n",
      "GD iter. 60/199: loss=(0.4084170076565865-0j)\n",
      "GD iter. 61/199: loss=(0.4080862685654885-0j)\n",
      "GD iter. 62/199: loss=(0.407766061625848-0j)\n",
      "GD iter. 63/199: loss=(0.40745593862793283-0j)\n",
      "GD iter. 64/199: loss=(0.4071554750548488-0j)\n",
      "GD iter. 65/199: loss=(0.40686426858444397-0j)\n",
      "GD iter. 66/199: loss=(0.4065819377014169-0j)\n",
      "GD iter. 67/199: loss=(0.40630812041039777-0j)\n",
      "GD iter. 68/199: loss=(0.40604247304163704-0j)\n",
      "GD iter. 69/199: loss=(0.4057846691417124-0j)\n",
      "GD iter. 70/199: loss=(0.4055343984423603-0j)\n",
      "GD iter. 71/199: loss=(0.40529136590116643-0j)\n",
      "GD iter. 72/199: loss=(0.40505529080840785-0j)\n",
      "GD iter. 73/199: loss=(0.4048259059548522-0j)\n",
      "GD iter. 74/199: loss=(0.4046029568557732-0j)\n",
      "GD iter. 75/199: loss=(0.4043862010268561-0j)\n",
      "GD iter. 76/199: loss=(0.40417540730803847-0j)\n",
      "GD iter. 77/199: loss=(0.4039703552316727-0j)\n",
      "GD iter. 78/199: loss=(0.40377083443169737-0j)\n",
      "GD iter. 79/199: loss=(0.40357664409078653-0j)\n",
      "GD iter. 80/199: loss=(0.4033875924226941-0j)\n",
      "GD iter. 81/199: loss=(0.40320349618724116-0j)\n",
      "GD iter. 82/199: loss=(0.4030241802356-0j)\n",
      "GD iter. 83/199: loss=(0.40284947708371865-0j)\n",
      "GD iter. 84/199: loss=(0.4026792265119026-0j)\n",
      "GD iter. 85/199: loss=(0.402513275188724-0j)\n",
      "GD iter. 86/199: loss=(0.4023514763175764-0j)\n",
      "GD iter. 87/199: loss=(0.40219368930432053-0j)\n",
      "GD iter. 88/199: loss=(0.40203977944458713-0j)\n",
      "GD iter. 89/199: loss=(0.40188961762941244-0j)\n",
      "GD iter. 90/199: loss=(0.40174308006798154-0j)\n",
      "GD iter. 91/199: loss=(0.4016000480263475-0j)\n",
      "GD iter. 92/199: loss=(0.4014604075810766-0j)\n",
      "GD iter. 93/199: loss=(0.40132404938684907-0j)\n",
      "GD iter. 94/199: loss=(0.4011908684571141-0j)\n",
      "GD iter. 95/199: loss=(0.40106076395696466-0j)\n",
      "GD iter. 96/199: loss=(0.4009336390074551-0j)\n",
      "GD iter. 97/199: loss=(0.4008094005006432-0j)\n",
      "GD iter. 98/199: loss=(0.40068795892468556-0j)\n",
      "GD iter. 99/199: loss=(0.40056922819836466-0j)\n",
      "GD iter. 100/199: loss=(0.40045312551446816-0j)\n",
      "GD iter. 101/199: loss=(0.4003395711914807-0j)\n",
      "GD iter. 102/199: loss=(0.4002284885330854-0j)\n",
      "GD iter. 103/199: loss=(0.4001198036950072-0j)\n",
      "GD iter. 104/199: loss=(0.4000134455587607-0j)\n",
      "GD iter. 105/199: loss=(0.3999093456118946-0j)\n",
      "GD iter. 106/199: loss=(0.39980743783435113-0j)\n",
      "GD iter. 107/199: loss=(0.39970765859058605-0j)\n",
      "GD iter. 108/199: loss=(0.399609946527116-0j)\n",
      "GD iter. 109/199: loss=(0.3995142424751814-0j)\n",
      "GD iter. 110/199: loss=(0.39942048935823465-0j)\n",
      "GD iter. 111/199: loss=(0.3993286321039805-0j)\n",
      "GD iter. 112/199: loss=(0.39923861756071427-0j)\n",
      "GD iter. 113/199: loss=(0.39915039441771716-0j)\n",
      "GD iter. 114/199: loss=(0.3990639131294854-0j)\n",
      "GD iter. 115/199: loss=(0.39897912584358264-0j)\n",
      "GD iter. 116/199: loss=(0.3988959863319176-0j)\n",
      "GD iter. 117/199: loss=(0.39881444992526255-0j)\n",
      "GD iter. 118/199: loss=(0.3987344734508372-0j)\n",
      "GD iter. 119/199: loss=(0.39865601517279503-0j)\n",
      "GD iter. 120/199: loss=(0.3985790347354585-0j)\n",
      "GD iter. 121/199: loss=(0.3985034931091575-0j)\n",
      "GD iter. 122/199: loss=(0.39842935253853523-0j)\n",
      "GD iter. 123/199: loss=(0.3983565764931938-0j)\n",
      "GD iter. 124/199: loss=(0.39828512962055823-0j)\n",
      "GD iter. 125/199: loss=(0.39821497770084496-0j)\n",
      "GD iter. 126/199: loss=(0.39814608760402764-0j)\n",
      "GD iter. 127/199: loss=(0.39807842724870107-0j)\n",
      "GD iter. 128/199: loss=(0.3980119655627434-0j)\n",
      "GD iter. 129/199: loss=(0.39794667244569226-0j)\n",
      "GD iter. 130/199: loss=(0.3978825187327453-0j)\n",
      "GD iter. 131/199: loss=(0.39781947616030744-0j)\n",
      "GD iter. 132/199: loss=(0.39775751733300807-0j)\n",
      "GD iter. 133/199: loss=(0.3976966156921169-0j)\n",
      "GD iter. 134/199: loss=(0.39763674548529115-0j)\n",
      "GD iter. 135/199: loss=(0.39757788173758885-0j)\n",
      "GD iter. 136/199: loss=(0.39752000022368933-0j)\n",
      "GD iter. 137/199: loss=(0.39746307744126214-0j)\n",
      "GD iter. 138/199: loss=(0.3974070905854311-0j)\n",
      "GD iter. 139/199: loss=(0.3973520175242823-0j)\n",
      "GD iter. 140/199: loss=(0.3972978367753656-0j)\n",
      "GD iter. 141/199: loss=(0.3972445274831462-0j)\n",
      "GD iter. 142/199: loss=(0.39719206939736035-0j)\n",
      "GD iter. 143/199: loss=(0.39714044285223515-0j)\n",
      "GD iter. 144/199: loss=(0.39708962874653186-0j)\n",
      "GD iter. 145/199: loss=(0.3970396085243773-0j)\n",
      "GD iter. 146/199: loss=(0.396990364156846-0j)\n",
      "GD iter. 147/199: loss=(0.3969418781242599-0j)\n",
      "GD iter. 148/199: loss=(0.3968941333991755-0j)\n",
      "GD iter. 149/199: loss=(0.39684711343002577-0j)\n",
      "GD iter. 150/199: loss=(0.39680080212538876-0j)\n",
      "GD iter. 151/199: loss=(0.396755183838857-0j)\n",
      "GD iter. 152/199: loss=(0.3967102433544791-0j)\n",
      "GD iter. 153/199: loss=(0.396665965872751-0j)\n",
      "GD iter. 154/199: loss=(0.3966223369971322-0j)\n",
      "GD iter. 155/199: loss=(0.3965793427210645-0j)\n",
      "GD iter. 156/199: loss=(0.39653696941547323-0j)\n",
      "GD iter. 157/199: loss=(0.39649520381672826-0j)\n",
      "GD iter. 158/199: loss=(0.39645403301504833-0j)\n",
      "GD iter. 159/199: loss=(0.3964134444433274-0j)\n",
      "GD iter. 160/199: loss=(0.3963734258663681-0j)\n",
      "GD iter. 161/199: loss=(0.39633396537050364-0j)\n",
      "GD iter. 162/199: loss=(0.3962950513535928-0j)\n",
      "GD iter. 163/199: loss=(0.39625667251537317-0j)\n",
      "GD iter. 164/199: loss=(0.3962188178481584-0j)\n",
      "GD iter. 165/199: loss=(0.3961814766278631-0j)\n",
      "GD iter. 166/199: loss=(0.3961446384053468-0j)\n",
      "GD iter. 167/199: loss=(0.39610829299805933-0j)\n",
      "GD iter. 168/199: loss=(0.39607243048197904-0j)\n",
      "GD iter. 169/199: loss=(0.3960370411838312-0j)\n",
      "GD iter. 170/199: loss=(0.396002115673575-0j)\n",
      "GD iter. 171/199: loss=(0.3959676447571497-0j)\n",
      "GD iter. 172/199: loss=(0.39593361946946937-0j)\n",
      "GD iter. 173/199: loss=(0.39590003106765614-0j)\n",
      "GD iter. 174/199: loss=(0.3958668710245041-0j)\n",
      "GD iter. 175/199: loss=(0.39583413102216297-0j)\n",
      "GD iter. 176/199: loss=(0.39580180294603606-0j)\n",
      "GD iter. 177/199: loss=(0.395769878878881-0j)\n",
      "GD iter. 178/199: loss=(0.3957383510951087-0j)\n",
      "GD iter. 179/199: loss=(0.3957072120552707-0j)\n",
      "GD iter. 180/199: loss=(0.39567645440072985-0j)\n",
      "GD iter. 181/199: loss=(0.3956460709485058-0j)\n",
      "GD iter. 182/199: loss=(0.3956160546862899-0j)\n",
      "GD iter. 183/199: loss=(0.39558639876762336-0j)\n",
      "GD iter. 184/199: loss=(0.3955570965072314-0j)\n",
      "GD iter. 185/199: loss=(0.3955281413765097-0j)\n",
      "GD iter. 186/199: loss=(0.39549952699915525-0j)\n",
      "GD iter. 187/199: loss=(0.39547124714693904-0j)\n",
      "GD iter. 188/199: loss=(0.395443295735613-0j)\n",
      "GD iter. 189/199: loss=(0.39541566682094803-0j)\n",
      "GD iter. 190/199: loss=(0.39538835459489735-0j)\n",
      "GD iter. 191/199: loss=(0.3953613533818822-0j)\n",
      "GD iter. 192/199: loss=(0.3953346576351934-0j)\n",
      "GD iter. 193/199: loss=(0.3953082619335067-0j)\n",
      "GD iter. 194/199: loss=(0.3952821609775066-0j)\n",
      "GD iter. 195/199: loss=(0.39525634958661615-0j)\n",
      "GD iter. 196/199: loss=(0.3952308226958268-0j)\n",
      "GD iter. 197/199: loss=(0.3952055753526276-0j)\n",
      "GD iter. 198/199: loss=(0.39518060271402833-0j)\n",
      "GD iter. 199/199: loss=(0.39515590004367374-0j)\n",
      "The Accuracy is: 0.8670\n",
      "The F1 score is: 0.4173\n",
      "The precision is: 0.3766\n",
      "The recall is: 0.4677\n",
      "GD iter. 0/199: loss=(0.6958964303439461-0j)\n",
      "GD iter. 1/199: loss=(0.5320940244032004-0j)\n",
      "GD iter. 2/199: loss=(0.5167565779191314-0j)\n",
      "GD iter. 3/199: loss=(0.5059461718275512-0j)\n",
      "GD iter. 4/199: loss=(0.4974186223122973-0j)\n",
      "GD iter. 5/199: loss=(0.49029950390911625-0j)\n",
      "GD iter. 6/199: loss=(0.48416052305590895-0j)\n",
      "GD iter. 7/199: loss=(0.47875258031992646-0j)\n",
      "GD iter. 8/199: loss=(0.47391668840603296-0j)\n",
      "GD iter. 9/199: loss=(0.4695450964871722-0j)\n",
      "GD iter. 10/199: loss=(0.46556117456205665-0j)\n",
      "GD iter. 11/199: loss=(0.4619080952864755-0j)\n",
      "GD iter. 12/199: loss=(0.45854215012240596-0j)\n",
      "GD iter. 13/199: loss=(0.455428655775858-0j)\n",
      "GD iter. 14/199: loss=(0.4525393641664955-0j)\n",
      "GD iter. 15/199: loss=(0.4498507716479197-0j)\n",
      "GD iter. 16/199: loss=(0.4473429807583941-0j)\n",
      "GD iter. 17/199: loss=(0.4449989103531213-0j)\n",
      "GD iter. 18/199: loss=(0.44280373108775123-0j)\n",
      "GD iter. 19/199: loss=(0.4407444504649455-0j)\n",
      "GD iter. 20/199: loss=(0.43880959974998984-0j)\n",
      "GD iter. 21/199: loss=(0.4369889920942907-0j)\n",
      "GD iter. 22/199: loss=(0.43527353172200817-0j)\n",
      "GD iter. 23/199: loss=(0.4336550606434644-0j)\n",
      "GD iter. 24/199: loss=(0.4321262335852171-0j)\n",
      "GD iter. 25/199: loss=(0.4306804145793978-0j)\n",
      "GD iter. 26/199: loss=(0.4293115904835627-0j)\n",
      "GD iter. 27/199: loss=(0.4280142979437352-0j)\n",
      "GD iter. 28/199: loss=(0.42678356117607813-0j)\n",
      "GD iter. 29/199: loss=(0.4256148385569656-0j)\n",
      "GD iter. 30/199: loss=(0.42450397645899435-0j)\n",
      "GD iter. 31/199: loss=(0.4234471691037014-0j)\n",
      "GD iter. 32/199: loss=(0.4224409234539648-0j)\n",
      "GD iter. 33/199: loss=(0.42148202836244614-0j)\n",
      "GD iter. 34/199: loss=(0.4205675273420992-0j)\n",
      "GD iter. 35/199: loss=(0.41969469444144003-0j)\n",
      "GD iter. 36/199: loss=(0.41886101279876936-0j)\n",
      "GD iter. 37/199: loss=(0.4180641555217245-0j)\n",
      "GD iter. 38/199: loss=(0.4173019685958954-0j)\n",
      "GD iter. 39/199: loss=(0.41657245557216027-0j)\n",
      "GD iter. 40/199: loss=(0.4158737638195079-0j)\n",
      "GD iter. 41/199: loss=(0.4152041721604112-0j)\n",
      "GD iter. 42/199: loss=(0.4145620797307882-0j)\n",
      "GD iter. 43/199: loss=(0.41394599592739917-0j)\n",
      "GD iter. 44/199: loss=(0.4133545313230344-0j)\n",
      "GD iter. 45/199: loss=(0.41278638944469576-0j)\n",
      "GD iter. 46/199: loss=(0.41224035932268205-0j)\n",
      "GD iter. 47/199: loss=(0.4117153087294172-0j)\n",
      "GD iter. 48/199: loss=(0.4112101780363271-0j)\n",
      "GD iter. 49/199: loss=(0.41072397462529997-0j)\n",
      "GD iter. 50/199: loss=(0.41025576779844725-0j)\n",
      "GD iter. 51/199: loss=(0.4098046841361729-0j)\n",
      "GD iter. 52/199: loss=(0.4093699032590799-0j)\n",
      "GD iter. 53/199: loss=(0.40895065395410085-0j)\n",
      "GD iter. 54/199: loss=(0.40854621062951807-0j)\n",
      "GD iter. 55/199: loss=(0.40815589006732184-0j)\n",
      "GD iter. 56/199: loss=(0.40777904844469354-0j)\n",
      "GD iter. 57/199: loss=(0.40741507859935844-0j)\n",
      "GD iter. 58/199: loss=(0.40706340751617665-0j)\n",
      "GD iter. 59/199: loss=(0.4067234940146643-0j)\n",
      "GD iter. 60/199: loss=(0.4063948266192045-0j)\n",
      "GD iter. 61/199: loss=(0.4060769215955472-0j)\n",
      "GD iter. 62/199: loss=(0.4057693211388291-0j)\n",
      "GD iter. 63/199: loss=(0.4054715916998101-0j)\n",
      "GD iter. 64/199: loss=(0.4051833224373166-0j)\n",
      "GD iter. 65/199: loss=(0.4049041237860489-0j)\n",
      "GD iter. 66/199: loss=(0.4046336261299495-0j)\n",
      "GD iter. 67/199: loss=(0.404371478572256-0j)\n",
      "GD iter. 68/199: loss=(0.40411734779419944-0j)\n",
      "GD iter. 69/199: loss=(0.4038709169950544-0j)\n",
      "GD iter. 70/199: loss=(0.4036318849069197-0j)\n",
      "GD iter. 71/199: loss=(0.40339996487821084-0j)\n",
      "GD iter. 72/199: loss=(0.40317488402039037-0j)\n",
      "GD iter. 73/199: loss=(0.40295638241294884-0j)\n",
      "GD iter. 74/199: loss=(0.4027442123620911-0j)\n",
      "GD iter. 75/199: loss=(0.402538137708983-0j)\n",
      "GD iter. 76/199: loss=(0.4023379331837676-0j)\n",
      "GD iter. 77/199: loss=(0.40214338380189035-0j)\n",
      "GD iter. 78/199: loss=(0.401954284299565-0j)\n",
      "GD iter. 79/199: loss=(0.40177043860547756-0j)\n",
      "GD iter. 80/199: loss=(0.40159165934606933-0j)\n",
      "GD iter. 81/199: loss=(0.401417767381958-0j)\n",
      "GD iter. 82/199: loss=(0.4012485913732568-0j)\n",
      "GD iter. 83/199: loss=(0.40108396737173074-0j)\n",
      "GD iter. 84/199: loss=(0.40092373843789786-0j)\n",
      "GD iter. 85/199: loss=(0.40076775428132844-0j)\n",
      "GD iter. 86/199: loss=(0.4006158709225376-0j)\n",
      "GD iter. 87/199: loss=(0.40046795037499006-0j)\n",
      "GD iter. 88/199: loss=(0.4003238603458465-0j)\n",
      "GD iter. 89/199: loss=(0.400183473954193-0j)\n",
      "GD iter. 90/199: loss=(0.40004666946558326-0j)\n",
      "GD iter. 91/199: loss=(0.3999133300418156-0j)\n",
      "GD iter. 92/199: loss=(0.3997833435049458-0j)\n",
      "GD iter. 93/199: loss=(0.39965660211460835-0j)\n",
      "GD iter. 94/199: loss=(0.39953300235779143-0j)\n",
      "GD iter. 95/199: loss=(0.39941244475026655-0j)\n",
      "GD iter. 96/199: loss=(0.3992948336489361-0j)\n",
      "GD iter. 97/199: loss=(0.3991800770744121-0j)\n",
      "GD iter. 98/199: loss=(0.39906808654318865-0j)\n",
      "GD iter. 99/199: loss=(0.3989587769088152-0j)\n",
      "GD iter. 100/199: loss=(0.39885206621151786-0j)\n",
      "GD iter. 101/199: loss=(0.3987478755357551-0j)\n",
      "GD iter. 102/199: loss=(0.3986461288752302-0j)\n",
      "GD iter. 103/199: loss=(0.39854675300491155-0j)\n",
      "GD iter. 104/199: loss=(0.398449677359647-0j)\n",
      "GD iter. 105/199: loss=(0.3983548339189807-0j)\n",
      "GD iter. 106/199: loss=(0.3982621570978121-0j)\n",
      "GD iter. 107/199: loss=(0.3981715836425556-0j)\n",
      "GD iter. 108/199: loss=(0.39808305253248544-0j)\n",
      "GD iter. 109/199: loss=(0.3979965048859678-0j)\n",
      "GD iter. 110/199: loss=(0.39791188387130344-0j)\n",
      "GD iter. 111/199: loss=(0.397829134621921-0j)\n",
      "GD iter. 112/199: loss=(0.3977482041556762-0j)\n",
      "GD iter. 113/199: loss=(0.3976690412980306-0j)\n",
      "GD iter. 114/199: loss=(0.39759159660889465-0j)\n",
      "GD iter. 115/199: loss=(0.39751582231293414-0j)\n",
      "GD iter. 116/199: loss=(0.3974416722331538-0j)\n",
      "GD iter. 117/199: loss=(0.39736910172757756-0j)\n",
      "GD iter. 118/199: loss=(0.3972980676288632-0j)\n",
      "GD iter. 119/199: loss=(0.3972285281866926-0j)\n",
      "GD iter. 120/199: loss=(0.39716044301279196-0j)\n",
      "GD iter. 121/199: loss=(0.39709377302844384-0j)\n",
      "GD iter. 122/199: loss=(0.3970284804143615-0j)\n",
      "GD iter. 123/199: loss=(0.3969645285628016-0j)\n",
      "GD iter. 124/199: loss=(0.3969018820318032-0j)\n",
      "GD iter. 125/199: loss=(0.3968405065014414-0j)\n",
      "GD iter. 126/199: loss=(0.3967803687319962-0j)\n",
      "GD iter. 127/199: loss=(0.3967214365239377-0j)\n",
      "GD iter. 128/199: loss=(0.39666367867963936-0j)\n",
      "GD iter. 129/199: loss=(0.39660706496673104-0j)\n",
      "GD iter. 130/199: loss=(0.3965515660830137-0j)\n",
      "GD iter. 131/199: loss=(0.396497153622856-0j)\n",
      "GD iter. 132/199: loss=(0.39644380004500435-0j)\n",
      "GD iter. 133/199: loss=(0.39639147864173496-0j)\n",
      "GD iter. 134/199: loss=(0.39634016350928564-0j)\n",
      "GD iter. 135/199: loss=(0.3962898295195055-0j)\n",
      "GD iter. 136/199: loss=(0.3962404522926644-0j)\n",
      "GD iter. 137/199: loss=(0.39619200817136907-0j)\n",
      "GD iter. 138/199: loss=(0.39614447419553284-0j)\n",
      "GD iter. 139/199: loss=(0.3960978280783508-0j)\n",
      "GD iter. 140/199: loss=(0.3960520481832332-0j)\n",
      "GD iter. 141/199: loss=(0.39600711350165524-0j)\n",
      "GD iter. 142/199: loss=(0.39596300363187836-0j)\n",
      "GD iter. 143/199: loss=(0.395919698758507-0j)\n",
      "GD iter. 144/199: loss=(0.39587717963284086-0j)\n",
      "GD iter. 145/199: loss=(0.3958354275539877-0j)\n",
      "GD iter. 146/199: loss=(0.3957944243507045-0j)\n",
      "GD iter. 147/199: loss=(0.39575415236393197-0j)\n",
      "GD iter. 148/199: loss=(0.39571459442999557-0j)\n",
      "GD iter. 149/199: loss=(0.39567573386443977-0j)\n",
      "GD iter. 150/199: loss=(0.3956375544464732-0j)\n",
      "GD iter. 151/199: loss=(0.3956000404039933-0j)\n",
      "GD iter. 152/199: loss=(0.3955631763991701-0j)\n",
      "GD iter. 153/199: loss=(0.39552694751456136-0j)\n",
      "GD iter. 154/199: loss=(0.39549133923974095-0j)\n",
      "GD iter. 155/199: loss=(0.3954563374584144-0j)\n",
      "GD iter. 156/199: loss=(0.3954219284360055-0j)\n",
      "GD iter. 157/199: loss=(0.3953880988076904-0j)\n",
      "GD iter. 158/199: loss=(0.39535483556686435-0j)\n",
      "GD iter. 159/199: loss=(0.395322126054021-0j)\n",
      "GD iter. 160/199: loss=(0.3952899579460278-0j)\n",
      "GD iter. 161/199: loss=(0.39525831924578275-0j)\n",
      "GD iter. 162/199: loss=(0.3952271982722347-0j)\n",
      "GD iter. 163/199: loss=(0.3951965836507553-0j)\n",
      "GD iter. 164/199: loss=(0.3951664643038464-0j)\n",
      "GD iter. 165/199: loss=(0.39513682944217104-0j)\n",
      "GD iter. 166/199: loss=(0.3951076685558937-0j)\n",
      "GD iter. 167/199: loss=(0.3950789714063202-0j)\n",
      "GD iter. 168/199: loss=(0.3950507280178218-0j)\n",
      "GD iter. 169/199: loss=(0.39502292867003685-0j)\n",
      "GD iter. 170/199: loss=(0.3949955638903347-0j)\n",
      "GD iter. 171/199: loss=(0.3949686244465365-0j)\n",
      "GD iter. 172/199: loss=(0.39494210133987806-0j)\n",
      "GD iter. 173/199: loss=(0.3949159857982104-0j)\n",
      "GD iter. 174/199: loss=(0.3948902692694248-0j)\n",
      "GD iter. 175/199: loss=(0.39486494341509704-0j)\n",
      "GD iter. 176/199: loss=(0.39484000010434067-0j)\n",
      "GD iter. 177/199: loss=(0.3948154314078624-0j)\n",
      "GD iter. 178/199: loss=(0.39479122959221147-0j)\n",
      "GD iter. 179/199: loss=(0.39476738711421666-0j)\n",
      "GD iter. 180/199: loss=(0.39474389661560333-0j)\n",
      "GD iter. 181/199: loss=(0.39472075091778397-0j)\n",
      "GD iter. 182/199: loss=(0.3946979430168161-0j)\n",
      "GD iter. 183/199: loss=(0.3946754660785217-0j)\n",
      "GD iter. 184/199: loss=(0.3946533134337617-0j)\n",
      "GD iter. 185/199: loss=(0.39463147857386066-0j)\n",
      "GD iter. 186/199: loss=(0.39460995514617503-0j)\n",
      "GD iter. 187/199: loss=(0.39458873694980195-0j)\n",
      "GD iter. 188/199: loss=(0.3945678179314214-0j)\n",
      "GD iter. 189/199: loss=(0.39454719218126877-0j)\n",
      "GD iter. 190/199: loss=(0.394526853929231-0j)\n",
      "GD iter. 191/199: loss=(0.3945067975410654-0j)\n",
      "GD iter. 192/199: loss=(0.39448701751473303-0j)\n",
      "GD iter. 193/199: loss=(0.3944675084768461-0j)\n",
      "GD iter. 194/199: loss=(0.39444826517922227-0j)\n",
      "GD iter. 195/199: loss=(0.39442928249554515-0j)\n",
      "GD iter. 196/199: loss=(0.3944105554181251-0j)\n",
      "GD iter. 197/199: loss=(0.3943920790547577-0j)\n",
      "GD iter. 198/199: loss=(0.3943738486256777-0j)\n",
      "GD iter. 199/199: loss=(0.3943558594606023-0j)\n",
      "The Accuracy is: 0.8374\n",
      "The F1 score is: 0.2556\n",
      "The precision is: 0.1954\n",
      "The recall is: 0.3696\n",
      "GD iter. 0/199: loss=(0.6955040897716468-0j)\n",
      "GD iter. 1/199: loss=(0.5367184698673717-0j)\n",
      "GD iter. 2/199: loss=(0.5222547578804911-0j)\n",
      "GD iter. 3/199: loss=(0.5117707196454726-0j)\n",
      "GD iter. 4/199: loss=(0.5034745188664729-0j)\n",
      "GD iter. 5/199: loss=(0.49657016871342136-0j)\n",
      "GD iter. 6/199: loss=(0.49063963577758457-0j)\n",
      "GD iter. 7/199: loss=(0.48543455090385484-0j)\n",
      "GD iter. 8/199: loss=(0.4807953715355451-0j)\n",
      "GD iter. 9/199: loss=(0.4766137492167225-0j)\n",
      "GD iter. 10/199: loss=(0.4728126513202169-0j)\n",
      "GD iter. 11/199: loss=(0.4693350647226085-0j)\n",
      "GD iter. 12/199: loss=(0.46613727219806994-0j)\n",
      "GD iter. 13/199: loss=(0.46318470708569265-0j)\n",
      "GD iter. 14/199: loss=(0.46044931753131374-0j)\n",
      "GD iter. 15/199: loss=(0.457907841037161-0j)\n",
      "GD iter. 16/199: loss=(0.4555406422915592-0j)\n",
      "GD iter. 17/199: loss=(0.4533309080437076-0j)\n",
      "GD iter. 18/199: loss=(0.4512640736543756-0j)\n",
      "GD iter. 19/199: loss=(0.44932740350966366-0j)\n",
      "GD iter. 20/199: loss=(0.447509676035933-0j)\n",
      "GD iter. 21/199: loss=(0.44580094151899896-0j)\n",
      "GD iter. 22/199: loss=(0.4441923318027428-0j)\n",
      "GD iter. 23/199: loss=(0.4426759078246237-0j)\n",
      "GD iter. 24/199: loss=(0.44124453537452996-0j)\n",
      "GD iter. 25/199: loss=(0.43989178236082455-0j)\n",
      "GD iter. 26/199: loss=(0.4386118327947641-0j)\n",
      "GD iter. 27/199: loss=(0.4373994140085122-0j)\n",
      "GD iter. 28/199: loss=(0.4362497345197673-0j)\n",
      "GD iter. 29/199: loss=(0.43515843058521214-0j)\n",
      "GD iter. 30/199: loss=(0.43412151993392284-0j)\n",
      "GD iter. 31/199: loss=(0.43313536149802656-0j)\n",
      "GD iter. 32/199: loss=(0.4321966201991184-0j)\n",
      "GD iter. 33/199: loss=(0.4313022360304882-0j)\n",
      "GD iter. 34/199: loss=(0.4304493968141508-0j)\n",
      "GD iter. 35/199: loss=(0.429635514119698-0j)\n",
      "GD iter. 36/199: loss=(0.4288582019172279-0j)\n",
      "GD iter. 37/199: loss=(0.4281152576047722-0j)\n",
      "GD iter. 38/199: loss=(0.4274046451058146-0j)\n",
      "GD iter. 39/199: loss=(0.42672447977763267-0j)\n",
      "GD iter. 40/199: loss=(0.426073014908482-0j)\n",
      "GD iter. 41/199: loss=(0.4254486296126921-0j)\n",
      "GD iter. 42/199: loss=(0.4248498179587936-0j)\n",
      "GD iter. 43/199: loss=(0.42427517918777863-0j)\n",
      "GD iter. 44/199: loss=(0.4237234088972616-0j)\n",
      "GD iter. 45/199: loss=(0.42319329108322173-0j)\n",
      "GD iter. 46/199: loss=(0.4226836909446305-0j)\n",
      "GD iter. 47/199: loss=(0.4221935483679855-0j)\n",
      "GD iter. 48/199: loss=(0.4217218720188692-0j)\n",
      "GD iter. 49/199: loss=(0.42126773397638295-0j)\n",
      "GD iter. 50/199: loss=(0.42083026485389013-0j)\n",
      "GD iter. 51/199: loss=(0.42040864935607447-0j)\n",
      "GD iter. 52/199: loss=(0.4200021222280704-0j)\n",
      "GD iter. 53/199: loss=(0.4196099645574225-0j)\n",
      "GD iter. 54/199: loss=(0.4192315003940204-0j)\n",
      "GD iter. 55/199: loss=(0.4188660936569956-0j)\n",
      "GD iter. 56/199: loss=(0.4185131453009444-0j)\n",
      "GD iter. 57/199: loss=(0.41817209071680855-0j)\n",
      "GD iter. 58/199: loss=(0.4178423973453644-0j)\n",
      "GD iter. 59/199: loss=(0.41752356248358163-0j)\n",
      "GD iter. 60/199: loss=(0.41721511126615596-0j)\n",
      "GD iter. 61/199: loss=(0.4169165948063304-0j)\n",
      "GD iter. 62/199: loss=(0.4166275884817264-0j)\n",
      "GD iter. 63/199: loss=(0.4163476903523299-0j)\n",
      "GD iter. 64/199: loss=(0.41607651969905146-0j)\n",
      "GD iter. 65/199: loss=(0.41581371567240555-0j)\n",
      "GD iter. 66/199: loss=(0.41555893604186733-0j)\n",
      "GD iter. 67/199: loss=(0.4153118560373665-0j)\n",
      "GD iter. 68/199: loss=(0.4150721672751843-0j)\n",
      "GD iter. 69/199: loss=(0.41483957676124295-0j)\n",
      "GD iter. 70/199: loss=(0.4146138059654274-0j)\n",
      "GD iter. 71/199: loss=(0.41439458996116113-0j)\n",
      "GD iter. 72/199: loss=(0.414181676624981-0j)\n",
      "GD iter. 73/199: loss=(0.41397482589133067-0j)\n",
      "GD iter. 74/199: loss=(0.4137738090582155-0j)\n",
      "GD iter. 75/199: loss=(0.4135784081397475-0j)\n",
      "GD iter. 76/199: loss=(0.41338841526195397-0j)\n",
      "GD iter. 77/199: loss=(0.41320363209853916-0j)\n",
      "GD iter. 78/199: loss=(0.4130238693435702-0j)\n",
      "GD iter. 79/199: loss=(0.4128489462183177-0j)\n",
      "GD iter. 80/199: loss=(0.41267869000971275-0j)\n",
      "GD iter. 81/199: loss=(0.41251293563809516-0j)\n",
      "GD iter. 82/199: loss=(0.41235152525211877-0j)\n",
      "GD iter. 83/199: loss=(0.41219430784885625-0j)\n",
      "GD iter. 84/199: loss=(0.4120411389173009-0j)\n",
      "GD iter. 85/199: loss=(0.41189188010361477-0j)\n",
      "GD iter. 86/199: loss=(0.4117463988965958-0j)\n",
      "GD iter. 87/199: loss=(0.4116045683319626-0j)\n",
      "GD iter. 88/199: loss=(0.4114662667141655-0j)\n",
      "GD iter. 89/199: loss=(0.4113313773545284-0j)\n",
      "GD iter. 90/199: loss=(0.4111997883246226-0j)\n",
      "GD iter. 91/199: loss=(0.41107139222385347-0j)\n",
      "GD iter. 92/199: loss=(0.41094608596032106-0j)\n",
      "GD iter. 93/199: loss=(0.41082377054408414-0j)\n",
      "GD iter. 94/199: loss=(0.41070435089202095-0j)\n",
      "GD iter. 95/199: loss=(0.41058773564354145-0j)\n",
      "GD iter. 96/199: loss=(0.4104738369864584-0j)\n",
      "GD iter. 97/199: loss=(0.4103625704923747-0j)\n",
      "GD iter. 98/199: loss=(0.4102538549609907-0j)\n",
      "GD iter. 99/199: loss=(0.41014761227277735-0j)\n",
      "GD iter. 100/199: loss=(0.4100437672495009-0j)\n",
      "GD iter. 101/199: loss=(0.40994224752211794-0j)\n",
      "GD iter. 102/199: loss=(0.409842983405595-0j)\n",
      "GD iter. 103/199: loss=(0.4097459077802382-0j)\n",
      "GD iter. 104/199: loss=(0.4096509559791442-0j)\n",
      "GD iter. 105/199: loss=(0.4095580656814099-0j)\n",
      "GD iter. 106/199: loss=(0.4094671768107659-0j)\n",
      "GD iter. 107/199: loss=(0.40937823143931634-0j)\n",
      "GD iter. 108/199: loss=(0.4092911736960913-0j)\n",
      "GD iter. 109/199: loss=(0.409205949680137-0j)\n",
      "GD iter. 110/199: loss=(0.4091225073778852-0j)\n",
      "GD iter. 111/199: loss=(0.4090407965845615-0j)\n",
      "GD iter. 112/199: loss=(0.40896076882940585-0j)\n",
      "GD iter. 113/199: loss=(0.40888237730449484-0j)\n",
      "GD iter. 114/199: loss=(0.4088055767969662-0j)\n",
      "GD iter. 115/199: loss=(0.40873032362446027-0j)\n",
      "GD iter. 116/199: loss=(0.408656575573603-0j)\n",
      "GD iter. 117/199: loss=(0.4085842918413683-0j)\n",
      "GD iter. 118/199: loss=(0.408513432979163-0j)\n",
      "GD iter. 119/199: loss=(0.408443960839492-0j)\n",
      "GD iter. 120/199: loss=(0.40837583852506726-0j)\n",
      "GD iter. 121/199: loss=(0.40830903034023086-0j)\n",
      "GD iter. 122/199: loss=(0.4082435017445737-0j)\n",
      "GD iter. 123/199: loss=(0.4081792193086362-0j)\n",
      "GD iter. 124/199: loss=(0.40811615067158263-0j)\n",
      "GD iter. 125/199: loss=(0.4080542645007501-0j)\n",
      "GD iter. 126/199: loss=(0.4079935304529768-0j)\n",
      "GD iter. 127/199: loss=(0.4079339191376198-0j)\n",
      "GD iter. 128/199: loss=(0.4078754020811779-0j)\n",
      "GD iter. 129/199: loss=(0.4078179516934411-0j)\n",
      "GD iter. 130/199: loss=(0.4077615412350894-0j)\n",
      "GD iter. 131/199: loss=(0.4077061447866717-0j)\n",
      "GD iter. 132/199: loss=(0.4076517372188971-0j)\n",
      "GD iter. 133/199: loss=(0.40759829416417476-0j)\n",
      "GD iter. 134/199: loss=(0.4075457919893428-0j)\n",
      "GD iter. 135/199: loss=(0.4074942077695292-0j)\n",
      "GD iter. 136/199: loss=(0.4074435192630917-0j)\n",
      "GD iter. 137/199: loss=(0.40739370488758453-0j)\n",
      "GD iter. 138/199: loss=(0.4073447436967052-0j)\n",
      "GD iter. 139/199: loss=(0.40729661535817513-0j)\n",
      "GD iter. 140/199: loss=(0.40724930013251087-0j)\n",
      "GD iter. 141/199: loss=(0.4072027788526451-0j)\n",
      "GD iter. 142/199: loss=(0.4071570329043585-0j)\n",
      "GD iter. 143/199: loss=(0.40711204420748587-0j)\n",
      "GD iter. 144/199: loss=(0.4070677951978614-0j)\n",
      "GD iter. 145/199: loss=(0.4070242688099696-0j)\n",
      "GD iter. 146/199: loss=(0.4069814484602712-0j)\n",
      "GD iter. 147/199: loss=(0.4069393180311732-0j)\n",
      "GD iter. 148/199: loss=(0.4068978618556153-0j)\n",
      "GD iter. 149/199: loss=(0.4068570647022454-0j)\n",
      "GD iter. 150/199: loss=(0.40681691176115886-0j)\n",
      "GD iter. 151/199: loss=(0.40677738863017576-0j)\n",
      "GD iter. 152/199: loss=(0.4067384813016361-0j)\n",
      "GD iter. 153/199: loss=(0.406700176149686-0j)\n",
      "GD iter. 154/199: loss=(0.40666245991803934-0j)\n",
      "GD iter. 155/199: loss=(0.40662531970818994-0j)\n",
      "GD iter. 156/199: loss=(0.40658874296805864-0j)\n",
      "GD iter. 157/199: loss=(0.4065527174810554-0j)\n",
      "GD iter. 158/199: loss=(0.40651723135553985-0j)\n",
      "GD iter. 159/199: loss=(0.40648227301466267-0j)\n",
      "GD iter. 160/199: loss=(0.406447831186574-0j)\n",
      "GD iter. 161/199: loss=(0.40641389489498264-0j)\n",
      "GD iter. 162/199: loss=(0.40638045345005075-0j)\n",
      "GD iter. 163/199: loss=(0.4063474964396129-0j)\n",
      "GD iter. 164/199: loss=(0.4063150137207046-0j)\n",
      "GD iter. 165/199: loss=(0.40628299541138824-0j)\n",
      "GD iter. 166/199: loss=(0.4062514318828648-0j)\n",
      "GD iter. 167/199: loss=(0.40622031375185963-0j)\n",
      "GD iter. 168/199: loss=(0.40618963187327234-0j)\n",
      "GD iter. 169/199: loss=(0.40615937733307933-0j)\n",
      "GD iter. 170/199: loss=(0.4061295414414795-0j)\n",
      "GD iter. 171/199: loss=(0.40610011572627397-0j)\n",
      "GD iter. 172/199: loss=(0.4060710919264705-0j)\n",
      "GD iter. 173/199: loss=(0.406042461986104-0j)\n",
      "GD iter. 174/199: loss=(0.4060142180482646-0j)\n",
      "GD iter. 175/199: loss=(0.40598635244932596-0j)\n",
      "GD iter. 176/199: loss=(0.4059588577133655-0j)\n",
      "GD iter. 177/199: loss=(0.40593172654676973-0j)\n",
      "GD iter. 178/199: loss=(0.40590495183301806-0j)\n",
      "GD iter. 179/199: loss=(0.40587852662763746-0j)\n",
      "GD iter. 180/199: loss=(0.40585244415332217-0j)\n",
      "GD iter. 181/199: loss=(0.4058266977952133-0j)\n",
      "GD iter. 182/199: loss=(0.4058012810963295-0j)\n",
      "GD iter. 183/199: loss=(0.40577618775314755-0j)\n",
      "GD iter. 184/199: loss=(0.4057514116113228-0j)\n",
      "GD iter. 185/199: loss=(0.4057269466615478-0j)\n",
      "GD iter. 186/199: loss=(0.40570278703554236-0j)\n",
      "GD iter. 187/199: loss=(0.40567892700217123-0j)\n",
      "GD iter. 188/199: loss=(0.4056553609636834-0j)\n",
      "GD iter. 189/199: loss=(0.4056320834520709-0j)\n",
      "GD iter. 190/199: loss=(0.40560908912554067-0j)\n",
      "GD iter. 191/199: loss=(0.4055863727650967-0j)\n",
      "GD iter. 192/199: loss=(0.40556392927122864-0j)\n",
      "GD iter. 193/199: loss=(0.40554175366070244-0j)\n",
      "GD iter. 194/199: loss=(0.4055198410634493-0j)\n",
      "GD iter. 195/199: loss=(0.40549818671955135-0j)\n",
      "GD iter. 196/199: loss=(0.40547678597631803-0j)\n",
      "GD iter. 197/199: loss=(0.4054556342854522-0j)\n",
      "GD iter. 198/199: loss=(0.4054347272003019-0j)\n",
      "GD iter. 199/199: loss=(0.4054140603731944-0j)\n",
      "The Accuracy is: 0.8752\n",
      "The F1 score is: 0.3871\n",
      "The precision is: 0.3243\n",
      "The recall is: 0.4800\n",
      "GD iter. 0/199: loss=(0.7085967591498652-0j)\n",
      "GD iter. 1/199: loss=(0.5417201236995017-0j)\n",
      "GD iter. 2/199: loss=(0.5265921575366872-0j)\n",
      "GD iter. 3/199: loss=(0.5160409854334193-0j)\n",
      "GD iter. 4/199: loss=(0.5076341545936238-0j)\n",
      "GD iter. 5/199: loss=(0.5005515571272913-0j)\n",
      "GD iter. 6/199: loss=(0.4944132367329377-0j)\n",
      "GD iter. 7/199: loss=(0.48899603430840455-0j)\n",
      "GD iter. 8/199: loss=(0.48415321837950026-0j)\n",
      "GD iter. 9/199: loss=(0.4797821409223122-0j)\n",
      "GD iter. 10/199: loss=(0.4758077036344788-0j)\n",
      "GD iter. 11/199: loss=(0.47217290651473226-0j)\n",
      "GD iter. 12/199: loss=(0.46883312390054666-0j)\n",
      "GD iter. 13/199: loss=(0.46575249671125596-0j)\n",
      "GD iter. 14/199: loss=(0.46290158006044935-0j)\n",
      "GD iter. 15/199: loss=(0.46025576059798334-0j)\n",
      "GD iter. 16/199: loss=(0.45779415973523385-0j)\n",
      "GD iter. 17/199: loss=(0.45549885206278873-0j)\n",
      "GD iter. 18/199: loss=(0.45335429365308855-0j)\n",
      "GD iter. 19/199: loss=(0.4513468936878747-0j)\n",
      "GD iter. 20/199: loss=(0.44946468634987086-0j)\n",
      "GD iter. 21/199: loss=(0.44769707448598894-0j)\n",
      "GD iter. 22/199: loss=(0.44603462577299374-0j)\n",
      "GD iter. 23/199: loss=(0.4444689080773543-0j)\n",
      "GD iter. 24/199: loss=(0.4429923546305381-0j)\n",
      "GD iter. 25/199: loss=(0.4415981522818147-0j)\n",
      "GD iter. 26/199: loss=(0.4402801478986089-0j)\n",
      "GD iter. 27/199: loss=(0.4390327692444453-0j)\n",
      "GD iter. 28/199: loss=(0.43785095755771175-0j)\n",
      "GD iter. 29/199: loss=(0.4367301096979817-0j)\n",
      "GD iter. 30/199: loss=(0.435666028197513-0j)\n",
      "GD iter. 31/199: loss=(0.43465487790523416-0j)\n",
      "GD iter. 32/199: loss=(0.433693148173937-0j)\n",
      "GD iter. 33/199: loss=(0.43277761974249856-0j)\n",
      "GD iter. 34/199: loss=(0.4319053356204953-0j)\n",
      "GD iter. 35/199: loss=(0.4310735754043386-0j)\n",
      "GD iter. 36/199: loss=(0.4302798325504921-0j)\n",
      "GD iter. 37/199: loss=(0.4295217942085225-0j)\n",
      "GD iter. 38/199: loss=(0.4287973232791289-0j)\n",
      "GD iter. 39/199: loss=(0.4281044424132163-0j)\n",
      "GD iter. 40/199: loss=(0.4274413197099446-0j)\n",
      "GD iter. 41/199: loss=(0.4268062559064035-0j)\n",
      "GD iter. 42/199: loss=(0.4261976728805273-0j)\n",
      "GD iter. 43/199: loss=(0.42561410331318594-0j)\n",
      "GD iter. 44/199: loss=(0.42505418137592127-0j)\n",
      "GD iter. 45/199: loss=(0.4245166343282242-0j)\n",
      "GD iter. 46/199: loss=(0.42400027492309666-0j)\n",
      "GD iter. 47/199: loss=(0.4235039945323543-0j)\n",
      "GD iter. 48/199: loss=(0.42302675691403924-0j)\n",
      "GD iter. 49/199: loss=(0.42256759255372656-0j)\n",
      "GD iter. 50/199: loss=(0.42212559351964046-0j)\n",
      "GD iter. 51/199: loss=(0.42169990877854907-0j)\n",
      "GD iter. 52/199: loss=(0.4212897399255416-0j)\n",
      "GD iter. 53/199: loss=(0.42089433728613435-0j)\n",
      "GD iter. 54/199: loss=(0.4205129963538218-0j)\n",
      "GD iter. 55/199: loss=(0.42014505453027445-0j)\n",
      "GD iter. 56/199: loss=(0.4197898881389738-0j)\n",
      "GD iter. 57/199: loss=(0.41944690968622345-0j)\n",
      "GD iter. 58/199: loss=(0.4191155653462564-0j)\n",
      "GD iter. 59/199: loss=(0.418795332649599-0j)\n",
      "GD iter. 60/199: loss=(0.41848571835602955-0j)\n",
      "GD iter. 61/199: loss=(0.4181862564953696-0j)\n",
      "GD iter. 62/199: loss=(0.41789650656106264-0j)\n",
      "GD iter. 63/199: loss=(0.41761605184298645-0j)\n",
      "GD iter. 64/199: loss=(0.41734449788730155-0j)\n",
      "GD iter. 65/199: loss=(0.4170814710723252-0j)\n",
      "GD iter. 66/199: loss=(0.41682661729048787-0j)\n",
      "GD iter. 67/199: loss=(0.4165796007273844-0j)\n",
      "GD iter. 68/199: loss=(0.4163401027297791-0j)\n",
      "GD iter. 69/199: loss=(0.4161078207551899-0j)\n",
      "GD iter. 70/199: loss=(0.4158824673963594-0j)\n",
      "GD iter. 71/199: loss=(0.4156637694745341-0j)\n",
      "GD iter. 72/199: loss=(0.4154514671960244-0j)\n",
      "GD iter. 73/199: loss=(0.4152453133670162-0j)\n",
      "GD iter. 74/199: loss=(0.41504507266205004-0j)\n",
      "GD iter. 75/199: loss=(0.4148505209419878-0j)\n",
      "GD iter. 76/199: loss=(0.41466144461765325-0j)\n",
      "GD iter. 77/199: loss=(0.4144776400556562-0j)\n",
      "GD iter. 78/199: loss=(0.4142989130232163-0j)\n",
      "GD iter. 79/199: loss=(0.4141250781690642-0j)\n",
      "GD iter. 80/199: loss=(0.4139559585377466-0j)\n",
      "GD iter. 81/199: loss=(0.41379138511488367-0j)\n",
      "GD iter. 82/199: loss=(0.41363119640112694-0j)\n",
      "GD iter. 83/199: loss=(0.41347523801275077-0j)\n",
      "GD iter. 84/199: loss=(0.41332336230697714-0j)\n",
      "GD iter. 85/199: loss=(0.41317542803028284-0j)\n",
      "GD iter. 86/199: loss=(0.413031299988083-0j)\n",
      "GD iter. 87/199: loss=(0.41289084873430304-0j)\n",
      "GD iter. 88/199: loss=(0.412753950279473-0j)\n",
      "GD iter. 89/199: loss=(0.41262048581608124-0j)\n",
      "GD iter. 90/199: loss=(0.41249034146002145-0j)\n",
      "GD iter. 91/199: loss=(0.41236340800705396-0j)\n",
      "GD iter. 92/199: loss=(0.41223958070328665-0j)\n",
      "GD iter. 93/199: loss=(0.4121187590287507-0j)\n",
      "GD iter. 94/199: loss=(0.41200084649321883-0j)\n",
      "GD iter. 95/199: loss=(0.41188575044347236-0j)\n",
      "GD iter. 96/199: loss=(0.4117733818812848-0j)\n",
      "GD iter. 97/199: loss=(0.41166365529143845-0j)\n",
      "GD iter. 98/199: loss=(0.41155648847914383-0j)\n",
      "GD iter. 99/199: loss=(0.411451802416272-0j)\n",
      "GD iter. 100/199: loss=(0.4113495210958543-0j)\n",
      "GD iter. 101/199: loss=(0.4112495713943414-0j)\n",
      "GD iter. 102/199: loss=(0.41115188294114835-0j)\n",
      "GD iter. 103/199: loss=(0.4110563879950439-0j)\n",
      "GD iter. 104/199: loss=(0.4109630213269759-0j)\n",
      "GD iter. 105/199: loss=(0.41087172010894696-0j)\n",
      "GD iter. 106/199: loss=(0.4107824238085866-0j)\n",
      "GD iter. 107/199: loss=(0.4106950740890824-0j)\n",
      "GD iter. 108/199: loss=(0.4106096147141634-0j)\n",
      "GD iter. 109/199: loss=(0.4105259914578409-0j)\n",
      "GD iter. 110/199: loss=(0.410444152018637-0j)\n",
      "GD iter. 111/199: loss=(0.4103640459380458-0j)\n",
      "GD iter. 112/199: loss=(0.4102856245229892-0j)\n",
      "GD iter. 113/199: loss=(0.410208840772044-0j)\n",
      "GD iter. 114/199: loss=(0.4101336493052325-0j)\n",
      "GD iter. 115/199: loss=(0.4100600062971793-0j)\n",
      "GD iter. 116/199: loss=(0.40998786941345233-0j)\n",
      "GD iter. 117/199: loss=(0.4099171977499147-0j)\n",
      "GD iter. 118/199: loss=(0.4098479517749267-0j)\n",
      "GD iter. 119/199: loss=(0.4097800932742447-0j)\n",
      "GD iter. 120/199: loss=(0.40971358529847557-0j)\n",
      "GD iter. 121/199: loss=(0.40964839211295295-0j)\n",
      "GD iter. 122/199: loss=(0.40958447914990675-0j)\n",
      "GD iter. 123/199: loss=(0.40952181296281015-0j)\n",
      "GD iter. 124/199: loss=(0.4094603611827904-0j)\n",
      "GD iter. 125/199: loss=(0.4094000924769988-0j)\n",
      "GD iter. 126/199: loss=(0.4093409765088412-0j)\n",
      "GD iter. 127/199: loss=(0.40928298389997403-0j)\n",
      "GD iter. 128/199: loss=(0.4092260861939796-0j)\n",
      "GD iter. 129/199: loss=(0.4091702558216356-0j)\n",
      "GD iter. 130/199: loss=(0.40911546606770155-0j)\n",
      "GD iter. 131/199: loss=(0.4090616910391471-0j)\n",
      "GD iter. 132/199: loss=(0.4090089056347525-0j)\n",
      "GD iter. 133/199: loss=(0.40895708551601473-0j)\n",
      "GD iter. 134/199: loss=(0.4089062070792959-0j)\n",
      "GD iter. 135/199: loss=(0.40885624742915616-0j)\n",
      "GD iter. 136/199: loss=(0.40880718435281327-0j)\n",
      "GD iter. 137/199: loss=(0.4087589962956753-0j)\n",
      "GD iter. 138/199: loss=(0.4087116623378986-0j)\n",
      "GD iter. 139/199: loss=(0.4086651621719192-0j)\n",
      "GD iter. 140/199: loss=(0.4086194760809164-0j)\n",
      "GD iter. 141/199: loss=(0.4085745849181615-0j)\n",
      "GD iter. 142/199: loss=(0.408530470087214-0j)\n",
      "GD iter. 143/199: loss=(0.4084871135229245-0j)\n",
      "GD iter. 144/199: loss=(0.40844449767320806-0j)\n",
      "GD iter. 145/199: loss=(0.4084026054815528-0j)\n",
      "GD iter. 146/199: loss=(0.4083614203702306-0j)\n",
      "GD iter. 147/199: loss=(0.40832092622417826-0j)\n",
      "GD iter. 148/199: loss=(0.40828110737551765-0j)\n",
      "GD iter. 149/199: loss=(0.40824194858868895-0j)\n",
      "GD iter. 150/199: loss=(0.40820343504616663-0j)\n",
      "GD iter. 151/199: loss=(0.40816555233473384-0j)\n",
      "GD iter. 152/199: loss=(0.40812828643229054-0j)\n",
      "GD iter. 153/199: loss=(0.4080916236951708-0j)\n",
      "GD iter. 154/199: loss=(0.4080555508459479-0j)\n",
      "GD iter. 155/199: loss=(0.40802005496170474-0j)\n",
      "GD iter. 156/199: loss=(0.40798512346275023-0j)\n",
      "GD iter. 157/199: loss=(0.4079507441017615-0j)\n",
      "GD iter. 158/199: loss=(0.40791690495333416-0j)\n",
      "GD iter. 159/199: loss=(0.4078835944039214-0j)\n",
      "GD iter. 160/199: loss=(0.4078508011421477-0j)\n",
      "GD iter. 161/199: loss=(0.40781851414947795-0j)\n",
      "GD iter. 162/199: loss=(0.40778672269122945-0j)\n",
      "GD iter. 163/199: loss=(0.4077554163079114-0j)\n",
      "GD iter. 164/199: loss=(0.4077245848068763-0j)\n",
      "GD iter. 165/199: loss=(0.40769421825427293-0j)\n",
      "GD iter. 166/199: loss=(0.4076643069672855-0j)\n",
      "GD iter. 167/199: loss=(0.4076348415066493-0j)\n",
      "GD iter. 168/199: loss=(0.4076058126694294-0j)\n",
      "GD iter. 169/199: loss=(0.407577211482053-0j)\n",
      "GD iter. 170/199: loss=(0.4075490291935833-0j)\n",
      "GD iter. 171/199: loss=(0.4075212572692275-0j)\n",
      "GD iter. 172/199: loss=(0.4074938873840663-0j)\n",
      "GD iter. 173/199: loss=(0.40746691141699803-0j)\n",
      "GD iter. 174/199: loss=(0.4074403214448885-0j)\n",
      "GD iter. 175/199: loss=(0.4074141097369175-0j)\n",
      "GD iter. 176/199: loss=(0.4073882687491144-0j)\n",
      "GD iter. 177/199: loss=(0.40736279111907664-0j)\n",
      "GD iter. 178/199: loss=(0.40733766966086105-0j)\n",
      "GD iter. 179/199: loss=(0.40731289736004456-0j)\n",
      "GD iter. 180/199: loss=(0.40728846736894536-0j)\n",
      "GD iter. 181/199: loss=(0.4072643730019992-0j)\n",
      "GD iter. 182/199: loss=(0.40724060773128495-0j)\n",
      "GD iter. 183/199: loss=(0.4072171651821935-0j)\n",
      "GD iter. 184/199: loss=(0.4071940391292348-0j)\n",
      "GD iter. 185/199: loss=(0.40717122349197743-0j)\n",
      "GD iter. 186/199: loss=(0.4071487123311163-0j)\n",
      "GD iter. 187/199: loss=(0.40712649984466387-0j)\n",
      "GD iter. 188/199: loss=(0.40710458036425895-0j)\n",
      "GD iter. 189/199: loss=(0.40708294835159115-0j)\n",
      "GD iter. 190/199: loss=(0.4070615983949344-0j)\n",
      "GD iter. 191/199: loss=(0.4070405252057876-0j)\n",
      "GD iter. 192/199: loss=(0.40701972361561684-0j)\n",
      "GD iter. 193/199: loss=(0.40699918857269723-0j)\n",
      "GD iter. 194/199: loss=(0.4069789151390498-0j)\n",
      "GD iter. 195/199: loss=(0.40695889848747-0j)\n",
      "GD iter. 196/199: loss=(0.4069391338986457-0j)\n",
      "GD iter. 197/199: loss=(0.40691961675836064-0j)\n",
      "GD iter. 198/199: loss=(0.4069003425547804-0j)\n",
      "GD iter. 199/199: loss=(0.4068813068758191-0j)\n",
      "The Accuracy is: 0.8982\n",
      "The F1 score is: 0.5079\n",
      "The precision is: 0.4051\n",
      "The recall is: 0.6809\n",
      "GD iter. 0/199: loss=(0.6702012750268803-0j)\n",
      "GD iter. 1/199: loss=(0.5432916164145895-0j)\n",
      "GD iter. 2/199: loss=(0.5281956958795023-0j)\n",
      "GD iter. 3/199: loss=(0.5172797576741897-0j)\n",
      "GD iter. 4/199: loss=(0.5086500436017624-0j)\n",
      "GD iter. 5/199: loss=(0.5014844838122412-0j)\n",
      "GD iter. 6/199: loss=(0.4953437762534085-0j)\n",
      "GD iter. 7/199: loss=(0.48996449150533683-0j)\n",
      "GD iter. 8/199: loss=(0.4851774587609494-0j)\n",
      "GD iter. 9/199: loss=(0.4808681878040767-0j)\n",
      "GD iter. 10/199: loss=(0.47695554637491916-0j)\n",
      "GD iter. 11/199: loss=(0.47337961176988846-0j)\n",
      "GD iter. 12/199: loss=(0.47009447738889615-0j)\n",
      "GD iter. 13/199: loss=(0.46706385098592523-0j)\n",
      "GD iter. 14/199: loss=(0.4642582762126919-0j)\n",
      "GD iter. 15/199: loss=(0.461653324292127-0j)\n",
      "GD iter. 16/199: loss=(0.4592283805431116-0j)\n",
      "GD iter. 17/199: loss=(0.4569658048742703-0j)\n",
      "GD iter. 18/199: loss=(0.45485033331647695-0j)\n",
      "GD iter. 19/199: loss=(0.45286863888855156-0j)\n",
      "GD iter. 20/199: loss=(0.45100900053302667-0j)\n",
      "GD iter. 21/199: loss=(0.44926104729119504-0j)\n",
      "GD iter. 22/199: loss=(0.44761555624516125-0j)\n",
      "GD iter. 23/199: loss=(0.4460642898743332-0j)\n",
      "GD iter. 24/199: loss=(0.4445998630132759-0j)\n",
      "GD iter. 25/199: loss=(0.44321563254350477-0j)\n",
      "GD iter. 26/199: loss=(0.44190560489893815-0j)\n",
      "GD iter. 27/199: loss=(0.44066435777737817-0j)\n",
      "GD iter. 28/199: loss=(0.43948697335383996-0j)\n",
      "GD iter. 29/199: loss=(0.43836898092688226-0j)\n",
      "GD iter. 30/199: loss=(0.43730630738569315-0j)\n",
      "GD iter. 31/199: loss=(0.43629523422090744-0j)\n",
      "GD iter. 32/199: loss=(0.43533236005329967-0j)\n",
      "GD iter. 33/199: loss=(0.4344145678462427-0j)\n",
      "GD iter. 34/199: loss=(0.43353899611671165-0j)\n",
      "GD iter. 35/199: loss=(0.43270301357696606-0j)\n",
      "GD iter. 36/199: loss=(0.431904196732746-0j)\n",
      "GD iter. 37/199: loss=(0.43114031003948555-0j)\n",
      "GD iter. 38/199: loss=(0.4304092882797479-0j)\n",
      "GD iter. 39/199: loss=(0.4297092208757958-0j)\n",
      "GD iter. 40/199: loss=(0.4290383378932223-0j)\n",
      "GD iter. 41/199: loss=(0.42839499752654797-0j)\n",
      "GD iter. 42/199: loss=(0.42777767488701424-0j)\n",
      "GD iter. 43/199: loss=(0.4271849519374877-0j)\n",
      "GD iter. 44/199: loss=(0.42661550844025325-0j)\n",
      "GD iter. 45/199: loss=(0.42606811380120685-0j)\n",
      "GD iter. 46/199: loss=(0.4255416197090504-0j)\n",
      "GD iter. 47/199: loss=(0.42503495348101267-0j)\n",
      "GD iter. 48/199: loss=(0.4245471120376904-0j)\n",
      "GD iter. 49/199: loss=(0.424077156439142-0j)\n",
      "GD iter. 50/199: loss=(0.42362420692258335-0j)\n",
      "GD iter. 51/199: loss=(0.4231874383891576-0j)\n",
      "GD iter. 52/199: loss=(0.42276607629341667-0j)\n",
      "GD iter. 53/199: loss=(0.42235939289452173-0j)\n",
      "GD iter. 54/199: loss=(0.4219667038328481-0j)\n",
      "GD iter. 55/199: loss=(0.421587364999763-0j)\n",
      "GD iter. 56/199: loss=(0.4212207696719229-0j)\n",
      "GD iter. 57/199: loss=(0.4208663458845721-0j)\n",
      "GD iter. 58/199: loss=(0.420523554021085-0j)\n",
      "GD iter. 59/199: loss=(0.42019188459841644-0j)\n",
      "GD iter. 60/199: loss=(0.41987085623027093-0j)\n",
      "GD iter. 61/199: loss=(0.41956001375169133-0j)\n",
      "GD iter. 62/199: loss=(0.41925892649044244-0j)\n",
      "GD iter. 63/199: loss=(0.4189671866720495-0j)\n",
      "GD iter. 64/199: loss=(0.4186844079466715-0j)\n",
      "GD iter. 65/199: loss=(0.41841022402715966-0j)\n",
      "GD iter. 66/199: loss=(0.4181442874286959-0j)\n",
      "GD iter. 67/199: loss=(0.41788626830133807-0j)\n",
      "GD iter. 68/199: loss=(0.41763585334762815-0j)\n",
      "GD iter. 69/199: loss=(0.4173927448181645-0j)\n",
      "GD iter. 70/199: loss=(0.41715665957870524-0j)\n",
      "GD iter. 71/199: loss=(0.416927328242963-0j)\n",
      "GD iter. 72/199: loss=(0.4167044943657918-0j)\n",
      "GD iter. 73/199: loss=(0.4164879136919427-0j)\n",
      "GD iter. 74/199: loss=(0.4162773534560029-0j)\n",
      "GD iter. 75/199: loss=(0.4160725917295176-0j)\n",
      "GD iter. 76/199: loss=(0.4158734168116488-0j)\n",
      "GD iter. 77/199: loss=(0.4156796266600424-0j)\n",
      "GD iter. 78/199: loss=(0.41549102835885954-0j)\n",
      "GD iter. 79/199: loss=(0.4153074376211895-0j)\n",
      "GD iter. 80/199: loss=(0.4151286783232949-0j)\n",
      "GD iter. 81/199: loss=(0.41495458206835345-0j)\n",
      "GD iter. 82/199: loss=(0.4147849877775546-0j)\n",
      "GD iter. 83/199: loss=(0.41461974130658186-0j)\n",
      "GD iter. 84/199: loss=(0.4144586950856747-0j)\n",
      "GD iter. 85/199: loss=(0.41430170778160585-0j)\n",
      "GD iter. 86/199: loss=(0.41414864398004475-0j)\n",
      "GD iter. 87/199: loss=(0.41399937388689523-0j)\n",
      "GD iter. 88/199: loss=(0.4138537730473098-0j)\n",
      "GD iter. 89/199: loss=(0.41371172208117796-0j)\n",
      "GD iter. 90/199: loss=(0.41357310643398276-0j)\n",
      "GD iter. 91/199: loss=(0.4134378161420006-0j)\n",
      "GD iter. 92/199: loss=(0.4133057456108975-0j)\n",
      "GD iter. 93/199: loss=(0.41317679340684543-0j)\n",
      "GD iter. 94/199: loss=(0.41305086205934655-0j)\n",
      "GD iter. 95/199: loss=(0.4129278578750122-0j)\n",
      "GD iter. 96/199: loss=(0.41280769076160095-0j)\n",
      "GD iter. 97/199: loss=(0.41269027406166403-0j)\n",
      "GD iter. 98/199: loss=(0.4125755243952003-0j)\n",
      "GD iter. 99/199: loss=(0.4124633615107585-0j)\n",
      "GD iter. 100/199: loss=(0.4123537081444682-0j)\n",
      "GD iter. 101/199: loss=(0.41224648988651463-0j)\n",
      "GD iter. 102/199: loss=(0.41214163505460755-0j)\n",
      "GD iter. 103/199: loss=(0.41203907457402217-0j)\n",
      "GD iter. 104/199: loss=(0.4119387418638235-0j)\n",
      "GD iter. 105/199: loss=(0.4118405727289062-0j)\n",
      "GD iter. 106/199: loss=(0.41174450525751005-0j)\n",
      "GD iter. 107/199: loss=(0.4116504797238924-0j)\n",
      "GD iter. 108/199: loss=(0.411558438495861-0j)\n",
      "GD iter. 109/199: loss=(0.41146832594688676-0j)\n",
      "GD iter. 110/199: loss=(0.4113800883725383-0j)\n",
      "GD iter. 111/199: loss=(0.4112936739109936-0j)\n",
      "GD iter. 112/199: loss=(0.4112090324674009-0j)\n",
      "GD iter. 113/199: loss=(0.41112611564187496-0j)\n",
      "GD iter. 114/199: loss=(0.41104487666092815-0j)\n",
      "GD iter. 115/199: loss=(0.4109652703121493-0j)\n",
      "GD iter. 116/199: loss=(0.4108872528819521-0j)\n",
      "GD iter. 117/199: loss=(0.4108107820962298-0j)\n",
      "GD iter. 118/199: loss=(0.41073581706375883-0j)\n",
      "GD iter. 119/199: loss=(0.4106623182222064-0j)\n",
      "GD iter. 120/199: loss=(0.4105902472866036-0j)\n",
      "GD iter. 121/199: loss=(0.410519567200156-0j)\n",
      "GD iter. 122/199: loss=(0.4104502420872695-0j)\n",
      "GD iter. 123/199: loss=(0.41038223720867656-0j)\n",
      "GD iter. 124/199: loss=(0.4103155189185565-0j)\n",
      "GD iter. 125/199: loss=(0.4102500546235457-0j)\n",
      "GD iter. 126/199: loss=(0.4101858127435456-0j)\n",
      "GD iter. 127/199: loss=(0.4101227626742347-0j)\n",
      "GD iter. 128/199: loss=(0.4100608747512017-0j)\n",
      "GD iter. 129/199: loss=(0.4100001202156181-0j)\n",
      "GD iter. 130/199: loss=(0.40994047118137567-0j)\n",
      "GD iter. 131/199: loss=(0.40988190060361607-0j)\n",
      "GD iter. 132/199: loss=(0.4098243822485848-0j)\n",
      "GD iter. 133/199: loss=(0.4097678906647468-0j)\n",
      "GD iter. 134/199: loss=(0.40971240115510177-0j)\n",
      "GD iter. 135/199: loss=(0.4096578897506431-0j)\n",
      "GD iter. 136/199: loss=(0.40960433318490475-0j)\n",
      "GD iter. 137/199: loss=(0.4095517088695471-0j)\n",
      "GD iter. 138/199: loss=(0.40949999487093064-0j)\n",
      "GD iter. 139/199: loss=(0.4094491698876338-0j)\n",
      "GD iter. 140/199: loss=(0.4093992132288702-0j)\n",
      "GD iter. 141/199: loss=(0.4093501047937634-0j)\n",
      "GD iter. 142/199: loss=(0.4093018250514413-0j)\n",
      "GD iter. 143/199: loss=(0.40925435502191343-0j)\n",
      "GD iter. 144/199: loss=(0.40920767625769316-0j)\n",
      "GD iter. 145/199: loss=(0.40916177082613475-0j)\n",
      "GD iter. 146/199: loss=(0.4091166212924511-0j)\n",
      "GD iter. 147/199: loss=(0.4090722107033829-0j)\n",
      "GD iter. 148/199: loss=(0.4090285225714915-0j)\n",
      "GD iter. 149/199: loss=(0.4089855408600455-0j)\n",
      "GD iter. 150/199: loss=(0.4089432499684787-0j)\n",
      "GD iter. 151/199: loss=(0.4089016347183917-0j)\n",
      "GD iter. 152/199: loss=(0.4088606803400758-0j)\n",
      "GD iter. 153/199: loss=(0.4088203724595363-0j)\n",
      "GD iter. 154/199: loss=(0.40878069708599274-0j)\n",
      "GD iter. 155/199: loss=(0.40874164059983875-0j)\n",
      "GD iter. 156/199: loss=(0.40870318974103925-0j)\n",
      "GD iter. 157/199: loss=(0.40866533159794804-0j)\n",
      "GD iter. 158/199: loss=(0.4086280535965293-0j)\n",
      "GD iter. 159/199: loss=(0.4085913434899641-0j)\n",
      "GD iter. 160/199: loss=(0.40855518934862817-0j)\n",
      "GD iter. 161/199: loss=(0.4085195795504246-0j)\n",
      "GD iter. 162/199: loss=(0.4084845027714577-0j)\n",
      "GD iter. 163/199: loss=(0.4084499479770331-0j)\n",
      "GD iter. 164/199: loss=(0.408415904412973-0j)\n",
      "GD iter. 165/199: loss=(0.408382361597232-0j)\n",
      "GD iter. 166/199: loss=(0.4083493093118029-0j)\n",
      "GD iter. 167/199: loss=(0.40831673759489956-0j)\n",
      "GD iter. 168/199: loss=(0.4082846367334086-0j)\n",
      "GD iter. 169/199: loss=(0.4082529972555954-0j)\n",
      "GD iter. 170/199: loss=(0.40822180992405915-0j)\n",
      "GD iter. 171/199: loss=(0.4081910657289233-0j)\n",
      "GD iter. 172/199: loss=(0.4081607558812546-0j)\n",
      "GD iter. 173/199: loss=(0.4081308718067017-0j)\n",
      "GD iter. 174/199: loss=(0.40810140513934423-0j)\n",
      "GD iter. 175/199: loss=(0.40807234771574485-0j)\n",
      "GD iter. 176/199: loss=(0.40804369156919706-0j)\n",
      "GD iter. 177/199: loss=(0.4080154289241602-0j)\n",
      "GD iter. 178/199: loss=(0.4079875521908764-0j)\n",
      "GD iter. 179/199: loss=(0.4079600539601603-0j)\n",
      "GD iter. 180/199: loss=(0.4079329269983585-0j)\n",
      "GD iter. 181/199: loss=(0.4079061642424687-0j)\n",
      "GD iter. 182/199: loss=(0.40787975879541627-0j)\n",
      "GD iter. 183/199: loss=(0.40785370392147924-0j)\n",
      "GD iter. 184/199: loss=(0.4078279930418595-0j)\n",
      "GD iter. 185/199: loss=(0.40780261973039245-0j)\n",
      "GD iter. 186/199: loss=(0.40777757770939077-0j)\n",
      "GD iter. 187/199: loss=(0.40775286084561924-0j)\n",
      "GD iter. 188/199: loss=(0.4077284631463931-0j)\n",
      "GD iter. 189/199: loss=(0.4077043787557978-0j)\n",
      "GD iter. 190/199: loss=(0.40768060195102473-0j)\n",
      "GD iter. 191/199: loss=(0.40765712713881985-0j)\n",
      "GD iter. 192/199: loss=(0.4076339488520397-0j)\n",
      "GD iter. 193/199: loss=(0.4076110617463133-0j)\n",
      "GD iter. 194/199: loss=(0.4075884605968036-0j)\n",
      "GD iter. 195/199: loss=(0.4075661402950683-0j)\n",
      "GD iter. 196/199: loss=(0.407544095846013-0j)\n",
      "GD iter. 197/199: loss=(0.4075223223649363-0j)\n",
      "GD iter. 198/199: loss=(0.40750081507466346-0j)\n",
      "GD iter. 199/199: loss=(0.4074795693027636-0j)\n",
      "The Accuracy is: 0.8818\n",
      "The F1 score is: 0.4375\n",
      "The precision is: 0.3944\n",
      "The recall is: 0.4912\n",
      "GD iter. 0/199: loss=(0.6825829259019192-0j)\n",
      "GD iter. 1/199: loss=(0.5344252644740147-0j)\n",
      "GD iter. 2/199: loss=(0.5199328109732259-0j)\n",
      "GD iter. 3/199: loss=(0.5095765010773411-0j)\n",
      "GD iter. 4/199: loss=(0.5013504101956106-0j)\n",
      "GD iter. 5/199: loss=(0.49446035723756576-0j)\n",
      "GD iter. 6/199: loss=(0.48850774065613095-0j)\n",
      "GD iter. 7/199: loss=(0.4832585111850963-0j)\n",
      "GD iter. 8/199: loss=(0.4785624146982961-0j)\n",
      "GD iter. 9/199: loss=(0.47431699615054906-0j)\n",
      "GD iter. 10/199: loss=(0.47044891534513533-0j)\n",
      "GD iter. 11/199: loss=(0.4669034349005919-0j)\n",
      "GD iter. 12/199: loss=(0.46363820846376974-0j)\n",
      "GD iter. 13/199: loss=(0.460619468873622-0j)\n",
      "GD iter. 14/199: loss=(0.4578196083534268-0j)\n",
      "GD iter. 15/199: loss=(0.455215591472349-0j)\n",
      "GD iter. 16/199: loss=(0.45278788052095703-0j)\n",
      "GD iter. 17/199: loss=(0.4505196848681676-0j)\n",
      "GD iter. 18/199: loss=(0.448396420748651-0j)\n",
      "GD iter. 19/199: loss=(0.4464053114481091-0j)\n",
      "GD iter. 20/199: loss=(0.4445350836971114-0j)\n",
      "GD iter. 21/199: loss=(0.4427757317483733-0j)\n",
      "GD iter. 22/199: loss=(0.4411183302966668-0j)\n",
      "GD iter. 23/199: loss=(0.43955488350564276-0j)\n",
      "GD iter. 24/199: loss=(0.4380782013302432-0j)\n",
      "GD iter. 25/199: loss=(0.4366817968963221-0j)\n",
      "GD iter. 26/199: loss=(0.43535980042045386-0j)\n",
      "GD iter. 27/199: loss=(0.43410688632873345-0j)\n",
      "GD iter. 28/199: loss=(0.43291821105374956-0j)\n",
      "GD iter. 29/199: loss=(0.4317893595735759-0j)\n",
      "GD iter. 30/199: loss=(0.43071629918195853-0j)\n",
      "GD iter. 31/199: loss=(0.42969533929436426-0j)\n",
      "GD iter. 32/199: loss=(0.4287230963326999-0j)\n",
      "GD iter. 33/199: loss=(0.4277964629140758-0j)\n",
      "GD iter. 34/199: loss=(0.4269125807108296-0j)\n",
      "GD iter. 35/199: loss=(0.42606881646049083-0j)\n",
      "GD iter. 36/199: loss=(0.425262740692851-0j)\n",
      "GD iter. 37/199: loss=(0.4244921088121607-0j)\n",
      "GD iter. 38/199: loss=(0.4237548442296872-0j)\n",
      "GD iter. 39/199: loss=(0.4230490232884244-0j)\n",
      "GD iter. 40/199: loss=(0.42237286175990346-0j)\n",
      "GD iter. 41/199: loss=(0.42172470272456586-0j)\n",
      "GD iter. 42/199: loss=(0.42110300567335873-0j)\n",
      "GD iter. 43/199: loss=(0.4205063366901425-0j)\n",
      "GD iter. 44/199: loss=(0.41993335959297423-0j)\n",
      "GD iter. 45/199: loss=(0.4193828279279818-0j)\n",
      "GD iter. 46/199: loss=(0.41885357772288445-0j)\n",
      "GD iter. 47/199: loss=(0.41834452091863344-0j)\n",
      "GD iter. 48/199: loss=(0.4178546394074711-0j)\n",
      "GD iter. 49/199: loss=(0.4173829796141906-0j)\n",
      "GD iter. 50/199: loss=(0.41692864756473635-0j)\n",
      "GD iter. 51/199: loss=(0.41649080439267894-0j)\n",
      "GD iter. 52/199: loss=(0.4160686622396801-0j)\n",
      "GD iter. 53/199: loss=(0.4156614805109452-0j)\n",
      "GD iter. 54/199: loss=(0.41526856245094057-0j)\n",
      "GD iter. 55/199: loss=(0.41488925200841403-0j)\n",
      "GD iter. 56/199: loss=(0.41452293096306897-0j)\n",
      "GD iter. 57/199: loss=(0.4141690162891699-0j)\n",
      "GD iter. 58/199: loss=(0.41382695773393674-0j)\n",
      "GD iter. 59/199: loss=(0.4134962355908775-0j)\n",
      "GD iter. 60/199: loss=(0.4131763586502392-0j)\n",
      "GD iter. 61/199: loss=(0.4128668623105577-0j)\n",
      "GD iter. 62/199: loss=(0.4125673068368962-0j)\n",
      "GD iter. 63/199: loss=(0.4122772757527871-0j)\n",
      "GD iter. 64/199: loss=(0.41199637435417263-0j)\n",
      "GD iter. 65/199: loss=(0.411724228334773-0j)\n",
      "GD iter. 66/199: loss=(0.4114604825133332-0j)\n",
      "GD iter. 67/199: loss=(0.41120479965411005-0j)\n",
      "GD iter. 68/199: loss=(0.41095685937277826-0j)\n",
      "GD iter. 69/199: loss=(0.41071635712066656-0j)\n",
      "GD iter. 70/199: loss=(0.41048300324089315-0j)\n",
      "GD iter. 71/199: loss=(0.41025652209056146-0j)\n",
      "GD iter. 72/199: loss=(0.4100366512237082-0j)\n",
      "GD iter. 73/199: loss=(0.40982314063017616-0j)\n",
      "GD iter. 74/199: loss=(0.4096157520260123-0j)\n",
      "GD iter. 75/199: loss=(0.40941425819138544-0j)\n",
      "GD iter. 76/199: loss=(0.40921844235236543-0j)\n",
      "GD iter. 77/199: loss=(0.4090280976032235-0j)\n",
      "GD iter. 78/199: loss=(0.4088430263662033-0j)\n",
      "GD iter. 79/199: loss=(0.40866303988596686-0j)\n",
      "GD iter. 80/199: loss=(0.408487957756161-0j)\n",
      "GD iter. 81/199: loss=(0.40831760747575513-0j)\n",
      "GD iter. 82/199: loss=(0.4081518240330048-0j)\n",
      "GD iter. 83/199: loss=(0.4079904495150605-0j)\n",
      "GD iter. 84/199: loss=(0.4078333327414098-0j)\n",
      "GD iter. 85/199: loss=(0.40768032891948053-0j)\n",
      "GD iter. 86/199: loss=(0.40753129932086785-0j)\n",
      "GD iter. 87/199: loss=(0.4073861109767683-0j)\n",
      "GD iter. 88/199: loss=(0.4072446363913132-0j)\n",
      "GD iter. 89/199: loss=(0.40710675327159496-0j)\n",
      "GD iter. 90/199: loss=(0.4069723442732718-0j)\n",
      "GD iter. 91/199: loss=(0.4068412967607205-0j)\n",
      "GD iter. 92/199: loss=(0.4067135025807834-0j)\n",
      "GD iter. 93/199: loss=(0.40658885784922666-0j)\n",
      "GD iter. 94/199: loss=(0.40646726274909145-0j)\n",
      "GD iter. 95/199: loss=(0.40634862134017985-0j)\n",
      "GD iter. 96/199: loss=(0.4062328413789709-0j)\n",
      "GD iter. 97/199: loss=(0.4061198341483124-0j)\n",
      "GD iter. 98/199: loss=(0.406009514296281-0j)\n",
      "GD iter. 99/199: loss=(0.40590179968364676-0j)\n",
      "GD iter. 100/199: loss=(0.40579661123941435-0j)\n",
      "GD iter. 101/199: loss=(0.4056938728239514-0j)\n",
      "GD iter. 102/199: loss=(0.40559351109924985-0j)\n",
      "GD iter. 103/199: loss=(0.4054954554058926-0j)\n",
      "GD iter. 104/199: loss=(0.4053996376463297-0j)\n",
      "GD iter. 105/199: loss=(0.4053059921740941-0j)\n",
      "GD iter. 106/199: loss=(0.40521445568861-0j)\n",
      "GD iter. 107/199: loss=(0.4051249671352721-0j)\n",
      "GD iter. 108/199: loss=(0.4050374676104918-0j)\n",
      "GD iter. 109/199: loss=(0.40495190027142985-0j)\n",
      "GD iter. 110/199: loss=(0.4048682102501483-0j)\n",
      "GD iter. 111/199: loss=(0.40478634457193635-0j)\n",
      "GD iter. 112/199: loss=(0.4047062520775765-0j)\n",
      "GD iter. 113/199: loss=(0.4046278833493344-0j)\n",
      "GD iter. 114/199: loss=(0.40455119064046674-0j)\n",
      "GD iter. 115/199: loss=(0.40447612780805803-0j)\n",
      "GD iter. 116/199: loss=(0.4044026502490043-0j)\n",
      "GD iter. 117/199: loss=(0.40433071483897604-0j)\n",
      "GD iter. 118/199: loss=(0.40426027987420177-0j)\n",
      "GD iter. 119/199: loss=(0.4041913050159226-0j)\n",
      "GD iter. 120/199: loss=(0.4041237512373774-0j)\n",
      "GD iter. 121/199: loss=(0.4040575807731882-0j)\n",
      "GD iter. 122/199: loss=(0.4039927570710183-0j)\n",
      "GD iter. 123/199: loss=(0.40392924474539027-0j)\n",
      "GD iter. 124/199: loss=(0.40386700953355026-0j)\n",
      "GD iter. 125/199: loss=(0.4038060182532766-0j)\n",
      "GD iter. 126/199: loss=(0.40374623876253357-0j)\n",
      "GD iter. 127/199: loss=(0.4036876399208789-0j)\n",
      "GD iter. 128/199: loss=(0.4036301915525378-0j)\n",
      "GD iter. 129/199: loss=(0.40357386441106013-0j)\n",
      "GD iter. 130/199: loss=(0.4035186301454844-0j)\n",
      "GD iter. 131/199: loss=(0.40346446126793417-0j)\n",
      "GD iter. 132/199: loss=(0.40341133112257777-0j)\n",
      "GD iter. 133/199: loss=(0.40335921385588636-0j)\n",
      "GD iter. 134/199: loss=(0.40330808438812643-0j)\n",
      "GD iter. 135/199: loss=(0.4032579183860308-0j)\n",
      "GD iter. 136/199: loss=(0.40320869223658995-0j)\n",
      "GD iter. 137/199: loss=(0.4031603830219124-0j)\n",
      "GD iter. 138/199: loss=(0.4031129684951047-0j)\n",
      "GD iter. 139/199: loss=(0.40306642705712176-0j)\n",
      "GD iter. 140/199: loss=(0.4030207377345457-0j)\n",
      "GD iter. 141/199: loss=(0.40297588015824837-0j)\n",
      "GD iter. 142/199: loss=(0.40293183454289855-0j)\n",
      "GD iter. 143/199: loss=(0.4028885816672746-0j)\n",
      "GD iter. 144/199: loss=(0.40284610285534855-0j)\n",
      "GD iter. 145/199: loss=(0.40280437995810386-0j)\n",
      "GD iter. 146/199: loss=(0.4027633953360573-0j)\n",
      "GD iter. 147/199: loss=(0.402723131842452-0j)\n",
      "GD iter. 148/199: loss=(0.40268357280709316-0j)\n",
      "GD iter. 149/199: loss=(0.402644702020798-0j)\n",
      "GD iter. 150/199: loss=(0.4026065037204327-0j)\n",
      "GD iter. 151/199: loss=(0.4025689625745127-0j)\n",
      "GD iter. 152/199: loss=(0.4025320636693404-0j)\n",
      "GD iter. 153/199: loss=(0.40249579249565803-0j)\n",
      "GD iter. 154/199: loss=(0.4024601349357943-0j)\n",
      "GD iter. 155/199: loss=(0.40242507725128307-0j)\n",
      "GD iter. 156/199: loss=(0.40239060607093524-0j)\n",
      "GD iter. 157/199: loss=(0.40235670837934445-0j)\n",
      "GD iter. 158/199: loss=(0.4023233715058073-0j)\n",
      "GD iter. 159/199: loss=(0.40229058311364485-0j)\n",
      "GD iter. 160/199: loss=(0.40225833118990373-0j)\n",
      "GD iter. 161/199: loss=(0.4022266040354256-0j)\n",
      "GD iter. 162/199: loss=(0.40219539025526796-0j)\n",
      "GD iter. 163/199: loss=(0.4021646787494624-0j)\n",
      "GD iter. 164/199: loss=(0.40213445870409725-0j)\n",
      "GD iter. 165/199: loss=(0.4021047195827112-0j)\n",
      "GD iter. 166/199: loss=(0.40207545111798526-0j)\n",
      "GD iter. 167/199: loss=(0.40204664330372264-0j)\n",
      "GD iter. 168/199: loss=(0.40201828638710285-0j)\n",
      "GD iter. 169/199: loss=(0.40199037086120215-0j)\n",
      "GD iter. 170/199: loss=(0.40196288745776737-0j)\n",
      "GD iter. 171/199: loss=(0.4019358271402351-0j)\n",
      "GD iter. 172/199: loss=(0.4019091810969858-0j)\n",
      "GD iter. 173/199: loss=(0.40188294073482417-0j)\n",
      "GD iter. 174/199: loss=(0.4018570976726772-0j)\n",
      "GD iter. 175/199: loss=(0.4018316437355015-0j)\n",
      "GD iter. 176/199: loss=(0.4018065709483922-0j)\n",
      "GD iter. 177/199: loss=(0.40178187153088524-0j)\n",
      "GD iter. 178/199: loss=(0.4017575378914468-0j)\n",
      "GD iter. 179/199: loss=(0.40173356262214205-0j)\n",
      "GD iter. 180/199: loss=(0.401709938493477-0j)\n",
      "GD iter. 181/199: loss=(0.4016866584494068-0j)\n",
      "GD iter. 182/199: loss=(0.40166371560250463-0j)\n",
      "GD iter. 183/199: loss=(0.4016411032292854-0j)\n",
      "GD iter. 184/199: loss=(0.4016188147656781-0j)\n",
      "GD iter. 185/199: loss=(0.40159684380264193-0j)\n",
      "GD iter. 186/199: loss=(0.4015751840819208-0j)\n",
      "GD iter. 187/199: loss=(0.4015538294919315-0j)\n",
      "GD iter. 188/199: loss=(0.40153277406378046-0j)\n",
      "GD iter. 189/199: loss=(0.40151201196740416-0j)\n",
      "GD iter. 190/199: loss=(0.4014915375078302-0j)\n",
      "GD iter. 191/199: loss=(0.40147134512155314-0j)\n",
      "GD iter. 192/199: loss=(0.4014514293730223-0j)\n",
      "GD iter. 193/199: loss=(0.40143178495123644-0j)\n",
      "GD iter. 194/199: loss=(0.40141240666644334-0j)\n",
      "GD iter. 195/199: loss=(0.4013932894469384-0j)\n",
      "GD iter. 196/199: loss=(0.4013744283359614-0j)\n",
      "GD iter. 197/199: loss=(0.40135581848868523-0j)\n",
      "GD iter. 198/199: loss=(0.40133745516929625-0j)\n",
      "GD iter. 199/199: loss=(0.401319333748161-0j)\n",
      "The Accuracy is: 0.8571\n",
      "The F1 score is: 0.4000\n",
      "The precision is: 0.3295\n",
      "The recall is: 0.5088\n",
      "GD iter. 0/199: loss=(0.6754387371484223-0j)\n",
      "GD iter. 1/199: loss=(0.5436899739759549-0j)\n",
      "GD iter. 2/199: loss=(0.529101137920453-0j)\n",
      "GD iter. 3/199: loss=(0.5185337827305825-0j)\n",
      "GD iter. 4/199: loss=(0.5101120870044511-0j)\n",
      "GD iter. 5/199: loss=(0.5030655367375478-0j)\n",
      "GD iter. 6/199: loss=(0.49699040663663036-0j)\n",
      "GD iter. 7/199: loss=(0.4916435328299525-0j)\n",
      "GD iter. 8/199: loss=(0.4868669755929462-0j)\n",
      "GD iter. 9/199: loss=(0.4825527137597117-0j)\n",
      "GD iter. 10/199: loss=(0.47862362453921176-0j)\n",
      "GD iter. 11/199: loss=(0.47502249576805144-0j)\n",
      "GD iter. 12/199: loss=(0.4717054076751744-0j)\n",
      "GD iter. 13/199: loss=(0.46863761741398224-0j)\n",
      "GD iter. 14/199: loss=(0.46579092603125966-0j)\n",
      "GD iter. 15/199: loss=(0.46314194725712216-0j)\n",
      "GD iter. 16/199: loss=(0.4606709381581988-0j)\n",
      "GD iter. 17/199: loss=(0.4583609878589774-0j)\n",
      "GD iter. 18/199: loss=(0.45619743955656-0j)\n",
      "GD iter. 19/199: loss=(0.454167467915874-0j)\n",
      "GD iter. 20/199: loss=(0.4522597622689855-0j)\n",
      "GD iter. 21/199: loss=(0.4504642834871388-0j)\n",
      "GD iter. 22/199: loss=(0.44877207331637325-0j)\n",
      "GD iter. 23/199: loss=(0.44717510191683063-0j)\n",
      "GD iter. 24/199: loss=(0.4456661438369368-0j)\n",
      "GD iter. 25/199: loss=(0.4442386756012209-0j)\n",
      "GD iter. 26/199: loss=(0.44288679005533055-0j)\n",
      "GD iter. 27/199: loss=(0.4416051239423078-0j)\n",
      "GD iter. 28/199: loss=(0.44038879609989573-0j)\n",
      "GD iter. 29/199: loss=(0.43923335430945787-0j)\n",
      "GD iter. 30/199: loss=(0.43813472928327346-0j)\n",
      "GD iter. 31/199: loss=(0.43708919460743645-0j)\n",
      "GD iter. 32/199: loss=(0.43609333170120324-0j)\n",
      "GD iter. 33/199: loss=(0.43514399903637413-0j)\n",
      "GD iter. 34/199: loss=(0.43423830499969607-0j)\n",
      "GD iter. 35/199: loss=(0.4333735838893415-0j)\n",
      "GD iter. 36/199: loss=(0.432547374621549-0j)\n",
      "GD iter. 37/199: loss=(0.43175740179136224-0j)\n",
      "GD iter. 38/199: loss=(0.43100155878620594-0j)\n",
      "GD iter. 39/199: loss=(0.43027789269580774-0j)\n",
      "GD iter. 40/199: loss=(0.42958459079890116-0j)\n",
      "GD iter. 41/199: loss=(0.42891996843786984-0j)\n",
      "GD iter. 42/199: loss=(0.42828245811823956-0j)\n",
      "GD iter. 43/199: loss=(0.427670599691654-0j)\n",
      "GD iter. 44/199: loss=(0.4270830314994001-0j)\n",
      "GD iter. 45/199: loss=(0.42651848236925916-0j)\n",
      "GD iter. 46/199: loss=(0.425975764371922-0j)\n",
      "GD iter. 47/199: loss=(0.4254537662547635-0j)\n",
      "GD iter. 48/199: loss=(0.42495144748075525-0j)\n",
      "GD iter. 49/199: loss=(0.4244678328089153-0j)\n",
      "GD iter. 50/199: loss=(0.42400200736018406-0j)\n",
      "GD iter. 51/199: loss=(0.4235531121191247-0j)\n",
      "GD iter. 52/199: loss=(0.42312033982751746-0j)\n",
      "GD iter. 53/199: loss=(0.4227029312308781-0j)\n",
      "GD iter. 54/199: loss=(0.42230017164326905-0j)\n",
      "GD iter. 55/199: loss=(0.42191138779957615-0j)\n",
      "GD iter. 56/199: loss=(0.42153594496777075-0j)\n",
      "GD iter. 57/199: loss=(0.42117324429662073-0j)\n",
      "GD iter. 58/199: loss=(0.42082272037690804-0j)\n",
      "GD iter. 59/199: loss=(0.4204838389965061-0j)\n",
      "GD iter. 60/199: loss=(0.4201560950716958-0j)\n",
      "GD iter. 61/199: loss=(0.4198390107388992-0j)\n",
      "GD iter. 62/199: loss=(0.41953213359260066-0j)\n",
      "GD iter. 63/199: loss=(0.4192350350566462-0j)\n",
      "GD iter. 64/199: loss=(0.4189473088773698-0j)\n",
      "GD iter. 65/199: loss=(0.4186685697281207-0j)\n",
      "GD iter. 66/199: loss=(0.41839845191576824-0j)\n",
      "GD iter. 67/199: loss=(0.41813660818065634-0j)\n",
      "GD iter. 68/199: loss=(0.41788270858228166-0j)\n",
      "GD iter. 69/199: loss=(0.41763643946368834-0j)\n",
      "GD iter. 70/199: loss=(0.4173975024882159-0j)\n",
      "GD iter. 71/199: loss=(0.4171656137428143-0j)\n",
      "GD iter. 72/199: loss=(0.41694050290266443-0j)\n",
      "GD iter. 73/199: loss=(0.4167219124523044-0j)\n",
      "GD iter. 74/199: loss=(0.41650959695889145-0j)\n",
      "GD iter. 75/199: loss=(0.41630332239360396-0j)\n",
      "GD iter. 76/199: loss=(0.41610286549753617-0j)\n",
      "GD iter. 77/199: loss=(0.415908013188747-0j)\n",
      "GD iter. 78/199: loss=(0.4157185620074088-0j)\n",
      "GD iter. 79/199: loss=(0.41553431759625464-0j)\n",
      "GD iter. 80/199: loss=(0.4153550942137549-0j)\n",
      "GD iter. 81/199: loss=(0.41518071427766556-0j)\n",
      "GD iter. 82/199: loss=(0.4150110079367825-0j)\n",
      "GD iter. 83/199: loss=(0.4148458126689042-0j)\n",
      "GD iter. 84/199: loss=(0.4146849729031744-0j)\n",
      "GD iter. 85/199: loss=(0.4145283396651098-0j)\n",
      "GD iter. 86/199: loss=(0.4143757702427567-0j)\n",
      "GD iter. 87/199: loss=(0.4142271278725384-0j)\n",
      "GD iter. 88/199: loss=(0.41408228144346576-0j)\n",
      "GD iter. 89/199: loss=(0.41394110521848476-0j)\n",
      "GD iter. 90/199: loss=(0.41380347857182354-0j)\n",
      "GD iter. 91/199: loss=(0.4136692857412932-0j)\n",
      "GD iter. 92/199: loss=(0.4135384155945655-0j)\n",
      "GD iter. 93/199: loss=(0.41341076140852845-0j)\n",
      "GD iter. 94/199: loss=(0.41328622066088316-0j)\n",
      "GD iter. 95/199: loss=(0.4131646948332064-0j)\n",
      "GD iter. 96/199: loss=(0.4130460892247565-0j)\n",
      "GD iter. 97/199: loss=(0.4129303127763563-0j)\n",
      "GD iter. 98/199: loss=(0.4128172779037265-0j)\n",
      "GD iter. 99/199: loss=(0.41270690033969376-0j)\n",
      "GD iter. 100/199: loss=(0.41259909898473096-0j)\n",
      "GD iter. 101/199: loss=(0.4124937957653292-0j)\n",
      "GD iter. 102/199: loss=(0.41239091549973184-0j)\n",
      "GD iter. 103/199: loss=(0.41229038577059496-0j)\n",
      "GD iter. 104/199: loss=(0.4121921368041638-0j)\n",
      "GD iter. 105/199: loss=(0.4120961013555885-0j)\n",
      "GD iter. 106/199: loss=(0.4120022146000189-0j)\n",
      "GD iter. 107/199: loss=(0.41191041402914946-0j)\n",
      "GD iter. 108/199: loss=(0.41182063935290136-0j)\n",
      "GD iter. 109/199: loss=(0.41173283240595054-0j)\n",
      "GD iter. 110/199: loss=(0.411646937058831-0j)\n",
      "GD iter. 111/199: loss=(0.4115628991333554-0j)\n",
      "GD iter. 112/199: loss=(0.411480666322116-0j)\n",
      "GD iter. 113/199: loss=(0.4114001881118413-0j)\n",
      "GD iter. 114/199: loss=(0.4113214157103953-0j)\n",
      "GD iter. 115/199: loss=(0.4112443019772258-0j)\n",
      "GD iter. 116/199: loss=(0.4111688013570732-0j)\n",
      "GD iter. 117/199: loss=(0.411094869816767-0j)\n",
      "GD iter. 118/199: loss=(0.41102246478494564-0j)\n",
      "GD iter. 119/199: loss=(0.4109515450945456-0j)\n",
      "GD iter. 120/199: loss=(0.410882070927915-0j)\n",
      "GD iter. 121/199: loss=(0.41081400376441546-0j)\n",
      "GD iter. 122/199: loss=(0.4107473063303837-0j)\n",
      "GD iter. 123/199: loss=(0.4106819425513318-0j)\n",
      "GD iter. 124/199: loss=(0.4106178775062715-0j)\n",
      "GD iter. 125/199: loss=(0.41055507738405717-0j)\n",
      "GD iter. 126/199: loss=(0.4104935094416431-0j)\n",
      "GD iter. 127/199: loss=(0.4104331419641619-0j)\n",
      "GD iter. 128/199: loss=(0.41037394422673307-0j)\n",
      "GD iter. 129/199: loss=(0.41031588645791495-0j)\n",
      "GD iter. 130/199: loss=(0.41025893980472206-0j)\n",
      "GD iter. 131/199: loss=(0.41020307629913016-0j)\n",
      "GD iter. 132/199: loss=(0.4101482688259972-0j)\n",
      "GD iter. 133/199: loss=(0.41009449109233215-0j)\n",
      "GD iter. 134/199: loss=(0.4100417175978478-0j)\n",
      "GD iter. 135/199: loss=(0.40998992360673603-0j)\n",
      "GD iter. 136/199: loss=(0.4099390851206081-0j)\n",
      "GD iter. 137/199: loss=(0.4098891788525457-0j)\n",
      "GD iter. 138/199: loss=(0.4098401822022098-0j)\n",
      "GD iter. 139/199: loss=(0.40979207323195943-0j)\n",
      "GD iter. 140/199: loss=(0.40974483064393413-0j)\n",
      "GD iter. 141/199: loss=(0.40969843375805376-0j)\n",
      "GD iter. 142/199: loss=(0.409652862490897-0j)\n",
      "GD iter. 143/199: loss=(0.40960809733541526-0j)\n",
      "GD iter. 144/199: loss=(0.40956411934144854-0j)\n",
      "GD iter. 145/199: loss=(0.4095209100970025-0j)\n",
      "GD iter. 146/199: loss=(0.40947845171025704-0j)\n",
      "GD iter. 147/199: loss=(0.40943672679227205-0j)\n",
      "GD iter. 148/199: loss=(0.40939571844035827-0j)\n",
      "GD iter. 149/199: loss=(0.40935541022208793-0j)\n",
      "GD iter. 150/199: loss=(0.4093157861599132-0j)\n",
      "GD iter. 151/199: loss=(0.409276830716369-0j)\n",
      "GD iter. 152/199: loss=(0.4092385287798329-0j)\n",
      "GD iter. 153/199: loss=(0.40920086565081987-0j)\n",
      "GD iter. 154/199: loss=(0.40916382702878784-0j)\n",
      "GD iter. 155/199: loss=(0.40912739899943323-0j)\n",
      "GD iter. 156/199: loss=(0.4090915680224552-0j)\n",
      "GD iter. 157/199: loss=(0.40905632091976896-0j)\n",
      "GD iter. 158/199: loss=(0.4090216448641493-0j)\n",
      "GD iter. 159/199: loss=(0.40898752736828753-0j)\n",
      "GD iter. 160/199: loss=(0.40895395627424225-0j)\n",
      "GD iter. 161/199: loss=(0.408920919743271-0j)\n",
      "GD iter. 162/199: loss=(0.4088884062460236-0j)\n",
      "GD iter. 163/199: loss=(0.40885640455308603-0j)\n",
      "GD iter. 164/199: loss=(0.4088249037258577-0j)\n",
      "GD iter. 165/199: loss=(0.40879389310774994-0j)\n",
      "GD iter. 166/199: loss=(0.408763362315693-0j)\n",
      "GD iter. 167/199: loss=(0.40873330123193846-0j)\n",
      "GD iter. 168/199: loss=(0.40870369999614603-0j)\n",
      "GD iter. 169/199: loss=(0.40867454899774314-0j)\n",
      "GD iter. 170/199: loss=(0.40864583886854605-0j)\n",
      "GD iter. 171/199: loss=(0.40861756047563336-0j)\n",
      "GD iter. 172/199: loss=(0.4085897049144611-0j)\n",
      "GD iter. 173/199: loss=(0.40856226350221075-0j)\n",
      "GD iter. 174/199: loss=(0.40853522777136025-0j)\n",
      "GD iter. 175/199: loss=(0.40850858946347024-0j)\n",
      "GD iter. 176/199: loss=(0.4084823405231773-0j)\n",
      "GD iter. 177/199: loss=(0.40845647309238553-0j)\n",
      "GD iter. 178/199: loss=(0.4084309795046502-0j)\n",
      "GD iter. 179/199: loss=(0.40840585227974424-0j)\n",
      "GD iter. 180/199: loss=(0.40838108411840396-0j)\n",
      "GD iter. 181/199: loss=(0.40835666789724295-0j)\n",
      "GD iter. 182/199: loss=(0.40833259666383215-0j)\n",
      "GD iter. 183/199: loss=(0.40830886363193725-0j)\n",
      "GD iter. 184/199: loss=(0.40828546217690853-0j)\n",
      "GD iter. 185/199: loss=(0.40826238583121777-0j)\n",
      "GD iter. 186/199: loss=(0.40823962828013616-0j)\n",
      "GD iter. 187/199: loss=(0.40821718335754886-0j)\n",
      "GD iter. 188/199: loss=(0.4081950450419012-0j)\n",
      "GD iter. 189/199: loss=(0.4081732074522705-0j)\n",
      "GD iter. 190/199: loss=(0.40815166484456156-0j)\n",
      "GD iter. 191/199: loss=(0.40813041160781915-0j)\n",
      "GD iter. 192/199: loss=(0.40810944226065365-0j)\n",
      "GD iter. 193/199: loss=(0.4080887514477776-0j)\n",
      "GD iter. 194/199: loss=(0.40806833393664715-0j)\n",
      "GD iter. 195/199: loss=(0.4080481846142058-0j)\n",
      "GD iter. 196/199: loss=(0.40802829848372735-0j)\n",
      "GD iter. 197/199: loss=(0.4080086706617529-0j)\n",
      "GD iter. 198/199: loss=(0.40798929637512094-0j)\n",
      "GD iter. 199/199: loss=(0.4079701709580857-0j)\n",
      "The Accuracy is: 0.8736\n",
      "The F1 score is: 0.4460\n",
      "The precision is: 0.3735\n",
      "The recall is: 0.5536\n",
      "GD iter. 0/199: loss=(0.706080338866752-0j)\n",
      "GD iter. 1/199: loss=(0.5464042190788867-0j)\n",
      "GD iter. 2/199: loss=(0.5298910775750041-0j)\n",
      "GD iter. 3/199: loss=(0.5186444639492516-0j)\n",
      "GD iter. 4/199: loss=(0.5099055501636456-0j)\n",
      "GD iter. 5/199: loss=(0.5026811866323311-0j)\n",
      "GD iter. 6/199: loss=(0.49650008616443114-0j)\n",
      "GD iter. 7/199: loss=(0.4910904946090961-0j)\n",
      "GD iter. 8/199: loss=(0.4862795126955038-0j)\n",
      "GD iter. 9/199: loss=(0.4819503223847229-0j)\n",
      "GD iter. 10/199: loss=(0.4780201969369806-0j)\n",
      "GD iter. 11/199: loss=(0.4744281244887375-0j)\n",
      "GD iter. 12/199: loss=(0.47112748273290295-0j)\n",
      "GD iter. 13/199: loss=(0.4680815398279234-0j)\n",
      "GD iter. 14/199: loss=(0.4652605997158995-0j)\n",
      "GD iter. 15/199: loss=(0.46264013407837296-0j)\n",
      "GD iter. 16/199: loss=(0.46019952281480164-0j)\n",
      "GD iter. 17/199: loss=(0.45792117980174-0j)\n",
      "GD iter. 18/199: loss=(0.45578992889635683-0j)\n",
      "GD iter. 19/199: loss=(0.45379254660836754-0j)\n",
      "GD iter. 20/199: loss=(0.4519174185545824-0j)\n",
      "GD iter. 21/199: loss=(0.45015427549536574-0j)\n",
      "GD iter. 22/199: loss=(0.4484939863552274-0j)\n",
      "GD iter. 23/199: loss=(0.44692839297417325-0j)\n",
      "GD iter. 24/199: loss=(0.44545017607359516-0j)\n",
      "GD iter. 25/199: loss=(0.4440527450336645-0j)\n",
      "GD iter. 26/199: loss=(0.4427301461638337-0j)\n",
      "GD iter. 27/199: loss=(0.4414769855704716-0j)\n",
      "GD iter. 28/199: loss=(0.4402883637146032-0j)\n",
      "GD iter. 29/199: loss=(0.4391598194531458-0j)\n",
      "GD iter. 30/199: loss=(0.43808728186208196-0j)\n",
      "GD iter. 31/199: loss=(0.4370670285104781-0j)\n",
      "GD iter. 32/199: loss=(0.43609564913038906-0j)\n",
      "GD iter. 33/199: loss=(0.43517001383659026-0j)\n",
      "GD iter. 34/199: loss=(0.43428724521029-0j)\n",
      "GD iter. 35/199: loss=(0.43344469368538835-0j)\n",
      "GD iter. 36/199: loss=(0.43263991577359323-0j)\n",
      "GD iter. 37/199: loss=(0.4318706547423076-0j)\n",
      "GD iter. 38/199: loss=(0.431134823421445-0j)\n",
      "GD iter. 39/199: loss=(0.43043048886570395-0j)\n",
      "GD iter. 40/199: loss=(0.42975585863996696-0j)\n",
      "GD iter. 41/199: loss=(0.4291092685293454-0j)\n",
      "GD iter. 42/199: loss=(0.42848917150347227-0j)\n",
      "GD iter. 43/199: loss=(0.42789412778809394-0j)\n",
      "GD iter. 44/199: loss=(0.4273227959167217-0j)\n",
      "GD iter. 45/199: loss=(0.42677392465176883-0j)\n",
      "GD iter. 46/199: loss=(0.4262463456787612-0j)\n",
      "GD iter. 47/199: loss=(0.4257389669893035-0j)\n",
      "GD iter. 48/199: loss=(0.4252507668788603-0j)\n",
      "GD iter. 49/199: loss=(0.4247807884943479-0j)\n",
      "GD iter. 50/199: loss=(0.4243281348742479-0j)\n",
      "GD iter. 51/199: loss=(0.4238919644306577-0j)\n",
      "GD iter. 52/199: loss=(0.4234714868285062-0j)\n",
      "GD iter. 53/199: loss=(0.423065959222246-0j)\n",
      "GD iter. 54/199: loss=(0.4226746828147649-0j)\n",
      "GD iter. 55/199: loss=(0.4222969997071511-0j)\n",
      "GD iter. 56/199: loss=(0.42193229001135574-0j)\n",
      "GD iter. 57/199: loss=(0.42157996920080026-0j)\n",
      "GD iter. 58/199: loss=(0.4212394856766222-0j)\n",
      "GD iter. 59/199: loss=(0.420910318529589-0j)\n",
      "GD iter. 60/199: loss=(0.42059197547977284-0j)\n",
      "GD iter. 61/199: loss=(0.420283990977917-0j)\n",
      "GD iter. 62/199: loss=(0.4199859244540379-0j)\n",
      "GD iter. 63/199: loss=(0.41969735870026065-0j)\n",
      "GD iter. 64/199: loss=(0.4194178983761621-0j)\n",
      "GD iter. 65/199: loss=(0.4191471686260437-0j)\n",
      "GD iter. 66/199: loss=(0.4188848137985764-0j)\n",
      "GD iter. 67/199: loss=(0.4186304962601727-0j)\n",
      "GD iter. 68/199: loss=(0.4183838952942567-0j)\n",
      "GD iter. 69/199: loss=(0.4181447060793356-0j)\n",
      "GD iter. 70/199: loss=(0.4179126387394303-0j)\n",
      "GD iter. 71/199: loss=(0.4176874174610127-0j)\n",
      "GD iter. 72/199: loss=(0.41746877967112916-0j)\n",
      "GD iter. 73/199: loss=(0.4172564752718612-0j)\n",
      "GD iter. 74/199: loss=(0.41705026592671024-0j)\n",
      "GD iter. 75/199: loss=(0.41684992439487534-0j)\n",
      "GD iter. 76/199: loss=(0.4166552339097487-0j)\n",
      "GD iter. 77/199: loss=(0.4164659875982634-0j)\n",
      "GD iter. 78/199: loss=(0.41628198793802135-0j)\n",
      "GD iter. 79/199: loss=(0.41610304624938366-0j)\n",
      "GD iter. 80/199: loss=(0.4159289822199437-0j)\n",
      "GD iter. 81/199: loss=(0.415759623459016-0j)\n",
      "GD iter. 82/199: loss=(0.41559480507996904-0j)\n",
      "GD iter. 83/199: loss=(0.41543436930840383-0j)\n",
      "GD iter. 84/199: loss=(0.4152781651143454-0j)\n",
      "GD iter. 85/199: loss=(0.41512604786675705-0j)\n",
      "GD iter. 86/199: loss=(0.41497787900882144-0j)\n",
      "GD iter. 87/199: loss=(0.4148335257525559-0j)\n",
      "GD iter. 88/199: loss=(0.4146928607914397-0j)\n",
      "GD iter. 89/199: loss=(0.4145557620298316-0j)\n",
      "GD iter. 90/199: loss=(0.4144221123280517-0j)\n",
      "GD iter. 91/199: loss=(0.41429179926208465-0j)\n",
      "GD iter. 92/199: loss=(0.4141647148969404-0j)\n",
      "GD iter. 93/199: loss=(0.41404075557278-0j)\n",
      "GD iter. 94/199: loss=(0.4139198217029794-0j)\n",
      "GD iter. 95/199: loss=(0.41380181758336604-0j)\n",
      "GD iter. 96/199: loss=(0.413686651211916-0j)\n",
      "GD iter. 97/199: loss=(0.4135742341182528-0j)\n",
      "GD iter. 98/199: loss=(0.41346448120233475-0j)\n",
      "GD iter. 99/199: loss=(0.41335731058176184-0j)\n",
      "GD iter. 100/199: loss=(0.4132526434471709-0j)\n",
      "GD iter. 101/199: loss=(0.41315040392522834-0j)\n",
      "GD iter. 102/199: loss=(0.41305051894875955-0j)\n",
      "GD iter. 103/199: loss=(0.4129529181335882-0j)\n",
      "GD iter. 104/199: loss=(0.41285753366168676-0j)\n",
      "GD iter. 105/199: loss=(0.41276430017026633-0j)\n",
      "GD iter. 106/199: loss=(0.4126731546464581-0j)\n",
      "GD iter. 107/199: loss=(0.41258403632726254-0j)\n",
      "GD iter. 108/199: loss=(0.4124968866044642-0j)\n",
      "GD iter. 109/199: loss=(0.41241164893422777-0j)\n",
      "GD iter. 110/199: loss=(0.4123282687511098-0j)\n",
      "GD iter. 111/199: loss=(0.41224669338624126-0j)\n",
      "GD iter. 112/199: loss=(0.4121668719894444-0j)\n",
      "GD iter. 113/199: loss=(0.4120887554550693-0j)\n",
      "GD iter. 114/199: loss=(0.4120122963513459-0j)\n",
      "GD iter. 115/199: loss=(0.4119374488530576-0j)\n",
      "GD iter. 116/199: loss=(0.41186416867736064-0j)\n",
      "GD iter. 117/199: loss=(0.41179241302257763-0j)\n",
      "GD iter. 118/199: loss=(0.4117221405098083-0j)\n",
      "GD iter. 119/199: loss=(0.41165331112720815-0j)\n",
      "GD iter. 120/199: loss=(0.4115858861767957-0j)\n",
      "GD iter. 121/199: loss=(0.4115198282236553-0j)\n",
      "GD iter. 122/199: loss=(0.41145510104741406-0j)\n",
      "GD iter. 123/199: loss=(0.41139166959587264-0j)\n",
      "GD iter. 124/199: loss=(0.4113294999406845-0j)\n",
      "GD iter. 125/199: loss=(0.41126855923497574-0j)\n",
      "GD iter. 126/199: loss=(0.41120881567281087-0j)\n",
      "GD iter. 127/199: loss=(0.4111502384504117-0j)\n",
      "GD iter. 128/199: loss=(0.4110927977290415-0j)\n",
      "GD iter. 129/199: loss=(0.4110364645994736-0j)\n",
      "GD iter. 130/199: loss=(0.41098121104796703-0j)\n",
      "GD iter. 131/199: loss=(0.4109270099236749-0j)\n",
      "GD iter. 132/199: loss=(0.4108738349074175-0j)\n",
      "GD iter. 133/199: loss=(0.4108216604817547-0j)\n",
      "GD iter. 134/199: loss=(0.41077046190229494-0j)\n",
      "GD iter. 135/199: loss=(0.4107202151701835-0j)\n",
      "GD iter. 136/199: loss=(0.41067089700571424-0j)\n",
      "GD iter. 137/199: loss=(0.4106224848230115-0j)\n",
      "GD iter. 138/199: loss=(0.41057495670573474-0j)\n",
      "GD iter. 139/199: loss=(0.4105282913837562-0j)\n",
      "GD iter. 140/199: loss=(0.4104824682107693-0j)\n",
      "GD iter. 141/199: loss=(0.41043746714278423-0j)\n",
      "GD iter. 142/199: loss=(0.4103932687174713-0j)\n",
      "GD iter. 143/199: loss=(0.410349854034313-0j)\n",
      "GD iter. 144/199: loss=(0.4103072047355311-0j)\n",
      "GD iter. 145/199: loss=(0.4102653029877506-0j)\n",
      "GD iter. 146/199: loss=(0.4102241314643726-0j)\n",
      "GD iter. 147/199: loss=(0.4101836733286214-0j)\n",
      "GD iter. 148/199: loss=(0.41014391221723867-0j)\n",
      "GD iter. 149/199: loss=(0.41010483222479616-0j)\n",
      "GD iter. 150/199: loss=(0.4100664178885997-0j)\n",
      "GD iter. 151/199: loss=(0.41002865417416107-0j)\n",
      "GD iter. 152/199: loss=(0.4099915264612113-0j)\n",
      "GD iter. 153/199: loss=(0.40995502053023475-0j)\n",
      "GD iter. 154/199: loss=(0.4099191225495013-0j)\n",
      "GD iter. 155/199: loss=(0.4098838190625753-0j)\n",
      "GD iter. 156/199: loss=(0.4098490969762832-0j)\n",
      "GD iter. 157/199: loss=(0.40981494354911885-0j)\n",
      "GD iter. 158/199: loss=(0.4097813463800711-0j)\n",
      "GD iter. 159/199: loss=(0.4097482933978541-0j)\n",
      "GD iter. 160/199: loss=(0.40971577285052646-0j)\n",
      "GD iter. 161/199: loss=(0.40968377329548095-0j)\n",
      "GD iter. 162/199: loss=(0.40965228358979294-0j)\n",
      "GD iter. 163/199: loss=(0.4096212928809109-0j)\n",
      "GD iter. 164/199: loss=(0.4095907905976768-0j)\n",
      "GD iter. 165/199: loss=(0.40956076644166317-0j)\n",
      "GD iter. 166/199: loss=(0.4095312103788145-0j)\n",
      "GD iter. 167/199: loss=(0.4095021126313809-0j)\n",
      "GD iter. 168/199: loss=(0.409473463670133-0j)\n",
      "GD iter. 169/199: loss=(0.40944525420684735-0j)\n",
      "GD iter. 170/199: loss=(0.40941747518705235-0j)\n",
      "GD iter. 171/199: loss=(0.4093901177830229-0j)\n",
      "GD iter. 172/199: loss=(0.4093631733870181-0j)\n",
      "GD iter. 173/199: loss=(0.40933663360474815-0j)\n",
      "GD iter. 174/199: loss=(0.4093104902490661-0j)\n",
      "GD iter. 175/199: loss=(0.4092847353338733-0j)\n",
      "GD iter. 176/199: loss=(0.40925936106823185-0j)\n",
      "GD iter. 177/199: loss=(0.40923435985067586-0j)\n",
      "GD iter. 178/199: loss=(0.4092097242637145-0j)\n",
      "GD iter. 179/199: loss=(0.40918544706851995-0j)\n",
      "GD iter. 180/199: loss=(0.40916152119979254-0j)\n",
      "GD iter. 181/199: loss=(0.4091379397607978-0j)\n",
      "GD iter. 182/199: loss=(0.40911469601856854-0j)\n",
      "GD iter. 183/199: loss=(0.4090917833992652-0j)\n",
      "GD iter. 184/199: loss=(0.40906919548369136-0j)\n",
      "GD iter. 185/199: loss=(0.40904692600295484-0j)\n",
      "GD iter. 186/199: loss=(0.4090249688342732-0j)\n",
      "GD iter. 187/199: loss=(0.40900331799691475-0j)\n",
      "GD iter. 188/199: loss=(0.4089819676482734-0j)\n",
      "GD iter. 189/199: loss=(0.4089609120800707-0j)\n",
      "GD iter. 190/199: loss=(0.4089401457146801-0j)\n",
      "GD iter. 191/199: loss=(0.40891966310157135-0j)\n",
      "GD iter. 192/199: loss=(0.40889945891386764-0j)\n",
      "GD iter. 193/199: loss=(0.4088795279450148-0j)\n",
      "GD iter. 194/199: loss=(0.40885986510555494-0j)\n",
      "GD iter. 195/199: loss=(0.40884046542000446-0j)\n",
      "GD iter. 196/199: loss=(0.40882132402382987-0j)\n",
      "GD iter. 197/199: loss=(0.40880243616051926-0j)\n",
      "GD iter. 198/199: loss=(0.4087837971787461-0j)\n",
      "GD iter. 199/199: loss=(0.4087654025296209-0j)\n",
      "The Accuracy is: 0.8736\n",
      "The F1 score is: 0.3906\n",
      "The precision is: 0.3425\n",
      "The recall is: 0.4545\n",
      "Average accuracy score is:  0.8699526018421677\n",
      "Average f1 score is:  0.39585189461328607\n"
     ]
    }
   ],
   "source": [
    "# logistic regression using pca feature selection #\n",
    "x_pca_t = add_bias(x_pca)\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_pca_t), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(10):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    # x_t, y_t, x_v, y_v = split_data(x_pca_t, y_train_processed, 0.9)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = logistic_regression(y_t, x_t, initial_w, max_iters=200, gamma=0.15)\n",
    "    y_pred = sigmoid(x_v @ w)\n",
    "    y_pred = (y_pred >= 0.75).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/499: loss=1.0386536008252525\n",
      "GD iter. 1/499: loss=1.033470228437925\n",
      "GD iter. 2/499: loss=1.0283024782126042\n",
      "GD iter. 3/499: loss=1.0231502979043203\n",
      "GD iter. 4/499: loss=1.0180136354565095\n",
      "GD iter. 5/499: loss=1.0128924390003058\n",
      "GD iter. 6/499: loss=1.0077866568538296\n",
      "GD iter. 7/499: loss=1.002696237521482\n",
      "GD iter. 8/499: loss=0.9976211296932405\n",
      "GD iter. 9/499: loss=0.9925612822439578\n",
      "GD iter. 10/499: loss=0.987516644232663\n",
      "GD iter. 11/499: loss=0.9824871649018672\n",
      "GD iter. 12/499: loss=0.9774727936768689\n",
      "GD iter. 13/499: loss=0.9724734801650645\n",
      "GD iter. 14/499: loss=0.9674891741552606\n",
      "GD iter. 15/499: loss=0.962519825616989\n",
      "GD iter. 16/499: loss=0.9575653846998238\n",
      "GD iter. 17/499: loss=0.9526258017327024\n",
      "GD iter. 18/499: loss=0.9477010272232479\n",
      "GD iter. 19/499: loss=0.9427910118570956\n",
      "GD iter. 20/499: loss=0.93789570649722\n",
      "GD iter. 21/499: loss=0.9330150621832664\n",
      "GD iter. 22/499: loss=0.9281490301308836\n",
      "GD iter. 23/499: loss=0.9232975617310607\n",
      "GD iter. 24/499: loss=0.9184606085494649\n",
      "GD iter. 25/499: loss=0.9136381223257836\n",
      "GD iter. 26/499: loss=0.9088300549730669\n",
      "GD iter. 27/499: loss=0.9040363585770741\n",
      "GD iter. 28/499: loss=0.8992569853956233\n",
      "GD iter. 29/499: loss=0.8944918878579414\n",
      "GD iter. 30/499: loss=0.889741018564019\n",
      "GD iter. 31/499: loss=0.885004330283966\n",
      "GD iter. 32/499: loss=0.8802817759573704\n",
      "GD iter. 33/499: loss=0.8755733086926605\n",
      "GD iter. 34/499: loss=0.8708788817664672\n",
      "GD iter. 35/499: loss=0.866198448622991\n",
      "GD iter. 36/499: loss=0.8615319628733709\n",
      "GD iter. 37/499: loss=0.8568793782950548\n",
      "GD iter. 38/499: loss=0.8522406488311737\n",
      "GD iter. 39/499: loss=0.8476157285899176\n",
      "GD iter. 40/499: loss=0.8430045718439135\n",
      "GD iter. 41/499: loss=0.8384071330296065\n",
      "GD iter. 42/499: loss=0.8338233667466424\n",
      "GD iter. 43/499: loss=0.8292532277572546\n",
      "GD iter. 44/499: loss=0.8246966709856509\n",
      "GD iter. 45/499: loss=0.8201536515174045\n",
      "GD iter. 46/499: loss=0.8156241245988459\n",
      "GD iter. 47/499: loss=0.8111080456364589\n",
      "GD iter. 48/499: loss=0.8066053701962773\n",
      "GD iter. 49/499: loss=0.8021160540032851\n",
      "GD iter. 50/499: loss=0.7976400529408182\n",
      "GD iter. 51/499: loss=0.7931773230499692\n",
      "GD iter. 52/499: loss=0.7887278205289934\n",
      "GD iter. 53/499: loss=0.7842915017327184\n",
      "GD iter. 54/499: loss=0.779868323171955\n",
      "GD iter. 55/499: loss=0.7754582415129108\n",
      "GD iter. 56/499: loss=0.7710612135766063\n",
      "GD iter. 57/499: loss=0.7666771963382925\n",
      "GD iter. 58/499: loss=0.7623061469268715\n",
      "GD iter. 59/499: loss=0.7579480226243189\n",
      "GD iter. 60/499: loss=0.7536027808651092\n",
      "GD iter. 61/499: loss=0.7492703792356417\n",
      "GD iter. 62/499: loss=0.7449507754736706\n",
      "GD iter. 63/499: loss=0.7406439274677362\n",
      "GD iter. 64/499: loss=0.7363497932565986\n",
      "GD iter. 65/499: loss=0.7320683310286734\n",
      "GD iter. 66/499: loss=0.72779949912147\n",
      "GD iter. 67/499: loss=0.7235432560210313\n",
      "GD iter. 68/499: loss=0.719299560361377\n",
      "GD iter. 69/499: loss=0.7150683709239462\n",
      "GD iter. 70/499: loss=0.7108496466370469\n",
      "GD iter. 71/499: loss=0.7066433465753021\n",
      "GD iter. 72/499: loss=0.7024494299591024\n",
      "GD iter. 73/499: loss=0.6982678561540585\n",
      "GD iter. 74/499: loss=0.6940985846704563\n",
      "GD iter. 75/499: loss=0.689941575162714\n",
      "GD iter. 76/499: loss=0.6857967874288424\n",
      "GD iter. 77/499: loss=0.681664181409905\n",
      "GD iter. 78/499: loss=0.677543717189483\n",
      "GD iter. 79/499: loss=0.6734353549931393\n",
      "GD iter. 80/499: loss=0.6693390551878883\n",
      "GD iter. 81/499: loss=0.665254778281663\n",
      "GD iter. 82/499: loss=0.6611824849227894\n",
      "GD iter. 83/499: loss=0.6571221358994582\n",
      "GD iter. 84/499: loss=0.6530736921392024\n",
      "GD iter. 85/499: loss=0.6490371147083736\n",
      "GD iter. 86/499: loss=0.6450123648116234\n",
      "GD iter. 87/499: loss=0.6409994037913841\n",
      "GD iter. 88/499: loss=0.6369981931273533\n",
      "GD iter. 89/499: loss=0.6330086944359798\n",
      "GD iter. 90/499: loss=0.6290308694699511\n",
      "GD iter. 91/499: loss=0.6250646801176835\n",
      "GD iter. 92/499: loss=0.6211100884028142\n",
      "GD iter. 93/499: loss=0.6171670564836945\n",
      "GD iter. 94/499: loss=0.6132355466528863\n",
      "GD iter. 95/499: loss=0.6093155213366589\n",
      "GD iter. 96/499: loss=0.6054069430944895\n",
      "GD iter. 97/499: loss=0.6015097746185644\n",
      "GD iter. 98/499: loss=0.5976239787332825\n",
      "GD iter. 99/499: loss=0.5937495183947609\n",
      "GD iter. 100/499: loss=0.5898863566903418\n",
      "GD iter. 101/499: loss=0.5860344568381023\n",
      "GD iter. 102/499: loss=0.5821937821863646\n",
      "GD iter. 103/499: loss=0.578364296213211\n",
      "GD iter. 104/499: loss=0.5745459625259958\n",
      "GD iter. 105/499: loss=0.5707387448608644\n",
      "GD iter. 106/499: loss=0.5669426070822716\n",
      "GD iter. 107/499: loss=0.5631575131825012\n",
      "GD iter. 108/499: loss=0.5593834272811886\n",
      "GD iter. 109/499: loss=0.5556203136248458\n",
      "GD iter. 110/499: loss=0.5518681365863864\n",
      "GD iter. 111/499: loss=0.548126860664654\n",
      "GD iter. 112/499: loss=0.5444167063530524\n",
      "GD iter. 113/499: loss=0.5410184947656022\n",
      "GD iter. 114/499: loss=0.5385103654337839\n",
      "GD iter. 115/499: loss=0.5371341752074952\n",
      "GD iter. 116/499: loss=0.5364337868178518\n",
      "GD iter. 117/499: loss=0.5360493620598445\n",
      "GD iter. 118/499: loss=0.5358179754005098\n",
      "GD iter. 119/499: loss=0.5356798318261278\n",
      "GD iter. 120/499: loss=0.5355811630456501\n",
      "GD iter. 121/499: loss=0.5355011471840755\n",
      "GD iter. 122/499: loss=0.5354347766586762\n",
      "GD iter. 123/499: loss=0.5353794606659577\n",
      "GD iter. 124/499: loss=0.5353313286793887\n",
      "GD iter. 125/499: loss=0.5352851532761054\n",
      "GD iter. 126/499: loss=0.5352413127264195\n",
      "GD iter. 127/499: loss=0.5351976409551952\n",
      "GD iter. 128/499: loss=0.5351541373008106\n",
      "GD iter. 129/499: loss=0.5351108011042618\n",
      "GD iter. 130/499: loss=0.5350676788169113\n",
      "GD iter. 131/499: loss=0.5350260436435501\n",
      "GD iter. 132/499: loss=0.534986979411525\n",
      "GD iter. 133/499: loss=0.5349473470656626\n",
      "GD iter. 134/499: loss=0.5349066794864621\n",
      "GD iter. 135/499: loss=0.534867420060331\n",
      "GD iter. 136/499: loss=0.5348259002294644\n",
      "GD iter. 137/499: loss=0.5347857915272299\n",
      "GD iter. 138/499: loss=0.5347447502735372\n",
      "GD iter. 139/499: loss=0.534703868051707\n",
      "GD iter. 140/499: loss=0.5346631455834301\n",
      "GD iter. 141/499: loss=0.5346237884567391\n",
      "GD iter. 142/499: loss=0.534583377152759\n",
      "GD iter. 143/499: loss=0.5345431223981914\n",
      "GD iter. 144/499: loss=0.5345030372769557\n",
      "GD iter. 145/499: loss=0.5344642903174328\n",
      "GD iter. 146/499: loss=0.5344244990923593\n",
      "GD iter. 147/499: loss=0.5343848741154608\n",
      "GD iter. 148/499: loss=0.5343466092676716\n",
      "GD iter. 149/499: loss=0.5343062834266415\n",
      "GD iter. 150/499: loss=0.5342683149521706\n",
      "GD iter. 151/499: loss=0.5342283009830446\n",
      "GD iter. 152/499: loss=0.5341906280879166\n",
      "GD iter. 153/499: loss=0.5341521239314618\n",
      "GD iter. 154/499: loss=0.5341136995184594\n",
      "GD iter. 155/499: loss=0.53407664538236\n",
      "GD iter. 156/499: loss=0.534038807394341\n",
      "GD iter. 157/499: loss=0.5340011528511699\n",
      "GD iter. 158/499: loss=0.5339652981079107\n",
      "GD iter. 159/499: loss=0.5339295858630331\n",
      "GD iter. 160/499: loss=0.5338940264294783\n",
      "GD iter. 161/499: loss=0.5338575197045287\n",
      "GD iter. 162/499: loss=0.5338228751849585\n",
      "GD iter. 163/499: loss=0.5337866613226114\n",
      "GD iter. 164/499: loss=0.5337522865048029\n",
      "GD iter. 165/499: loss=0.5337163632229163\n",
      "GD iter. 166/499: loss=0.5336822559217212\n",
      "GD iter. 167/499: loss=0.5336466309616915\n",
      "GD iter. 168/499: loss=0.5336135983641577\n",
      "GD iter. 169/499: loss=0.5335782300141941\n",
      "GD iter. 170/499: loss=0.5335454867857456\n",
      "GD iter. 171/499: loss=0.5335103729350265\n",
      "GD iter. 172/499: loss=0.5334779570714325\n",
      "GD iter. 173/499: loss=0.5334417521933439\n",
      "GD iter. 174/499: loss=0.5334082283419614\n",
      "GD iter. 175/499: loss=0.5333748378234826\n",
      "GD iter. 176/499: loss=0.5333415809439712\n",
      "GD iter. 177/499: loss=0.5333067726141841\n",
      "GD iter. 178/499: loss=0.5332737822648729\n",
      "GD iter. 179/499: loss=0.5332409231283437\n",
      "GD iter. 180/499: loss=0.5332081946815036\n",
      "GD iter. 181/499: loss=0.533175596403347\n",
      "GD iter. 182/499: loss=0.5331431483873423\n",
      "GD iter. 183/499: loss=0.5331077556806161\n",
      "GD iter. 184/499: loss=0.533075552329928\n",
      "GD iter. 185/499: loss=0.5330421234536887\n",
      "GD iter. 186/499: loss=0.5330102108278358\n",
      "GD iter. 187/499: loss=0.5329770278015032\n",
      "GD iter. 188/499: loss=0.5329439740327195\n",
      "GD iter. 189/499: loss=0.5329110671700145\n",
      "GD iter. 190/499: loss=0.5328807862943782\n",
      "GD iter. 191/499: loss=0.5328481217705727\n",
      "GD iter. 192/499: loss=0.5328166980482993\n",
      "GD iter. 193/499: loss=0.5327853987921539\n",
      "GD iter. 194/499: loss=0.5327542235069762\n",
      "GD iter. 195/499: loss=0.5327231716995798\n",
      "GD iter. 196/499: loss=0.5326922439140676\n",
      "GD iter. 197/499: loss=0.5326603419804844\n",
      "GD iter. 198/499: loss=0.5326296598539453\n",
      "GD iter. 199/499: loss=0.5325990992476624\n",
      "GD iter. 200/499: loss=0.5325686613905877\n",
      "GD iter. 201/499: loss=0.5325377529342302\n",
      "GD iter. 202/499: loss=0.5325069728777035\n",
      "GD iter. 203/499: loss=0.5324775171480104\n",
      "GD iter. 204/499: loss=0.5324469801055619\n",
      "GD iter. 205/499: loss=0.5324177918710545\n",
      "GD iter. 206/499: loss=0.5323865353555628\n",
      "GD iter. 207/499: loss=0.5323566083592962\n",
      "GD iter. 208/499: loss=0.5323267991710755\n",
      "GD iter. 209/499: loss=0.5322971073237005\n",
      "GD iter. 210/499: loss=0.5322675323518304\n",
      "GD iter. 211/499: loss=0.5322380846588258\n",
      "GD iter. 212/499: loss=0.5322097688871708\n",
      "GD iter. 213/499: loss=0.5321800002524707\n",
      "GD iter. 214/499: loss=0.5321519394403702\n",
      "GD iter. 215/499: loss=0.532122386261692\n",
      "GD iter. 216/499: loss=0.5320929584247385\n",
      "GD iter. 217/499: loss=0.5320652427846184\n",
      "GD iter. 218/499: loss=0.5320360305514069\n",
      "GD iter. 219/499: loss=0.5320069394588455\n",
      "GD iter. 220/499: loss=0.5319795649192008\n",
      "GD iter. 221/499: loss=0.5319506895932703\n",
      "GD iter. 222/499: loss=0.5319219312400497\n",
      "GD iter. 223/499: loss=0.5318948937776916\n",
      "GD iter. 224/499: loss=0.5318663513688766\n",
      "GD iter. 225/499: loss=0.5318379302607664\n",
      "GD iter. 226/499: loss=0.531809444143194\n",
      "GD iter. 227/499: loss=0.5317828859266414\n",
      "GD iter. 228/499: loss=0.5317530215074525\n",
      "GD iter. 229/499: loss=0.5317277557175677\n",
      "GD iter. 230/499: loss=0.5316980976497409\n",
      "GD iter. 231/499: loss=0.5316741064722469\n",
      "GD iter. 232/499: loss=0.5316445793622631\n",
      "GD iter. 233/499: loss=0.5316179496146303\n",
      "GD iter. 234/499: loss=0.531589859027971\n",
      "GD iter. 235/499: loss=0.531566306567163\n",
      "GD iter. 236/499: loss=0.5315371999498193\n",
      "GD iter. 237/499: loss=0.5315104043067629\n",
      "GD iter. 238/499: loss=0.5314853350729177\n",
      "GD iter. 239/499: loss=0.5314566572829352\n",
      "GD iter. 240/499: loss=0.5314345335280172\n",
      "GD iter. 241/499: loss=0.5314059433288614\n",
      "GD iter. 242/499: loss=0.5313794921582649\n",
      "GD iter. 243/499: loss=0.5313551617405863\n",
      "GD iter. 244/499: loss=0.5313279065569544\n",
      "GD iter. 245/499: loss=0.5313054806520975\n",
      "GD iter. 246/499: loss=0.5312763699347759\n",
      "GD iter. 247/499: loss=0.5312531843351769\n",
      "GD iter. 248/499: loss=0.5312254460537309\n",
      "GD iter. 249/499: loss=0.5312060134748597\n",
      "GD iter. 250/499: loss=0.5311756454852549\n",
      "GD iter. 251/499: loss=0.5311542015114626\n",
      "GD iter. 252/499: loss=0.5311257236563562\n",
      "GD iter. 253/499: loss=0.5311044087878747\n",
      "GD iter. 254/499: loss=0.5310774680704944\n",
      "GD iter. 255/499: loss=0.5310516781041943\n",
      "GD iter. 256/499: loss=0.5310343149227715\n",
      "GD iter. 257/499: loss=0.5310033962593904\n",
      "GD iter. 258/499: loss=0.5309803104913093\n",
      "GD iter. 259/499: loss=0.5309563353777736\n",
      "GD iter. 260/499: loss=0.5309324604871453\n",
      "GD iter. 261/499: loss=0.5309071837165068\n",
      "GD iter. 262/499: loss=0.5308844854086973\n",
      "GD iter. 263/499: loss=0.5308608884598184\n",
      "GD iter. 264/499: loss=0.530835906595855\n",
      "GD iter. 265/499: loss=0.5308135883012651\n",
      "GD iter. 266/499: loss=0.5307902849981961\n",
      "GD iter. 267/499: loss=0.5307642564024584\n",
      "GD iter. 268/499: loss=0.5307478616977407\n",
      "GD iter. 269/499: loss=0.5307186539139779\n",
      "GD iter. 270/499: loss=0.5307014915270082\n",
      "GD iter. 271/499: loss=0.5306724591113444\n",
      "GD iter. 272/499: loss=0.530651788971538\n",
      "GD iter. 273/499: loss=0.5306276055925695\n",
      "GD iter. 274/499: loss=0.5306050193866819\n",
      "GD iter. 275/499: loss=0.5305825224835591\n",
      "GD iter. 276/499: loss=0.5305601298658983\n",
      "GD iter. 277/499: loss=0.5305356593121054\n",
      "GD iter. 278/499: loss=0.5305191207757399\n",
      "GD iter. 279/499: loss=0.5304908863553754\n",
      "GD iter. 280/499: loss=0.5304745461722217\n",
      "GD iter. 281/499: loss=0.5304464687943333\n",
      "GD iter. 282/499: loss=0.5304294532679698\n",
      "GD iter. 283/499: loss=0.5304028042933926\n",
      "GD iter. 284/499: loss=0.5303814076932812\n",
      "GD iter. 285/499: loss=0.5303600986598114\n",
      "GD iter. 286/499: loss=0.5303377878310517\n",
      "GD iter. 287/499: loss=0.5303166482566797\n",
      "GD iter. 288/499: loss=0.5302945493204841\n",
      "GD iter. 289/499: loss=0.5302786373946149\n",
      "GD iter. 290/499: loss=0.530250553249012\n",
      "GD iter. 291/499: loss=0.53023863022746\n",
      "GD iter. 292/499: loss=0.5302094350974402\n",
      "GD iter. 293/499: loss=0.5301917385648498\n",
      "GD iter. 294/499: loss=0.5301681221444707\n",
      "GD iter. 295/499: loss=0.5301495119887455\n",
      "GD iter. 296/499: loss=0.5301261218895635\n",
      "GD iter. 297/499: loss=0.5301105914338592\n",
      "GD iter. 298/499: loss=0.530081028274936\n",
      "GD iter. 299/499: loss=0.5300721567517005\n",
      "GD iter. 300/499: loss=0.5300411736683669\n",
      "GD iter. 301/499: loss=0.5300308272767976\n",
      "GD iter. 302/499: loss=0.5300012850633229\n",
      "GD iter. 303/499: loss=0.5299915098329214\n",
      "GD iter. 304/499: loss=0.5299617144865058\n",
      "GD iter. 305/499: loss=0.5299501449074558\n",
      "GD iter. 306/499: loss=0.5299208833490769\n",
      "GD iter. 307/499: loss=0.5299128816439446\n",
      "GD iter. 308/499: loss=0.5298830862716439\n",
      "GD iter. 309/499: loss=0.5298657040128568\n",
      "GD iter. 310/499: loss=0.5298429786375636\n",
      "GD iter. 311/499: loss=0.5298316838071029\n",
      "GD iter. 312/499: loss=0.5298041516529705\n",
      "GD iter. 313/499: loss=0.5297961912234029\n",
      "GD iter. 314/499: loss=0.5297667398830606\n",
      "GD iter. 315/499: loss=0.5297488454173374\n",
      "GD iter. 316/499: loss=0.5297262228925048\n",
      "GD iter. 317/499: loss=0.529721919554589\n",
      "GD iter. 318/499: loss=0.529692531171408\n",
      "GD iter. 319/499: loss=0.5296692768533798\n",
      "GD iter. 320/499: loss=0.5296641034524154\n",
      "GD iter. 321/499: loss=0.5296351450835803\n",
      "GD iter. 322/499: loss=0.5296170530632562\n",
      "GD iter. 323/499: loss=0.5295969733766093\n",
      "GD iter. 324/499: loss=0.529582814885455\n",
      "GD iter. 325/499: loss=0.5295582862533514\n",
      "GD iter. 326/499: loss=0.5295524748180797\n",
      "GD iter. 327/499: loss=0.5295248024817119\n",
      "GD iter. 328/499: loss=0.5295072415227831\n",
      "GD iter. 329/499: loss=0.5294885573600935\n",
      "GD iter. 330/499: loss=0.5294711489880607\n",
      "GD iter. 331/499: loss=0.5294512638955244\n",
      "GD iter. 332/499: loss=0.5294395746193435\n",
      "GD iter. 333/499: loss=0.5294136927732547\n",
      "GD iter. 334/499: loss=0.5294095776552517\n",
      "GD iter. 335/499: loss=0.5293806973365875\n",
      "GD iter. 336/499: loss=0.5293666639876716\n",
      "GD iter. 337/499: loss=0.5293444507255385\n",
      "GD iter. 338/499: loss=0.5293391751671397\n",
      "GD iter. 339/499: loss=0.5293106408048626\n",
      "GD iter. 340/499: loss=0.5292981834217345\n",
      "GD iter. 341/499: loss=0.5292749741353712\n",
      "GD iter. 342/499: loss=0.529269902913854\n",
      "GD iter. 343/499: loss=0.5292405506610521\n",
      "GD iter. 344/499: loss=0.5292355390780005\n",
      "GD iter. 345/499: loss=0.5292063978360696\n",
      "GD iter. 346/499: loss=0.529201449422153\n",
      "GD iter. 347/499: loss=0.529171936542285\n",
      "GD iter. 348/499: loss=0.5291698918480057\n",
      "GD iter. 349/499: loss=0.5291397724254904\n",
      "GD iter. 350/499: loss=0.5291294085320841\n",
      "GD iter. 351/499: loss=0.5291076119219376\n",
      "GD iter. 352/499: loss=0.5290940154556304\n",
      "GD iter. 353/499: loss=0.52907471351364\n",
      "GD iter. 354/499: loss=0.529063153039795\n",
      "GD iter. 355/499: loss=0.5290428183515359\n",
      "GD iter. 356/499: loss=0.5290283875699269\n",
      "GD iter. 357/499: loss=0.5290105267424331\n",
      "GD iter. 358/499: loss=0.5289981711097377\n",
      "GD iter. 359/499: loss=0.5289765019326796\n",
      "GD iter. 360/499: loss=0.5289716119882809\n",
      "GD iter. 361/499: loss=0.528942017794303\n",
      "GD iter. 362/499: loss=0.5289398902322433\n",
      "GD iter. 363/499: loss=0.5289111891808116\n",
      "GD iter. 364/499: loss=0.528907691310099\n",
      "GD iter. 365/499: loss=0.5288789488641085\n",
      "GD iter. 366/499: loss=0.5288771803465347\n",
      "GD iter. 367/499: loss=0.5288467009607027\n",
      "GD iter. 368/499: loss=0.5288456466435937\n",
      "GD iter. 369/499: loss=0.5288159957522774\n",
      "GD iter. 370/499: loss=0.5288152843239409\n",
      "GD iter. 371/499: loss=0.528783769553682\n",
      "GD iter. 372/499: loss=0.528784661345262\n",
      "GD iter. 373/499: loss=0.5287551027474549\n",
      "GD iter. 374/499: loss=0.5287554723202504\n",
      "GD iter. 375/499: loss=0.5287236837257885\n",
      "GD iter. 376/499: loss=0.5287253110642999\n",
      "GD iter. 377/499: loss=0.5286945046037084\n",
      "GD iter. 378/499: loss=0.5286946234909408\n",
      "GD iter. 379/499: loss=0.528667077246536\n",
      "GD iter. 380/499: loss=0.5286632142990421\n",
      "GD iter. 381/499: loss=0.5286377163647779\n",
      "GD iter. 382/499: loss=0.528634030463889\n",
      "GD iter. 383/499: loss=0.5286074047234524\n",
      "GD iter. 384/499: loss=0.5286067650584155\n",
      "GD iter. 385/499: loss=0.5285773156402077\n",
      "GD iter. 386/499: loss=0.5285779223854482\n",
      "GD iter. 387/499: loss=0.5285485805312037\n",
      "GD iter. 388/499: loss=0.5285482766866657\n",
      "GD iter. 389/499: loss=0.5285205674831279\n",
      "GD iter. 390/499: loss=0.5285197718876875\n",
      "GD iter. 391/499: loss=0.5284916534344989\n",
      "GD iter. 392/499: loss=0.5284925436478356\n",
      "GD iter. 393/499: loss=0.5284630438154423\n",
      "GD iter. 394/499: loss=0.5284635749710069\n",
      "GD iter. 395/499: loss=0.5284367887267825\n",
      "GD iter. 396/499: loss=0.5284356120753602\n",
      "GD iter. 397/499: loss=0.528409922303681\n",
      "GD iter. 398/499: loss=0.5284066507947462\n",
      "GD iter. 399/499: loss=0.5283838287576013\n",
      "GD iter. 400/499: loss=0.5283781816935262\n",
      "GD iter. 401/499: loss=0.528357564792181\n",
      "GD iter. 402/499: loss=0.5283517044501332\n",
      "GD iter. 403/499: loss=0.5283290204157254\n",
      "GD iter. 404/499: loss=0.5283258844034807\n",
      "GD iter. 405/499: loss=0.5282988871173756\n",
      "GD iter. 406/499: loss=0.5282997920182894\n",
      "GD iter. 407/499: loss=0.5282726995646646\n",
      "GD iter. 408/499: loss=0.5282740440927323\n",
      "GD iter. 409/499: loss=0.5282458166470718\n",
      "GD iter. 410/499: loss=0.5282468216488913\n",
      "GD iter. 411/499: loss=0.5282219770059361\n",
      "GD iter. 412/499: loss=0.5282218687636048\n",
      "GD iter. 413/499: loss=0.5281946954504511\n",
      "GD iter. 414/499: loss=0.5281956964431386\n",
      "GD iter. 415/499: loss=0.5281678832222915\n",
      "GD iter. 416/499: loss=0.5281715918477228\n",
      "GD iter. 417/499: loss=0.5281431391745923\n",
      "GD iter. 418/499: loss=0.5281463596436062\n",
      "GD iter. 419/499: loss=0.5281179294312075\n",
      "GD iter. 420/499: loss=0.5281202632401334\n",
      "GD iter. 421/499: loss=0.5280923915405594\n",
      "GD iter. 422/499: loss=0.5280954318163321\n",
      "GD iter. 423/499: loss=0.5280675792190768\n",
      "GD iter. 424/499: loss=0.5280719026053364\n",
      "GD iter. 425/499: loss=0.5280425749197794\n",
      "GD iter. 426/499: loss=0.5280466738111527\n",
      "GD iter. 427/499: loss=0.5280187738402685\n",
      "GD iter. 428/499: loss=0.528019823080485\n",
      "GD iter. 429/499: loss=0.5279994679849681\n",
      "GD iter. 430/499: loss=0.5279952450318111\n",
      "GD iter. 431/499: loss=0.5279739119827404\n",
      "GD iter. 432/499: loss=0.527970549050383\n",
      "GD iter. 433/499: loss=0.5279509455385534\n",
      "GD iter. 434/499: loss=0.5279468634626483\n",
      "GD iter. 435/499: loss=0.5279235043688398\n",
      "GD iter. 436/499: loss=0.5279231422598606\n",
      "GD iter. 437/499: loss=0.5279012961533792\n",
      "GD iter. 438/499: loss=0.5278999787985067\n",
      "GD iter. 439/499: loss=0.5278767045148207\n",
      "GD iter. 440/499: loss=0.5278767563479899\n",
      "GD iter. 441/499: loss=0.5278525750123392\n",
      "GD iter. 442/499: loss=0.5278611221879909\n",
      "GD iter. 443/499: loss=0.5278263068767942\n",
      "GD iter. 444/499: loss=0.5278365415196594\n",
      "GD iter. 445/499: loss=0.5278045449816113\n",
      "GD iter. 446/499: loss=0.5278151960828145\n",
      "GD iter. 447/499: loss=0.5277812094867684\n",
      "GD iter. 448/499: loss=0.5277924382497052\n",
      "GD iter. 449/499: loss=0.5277598659804567\n",
      "GD iter. 450/499: loss=0.527766469705607\n",
      "GD iter. 451/499: loss=0.5277377180084462\n",
      "GD iter. 452/499: loss=0.5277446097431397\n",
      "GD iter. 453/499: loss=0.5277149304243228\n",
      "GD iter. 454/499: loss=0.5277260055924357\n",
      "GD iter. 455/499: loss=0.5276923712025287\n",
      "GD iter. 456/499: loss=0.5277053029365167\n",
      "GD iter. 457/499: loss=0.5276698844534071\n",
      "GD iter. 458/499: loss=0.5276847940711651\n",
      "GD iter. 459/499: loss=0.5276486317717455\n",
      "GD iter. 460/499: loss=0.5276626391440097\n",
      "GD iter. 461/499: loss=0.5276281674606784\n",
      "GD iter. 462/499: loss=0.5276355002111741\n",
      "GD iter. 463/499: loss=0.527609396227346\n",
      "GD iter. 464/499: loss=0.527609008828395\n",
      "GD iter. 465/499: loss=0.5275933411054956\n",
      "GD iter. 466/499: loss=0.5275935723327346\n",
      "GD iter. 467/499: loss=0.5275648603910812\n",
      "GD iter. 468/499: loss=0.5275753242594498\n",
      "GD iter. 469/499: loss=0.527543659369409\n",
      "GD iter. 470/499: loss=0.5275605953542838\n",
      "GD iter. 471/499: loss=0.5275229380390248\n",
      "GD iter. 472/499: loss=0.5275411407571706\n",
      "GD iter. 473/499: loss=0.5275023763942838\n",
      "GD iter. 474/499: loss=0.5275202918596203\n",
      "GD iter. 475/499: loss=0.527481991927374\n",
      "GD iter. 476/499: loss=0.5275028862348119\n",
      "GD iter. 477/499: loss=0.5274628018241588\n",
      "GD iter. 478/499: loss=0.5274742528032041\n",
      "GD iter. 479/499: loss=0.5274426545270157\n",
      "GD iter. 480/499: loss=0.5274555370439526\n",
      "GD iter. 481/499: loss=0.5274227261867366\n",
      "GD iter. 482/499: loss=0.5274353714754966\n",
      "GD iter. 483/499: loss=0.5274044250044294\n",
      "GD iter. 484/499: loss=0.527410520047934\n",
      "GD iter. 485/499: loss=0.5273901927008232\n",
      "GD iter. 486/499: loss=0.5273893719002695\n",
      "GD iter. 487/499: loss=0.5273707662896907\n",
      "GD iter. 488/499: loss=0.5273700278677347\n",
      "GD iter. 489/499: loss=0.5273514974322322\n",
      "GD iter. 490/499: loss=0.5273530873663045\n",
      "GD iter. 491/499: loss=0.5273306909971359\n",
      "GD iter. 492/499: loss=0.5273333390860303\n",
      "GD iter. 493/499: loss=0.5273113286366958\n",
      "GD iter. 494/499: loss=0.5273137568727223\n",
      "GD iter. 495/499: loss=0.527293013566528\n",
      "GD iter. 496/499: loss=0.5272958416298438\n",
      "GD iter. 497/499: loss=0.5272718964322319\n",
      "GD iter. 498/499: loss=0.527289315854256\n",
      "GD iter. 499/499: loss=0.5272517492073447\n",
      "The Accuracy is: 0.8982\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zewzhang/Course/ML/Project/ML_proj/project1/implementations.py:587: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/499: loss=1.0231306769963595\n",
      "GD iter. 1/499: loss=1.017992292847826\n",
      "GD iter. 2/499: loss=1.0128693795736143\n",
      "GD iter. 3/499: loss=1.0077618854763268\n",
      "GD iter. 4/499: loss=1.002669759044903\n",
      "GD iter. 5/499: loss=0.99759294895391\n",
      "GD iter. 6/499: loss=0.9925314040628456\n",
      "GD iter. 7/499: loss=0.9874850734154373\n",
      "GD iter. 8/499: loss=0.9824539062389468\n",
      "GD iter. 9/499: loss=0.9774378519434769\n",
      "GD iter. 10/499: loss=0.9724368601212814\n",
      "GD iter. 11/499: loss=0.967450880546076\n",
      "GD iter. 12/499: loss=0.9624798631723547\n",
      "GD iter. 13/499: loss=0.9575237581347061\n",
      "GD iter. 14/499: loss=0.952582515747134\n",
      "GD iter. 15/499: loss=0.9476560865023801\n",
      "GD iter. 16/499: loss=0.9427444210712489\n",
      "GD iter. 17/499: loss=0.9378474703019366\n",
      "GD iter. 18/499: loss=0.9329651852193612\n",
      "GD iter. 19/499: loss=0.9280975170244955\n",
      "GD iter. 20/499: loss=0.9232444170937031\n",
      "GD iter. 21/499: loss=0.9184058369780769\n",
      "GD iter. 22/499: loss=0.9135817284027796\n",
      "GD iter. 23/499: loss=0.908772043266388\n",
      "GD iter. 24/499: loss=0.9039767336402382\n",
      "GD iter. 25/499: loss=0.899195751767774\n",
      "GD iter. 26/499: loss=0.8944290500638991\n",
      "GD iter. 27/499: loss=0.8896765811143296\n",
      "GD iter. 28/499: loss=0.8849382976749508\n",
      "GD iter. 29/499: loss=0.8802141526711756\n",
      "GD iter. 30/499: loss=0.8755040991973051\n",
      "GD iter. 31/499: loss=0.870808090515893\n",
      "GD iter. 32/499: loss=0.866126080057111\n",
      "GD iter. 33/499: loss=0.8614580214181179\n",
      "GD iter. 34/499: loss=0.8568038683624293\n",
      "GD iter. 35/499: loss=0.8521635748192927\n",
      "GD iter. 36/499: loss=0.8475370948830622\n",
      "GD iter. 37/499: loss=0.8429243828125764\n",
      "GD iter. 38/499: loss=0.83832539303054\n",
      "GD iter. 39/499: loss=0.8337400801229068\n",
      "GD iter. 40/499: loss=0.8291683988382635\n",
      "GD iter. 41/499: loss=0.8246103040872199\n",
      "GD iter. 42/499: loss=0.8200657509417975\n",
      "GD iter. 43/499: loss=0.8155346946348216\n",
      "GD iter. 44/499: loss=0.8110170905593183\n",
      "GD iter. 45/499: loss=0.8065128942679092\n",
      "GD iter. 46/499: loss=0.8020220614722129\n",
      "GD iter. 47/499: loss=0.7975445480422458\n",
      "GD iter. 48/499: loss=0.7930803100058267\n",
      "GD iter. 49/499: loss=0.7886293035479833\n",
      "GD iter. 50/499: loss=0.7841914850103607\n",
      "GD iter. 51/499: loss=0.779766810890633\n",
      "GD iter. 52/499: loss=0.7753552378419161\n",
      "GD iter. 53/499: loss=0.7709567226721838\n",
      "GD iter. 54/499: loss=0.7665712223436856\n",
      "GD iter. 55/499: loss=0.7621986939723667\n",
      "GD iter. 56/499: loss=0.7578390948272905\n",
      "GD iter. 57/499: loss=0.7534923823300632\n",
      "GD iter. 58/499: loss=0.7491585140542605\n",
      "GD iter. 59/499: loss=0.7448374477248572\n",
      "GD iter. 60/499: loss=0.7405291412176579\n",
      "GD iter. 61/499: loss=0.7362335525587306\n",
      "GD iter. 62/499: loss=0.7319506399238429\n",
      "GD iter. 63/499: loss=0.7276803616378992\n",
      "GD iter. 64/499: loss=0.7234226761743809\n",
      "GD iter. 65/499: loss=0.7191775421547889\n",
      "GD iter. 66/499: loss=0.7149449183480876\n",
      "GD iter. 67/499: loss=0.7107247636701516\n",
      "GD iter. 68/499: loss=0.7065170371832141\n",
      "GD iter. 69/499: loss=0.7023216980953182\n",
      "GD iter. 70/499: loss=0.6981387057597696\n",
      "GD iter. 71/499: loss=0.6939680196745909\n",
      "GD iter. 72/499: loss=0.6898095994819796\n",
      "GD iter. 73/499: loss=0.6856634049677672\n",
      "GD iter. 74/499: loss=0.6815293960608801\n",
      "GD iter. 75/499: loss=0.6774075328328034\n",
      "GD iter. 76/499: loss=0.6732977754970462\n",
      "GD iter. 77/499: loss=0.6692000844086089\n",
      "GD iter. 78/499: loss=0.6651144200634533\n",
      "GD iter. 79/499: loss=0.6610407430979739\n",
      "GD iter. 80/499: loss=0.6569790142884717\n",
      "GD iter. 81/499: loss=0.6529291945506295\n",
      "GD iter. 82/499: loss=0.6488912449389903\n",
      "GD iter. 83/499: loss=0.644865126646437\n",
      "GD iter. 84/499: loss=0.6408508010036736\n",
      "GD iter. 85/499: loss=0.6368482294787091\n",
      "GD iter. 86/499: loss=0.6328573736763439\n",
      "GD iter. 87/499: loss=0.6288781953376569\n",
      "GD iter. 88/499: loss=0.6249106563394955\n",
      "GD iter. 89/499: loss=0.620954718693967\n",
      "GD iter. 90/499: loss=0.6170103445479328\n",
      "GD iter. 91/499: loss=0.6130774961825037\n",
      "GD iter. 92/499: loss=0.6091561360125374\n",
      "GD iter. 93/499: loss=0.6052462265861378\n",
      "GD iter. 94/499: loss=0.6013477305841569\n",
      "GD iter. 95/499: loss=0.5974606108196985\n",
      "GD iter. 96/499: loss=0.5935848302376221\n",
      "GD iter. 97/499: loss=0.5897203519140513\n",
      "GD iter. 98/499: loss=0.5858671390558825\n",
      "GD iter. 99/499: loss=0.5820251550002962\n",
      "GD iter. 100/499: loss=0.5781943632142688\n",
      "GD iter. 101/499: loss=0.574374727294089\n",
      "GD iter. 102/499: loss=0.5705662109648729\n",
      "GD iter. 103/499: loss=0.5667687780800834\n",
      "GD iter. 104/499: loss=0.5629823926210505\n",
      "GD iter. 105/499: loss=0.5592070186964926\n",
      "GD iter. 106/499: loss=0.5554426205420417\n",
      "GD iter. 107/499: loss=0.5516891625197687\n",
      "GD iter. 108/499: loss=0.5479503401640117\n",
      "GD iter. 109/499: loss=0.5442526474185844\n",
      "GD iter. 110/499: loss=0.5408185409905428\n",
      "GD iter. 111/499: loss=0.5381534624073329\n",
      "GD iter. 112/499: loss=0.5367199200733865\n",
      "GD iter. 113/499: loss=0.5360720919389768\n",
      "GD iter. 114/499: loss=0.535753904125745\n",
      "GD iter. 115/499: loss=0.5355633522561721\n",
      "GD iter. 116/499: loss=0.5354423936227214\n",
      "GD iter. 117/499: loss=0.5353514629654715\n",
      "GD iter. 118/499: loss=0.5352789636949017\n",
      "GD iter. 119/499: loss=0.5352177122353357\n",
      "GD iter. 120/499: loss=0.5351655168551507\n",
      "GD iter. 121/499: loss=0.5351180741866328\n",
      "GD iter. 122/499: loss=0.5350758610482331\n",
      "GD iter. 123/499: loss=0.5350338147715779\n",
      "GD iter. 124/499: loss=0.5349932818704708\n",
      "GD iter. 125/499: loss=0.5349529086831342\n",
      "GD iter. 126/499: loss=0.5349126945758638\n",
      "GD iter. 127/499: loss=0.5348726389174777\n",
      "GD iter. 128/499: loss=0.5348327410793077\n",
      "GD iter. 129/499: loss=0.5347930004351884\n",
      "GD iter. 130/499: loss=0.5347534274807099\n",
      "GD iter. 131/499: loss=0.5347150008740404\n",
      "GD iter. 132/499: loss=0.5346757407641995\n",
      "GD iter. 133/499: loss=0.5346376218348465\n",
      "GD iter. 134/499: loss=0.5345996720295588\n",
      "GD iter. 135/499: loss=0.5345601664188647\n",
      "GD iter. 136/499: loss=0.5345225095066676\n",
      "GD iter. 137/499: loss=0.534484310006368\n",
      "GD iter. 138/499: loss=0.5344462615509693\n",
      "GD iter. 139/499: loss=0.5344083640760703\n",
      "GD iter. 140/499: loss=0.5343713152831664\n",
      "GD iter. 141/499: loss=0.5343337149683831\n",
      "GD iter. 142/499: loss=0.5342962698169519\n",
      "GD iter. 143/499: loss=0.5342580751887887\n",
      "GD iter. 144/499: loss=0.5342216253624457\n",
      "GD iter. 145/499: loss=0.5341837350650982\n",
      "GD iter. 146/499: loss=0.5341484894641432\n",
      "GD iter. 147/499: loss=0.5341108681551787\n",
      "GD iter. 148/499: loss=0.5340743102520915\n",
      "GD iter. 149/499: loss=0.5340386087545672\n",
      "GD iter. 150/499: loss=0.534001430763862\n",
      "GD iter. 151/499: loss=0.5339660142906224\n",
      "GD iter. 152/499: loss=0.5339285455617041\n",
      "GD iter. 153/499: loss=0.5338947346486115\n",
      "GD iter. 154/499: loss=0.5338575269808766\n",
      "GD iter. 155/499: loss=0.5338220869256145\n",
      "GD iter. 156/499: loss=0.5337881520793072\n",
      "GD iter. 157/499: loss=0.533748694177067\n",
      "GD iter. 158/499: loss=0.5337177287909602\n",
      "GD iter. 159/499: loss=0.5336774326281808\n",
      "GD iter. 160/499: loss=0.5336466218165994\n",
      "GD iter. 161/499: loss=0.5336118442318082\n",
      "GD iter. 162/499: loss=0.5335772039520662\n",
      "GD iter. 163/499: loss=0.5335427004320351\n",
      "GD iter. 164/499: loss=0.5335083331285487\n",
      "GD iter. 165/499: loss=0.5334741015006048\n",
      "GD iter. 166/499: loss=0.533440007292799\n",
      "GD iter. 167/499: loss=0.5334047752269279\n",
      "GD iter. 168/499: loss=0.5333722306202525\n",
      "GD iter. 169/499: loss=0.5333373689942688\n",
      "GD iter. 170/499: loss=0.5333050993152568\n",
      "GD iter. 171/499: loss=0.5332695233319134\n",
      "GD iter. 172/499: loss=0.5332400501532669\n",
      "GD iter. 173/499: loss=0.5332014878509864\n",
      "GD iter. 174/499: loss=0.5331720040862697\n",
      "GD iter. 175/499: loss=0.5331391081853305\n",
      "GD iter. 176/499: loss=0.5331064391415458\n",
      "GD iter. 177/499: loss=0.5330725379746604\n",
      "GD iter. 178/499: loss=0.5330413053687679\n",
      "GD iter. 179/499: loss=0.5330056534492025\n",
      "GD iter. 180/499: loss=0.5329773371681783\n",
      "GD iter. 181/499: loss=0.5329400851268827\n",
      "GD iter. 182/499: loss=0.5329133364094194\n",
      "GD iter. 183/499: loss=0.5328790209973009\n",
      "GD iter. 184/499: loss=0.532850413328618\n",
      "GD iter. 185/499: loss=0.532812083438105\n",
      "GD iter. 186/499: loss=0.5327862563975483\n",
      "GD iter. 187/499: loss=0.5327548909415942\n",
      "GD iter. 188/499: loss=0.5327243149680901\n",
      "GD iter. 189/499: loss=0.5326881327223153\n",
      "GD iter. 190/499: loss=0.5326608852962574\n",
      "GD iter. 191/499: loss=0.5326333573026119\n",
      "GD iter. 192/499: loss=0.5325949139267113\n",
      "GD iter. 193/499: loss=0.5325715386288019\n",
      "GD iter. 194/499: loss=0.5325350880102103\n",
      "GD iter. 195/499: loss=0.5325076255944658\n",
      "GD iter. 196/499: loss=0.5324805757219832\n",
      "GD iter. 197/499: loss=0.5324456814305261\n",
      "GD iter. 198/499: loss=0.5324186372276688\n",
      "GD iter. 199/499: loss=0.5323888500487225\n",
      "GD iter. 200/499: loss=0.5323567921357428\n",
      "GD iter. 201/499: loss=0.5323301827177496\n",
      "GD iter. 202/499: loss=0.5322969088109615\n",
      "GD iter. 203/499: loss=0.5322725604862482\n",
      "GD iter. 204/499: loss=0.5322361454880684\n",
      "GD iter. 205/499: loss=0.5322157732641741\n",
      "GD iter. 206/499: loss=0.5321792272423794\n",
      "GD iter. 207/499: loss=0.5321550989116899\n",
      "GD iter. 208/499: loss=0.5321243706780306\n",
      "GD iter. 209/499: loss=0.5320984060730993\n",
      "GD iter. 210/499: loss=0.5320659465091488\n",
      "GD iter. 211/499: loss=0.5320426144276716\n",
      "GD iter. 212/499: loss=0.532008196001106\n",
      "GD iter. 213/499: loss=0.5319848603306365\n",
      "GD iter. 214/499: loss=0.5319528615811048\n",
      "GD iter. 215/499: loss=0.5319299646222146\n",
      "GD iter. 216/499: loss=0.5318959402931824\n",
      "GD iter. 217/499: loss=0.5318746452860101\n",
      "GD iter. 218/499: loss=0.531839078035607\n",
      "GD iter. 219/499: loss=0.5318191909501008\n",
      "GD iter. 220/499: loss=0.5317868615339267\n",
      "GD iter. 221/499: loss=0.5317632136185738\n",
      "GD iter. 222/499: loss=0.5317321135540519\n",
      "GD iter. 223/499: loss=0.5317100474309744\n",
      "GD iter. 224/499: loss=0.5316753051807532\n",
      "GD iter. 225/499: loss=0.5316590763883473\n",
      "GD iter. 226/499: loss=0.5316221384348949\n",
      "GD iter. 227/499: loss=0.5316037648608898\n",
      "GD iter. 228/499: loss=0.5315722212465603\n",
      "GD iter. 229/499: loss=0.5315500436811872\n",
      "GD iter. 230/499: loss=0.531519653770894\n",
      "GD iter. 231/499: loss=0.5314959012748677\n",
      "GD iter. 232/499: loss=0.5314679610798597\n",
      "GD iter. 233/499: loss=0.5314428127726085\n",
      "GD iter. 234/499: loss=0.53141658237765\n",
      "GD iter. 235/499: loss=0.5313921229695174\n",
      "GD iter. 236/499: loss=0.5313634624839073\n",
      "GD iter. 237/499: loss=0.5313460780342414\n",
      "GD iter. 238/499: loss=0.5313082806475063\n",
      "GD iter. 239/499: loss=0.5312950354372824\n",
      "GD iter. 240/499: loss=0.5312575649152251\n",
      "GD iter. 241/499: loss=0.5312443946531791\n",
      "GD iter. 242/499: loss=0.5312072502410268\n",
      "GD iter. 243/499: loss=0.5311934119282413\n",
      "GD iter. 244/499: loss=0.5311578735320698\n",
      "GD iter. 245/499: loss=0.5311426197829118\n",
      "GD iter. 246/499: loss=0.5311104673896608\n",
      "GD iter. 247/499: loss=0.5310965908306533\n",
      "GD iter. 248/499: loss=0.5310584753236313\n",
      "GD iter. 249/499: loss=0.5310455171728722\n",
      "GD iter. 250/499: loss=0.5310095137864895\n",
      "GD iter. 251/499: loss=0.5309965891533617\n",
      "GD iter. 252/499: loss=0.5309646708522783\n",
      "GD iter. 253/499: loss=0.5309497466691973\n",
      "GD iter. 254/499: loss=0.5309128331318711\n",
      "GD iter. 255/499: loss=0.5309033073465417\n",
      "GD iter. 256/499: loss=0.5308660588241941\n",
      "GD iter. 257/499: loss=0.5308515830028121\n",
      "GD iter. 258/499: loss=0.5308248915576571\n",
      "GD iter. 259/499: loss=0.5308053732107009\n",
      "GD iter. 260/499: loss=0.5307765029569227\n",
      "GD iter. 261/499: loss=0.530761783110695\n",
      "GD iter. 262/499: loss=0.530725416013497\n",
      "GD iter. 263/499: loss=0.5307160552851478\n",
      "GD iter. 264/499: loss=0.5306797086842926\n",
      "GD iter. 265/499: loss=0.5306688558652531\n",
      "GD iter. 266/499: loss=0.5306392515913849\n",
      "GD iter. 267/499: loss=0.5306197223211024\n",
      "GD iter. 268/499: loss=0.5305936637939945\n",
      "GD iter. 269/499: loss=0.5305792051106454\n",
      "GD iter. 270/499: loss=0.5305435908416741\n",
      "GD iter. 271/499: loss=0.5305367342699021\n",
      "GD iter. 272/499: loss=0.5305003549369525\n",
      "GD iter. 273/499: loss=0.5304908599666647\n",
      "GD iter. 274/499: loss=0.5304553703983637\n",
      "GD iter. 275/499: loss=0.530447166239684\n",
      "GD iter. 276/499: loss=0.5304132718621222\n",
      "GD iter. 277/499: loss=0.5304014660310933\n",
      "GD iter. 278/499: loss=0.5303735143995554\n",
      "GD iter. 279/499: loss=0.5303520427236128\n",
      "GD iter. 280/499: loss=0.530330656174845\n",
      "GD iter. 281/499: loss=0.530309358634368\n",
      "GD iter. 282/499: loss=0.5302874705019887\n",
      "GD iter. 283/499: loss=0.5302701911715308\n",
      "GD iter. 284/499: loss=0.5302432049661356\n",
      "GD iter. 285/499: loss=0.5302309063239051\n",
      "GD iter. 286/499: loss=0.5301972089177148\n",
      "GD iter. 287/499: loss=0.5301892531071272\n",
      "GD iter. 288/499: loss=0.5301556397436579\n",
      "GD iter. 289/499: loss=0.530147929425747\n",
      "GD iter. 290/499: loss=0.5301143991174595\n",
      "GD iter. 291/499: loss=0.5301069326621366\n",
      "GD iter. 292/499: loss=0.5300734844306366\n",
      "GD iter. 293/499: loss=0.5300662656913762\n",
      "GD iter. 294/499: loss=0.5300322500300234\n",
      "GD iter. 295/499: loss=0.5300267240315276\n",
      "GD iter. 296/499: loss=0.5299919764055107\n",
      "GD iter. 297/499: loss=0.5299848433025004\n",
      "GD iter. 298/499: loss=0.5299562887074427\n",
      "GD iter. 299/499: loss=0.5299421990312456\n",
      "GD iter. 300/499: loss=0.5299170907611047\n",
      "GD iter. 301/499: loss=0.5299000583999899\n",
      "GD iter. 302/499: loss=0.529877053657881\n",
      "GD iter. 303/499: loss=0.5298674312482785\n",
      "GD iter. 304/499: loss=0.5298335214578653\n",
      "GD iter. 305/499: loss=0.5298262309905349\n",
      "GD iter. 306/499: loss=0.5297934944468703\n",
      "GD iter. 307/499: loss=0.5297897363539241\n",
      "GD iter. 308/499: loss=0.5297552471817792\n",
      "GD iter. 309/499: loss=0.5297499309373298\n",
      "GD iter. 310/499: loss=0.5297171810043833\n",
      "GD iter. 311/499: loss=0.5297098103993069\n",
      "GD iter. 312/499: loss=0.5296838322668131\n",
      "GD iter. 313/499: loss=0.5296727198097186\n",
      "GD iter. 314/499: loss=0.5296467032997719\n",
      "GD iter. 315/499: loss=0.5296318560129604\n",
      "GD iter. 316/499: loss=0.5296089593786534\n",
      "GD iter. 317/499: loss=0.5295989119024148\n",
      "GD iter. 318/499: loss=0.5295658353131998\n",
      "GD iter. 319/499: loss=0.5295625612035872\n",
      "GD iter. 320/499: loss=0.5295293697448002\n",
      "GD iter. 321/499: loss=0.5295251885543523\n",
      "GD iter. 322/499: loss=0.5294925252910507\n",
      "GD iter. 323/499: loss=0.5294894181611485\n",
      "GD iter. 324/499: loss=0.5294566666140378\n",
      "GD iter. 325/499: loss=0.5294509516667137\n",
      "GD iter. 326/499: loss=0.529425012841512\n",
      "GD iter. 327/499: loss=0.5294194199448096\n",
      "GD iter. 328/499: loss=0.529383907523425\n",
      "GD iter. 329/499: loss=0.5293805753296531\n",
      "GD iter. 330/499: loss=0.5293546123402314\n",
      "GD iter. 331/499: loss=0.5293486941844551\n",
      "GD iter. 332/499: loss=0.529313427596818\n",
      "GD iter. 333/499: loss=0.5293103821299182\n",
      "GD iter. 334/499: loss=0.5292847171540922\n",
      "GD iter. 335/499: loss=0.5292767527803689\n",
      "GD iter. 336/499: loss=0.5292450506354014\n",
      "GD iter. 337/499: loss=0.5292388893649289\n",
      "GD iter. 338/499: loss=0.5292163152757281\n",
      "GD iter. 339/499: loss=0.529205607290913\n",
      "GD iter. 340/499: loss=0.5291808745025576\n",
      "GD iter. 341/499: loss=0.5291762651184255\n",
      "GD iter. 342/499: loss=0.5291410264188722\n",
      "GD iter. 343/499: loss=0.529139292628627\n",
      "GD iter. 344/499: loss=0.529112346030583\n",
      "GD iter. 345/499: loss=0.5291068991232604\n",
      "GD iter. 346/499: loss=0.529074202765297\n",
      "GD iter. 347/499: loss=0.5290725942837348\n",
      "GD iter. 348/499: loss=0.5290473077909287\n",
      "GD iter. 349/499: loss=0.5290430973905814\n",
      "GD iter. 350/499: loss=0.5290085336930249\n",
      "GD iter. 351/499: loss=0.5290072016213173\n",
      "GD iter. 352/499: loss=0.528980871491127\n",
      "GD iter. 353/499: loss=0.528975819912195\n",
      "GD iter. 354/499: loss=0.5289447656332372\n",
      "GD iter. 355/499: loss=0.5289431858366272\n",
      "GD iter. 356/499: loss=0.5289176623639104\n",
      "GD iter. 357/499: loss=0.5289136615954543\n",
      "GD iter. 358/499: loss=0.5288800661406167\n",
      "GD iter. 359/499: loss=0.5288815651047506\n",
      "GD iter. 360/499: loss=0.5288484785010376\n",
      "GD iter. 361/499: loss=0.5288497572111134\n",
      "GD iter. 362/499: loss=0.5288190394892854\n",
      "GD iter. 363/499: loss=0.5288170777760521\n",
      "GD iter. 364/499: loss=0.5287981636315975\n",
      "GD iter. 365/499: loss=0.5287825431738192\n",
      "GD iter. 366/499: loss=0.5287648418349001\n",
      "GD iter. 367/499: loss=0.5287514065590596\n",
      "GD iter. 368/499: loss=0.5287350809118688\n",
      "GD iter. 369/499: loss=0.5287218681196619\n",
      "GD iter. 370/499: loss=0.5287043200679242\n",
      "GD iter. 371/499: loss=0.5286912178017222\n",
      "GD iter. 372/499: loss=0.5286722033127854\n",
      "GD iter. 373/499: loss=0.528668905133676\n",
      "GD iter. 374/499: loss=0.5286355536344047\n",
      "GD iter. 375/499: loss=0.5286364982594121\n",
      "GD iter. 376/499: loss=0.5286049371406624\n",
      "GD iter. 377/499: loss=0.5286076872454436\n",
      "GD iter. 378/499: loss=0.5285760122877348\n",
      "GD iter. 379/499: loss=0.528578266352387\n",
      "GD iter. 380/499: loss=0.5285460776117347\n",
      "GD iter. 381/499: loss=0.528547985764986\n",
      "GD iter. 382/499: loss=0.5285243043786392\n",
      "GD iter. 383/499: loss=0.5285226214034346\n",
      "GD iter. 384/499: loss=0.5284895495595903\n",
      "GD iter. 385/499: loss=0.5284905325522414\n",
      "GD iter. 386/499: loss=0.5284661579183593\n",
      "GD iter. 387/499: loss=0.5284647476890791\n",
      "GD iter. 388/499: loss=0.5284319733481116\n",
      "GD iter. 389/499: loss=0.5284339177477116\n",
      "GD iter. 390/499: loss=0.5284115632786218\n",
      "GD iter. 391/499: loss=0.528410479132573\n",
      "GD iter. 392/499: loss=0.5283759227184064\n",
      "GD iter. 393/499: loss=0.528377615358327\n",
      "GD iter. 394/499: loss=0.5283576802896145\n",
      "GD iter. 395/499: loss=0.5283545622258127\n",
      "GD iter. 396/499: loss=0.5283208069844648\n",
      "GD iter. 397/499: loss=0.5283223898726169\n",
      "GD iter. 398/499: loss=0.5283079641693611\n",
      "GD iter. 399/499: loss=0.5282943956414148\n",
      "GD iter. 400/499: loss=0.5282808803221712\n",
      "GD iter. 401/499: loss=0.5282674468809212\n",
      "GD iter. 402/499: loss=0.5282503022102187\n",
      "GD iter. 403/499: loss=0.5282485743094149\n",
      "GD iter. 404/499: loss=0.5282142874326342\n",
      "GD iter. 405/499: loss=0.528220252952028\n",
      "GD iter. 406/499: loss=0.5281874837672547\n",
      "GD iter. 407/499: loss=0.5281922240105003\n",
      "GD iter. 408/499: loss=0.5281640536165896\n",
      "GD iter. 409/499: loss=0.5281666105521087\n",
      "GD iter. 410/499: loss=0.5281397212544765\n",
      "GD iter. 411/499: loss=0.5281427033922182\n",
      "GD iter. 412/499: loss=0.5281103770846838\n",
      "GD iter. 413/499: loss=0.5281153870068276\n",
      "GD iter. 414/499: loss=0.5280907928121775\n",
      "GD iter. 415/499: loss=0.5280923971369039\n",
      "GD iter. 416/499: loss=0.5280601103918958\n",
      "GD iter. 417/499: loss=0.5280666726121818\n",
      "GD iter. 418/499: loss=0.5280366257541842\n",
      "GD iter. 419/499: loss=0.5280419222212558\n",
      "GD iter. 420/499: loss=0.5280139412682221\n",
      "GD iter. 421/499: loss=0.5280161264713624\n",
      "GD iter. 422/499: loss=0.52799405231028\n",
      "GD iter. 423/499: loss=0.5279940683639145\n",
      "GD iter. 424/499: loss=0.527961886121698\n",
      "GD iter. 425/499: loss=0.5279691164808812\n",
      "GD iter. 426/499: loss=0.5279383432595298\n",
      "GD iter. 427/499: loss=0.5279451580463989\n",
      "GD iter. 428/499: loss=0.5279165888002003\n",
      "GD iter. 429/499: loss=0.527923063857209\n",
      "GD iter. 430/499: loss=0.5278893409067086\n",
      "GD iter. 431/499: loss=0.5278967758231339\n",
      "GD iter. 432/499: loss=0.5278750789074721\n",
      "GD iter. 433/499: loss=0.5278761144161529\n",
      "GD iter. 434/499: loss=0.5278431756240748\n",
      "GD iter. 435/499: loss=0.5278495123878544\n",
      "GD iter. 436/499: loss=0.5278278220196404\n",
      "GD iter. 437/499: loss=0.5278285317693314\n",
      "GD iter. 438/499: loss=0.5277965468114053\n",
      "GD iter. 439/499: loss=0.5278049853238501\n",
      "GD iter. 440/499: loss=0.5277778250599973\n",
      "GD iter. 441/499: loss=0.527782482818627\n",
      "GD iter. 442/499: loss=0.5277553324680935\n",
      "GD iter. 443/499: loss=0.5277603455837091\n",
      "GD iter. 444/499: loss=0.5277288280017182\n",
      "GD iter. 445/499: loss=0.5277379280381503\n",
      "GD iter. 446/499: loss=0.5277099549972617\n",
      "GD iter. 447/499: loss=0.5277168797717575\n",
      "GD iter. 448/499: loss=0.5276842296158779\n",
      "GD iter. 449/499: loss=0.5276928877509216\n",
      "GD iter. 450/499: loss=0.5276726583761866\n",
      "GD iter. 451/499: loss=0.5276678435862051\n",
      "GD iter. 452/499: loss=0.5276518600032399\n",
      "GD iter. 453/499: loss=0.5276446996877626\n",
      "GD iter. 454/499: loss=0.5276297738818625\n",
      "GD iter. 455/499: loss=0.5276227039138449\n",
      "GD iter. 456/499: loss=0.5276077101949957\n",
      "GD iter. 457/499: loss=0.5276049815156199\n",
      "GD iter. 458/499: loss=0.5275779235513981\n",
      "GD iter. 459/499: loss=0.5275885038617628\n",
      "GD iter. 460/499: loss=0.5275553575532972\n",
      "GD iter. 461/499: loss=0.527566447845194\n",
      "GD iter. 462/499: loss=0.5275342466209336\n",
      "GD iter. 463/499: loss=0.5275445725196061\n",
      "GD iter. 464/499: loss=0.5275140123408945\n",
      "GD iter. 465/499: loss=0.5275238102382204\n",
      "GD iter. 466/499: loss=0.5274952205575991\n",
      "GD iter. 467/499: loss=0.5275060141933591\n",
      "GD iter. 468/499: loss=0.5274732742370659\n",
      "GD iter. 469/499: loss=0.5274847710344631\n",
      "GD iter. 470/499: loss=0.5274528719370344\n",
      "GD iter. 471/499: loss=0.5274650703686978\n",
      "GD iter. 472/499: loss=0.5274325086981309\n",
      "GD iter. 473/499: loss=0.5274455715999963\n",
      "GD iter. 474/499: loss=0.527415327168612\n",
      "GD iter. 475/499: loss=0.5274267699844187\n",
      "GD iter. 476/499: loss=0.5273929276615144\n",
      "GD iter. 477/499: loss=0.5274059317816494\n",
      "GD iter. 478/499: loss=0.5273776080218019\n",
      "GD iter. 479/499: loss=0.5273846613124942\n",
      "GD iter. 480/499: loss=0.5273630441923117\n",
      "GD iter. 481/499: loss=0.5273648310230903\n",
      "GD iter. 482/499: loss=0.5273434164755259\n",
      "GD iter. 483/499: loss=0.5273451689094851\n",
      "GD iter. 484/499: loss=0.5273233121333181\n",
      "GD iter. 485/499: loss=0.5273287876568435\n",
      "GD iter. 486/499: loss=0.5272962578144814\n",
      "GD iter. 487/499: loss=0.5273129256630901\n",
      "GD iter. 488/499: loss=0.5272770091348273\n",
      "GD iter. 489/499: loss=0.5272918685294358\n",
      "GD iter. 490/499: loss=0.527259446922207\n",
      "GD iter. 491/499: loss=0.5272737557134708\n",
      "GD iter. 492/499: loss=0.5272395854402574\n",
      "GD iter. 493/499: loss=0.5272533567919336\n",
      "GD iter. 494/499: loss=0.5272325421152884\n",
      "GD iter. 495/499: loss=0.5272308580576626\n",
      "GD iter. 496/499: loss=0.5272150807654266\n",
      "GD iter. 497/499: loss=0.5272119006101876\n",
      "GD iter. 498/499: loss=0.5272000863974778\n",
      "GD iter. 499/499: loss=0.5271917171350657\n",
      "The Accuracy is: 0.9031\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "GD iter. 0/499: loss=1.0093312438802349\n",
      "GD iter. 1/499: loss=1.00423259646404\n",
      "GD iter. 2/499: loss=0.999149289008218\n",
      "GD iter. 3/499: loss=0.9940812702827568\n",
      "GD iter. 4/499: loss=0.9890284892422223\n",
      "GD iter. 5/499: loss=0.9839908950250615\n",
      "GD iter. 6/499: loss=0.9789684369529075\n",
      "GD iter. 7/499: loss=0.9739610645298875\n",
      "GD iter. 8/499: loss=0.9689687274419346\n",
      "GD iter. 9/499: loss=0.9639913755560999\n",
      "GD iter. 10/499: loss=0.9590289589198695\n",
      "GD iter. 11/499: loss=0.9540814277604833\n",
      "GD iter. 12/499: loss=0.9491487324842558\n",
      "GD iter. 13/499: loss=0.9442308236759004\n",
      "GD iter. 14/499: loss=0.9393276520978566\n",
      "GD iter. 15/499: loss=0.9344391686896185\n",
      "GD iter. 16/499: loss=0.9295653245670672\n",
      "GD iter. 17/499: loss=0.9247060710218054\n",
      "GD iter. 18/499: loss=0.9198613595204936\n",
      "GD iter. 19/499: loss=0.9150311417041906\n",
      "GD iter. 20/499: loss=0.9102153693876956\n",
      "GD iter. 21/499: loss=0.9054139945588926\n",
      "GD iter. 22/499: loss=0.9006269693780973\n",
      "GD iter. 23/499: loss=0.8958542461774084\n",
      "GD iter. 24/499: loss=0.891095777460058\n",
      "GD iter. 25/499: loss=0.886351515899768\n",
      "GD iter. 26/499: loss=0.8816214143401071\n",
      "GD iter. 27/499: loss=0.8769054257938491\n",
      "GD iter. 28/499: loss=0.8722035034423377\n",
      "GD iter. 29/499: loss=0.8675156006348497\n",
      "GD iter. 30/499: loss=0.8628416708879625\n",
      "GD iter. 31/499: loss=0.8581816678849242\n",
      "GD iter. 32/499: loss=0.8535355454750249\n",
      "GD iter. 33/499: loss=0.8489032576729721\n",
      "GD iter. 34/499: loss=0.844284758658268\n",
      "GD iter. 35/499: loss=0.8396800027745871\n",
      "GD iter. 36/499: loss=0.8350889445291602\n",
      "GD iter. 37/499: loss=0.8305115385921573\n",
      "GD iter. 38/499: loss=0.8259477397960748\n",
      "GD iter. 39/499: loss=0.8213975031351237\n",
      "GD iter. 40/499: loss=0.8168607837646219\n",
      "GD iter. 41/499: loss=0.8123375370003874\n",
      "GD iter. 42/499: loss=0.8078277183181336\n",
      "GD iter. 43/499: loss=0.8033312833528699\n",
      "GD iter. 44/499: loss=0.7988481878982996\n",
      "GD iter. 45/499: loss=0.7943783879062251\n",
      "GD iter. 46/499: loss=0.7899218394859518\n",
      "GD iter. 47/499: loss=0.7854784989036971\n",
      "GD iter. 48/499: loss=0.7810483225819992\n",
      "GD iter. 49/499: loss=0.7766312670991294\n",
      "GD iter. 50/499: loss=0.7722272891885077\n",
      "GD iter. 51/499: loss=0.7678363457381178\n",
      "GD iter. 52/499: loss=0.763458393789928\n",
      "GD iter. 53/499: loss=0.7590933905393111\n",
      "GD iter. 54/499: loss=0.7547412933344684\n",
      "GD iter. 55/499: loss=0.7504020596758552\n",
      "GD iter. 56/499: loss=0.7460756472156094\n",
      "GD iter. 57/499: loss=0.7417620137569809\n",
      "GD iter. 58/499: loss=0.7374611172537645\n",
      "GD iter. 59/499: loss=0.7331729158097342\n",
      "GD iter. 60/499: loss=0.7288973676780797\n",
      "GD iter. 61/499: loss=0.7246344312608463\n",
      "GD iter. 62/499: loss=0.7203840651083745\n",
      "GD iter. 63/499: loss=0.7161462279187452\n",
      "GD iter. 64/499: loss=0.7119208785372229\n",
      "GD iter. 65/499: loss=0.7077079759557056\n",
      "GD iter. 66/499: loss=0.7035074793121723\n",
      "GD iter. 67/499: loss=0.6993193478901364\n",
      "GD iter. 68/499: loss=0.6951435411180992\n",
      "GD iter. 69/499: loss=0.690980018569005\n",
      "GD iter. 70/499: loss=0.6868287399597017\n",
      "GD iter. 71/499: loss=0.6826896651503979\n",
      "GD iter. 72/499: loss=0.6785627541441274\n",
      "GD iter. 73/499: loss=0.6744479670862132\n",
      "GD iter. 74/499: loss=0.670345264263734\n",
      "GD iter. 75/499: loss=0.6662546061049921\n",
      "GD iter. 76/499: loss=0.6621759531789845\n",
      "GD iter. 77/499: loss=0.658109266194876\n",
      "GD iter. 78/499: loss=0.654054506001473\n",
      "GD iter. 79/499: loss=0.6500116335867008\n",
      "GD iter. 80/499: loss=0.6459806100770822\n",
      "GD iter. 81/499: loss=0.6419613967372184\n",
      "GD iter. 82/499: loss=0.6379539549692712\n",
      "GD iter. 83/499: loss=0.6339582463124488\n",
      "GD iter. 84/499: loss=0.6299742324424911\n",
      "GD iter. 85/499: loss=0.6260018751711598\n",
      "GD iter. 86/499: loss=0.6220411364457282\n",
      "GD iter. 87/499: loss=0.6180919783484746\n",
      "GD iter. 88/499: loss=0.6141543630961763\n",
      "GD iter. 89/499: loss=0.6102282530396066\n",
      "GD iter. 90/499: loss=0.6063136106630328\n",
      "GD iter. 91/499: loss=0.6024103985837173\n",
      "GD iter. 92/499: loss=0.5985185795514195\n",
      "GD iter. 93/499: loss=0.5946381164479005\n",
      "GD iter. 94/499: loss=0.5907689722864292\n",
      "GD iter. 95/499: loss=0.5869111102112902\n",
      "GD iter. 96/499: loss=0.5830644934972935\n",
      "GD iter. 97/499: loss=0.5792290855492879\n",
      "GD iter. 98/499: loss=0.5754048499016732\n",
      "GD iter. 99/499: loss=0.5715917502179164\n",
      "GD iter. 100/499: loss=0.56778975029007\n",
      "GD iter. 101/499: loss=0.5639988140382893\n",
      "GD iter. 102/499: loss=0.5602189055103571\n",
      "GD iter. 103/499: loss=0.556449988881203\n",
      "GD iter. 104/499: loss=0.5526920284524311\n",
      "GD iter. 105/499: loss=0.5489449886518456\n",
      "GD iter. 106/499: loss=0.5452221153336421\n",
      "GD iter. 107/499: loss=0.5416511158231943\n",
      "GD iter. 108/499: loss=0.5387965017975563\n",
      "GD iter. 109/499: loss=0.5372582773565272\n",
      "GD iter. 110/499: loss=0.5365849710302478\n",
      "GD iter. 111/499: loss=0.5362602428474649\n",
      "GD iter. 112/499: loss=0.5360731351202422\n",
      "GD iter. 113/499: loss=0.5359458680269272\n",
      "GD iter. 114/499: loss=0.5358495743610767\n",
      "GD iter. 115/499: loss=0.5357728082547264\n",
      "GD iter. 116/499: loss=0.535713693866707\n",
      "GD iter. 117/499: loss=0.5356574675286259\n",
      "GD iter. 118/499: loss=0.535604089078007\n",
      "GD iter. 119/499: loss=0.5355543964481336\n",
      "GD iter. 120/499: loss=0.5355060364155304\n",
      "GD iter. 121/499: loss=0.5354601461015923\n",
      "GD iter. 122/499: loss=0.5354150006415599\n",
      "GD iter. 123/499: loss=0.535371533813588\n",
      "GD iter. 124/499: loss=0.535333633028995\n",
      "GD iter. 125/499: loss=0.5352890793444294\n",
      "GD iter. 126/499: loss=0.5352489420529195\n",
      "GD iter. 127/499: loss=0.5352062555119859\n",
      "GD iter. 128/499: loss=0.5351689891500189\n",
      "GD iter. 129/499: loss=0.5351265590562075\n",
      "GD iter. 130/499: loss=0.5350843577471485\n",
      "GD iter. 131/499: loss=0.5350449980312948\n",
      "GD iter. 132/499: loss=0.535004520052958\n",
      "GD iter. 133/499: loss=0.5349655261796181\n",
      "GD iter. 134/499: loss=0.5349239526841294\n",
      "GD iter. 135/499: loss=0.5348852395657347\n",
      "GD iter. 136/499: loss=0.5348440404767599\n",
      "GD iter. 137/499: loss=0.5348064176630726\n",
      "GD iter. 138/499: loss=0.5347655300079581\n",
      "GD iter. 139/499: loss=0.534728220761396\n",
      "GD iter. 140/499: loss=0.5346876420567779\n",
      "GD iter. 141/499: loss=0.5346506484000658\n",
      "GD iter. 142/499: loss=0.5346093305421905\n",
      "GD iter. 143/499: loss=0.5345739192004915\n",
      "GD iter. 144/499: loss=0.5345328654133981\n",
      "GD iter. 145/499: loss=0.5344964584316338\n",
      "GD iter. 146/499: loss=0.5344577850697557\n",
      "GD iter. 147/499: loss=0.5344217754619457\n",
      "GD iter. 148/499: loss=0.5343820581371271\n",
      "GD iter. 149/499: loss=0.5343463673197185\n",
      "GD iter. 150/499: loss=0.5343050470593428\n",
      "GD iter. 151/499: loss=0.5342708426334207\n",
      "GD iter. 152/499: loss=0.5342298859854416\n",
      "GD iter. 153/499: loss=0.5341972419390146\n",
      "GD iter. 154/499: loss=0.5341555129475207\n",
      "GD iter. 155/499: loss=0.5341231531904225\n",
      "GD iter. 156/499: loss=0.5340843195401879\n",
      "GD iter. 157/499: loss=0.5340466134585811\n",
      "GD iter. 158/499: loss=0.5340111213279407\n",
      "GD iter. 159/499: loss=0.5339747893931923\n",
      "GD iter. 160/499: loss=0.5339375222786954\n",
      "GD iter. 161/499: loss=0.5339038066487912\n",
      "GD iter. 162/499: loss=0.533865868893683\n",
      "GD iter. 163/499: loss=0.5338314245937057\n",
      "GD iter. 164/499: loss=0.5337938456893889\n",
      "GD iter. 165/499: loss=0.5337619024814619\n",
      "GD iter. 166/499: loss=0.5337244894794279\n",
      "GD iter. 167/499: loss=0.5336892903260072\n",
      "GD iter. 168/499: loss=0.5336533759077903\n",
      "GD iter. 169/499: loss=0.5336209896168679\n",
      "GD iter. 170/499: loss=0.5335841612337552\n",
      "GD iter. 171/499: loss=0.5335520939194015\n",
      "GD iter. 172/499: loss=0.533515503473795\n",
      "GD iter. 173/499: loss=0.5334814474259425\n",
      "GD iter. 174/499: loss=0.5334452989506512\n",
      "GD iter. 175/499: loss=0.5334136479066948\n",
      "GD iter. 176/499: loss=0.5333777236264053\n",
      "GD iter. 177/499: loss=0.5333463853927051\n",
      "GD iter. 178/499: loss=0.5333106833482851\n",
      "GD iter. 179/499: loss=0.5332796556158508\n",
      "GD iter. 180/499: loss=0.5332441801383804\n",
      "GD iter. 181/499: loss=0.5332143684577092\n",
      "GD iter. 182/499: loss=0.5331772993673651\n",
      "GD iter. 183/499: loss=0.533147684519377\n",
      "GD iter. 184/499: loss=0.5331109444814793\n",
      "GD iter. 185/499: loss=0.5330815246461471\n",
      "GD iter. 186/499: loss=0.5330451400135623\n",
      "GD iter. 187/499: loss=0.5330178724150725\n",
      "GD iter. 188/499: loss=0.5329808501639153\n",
      "GD iter. 189/499: loss=0.532953889729675\n",
      "GD iter. 190/499: loss=0.5329159122160787\n",
      "GD iter. 191/499: loss=0.5328897485181534\n",
      "GD iter. 192/499: loss=0.5328540318001483\n",
      "GD iter. 193/499: loss=0.5328248278964761\n",
      "GD iter. 194/499: loss=0.5327912436981053\n",
      "GD iter. 195/499: loss=0.5327597340441846\n",
      "GD iter. 196/499: loss=0.5327296802699408\n",
      "GD iter. 197/499: loss=0.5326965726049014\n",
      "GD iter. 198/499: loss=0.53267151167653\n",
      "GD iter. 199/499: loss=0.5326356591826992\n",
      "GD iter. 200/499: loss=0.5326047700677295\n",
      "GD iter. 201/499: loss=0.5325765973561418\n",
      "GD iter. 202/499: loss=0.5325439650145414\n",
      "GD iter. 203/499: loss=0.5325117285653189\n",
      "GD iter. 204/499: loss=0.5324872111536746\n",
      "GD iter. 205/499: loss=0.5324535821925108\n",
      "GD iter. 206/499: loss=0.5324216323377153\n",
      "GD iter. 207/499: loss=0.5323975433169805\n",
      "GD iter. 208/499: loss=0.5323642701194388\n",
      "GD iter. 209/499: loss=0.5323326071251228\n",
      "GD iter. 210/499: loss=0.5323081059227367\n",
      "GD iter. 211/499: loss=0.5322765389721468\n",
      "GD iter. 212/499: loss=0.5322451411360781\n",
      "GD iter. 213/499: loss=0.5322182640167633\n",
      "GD iter. 214/499: loss=0.5321861352125931\n",
      "GD iter. 215/499: loss=0.5321613424995041\n",
      "GD iter. 216/499: loss=0.5321293035695548\n",
      "GD iter. 217/499: loss=0.5321030772181871\n",
      "GD iter. 218/499: loss=0.5320713642763044\n",
      "GD iter. 219/499: loss=0.5320465556097587\n",
      "GD iter. 220/499: loss=0.5320150185186074\n",
      "GD iter. 221/499: loss=0.5319892083567429\n",
      "GD iter. 222/499: loss=0.5319571811451215\n",
      "GD iter. 223/499: loss=0.5319370895273691\n",
      "GD iter. 224/499: loss=0.5319020599791023\n",
      "GD iter. 225/499: loss=0.5318824161581688\n",
      "GD iter. 226/499: loss=0.5318474479905659\n",
      "GD iter. 227/499: loss=0.5318224206572186\n",
      "GD iter. 228/499: loss=0.531792187211632\n",
      "GD iter. 229/499: loss=0.5317722559039743\n",
      "GD iter. 230/499: loss=0.531737828749964\n",
      "GD iter. 231/499: loss=0.531714877108672\n",
      "GD iter. 232/499: loss=0.5316849937481524\n",
      "GD iter. 233/499: loss=0.5316574426594189\n",
      "GD iter. 234/499: loss=0.5316309996132587\n",
      "GD iter. 235/499: loss=0.5316037229736366\n",
      "GD iter. 236/499: loss=0.5315844973010974\n",
      "GD iter. 237/499: loss=0.5315508550509275\n",
      "GD iter. 238/499: loss=0.5315287082708566\n",
      "GD iter. 239/499: loss=0.5314996023921419\n",
      "GD iter. 240/499: loss=0.5314721405183135\n",
      "GD iter. 241/499: loss=0.531452227695226\n",
      "GD iter. 242/499: loss=0.5314195076133955\n",
      "GD iter. 243/499: loss=0.5314045823922989\n",
      "GD iter. 244/499: loss=0.531371435184814\n",
      "GD iter. 245/499: loss=0.5313444008407355\n",
      "GD iter. 246/499: loss=0.5313189018471276\n",
      "GD iter. 247/499: loss=0.5312945340713016\n",
      "GD iter. 248/499: loss=0.5312678672416883\n",
      "GD iter. 249/499: loss=0.5312471788323148\n",
      "GD iter. 250/499: loss=0.5312191630469222\n",
      "GD iter. 251/499: loss=0.5311927996463327\n",
      "GD iter. 252/499: loss=0.5311724064823683\n",
      "GD iter. 253/499: loss=0.5311446849020564\n",
      "GD iter. 254/499: loss=0.5311186213490924\n",
      "GD iter. 255/499: loss=0.5310985199051932\n",
      "GD iter. 256/499: loss=0.5310710917036959\n",
      "GD iter. 257/499: loss=0.5310466844602796\n",
      "GD iter. 258/499: loss=0.5310210248418422\n",
      "GD iter. 259/499: loss=0.531002499930157\n",
      "GD iter. 260/499: loss=0.530972799876161\n",
      "GD iter. 261/499: loss=0.5309532679924712\n",
      "GD iter. 262/499: loss=0.5309264137320633\n",
      "GD iter. 263/499: loss=0.5309025821399663\n",
      "GD iter. 264/499: loss=0.530877502315375\n",
      "GD iter. 265/499: loss=0.5308587728093872\n",
      "GD iter. 266/499: loss=0.5308303289492861\n",
      "GD iter. 267/499: loss=0.530813383005613\n",
      "GD iter. 268/499: loss=0.5307824258848296\n",
      "GD iter. 269/499: loss=0.5307689976422418\n",
      "GD iter. 270/499: loss=0.5307370319152334\n",
      "GD iter. 271/499: loss=0.5307196527490173\n",
      "GD iter. 272/499: loss=0.5306908871222697\n",
      "GD iter. 273/499: loss=0.5306724753644546\n",
      "GD iter. 274/499: loss=0.5306447614562375\n",
      "GD iter. 275/499: loss=0.5306296986755796\n",
      "GD iter. 276/499: loss=0.5305987007659061\n",
      "GD iter. 277/499: loss=0.5305885834434738\n",
      "GD iter. 278/499: loss=0.53055502515383\n",
      "GD iter. 279/499: loss=0.5305357174769849\n",
      "GD iter. 280/499: loss=0.5305094223861352\n",
      "GD iter. 281/499: loss=0.5304962498060265\n",
      "GD iter. 282/499: loss=0.530465132063174\n",
      "GD iter. 283/499: loss=0.5304533942746165\n",
      "GD iter. 284/499: loss=0.530421694845031\n",
      "GD iter. 285/499: loss=0.5304071797148645\n",
      "GD iter. 286/499: loss=0.53037899315395\n",
      "GD iter. 287/499: loss=0.5303602839431912\n",
      "GD iter. 288/499: loss=0.5303343147324148\n",
      "GD iter. 289/499: loss=0.5303244380598275\n",
      "GD iter. 290/499: loss=0.5302921382422017\n",
      "GD iter. 291/499: loss=0.5302781650441447\n",
      "GD iter. 292/499: loss=0.5302492570690128\n",
      "GD iter. 293/499: loss=0.5302376430802935\n",
      "GD iter. 294/499: loss=0.5302072880292812\n",
      "GD iter. 295/499: loss=0.5301928191435605\n",
      "GD iter. 296/499: loss=0.5301645387983435\n",
      "GD iter. 297/499: loss=0.5301543587540544\n",
      "GD iter. 298/499: loss=0.5301233920345063\n",
      "GD iter. 299/499: loss=0.5301130018438416\n",
      "GD iter. 300/499: loss=0.5300833336395092\n",
      "GD iter. 301/499: loss=0.5300655077183767\n",
      "GD iter. 302/499: loss=0.5300440342493704\n",
      "GD iter. 303/499: loss=0.5300226596418407\n",
      "GD iter. 304/499: loss=0.5300022743442919\n",
      "GD iter. 305/499: loss=0.5299843028583296\n",
      "GD iter. 306/499: loss=0.5299617732603559\n",
      "GD iter. 307/499: loss=0.5299474721794586\n",
      "GD iter. 308/499: loss=0.5299204242452727\n",
      "GD iter. 309/499: loss=0.5299119597108451\n",
      "GD iter. 310/499: loss=0.5298790188979569\n",
      "GD iter. 311/499: loss=0.5298720761734387\n",
      "GD iter. 312/499: loss=0.5298424694224912\n",
      "GD iter. 313/499: loss=0.5298305534304241\n",
      "GD iter. 314/499: loss=0.5298035133645417\n",
      "GD iter. 315/499: loss=0.5297890682715053\n",
      "GD iter. 316/499: loss=0.5297640578460081\n",
      "GD iter. 317/499: loss=0.5297558329371004\n",
      "GD iter. 318/499: loss=0.5297266008875062\n",
      "GD iter. 319/499: loss=0.5297096130726072\n",
      "GD iter. 320/499: loss=0.529688587070937\n",
      "GD iter. 321/499: loss=0.5296762035626524\n",
      "GD iter. 322/499: loss=0.5296492616280586\n",
      "GD iter. 323/499: loss=0.5296423513791448\n",
      "GD iter. 324/499: loss=0.5296135080356792\n",
      "GD iter. 325/499: loss=0.5295969703564454\n",
      "GD iter. 326/499: loss=0.5295763821757462\n",
      "GD iter. 327/499: loss=0.5295609329827936\n",
      "GD iter. 328/499: loss=0.5295404263079577\n",
      "GD iter. 329/499: loss=0.5295214066945069\n",
      "GD iter. 330/499: loss=0.529508208307628\n",
      "GD iter. 331/499: loss=0.5294832739545415\n",
      "GD iter. 332/499: loss=0.5294760335829043\n",
      "GD iter. 333/499: loss=0.5294467972271718\n",
      "GD iter. 334/499: loss=0.5294406066592411\n",
      "GD iter. 335/499: loss=0.5294113044281614\n",
      "GD iter. 336/499: loss=0.5294043631850738\n",
      "GD iter. 337/499: loss=0.5293753902263187\n",
      "GD iter. 338/499: loss=0.529369501435918\n",
      "GD iter. 339/499: loss=0.5293404653732697\n",
      "GD iter. 340/499: loss=0.5293338186493375\n",
      "GD iter. 341/499: loss=0.529305108623171\n",
      "GD iter. 342/499: loss=0.5292984150798278\n",
      "GD iter. 343/499: loss=0.5292700358298776\n",
      "GD iter. 344/499: loss=0.5292643865780251\n",
      "GD iter. 345/499: loss=0.5292359347145006\n",
      "GD iter. 346/499: loss=0.5292295304454238\n",
      "GD iter. 347/499: loss=0.5292014121870768\n",
      "GD iter. 348/499: loss=0.5291975619636939\n",
      "GD iter. 349/499: loss=0.5291664386875151\n",
      "GD iter. 350/499: loss=0.5291620180394939\n",
      "GD iter. 351/499: loss=0.5291341482260453\n",
      "GD iter. 352/499: loss=0.5291296103462055\n",
      "GD iter. 353/499: loss=0.5290999985321662\n",
      "GD iter. 354/499: loss=0.5290962229818755\n",
      "GD iter. 355/499: loss=0.5290668167850747\n",
      "GD iter. 356/499: loss=0.5290650054710833\n",
      "GD iter. 357/499: loss=0.5290340642304705\n",
      "GD iter. 358/499: loss=0.529033543400495\n",
      "GD iter. 359/499: loss=0.5290020151528948\n",
      "GD iter. 360/499: loss=0.5289977775925963\n",
      "GD iter. 361/499: loss=0.528969641817097\n",
      "GD iter. 362/499: loss=0.5289666713390284\n",
      "GD iter. 363/499: loss=0.5289374078964008\n",
      "GD iter. 364/499: loss=0.5289402123493556\n",
      "GD iter. 365/499: loss=0.5289054702598249\n",
      "GD iter. 366/499: loss=0.5289070199971714\n",
      "GD iter. 367/499: loss=0.5288740675597974\n",
      "GD iter. 368/499: loss=0.5288727157311266\n",
      "GD iter. 369/499: loss=0.5288436941862455\n",
      "GD iter. 370/499: loss=0.5288387766849009\n",
      "GD iter. 371/499: loss=0.5288124874962364\n",
      "GD iter. 372/499: loss=0.5288104509672481\n",
      "GD iter. 373/499: loss=0.5287817911071037\n",
      "GD iter. 374/499: loss=0.5287819570031275\n",
      "GD iter. 375/499: loss=0.5287501742927948\n",
      "GD iter. 376/499: loss=0.5287551525975284\n",
      "GD iter. 377/499: loss=0.5287189836591181\n",
      "GD iter. 378/499: loss=0.5287280104389733\n",
      "GD iter. 379/499: loss=0.5286880556550251\n",
      "GD iter. 380/499: loss=0.5286956466488465\n",
      "GD iter. 381/499: loss=0.5286595719372456\n",
      "GD iter. 382/499: loss=0.5286666835182529\n",
      "GD iter. 383/499: loss=0.5286285574415973\n",
      "GD iter. 384/499: loss=0.5286359902435963\n",
      "GD iter. 385/499: loss=0.5286005307068072\n",
      "GD iter. 386/499: loss=0.5286090970792362\n",
      "GD iter. 387/499: loss=0.5285705900604732\n",
      "GD iter. 388/499: loss=0.5285788343466585\n",
      "GD iter. 389/499: loss=0.5285419820710366\n",
      "GD iter. 390/499: loss=0.5285506811675288\n",
      "GD iter. 391/499: loss=0.528513611148143\n",
      "GD iter. 392/499: loss=0.5285206046668774\n",
      "GD iter. 393/499: loss=0.5284860109669405\n",
      "GD iter. 394/499: loss=0.5284899341464631\n",
      "GD iter. 395/499: loss=0.5284594322608592\n",
      "GD iter. 396/499: loss=0.5284560083143771\n",
      "GD iter. 397/499: loss=0.5284317273021362\n",
      "GD iter. 398/499: loss=0.5284272710151819\n",
      "GD iter. 399/499: loss=0.5284049529640601\n",
      "GD iter. 400/499: loss=0.528400720076675\n",
      "GD iter. 401/499: loss=0.5283758928905151\n",
      "GD iter. 402/499: loss=0.5283793484278575\n",
      "GD iter. 403/499: loss=0.5283472352161112\n",
      "GD iter. 404/499: loss=0.5283551220581875\n",
      "GD iter. 405/499: loss=0.5283195639518058\n",
      "GD iter. 406/499: loss=0.5283275867646297\n",
      "GD iter. 407/499: loss=0.5282930141126279\n",
      "GD iter. 408/499: loss=0.5283005693905156\n",
      "GD iter. 409/499: loss=0.5282670608446589\n",
      "GD iter. 410/499: loss=0.5282729340684605\n",
      "GD iter. 411/499: loss=0.5282407878343033\n",
      "GD iter. 412/499: loss=0.5282464717618319\n",
      "GD iter. 413/499: loss=0.5282145394047566\n",
      "GD iter. 414/499: loss=0.5282202172719151\n",
      "GD iter. 415/499: loss=0.5281885050604846\n",
      "GD iter. 416/499: loss=0.5281948623663423\n",
      "GD iter. 417/499: loss=0.5281629994148176\n",
      "GD iter. 418/499: loss=0.5281649120580335\n",
      "GD iter. 419/499: loss=0.5281383781719192\n",
      "GD iter. 420/499: loss=0.5281358341541773\n",
      "GD iter. 421/499: loss=0.528119221926513\n",
      "GD iter. 422/499: loss=0.5281109612192585\n",
      "GD iter. 423/499: loss=0.5280900896840462\n",
      "GD iter. 424/499: loss=0.5280852895402084\n",
      "GD iter. 425/499: loss=0.5280674575506368\n",
      "GD iter. 426/499: loss=0.5280591008831201\n",
      "GD iter. 427/499: loss=0.5280462936268989\n",
      "GD iter. 428/499: loss=0.5280345451615709\n",
      "GD iter. 429/499: loss=0.5280205092472792\n",
      "GD iter. 430/499: loss=0.5280106615941019\n",
      "GD iter. 431/499: loss=0.5279929031227881\n",
      "GD iter. 432/499: loss=0.5279891411711926\n",
      "GD iter. 433/499: loss=0.5279642667109423\n",
      "GD iter. 434/499: loss=0.5279735776109791\n",
      "GD iter. 435/499: loss=0.5279396735034046\n",
      "GD iter. 436/499: loss=0.5279482068147879\n",
      "GD iter. 437/499: loss=0.527915271675767\n",
      "GD iter. 438/499: loss=0.5279230535571927\n",
      "GD iter. 439/499: loss=0.5278918217193377\n",
      "GD iter. 440/499: loss=0.5278999300491255\n",
      "GD iter. 441/499: loss=0.5278685570679374\n",
      "GD iter. 442/499: loss=0.5278769901755703\n",
      "GD iter. 443/499: loss=0.5278454762504908\n",
      "GD iter. 444/499: loss=0.5278542346687658\n",
      "GD iter. 445/499: loss=0.5278218326467107\n",
      "GD iter. 446/499: loss=0.5278318143031159\n",
      "GD iter. 447/499: loss=0.5277991783299424\n",
      "GD iter. 448/499: loss=0.5278095875940296\n",
      "GD iter. 449/499: loss=0.5277759359735517\n",
      "GD iter. 450/499: loss=0.5277855454791918\n",
      "GD iter. 451/499: loss=0.5277543974583381\n",
      "GD iter. 452/499: loss=0.5277629623774271\n",
      "GD iter. 453/499: loss=0.5277312132829396\n",
      "GD iter. 454/499: loss=0.5277411325897299\n",
      "GD iter. 455/499: loss=0.5277092390187638\n",
      "GD iter. 456/499: loss=0.5277199158199277\n",
      "GD iter. 457/499: loss=0.5276876198205412\n",
      "GD iter. 458/499: loss=0.5276973170747363\n",
      "GD iter. 459/499: loss=0.5276665601126195\n",
      "GD iter. 460/499: loss=0.527670010206443\n",
      "GD iter. 461/499: loss=0.5276538254837605\n",
      "GD iter. 462/499: loss=0.5276448158647992\n",
      "GD iter. 463/499: loss=0.5276334599869515\n",
      "GD iter. 464/499: loss=0.5276235637423495\n",
      "GD iter. 465/499: loss=0.5276122904634444\n",
      "GD iter. 466/499: loss=0.5276010720134311\n",
      "GD iter. 467/499: loss=0.5275913044361467\n",
      "GD iter. 468/499: loss=0.5275786261494987\n",
      "GD iter. 469/499: loss=0.527571761308689\n",
      "GD iter. 470/499: loss=0.5275579704822999\n",
      "GD iter. 471/499: loss=0.527550165369948\n",
      "GD iter. 472/499: loss=0.5275375815426158\n",
      "GD iter. 473/499: loss=0.527529930210832\n",
      "GD iter. 474/499: loss=0.5275147349166898\n",
      "GD iter. 475/499: loss=0.5275145385081731\n",
      "GD iter. 476/499: loss=0.5274885837270646\n",
      "GD iter. 477/499: loss=0.5275018620886661\n",
      "GD iter. 478/499: loss=0.527467301657214\n",
      "GD iter. 479/499: loss=0.5274802021445095\n",
      "GD iter. 480/499: loss=0.5274480536213717\n",
      "GD iter. 481/499: loss=0.5274608157816736\n",
      "GD iter. 482/499: loss=0.5274275592420782\n",
      "GD iter. 483/499: loss=0.5274408292969677\n",
      "GD iter. 484/499: loss=0.527407324517121\n",
      "GD iter. 485/499: loss=0.5274223886512086\n",
      "GD iter. 486/499: loss=0.5273877219400556\n",
      "GD iter. 487/499: loss=0.5274011135646557\n",
      "GD iter. 488/499: loss=0.5273705428172697\n",
      "GD iter. 489/499: loss=0.527381207724295\n",
      "GD iter. 490/499: loss=0.5273510832096796\n",
      "GD iter. 491/499: loss=0.5273628864255512\n",
      "GD iter. 492/499: loss=0.5273303338491676\n",
      "GD iter. 493/499: loss=0.5273444608289904\n",
      "GD iter. 494/499: loss=0.5273106452900566\n",
      "GD iter. 495/499: loss=0.5273248704112821\n",
      "GD iter. 496/499: loss=0.5272934252159445\n",
      "GD iter. 497/499: loss=0.5273064761902965\n",
      "GD iter. 498/499: loss=0.5272744999957223\n",
      "GD iter. 499/499: loss=0.5272880951494915\n",
      "The Accuracy is: 0.9179\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "GD iter. 0/499: loss=1.0039326779409425\n",
      "GD iter. 1/499: loss=0.9988553582393398\n",
      "GD iter. 2/499: loss=0.9937933061169522\n",
      "GD iter. 3/499: loss=0.9887464706073134\n",
      "GD iter. 4/499: loss=0.9837148009275342\n",
      "GD iter. 5/499: loss=0.9786982464776078\n",
      "GD iter. 6/499: loss=0.9736967568397207\n",
      "GD iter. 7/499: loss=0.9687102817775622\n",
      "GD iter. 8/499: loss=0.9637387712356423\n",
      "GD iter. 9/499: loss=0.9587821753386059\n",
      "GD iter. 10/499: loss=0.9538404443905547\n",
      "GD iter. 11/499: loss=0.9489135288743688\n",
      "GD iter. 12/499: loss=0.9440013794510328\n",
      "GD iter. 13/499: loss=0.939103946958962\n",
      "GD iter. 14/499: loss=0.9342211824133356\n",
      "GD iter. 15/499: loss=0.9293530370054269\n",
      "GD iter. 16/499: loss=0.9244994621019405\n",
      "GD iter. 17/499: loss=0.9196604092443496\n",
      "GD iter. 18/499: loss=0.9148358301482379\n",
      "GD iter. 19/499: loss=0.910025676702642\n",
      "GD iter. 20/499: loss=0.9052299009693975\n",
      "GD iter. 21/499: loss=0.9004484551824877\n",
      "GD iter. 22/499: loss=0.8956812917473937\n",
      "GD iter. 23/499: loss=0.8909283632404489\n",
      "GD iter. 24/499: loss=0.8861896224081944\n",
      "GD iter. 25/499: loss=0.8814650221667372\n",
      "GD iter. 26/499: loss=0.8767545156011114\n",
      "GD iter. 27/499: loss=0.8720580559646424\n",
      "GD iter. 28/499: loss=0.8673755966783112\n",
      "GD iter. 29/499: loss=0.8627070913301236\n",
      "GD iter. 30/499: loss=0.8580524936744813\n",
      "GD iter. 31/499: loss=0.8534117576315547\n",
      "GD iter. 32/499: loss=0.848784837286658\n",
      "GD iter. 33/499: loss=0.8441716868896285\n",
      "GD iter. 34/499: loss=0.8395722608542064\n",
      "GD iter. 35/499: loss=0.8349865137574172\n",
      "GD iter. 36/499: loss=0.8304144003389577\n",
      "GD iter. 37/499: loss=0.8258558755005836\n",
      "GD iter. 38/499: loss=0.8213108943054989\n",
      "GD iter. 39/499: loss=0.8167794119777493\n",
      "GD iter. 40/499: loss=0.8122613839016156\n",
      "GD iter. 41/499: loss=0.8077567656210121\n",
      "GD iter. 42/499: loss=0.8032655128388851\n",
      "GD iter. 43/499: loss=0.7987875814166152\n",
      "GD iter. 44/499: loss=0.7943229273734209\n",
      "GD iter. 45/499: loss=0.7898715068857649\n",
      "GD iter. 46/499: loss=0.7854332762867636\n",
      "GD iter. 47/499: loss=0.7810081920655971\n",
      "GD iter. 48/499: loss=0.7765962108669227\n",
      "GD iter. 49/499: loss=0.772197289490291\n",
      "GD iter. 50/499: loss=0.7678113848895621\n",
      "GD iter. 51/499: loss=0.7634384541723281\n",
      "GD iter. 52/499: loss=0.7590784545993325\n",
      "GD iter. 53/499: loss=0.7547313435838974\n",
      "GD iter. 54/499: loss=0.7503970786913478\n",
      "GD iter. 55/499: loss=0.746075617638442\n",
      "GD iter. 56/499: loss=0.7417669182928026\n",
      "GD iter. 57/499: loss=0.73747093867235\n",
      "GD iter. 58/499: loss=0.7331876369447367\n",
      "GD iter. 59/499: loss=0.7289169714267865\n",
      "GD iter. 60/499: loss=0.7246589005839335\n",
      "GD iter. 61/499: loss=0.7204133830296648\n",
      "GD iter. 62/499: loss=0.7161803775249643\n",
      "GD iter. 63/499: loss=0.7119598429777593\n",
      "GD iter. 64/499: loss=0.7077517384423679\n",
      "GD iter. 65/499: loss=0.7035560231189519\n",
      "GD iter. 66/499: loss=0.6993726563529679\n",
      "GD iter. 67/499: loss=0.6952015976346219\n",
      "GD iter. 68/499: loss=0.6910428065983278\n",
      "GD iter. 69/499: loss=0.6868962430221652\n",
      "GD iter. 70/499: loss=0.6827618668273411\n",
      "GD iter. 71/499: loss=0.6786396380776532\n",
      "GD iter. 72/499: loss=0.6745295169789548\n",
      "GD iter. 73/499: loss=0.6704314638786232\n",
      "GD iter. 74/499: loss=0.6663454392650272\n",
      "GD iter. 75/499: loss=0.6622714037670016\n",
      "GD iter. 76/499: loss=0.6582093181533174\n",
      "GD iter. 77/499: loss=0.6541591433321603\n",
      "GD iter. 78/499: loss=0.6501208403506065\n",
      "GD iter. 79/499: loss=0.6460943703941027\n",
      "GD iter. 80/499: loss=0.6420796947859481\n",
      "GD iter. 81/499: loss=0.638076774986778\n",
      "GD iter. 82/499: loss=0.6340855725940491\n",
      "GD iter. 83/499: loss=0.6301060493415276\n",
      "GD iter. 84/499: loss=0.6261381670987782\n",
      "GD iter. 85/499: loss=0.6221818878706562\n",
      "GD iter. 86/499: loss=0.6182371737968009\n",
      "GD iter. 87/499: loss=0.6143039871511304\n",
      "GD iter. 88/499: loss=0.610382290341341\n",
      "GD iter. 89/499: loss=0.6064720459084035\n",
      "GD iter. 90/499: loss=0.602573216526068\n",
      "GD iter. 91/499: loss=0.5986857650003642\n",
      "GD iter. 92/499: loss=0.5948096542691081\n",
      "GD iter. 93/499: loss=0.5909448474014092\n",
      "GD iter. 94/499: loss=0.587091307597178\n",
      "GD iter. 95/499: loss=0.5832489981866389\n",
      "GD iter. 96/499: loss=0.5794178826298412\n",
      "GD iter. 97/499: loss=0.5755979245161748\n",
      "GD iter. 98/499: loss=0.571789087563886\n",
      "GD iter. 99/499: loss=0.5679913356195966\n",
      "GD iter. 100/499: loss=0.5642046326578236\n",
      "GD iter. 101/499: loss=0.5604289427805013\n",
      "GD iter. 102/499: loss=0.5566642302165056\n",
      "GD iter. 103/499: loss=0.5529104593211789\n",
      "GD iter. 104/499: loss=0.5491688777959445\n",
      "GD iter. 105/499: loss=0.5454462097931144\n",
      "GD iter. 106/499: loss=0.5417915450461627\n",
      "GD iter. 107/499: loss=0.538605619831515\n",
      "GD iter. 108/499: loss=0.5367840100755256\n",
      "GD iter. 109/499: loss=0.5359586341545041\n",
      "GD iter. 110/499: loss=0.5356099121452023\n",
      "GD iter. 111/499: loss=0.5354279164941409\n",
      "GD iter. 112/499: loss=0.5353213130176414\n",
      "GD iter. 113/499: loss=0.5352477908289928\n",
      "GD iter. 114/499: loss=0.5351914339366097\n",
      "GD iter. 115/499: loss=0.5351411559613267\n",
      "GD iter. 116/499: loss=0.535096418219495\n",
      "GD iter. 117/499: loss=0.5350556565016149\n",
      "GD iter. 118/499: loss=0.5350134295031322\n",
      "GD iter. 119/499: loss=0.5349729778263601\n",
      "GD iter. 120/499: loss=0.534931087125369\n",
      "GD iter. 121/499: loss=0.5348909429931765\n",
      "GD iter. 122/499: loss=0.5348493859932536\n",
      "GD iter. 123/499: loss=0.5348095578447843\n",
      "GD iter. 124/499: loss=0.5347699079057461\n",
      "GD iter. 125/499: loss=0.5347288059822779\n",
      "GD iter. 126/499: loss=0.5346894572838023\n",
      "GD iter. 127/499: loss=0.534648682821563\n",
      "GD iter. 128/499: loss=0.5346096389884015\n",
      "GD iter. 129/499: loss=0.5345707751205184\n",
      "GD iter. 130/499: loss=0.5345304464677734\n",
      "GD iter. 131/499: loss=0.53449187766049\n",
      "GD iter. 132/499: loss=0.5344518703523043\n",
      "GD iter. 133/499: loss=0.5344135954634431\n",
      "GD iter. 134/499: loss=0.5343755130954025\n",
      "GD iter. 135/499: loss=0.5343373923182989\n",
      "GD iter. 136/499: loss=0.5342994213176149\n",
      "GD iter. 137/499: loss=0.534261599499952\n",
      "GD iter. 138/499: loss=0.5342239262742733\n",
      "GD iter. 139/499: loss=0.5341864027429577\n",
      "GD iter. 140/499: loss=0.5341496410903759\n",
      "GD iter. 141/499: loss=0.5341130250214854\n",
      "GD iter. 142/499: loss=0.5340765816903464\n",
      "GD iter. 143/499: loss=0.5340384764950752\n",
      "GD iter. 144/499: loss=0.5340023221864548\n",
      "GD iter. 145/499: loss=0.5339645186666203\n",
      "GD iter. 146/499: loss=0.5339291839864485\n",
      "GD iter. 147/499: loss=0.5338911990870978\n",
      "GD iter. 148/499: loss=0.5338578193409205\n",
      "GD iter. 149/499: loss=0.5338193107615151\n",
      "GD iter. 150/499: loss=0.5337828588550992\n",
      "GD iter. 151/499: loss=0.533747038868958\n",
      "GD iter. 152/499: loss=0.5337125218145412\n",
      "GD iter. 153/499: loss=0.5336746750621082\n",
      "GD iter. 154/499: loss=0.5336421284890542\n",
      "GD iter. 155/499: loss=0.5336044825992078\n",
      "GD iter. 156/499: loss=0.5335705143755319\n",
      "GD iter. 157/499: loss=0.5335332300401425\n",
      "GD iter. 158/499: loss=0.5335005018087838\n",
      "GD iter. 159/499: loss=0.5334634763378497\n",
      "GD iter. 160/499: loss=0.5334310367094043\n",
      "GD iter. 161/499: loss=0.5333942800467716\n",
      "GD iter. 162/499: loss=0.5333632925511376\n",
      "GD iter. 163/499: loss=0.5333267745973523\n",
      "GD iter. 164/499: loss=0.5332934143597406\n",
      "GD iter. 165/499: loss=0.53325840855823\n",
      "GD iter. 166/499: loss=0.5332281369499321\n",
      "GD iter. 167/499: loss=0.5331933219217778\n",
      "GD iter. 168/499: loss=0.5331586830047659\n",
      "GD iter. 169/499: loss=0.5331281242600243\n",
      "GD iter. 170/499: loss=0.5330937421578446\n",
      "GD iter. 171/499: loss=0.53306041147962\n",
      "GD iter. 172/499: loss=0.5330272351037615\n",
      "GD iter. 173/499: loss=0.5329969736875464\n",
      "GD iter. 174/499: loss=0.5329640262971015\n",
      "GD iter. 175/499: loss=0.5329312075594195\n",
      "GD iter. 176/499: loss=0.5328985169664142\n",
      "GD iter. 177/499: loss=0.532865988171414\n",
      "GD iter. 178/499: loss=0.5328382415438442\n",
      "GD iter. 179/499: loss=0.5328035592140083\n",
      "GD iter. 180/499: loss=0.5327713882332948\n",
      "GD iter. 181/499: loss=0.5327403527561261\n",
      "GD iter. 182/499: loss=0.5327094591293388\n",
      "GD iter. 183/499: loss=0.5326764841963458\n",
      "GD iter. 184/499: loss=0.5326505140879119\n",
      "GD iter. 185/499: loss=0.5326165515771994\n",
      "GD iter. 186/499: loss=0.5325838712570168\n",
      "GD iter. 187/499: loss=0.5325566521738236\n",
      "GD iter. 188/499: loss=0.5325241267372824\n",
      "GD iter. 189/499: loss=0.5324928288781876\n",
      "GD iter. 190/499: loss=0.5324642634473946\n",
      "GD iter. 191/499: loss=0.5324331806627471\n",
      "GD iter. 192/499: loss=0.5324022730434433\n",
      "GD iter. 193/499: loss=0.5323775158695504\n",
      "GD iter. 194/499: loss=0.5323457414446232\n",
      "GD iter. 195/499: loss=0.5323151284587597\n",
      "GD iter. 196/499: loss=0.5322846343339624\n",
      "GD iter. 197/499: loss=0.5322542971587232\n",
      "GD iter. 198/499: loss=0.5322284957405915\n",
      "GD iter. 199/499: loss=0.532198346307351\n",
      "GD iter. 200/499: loss=0.5321683138927431\n",
      "GD iter. 201/499: loss=0.532138399137538\n",
      "GD iter. 202/499: loss=0.5321098230939405\n",
      "GD iter. 203/499: loss=0.5320802372897793\n",
      "GD iter. 204/499: loss=0.5320583250686327\n",
      "GD iter. 205/499: loss=0.5320288527167707\n",
      "GD iter. 206/499: loss=0.5319994946837652\n",
      "GD iter. 207/499: loss=0.5319702505197035\n",
      "GD iter. 208/499: loss=0.5319411197764565\n",
      "GD iter. 209/499: loss=0.5319121020076728\n",
      "GD iter. 210/499: loss=0.5318832310647326\n",
      "GD iter. 211/499: loss=0.5318588985644777\n",
      "GD iter. 212/499: loss=0.53183020796775\n",
      "GD iter. 213/499: loss=0.5318016285962917\n",
      "GD iter. 214/499: loss=0.5317731881827239\n",
      "GD iter. 215/499: loss=0.5317461682832363\n",
      "GD iter. 216/499: loss=0.5317192642451052\n",
      "GD iter. 217/499: loss=0.5316911768157042\n",
      "GD iter. 218/499: loss=0.5316724831130515\n",
      "GD iter. 219/499: loss=0.5316390590292025\n",
      "GD iter. 220/499: loss=0.5316111630628814\n",
      "GD iter. 221/499: loss=0.5315835347278975\n",
      "GD iter. 222/499: loss=0.5315652261915811\n",
      "GD iter. 223/499: loss=0.5315322301489507\n",
      "GD iter. 224/499: loss=0.5315047633354885\n",
      "GD iter. 225/499: loss=0.5314788205975093\n",
      "GD iter. 226/499: loss=0.5314516915047729\n",
      "GD iter. 227/499: loss=0.5314339714039599\n",
      "GD iter. 228/499: loss=0.5314014617395837\n",
      "GD iter. 229/499: loss=0.5313725266761167\n",
      "GD iter. 230/499: loss=0.5313547617262651\n",
      "GD iter. 231/499: loss=0.5313255057656674\n",
      "GD iter. 232/499: loss=0.5312965567958404\n",
      "GD iter. 233/499: loss=0.5312794284172278\n",
      "GD iter. 234/499: loss=0.5312489651802068\n",
      "GD iter. 235/499: loss=0.531220318713461\n",
      "GD iter. 236/499: loss=0.5312017570199998\n",
      "GD iter. 237/499: loss=0.5311731030933908\n",
      "GD iter. 238/499: loss=0.5311449063475276\n",
      "GD iter. 239/499: loss=0.5311301077339838\n",
      "GD iter. 240/499: loss=0.5311001893272839\n",
      "GD iter. 241/499: loss=0.5310720556328838\n",
      "GD iter. 242/499: loss=0.5310481783709133\n",
      "GD iter. 243/499: loss=0.5310222431252944\n",
      "GD iter. 244/499: loss=0.5310079198975148\n",
      "GD iter. 245/499: loss=0.5309785213168066\n",
      "GD iter. 246/499: loss=0.5309526567949557\n",
      "GD iter. 247/499: loss=0.5309269017255344\n",
      "GD iter. 248/499: loss=0.5309036101165583\n",
      "GD iter. 249/499: loss=0.5308782301804227\n",
      "GD iter. 250/499: loss=0.5308644182413566\n",
      "GD iter. 251/499: loss=0.5308356593401863\n",
      "GD iter. 252/499: loss=0.5308103590351684\n",
      "GD iter. 253/499: loss=0.5307851667805958\n",
      "GD iter. 254/499: loss=0.530762849000706\n",
      "GD iter. 255/499: loss=0.5307379285235461\n",
      "GD iter. 256/499: loss=0.5307232974194949\n",
      "GD iter. 257/499: loss=0.5306969174042577\n",
      "GD iter. 258/499: loss=0.5306706953774162\n",
      "GD iter. 259/499: loss=0.530646103002719\n",
      "GD iter. 260/499: loss=0.5306288986723304\n",
      "GD iter. 261/499: loss=0.5306029238120353\n",
      "GD iter. 262/499: loss=0.5305785548632044\n",
      "GD iter. 263/499: loss=0.5305566328366897\n",
      "GD iter. 264/499: loss=0.530532658109802\n",
      "GD iter. 265/499: loss=0.5305202572772585\n",
      "GD iter. 266/499: loss=0.5304940062465374\n",
      "GD iter. 267/499: loss=0.5304679265724822\n",
      "GD iter. 268/499: loss=0.5304442170621604\n",
      "GD iter. 269/499: loss=0.5304336349384785\n",
      "GD iter. 270/499: loss=0.5304055476392556\n",
      "GD iter. 271/499: loss=0.5303798375750153\n",
      "GD iter. 272/499: loss=0.5303576551689269\n",
      "GD iter. 273/499: loss=0.5303411328163292\n",
      "GD iter. 274/499: loss=0.5303156625841915\n",
      "GD iter. 275/499: loss=0.5302950083061734\n",
      "GD iter. 276/499: loss=0.5302724655247938\n",
      "GD iter. 277/499: loss=0.5302534198530858\n",
      "GD iter. 278/499: loss=0.5302298498868872\n",
      "GD iter. 279/499: loss=0.5302149372078764\n",
      "GD iter. 280/499: loss=0.5301888778783705\n",
      "GD iter. 281/499: loss=0.5301681367329069\n",
      "GD iter. 282/499: loss=0.5301461756323387\n",
      "GD iter. 283/499: loss=0.5301366950479849\n",
      "GD iter. 284/499: loss=0.53010279090636\n",
      "GD iter. 285/499: loss=0.5300964933549532\n",
      "GD iter. 286/499: loss=0.5300622944864244\n",
      "GD iter. 287/499: loss=0.5300553577762903\n",
      "GD iter. 288/499: loss=0.5300213802128504\n",
      "GD iter. 289/499: loss=0.5300150494869484\n",
      "GD iter. 290/499: loss=0.5299810936376276\n",
      "GD iter. 291/499: loss=0.5299750596189724\n",
      "GD iter. 292/499: loss=0.5299411362014135\n",
      "GD iter. 293/499: loss=0.52993416332054\n",
      "GD iter. 294/499: loss=0.5299017739240228\n",
      "GD iter. 295/499: loss=0.5298890204017841\n",
      "GD iter. 296/499: loss=0.5298624900765164\n",
      "GD iter. 297/499: loss=0.5298518202666019\n",
      "GD iter. 298/499: loss=0.5298230445575635\n",
      "GD iter. 299/499: loss=0.5298167649909861\n",
      "GD iter. 300/499: loss=0.5297846090147237\n",
      "GD iter. 301/499: loss=0.5297725057950222\n",
      "GD iter. 302/499: loss=0.5297462586368278\n",
      "GD iter. 303/499: loss=0.5297352928062233\n",
      "GD iter. 304/499: loss=0.529708066638344\n",
      "GD iter. 305/499: loss=0.5297017621190184\n",
      "GD iter. 306/499: loss=0.5296700237024382\n",
      "GD iter. 307/499: loss=0.5296652807667283\n",
      "GD iter. 308/499: loss=0.5296323603690248\n",
      "GD iter. 309/499: loss=0.5296277209706446\n",
      "GD iter. 310/499: loss=0.5295950049677467\n",
      "GD iter. 311/499: loss=0.5295888385512004\n",
      "GD iter. 312/499: loss=0.529556978601835\n",
      "GD iter. 313/499: loss=0.5295531282105659\n",
      "GD iter. 314/499: loss=0.5295209456494673\n",
      "GD iter. 315/499: loss=0.5295148575015354\n",
      "GD iter. 316/499: loss=0.5294844017462842\n",
      "GD iter. 317/499: loss=0.5294798453178179\n",
      "GD iter. 318/499: loss=0.5294485427116069\n",
      "GD iter. 319/499: loss=0.5294401558485077\n",
      "GD iter. 320/499: loss=0.5294140292965661\n",
      "GD iter. 321/499: loss=0.5293951772895621\n",
      "GD iter. 322/499: loss=0.5293868414929543\n",
      "GD iter. 323/499: loss=0.529359799949221\n",
      "GD iter. 324/499: loss=0.5293515971486087\n",
      "GD iter. 325/499: loss=0.5293230917758327\n",
      "GD iter. 326/499: loss=0.5293209479035557\n",
      "GD iter. 327/499: loss=0.5292872298686846\n",
      "GD iter. 328/499: loss=0.5292860059497049\n",
      "GD iter. 329/499: loss=0.5292525335776441\n",
      "GD iter. 330/499: loss=0.5292528249035834\n",
      "GD iter. 331/499: loss=0.5292173414293587\n",
      "GD iter. 332/499: loss=0.5292181688018526\n",
      "GD iter. 333/499: loss=0.5291851364760827\n",
      "GD iter. 334/499: loss=0.529183287981903\n",
      "GD iter. 335/499: loss=0.5291508545770792\n",
      "GD iter. 336/499: loss=0.5291501170495966\n",
      "GD iter. 337/499: loss=0.5291176108307295\n",
      "GD iter. 338/499: loss=0.5291186168369278\n",
      "GD iter. 339/499: loss=0.5290815229626907\n",
      "GD iter. 340/499: loss=0.5290833007492659\n",
      "GD iter. 341/499: loss=0.5290512603320519\n",
      "GD iter. 342/499: loss=0.5290512013990732\n",
      "GD iter. 343/499: loss=0.5290169014720305\n",
      "GD iter. 344/499: loss=0.5290180506037288\n",
      "GD iter. 345/499: loss=0.5289853938035373\n",
      "GD iter. 346/499: loss=0.5289858335415237\n",
      "GD iter. 347/499: loss=0.5289532747571697\n",
      "GD iter. 348/499: loss=0.5289538773471303\n",
      "GD iter. 349/499: loss=0.5289204912059792\n",
      "GD iter. 350/499: loss=0.5289233308357848\n",
      "GD iter. 351/499: loss=0.5288871768242231\n",
      "GD iter. 352/499: loss=0.5288938213925877\n",
      "GD iter. 353/499: loss=0.5288548249392409\n",
      "GD iter. 354/499: loss=0.5288606063336825\n",
      "GD iter. 355/499: loss=0.5288257673876169\n",
      "GD iter. 356/499: loss=0.5288314495127621\n",
      "GD iter. 357/499: loss=0.528792931977894\n",
      "GD iter. 358/499: loss=0.5287988963225349\n",
      "GD iter. 359/499: loss=0.5287652085573616\n",
      "GD iter. 360/499: loss=0.52877099153856\n",
      "GD iter. 361/499: loss=0.5287315374332404\n",
      "GD iter. 362/499: loss=0.5287403777439226\n",
      "GD iter. 363/499: loss=0.5287030978030212\n",
      "GD iter. 364/499: loss=0.5287094350803252\n",
      "GD iter. 365/499: loss=0.5286731807487093\n",
      "GD iter. 366/499: loss=0.5286812568487731\n",
      "GD iter. 367/499: loss=0.5286426481850692\n",
      "GD iter. 368/499: loss=0.5286516502621293\n",
      "GD iter. 369/499: loss=0.5286142358913651\n",
      "GD iter. 370/499: loss=0.5286202934380072\n",
      "GD iter. 371/499: loss=0.5285857782733796\n",
      "GD iter. 372/499: loss=0.5285929043421297\n",
      "GD iter. 373/499: loss=0.5285569002480501\n",
      "GD iter. 374/499: loss=0.5285640235583599\n",
      "GD iter. 375/499: loss=0.5285279162496207\n",
      "GD iter. 376/499: loss=0.5285364185123484\n",
      "GD iter. 377/499: loss=0.5284985165376078\n",
      "GD iter. 378/499: loss=0.5285071050253141\n",
      "GD iter. 379/499: loss=0.5284723363003972\n",
      "GD iter. 380/499: loss=0.5284807102207899\n",
      "GD iter. 381/499: loss=0.5284411277080119\n",
      "GD iter. 382/499: loss=0.5284497043632267\n",
      "GD iter. 383/499: loss=0.5284183167987279\n",
      "GD iter. 384/499: loss=0.5284211258240179\n",
      "GD iter. 385/499: loss=0.5283911198064983\n",
      "GD iter. 386/499: loss=0.5283966027445517\n",
      "GD iter. 387/499: loss=0.5283592200317228\n",
      "GD iter. 388/499: loss=0.5283682582915888\n",
      "GD iter. 389/499: loss=0.528334201636764\n",
      "GD iter. 390/499: loss=0.5283435984049855\n",
      "GD iter. 391/499: loss=0.5283044947313724\n",
      "GD iter. 392/499: loss=0.5283128102794636\n",
      "GD iter. 393/499: loss=0.5282827720325842\n",
      "GD iter. 394/499: loss=0.5282876837111574\n",
      "GD iter. 395/499: loss=0.5282542393486034\n",
      "GD iter. 396/499: loss=0.5282636461977552\n",
      "GD iter. 397/499: loss=0.528226003713118\n",
      "GD iter. 398/499: loss=0.5282333520584264\n",
      "GD iter. 399/499: loss=0.528203961332045\n",
      "GD iter. 400/499: loss=0.5282103415354321\n",
      "GD iter. 401/499: loss=0.5281756759933395\n",
      "GD iter. 402/499: loss=0.5281842116050638\n",
      "GD iter. 403/499: loss=0.5281499554494722\n",
      "GD iter. 404/499: loss=0.5281582974591348\n",
      "GD iter. 405/499: loss=0.5281263013035629\n",
      "GD iter. 406/499: loss=0.5281383444610893\n",
      "GD iter. 407/499: loss=0.5280958868156097\n",
      "GD iter. 408/499: loss=0.5281040257872951\n",
      "GD iter. 409/499: loss=0.5280794926769181\n",
      "GD iter. 410/499: loss=0.5280795191229657\n",
      "GD iter. 411/499: loss=0.5280548828624188\n",
      "GD iter. 412/499: loss=0.528055230605136\n",
      "GD iter. 413/499: loss=0.5280288484837208\n",
      "GD iter. 414/499: loss=0.5280336476030354\n",
      "GD iter. 415/499: loss=0.5280012081795816\n",
      "GD iter. 416/499: loss=0.5280109065524676\n",
      "GD iter. 417/499: loss=0.5279754920221047\n",
      "GD iter. 418/499: loss=0.5279850747788156\n",
      "GD iter. 419/499: loss=0.5279541548671968\n",
      "GD iter. 420/499: loss=0.5279663783917463\n",
      "GD iter. 421/499: loss=0.5279239263816631\n",
      "GD iter. 422/499: loss=0.5279342469523255\n",
      "GD iter. 423/499: loss=0.5279107252468684\n",
      "GD iter. 424/499: loss=0.5279117374286603\n",
      "GD iter. 425/499: loss=0.527884476699611\n",
      "GD iter. 426/499: loss=0.5278966277024452\n",
      "GD iter. 427/499: loss=0.5278538111192332\n",
      "GD iter. 428/499: loss=0.5278638022421758\n",
      "GD iter. 429/499: loss=0.5278415812442205\n",
      "GD iter. 430/499: loss=0.5278372194251064\n",
      "GD iter. 431/499: loss=0.5278212625584616\n",
      "GD iter. 432/499: loss=0.5278131559277647\n",
      "GD iter. 433/499: loss=0.5277984553229365\n",
      "GD iter. 434/499: loss=0.5277904680628543\n",
      "GD iter. 435/499: loss=0.5277748419597745\n",
      "GD iter. 436/499: loss=0.5277691019834739\n",
      "GD iter. 437/499: loss=0.5277504111387207\n",
      "GD iter. 438/499: loss=0.5277543892613766\n",
      "GD iter. 439/499: loss=0.5277213067329038\n",
      "GD iter. 440/499: loss=0.5277337932433455\n",
      "GD iter. 441/499: loss=0.5276980478114339\n",
      "GD iter. 442/499: loss=0.5277124103692172\n",
      "GD iter. 443/499: loss=0.5276755070429604\n",
      "GD iter. 444/499: loss=0.5276884299771718\n",
      "GD iter. 445/499: loss=0.5276565090339885\n",
      "GD iter. 446/499: loss=0.5276681864141277\n",
      "GD iter. 447/499: loss=0.5276345346423228\n",
      "GD iter. 448/499: loss=0.5276465620597521\n",
      "GD iter. 449/499: loss=0.5276137353412966\n",
      "GD iter. 450/499: loss=0.5276251269903426\n",
      "GD iter. 451/499: loss=0.5275931759277622\n",
      "GD iter. 452/499: loss=0.5276059488986871\n",
      "GD iter. 453/499: loss=0.527569870086222\n",
      "GD iter. 454/499: loss=0.5275835604029471\n",
      "GD iter. 455/499: loss=0.5275521945230798\n",
      "GD iter. 456/499: loss=0.527564227316401\n",
      "GD iter. 457/499: loss=0.5275290601626192\n",
      "GD iter. 458/499: loss=0.5275431218075474\n",
      "GD iter. 459/499: loss=0.5275098904993677\n",
      "GD iter. 460/499: loss=0.527521809736125\n",
      "GD iter. 461/499: loss=0.5274916014892115\n",
      "GD iter. 462/499: loss=0.5275046299485517\n",
      "GD iter. 463/499: loss=0.5274677772015601\n",
      "GD iter. 464/499: loss=0.5274883796775252\n",
      "GD iter. 465/499: loss=0.5274474727876258\n",
      "GD iter. 466/499: loss=0.5274691918604497\n",
      "GD iter. 467/499: loss=0.527427860234865\n",
      "GD iter. 468/499: loss=0.5274501586171986\n",
      "GD iter. 469/499: loss=0.5274088333708695\n",
      "GD iter. 470/499: loss=0.5274267057509373\n",
      "GD iter. 471/499: loss=0.5273895437780592\n",
      "GD iter. 472/499: loss=0.5274078366066526\n",
      "GD iter. 473/499: loss=0.5273703966571462\n",
      "GD iter. 474/499: loss=0.5273885039167833\n",
      "GD iter. 475/499: loss=0.5273524878992457\n",
      "GD iter. 476/499: loss=0.5273678466927356\n",
      "GD iter. 477/499: loss=0.5273366231717402\n",
      "GD iter. 478/499: loss=0.5273480972573531\n",
      "GD iter. 479/499: loss=0.5273167471820623\n",
      "GD iter. 480/499: loss=0.527329245758345\n",
      "GD iter. 481/499: loss=0.5272967328186011\n",
      "GD iter. 482/499: loss=0.5273133051598143\n",
      "GD iter. 483/499: loss=0.5272755201964817\n",
      "GD iter. 484/499: loss=0.5272959115523713\n",
      "GD iter. 485/499: loss=0.5272569268997864\n",
      "GD iter. 486/499: loss=0.5272781244397665\n",
      "GD iter. 487/499: loss=0.5272388246122336\n",
      "GD iter. 488/499: loss=0.5272603721011965\n",
      "GD iter. 489/499: loss=0.5272205252729965\n",
      "GD iter. 490/499: loss=0.5272432541126879\n",
      "GD iter. 491/499: loss=0.5272030984212119\n",
      "GD iter. 492/499: loss=0.5272247344367844\n",
      "GD iter. 493/499: loss=0.5271855432964367\n",
      "GD iter. 494/499: loss=0.5272072609171581\n",
      "GD iter. 495/499: loss=0.5271683051551521\n",
      "GD iter. 496/499: loss=0.5271892240386896\n",
      "GD iter. 497/499: loss=0.5271527626587132\n",
      "GD iter. 498/499: loss=0.5271690922029371\n",
      "GD iter. 499/499: loss=0.5271372930220923\n",
      "The Accuracy is: 0.9113\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "GD iter. 0/499: loss=1.0158869453759323\n",
      "GD iter. 1/499: loss=1.0107719606775387\n",
      "GD iter. 2/499: loss=1.0056723674393713\n",
      "GD iter. 3/499: loss=1.0005881142531918\n",
      "GD iter. 4/499: loss=0.995519149895998\n",
      "GD iter. 5/499: loss=0.9904654233293225\n",
      "GD iter. 6/499: loss=0.9854268836985368\n",
      "GD iter. 7/499: loss=0.9804034803321556\n",
      "GD iter. 8/499: loss=0.9753951627411468\n",
      "GD iter. 9/499: loss=0.9704018806182404\n",
      "GD iter. 10/499: loss=0.9654235838372442\n",
      "GD iter. 11/499: loss=0.9604602224523586\n",
      "GD iter. 12/499: loss=0.9555117466974968\n",
      "GD iter. 13/499: loss=0.9505781069856051\n",
      "GD iter. 14/499: loss=0.9456592539079879\n",
      "GD iter. 15/499: loss=0.9407551382336349\n",
      "GD iter. 16/499: loss=0.9358657109085501\n",
      "GD iter. 17/499: loss=0.9309909230550832\n",
      "GD iter. 18/499: loss=0.9261307259712659\n",
      "GD iter. 19/499: loss=0.9212850711301482\n",
      "GD iter. 20/499: loss=0.9164539101791376\n",
      "GD iter. 21/499: loss=0.9116371949393425\n",
      "GD iter. 22/499: loss=0.9068348774049169\n",
      "GD iter. 23/499: loss=0.9020469097424074\n",
      "GD iter. 24/499: loss=0.8972732442901039\n",
      "GD iter. 25/499: loss=0.8925138335573918\n",
      "GD iter. 26/499: loss=0.8877686302241061\n",
      "GD iter. 27/499: loss=0.8830375871398911\n",
      "GD iter. 28/499: loss=0.8783206573235579\n",
      "GD iter. 29/499: loss=0.873617793962449\n",
      "GD iter. 30/499: loss=0.8689289504118016\n",
      "GD iter. 31/499: loss=0.8642540801941164\n",
      "GD iter. 32/499: loss=0.8595931369985264\n",
      "GD iter. 33/499: loss=0.8549460746801703\n",
      "GD iter. 34/499: loss=0.850312847259566\n",
      "GD iter. 35/499: loss=0.8456934089219881\n",
      "GD iter. 36/499: loss=0.8410877140168493\n",
      "GD iter. 37/499: loss=0.8364957170570793\n",
      "GD iter. 38/499: loss=0.8319173727185113\n",
      "GD iter. 39/499: loss=0.8273526358392685\n",
      "GD iter. 40/499: loss=0.8228014614191527\n",
      "GD iter. 41/499: loss=0.818263804619036\n",
      "GD iter. 42/499: loss=0.813739620760255\n",
      "GD iter. 43/499: loss=0.8092288653240065\n",
      "GD iter. 44/499: loss=0.8047314939507465\n",
      "GD iter. 45/499: loss=0.8002474624395907\n",
      "GD iter. 46/499: loss=0.7957767267477177\n",
      "GD iter. 47/499: loss=0.7913192429897754\n",
      "GD iter. 48/499: loss=0.7868749674372871\n",
      "GD iter. 49/499: loss=0.7824438565180636\n",
      "GD iter. 50/499: loss=0.7780258668156137\n",
      "GD iter. 51/499: loss=0.7736209550685602\n",
      "GD iter. 52/499: loss=0.7692290781700561\n",
      "GD iter. 53/499: loss=0.7648501931672046\n",
      "GD iter. 54/499: loss=0.760484257260479\n",
      "GD iter. 55/499: loss=0.7561312278031481\n",
      "GD iter. 56/499: loss=0.7517910623007014\n",
      "GD iter. 57/499: loss=0.7474637184102765\n",
      "GD iter. 58/499: loss=0.7431491539400904\n",
      "GD iter. 59/499: loss=0.7388473268488717\n",
      "GD iter. 60/499: loss=0.7345581952452955\n",
      "GD iter. 61/499: loss=0.73028171738742\n",
      "GD iter. 62/499: loss=0.7260178516821255\n",
      "GD iter. 63/499: loss=0.7217665566845568\n",
      "GD iter. 64/499: loss=0.7175277910975644\n",
      "GD iter. 65/499: loss=0.7133015137711524\n",
      "GD iter. 66/499: loss=0.7090876837019249\n",
      "GD iter. 67/499: loss=0.7048862600325362\n",
      "GD iter. 68/499: loss=0.7006972020511428\n",
      "GD iter. 69/499: loss=0.6965204691908576\n",
      "GD iter. 70/499: loss=0.6923560210292055\n",
      "GD iter. 71/499: loss=0.6882038172875828\n",
      "GD iter. 72/499: loss=0.6840638178307165\n",
      "GD iter. 73/499: loss=0.6799359826661276\n",
      "GD iter. 74/499: loss=0.6758202719435955\n",
      "GD iter. 75/499: loss=0.6717166459546238\n",
      "GD iter. 76/499: loss=0.6676250651319103\n",
      "GD iter. 77/499: loss=0.6635454900488174\n",
      "GD iter. 78/499: loss=0.6594778814188437\n",
      "GD iter. 79/499: loss=0.6554222000951008\n",
      "GD iter. 80/499: loss=0.6513784070697887\n",
      "GD iter. 81/499: loss=0.6473464634736747\n",
      "GD iter. 82/499: loss=0.6433263305755755\n",
      "GD iter. 83/499: loss=0.6393179697818393\n",
      "GD iter. 84/499: loss=0.63532134263583\n",
      "GD iter. 85/499: loss=0.6313364108174154\n",
      "GD iter. 86/499: loss=0.6273631361424556\n",
      "GD iter. 87/499: loss=0.6234014805622933\n",
      "GD iter. 88/499: loss=0.6194514061632475\n",
      "GD iter. 89/499: loss=0.6155128751661075\n",
      "GD iter. 90/499: loss=0.61158584992563\n",
      "GD iter. 91/499: loss=0.607670292930038\n",
      "GD iter. 92/499: loss=0.6037661668005205\n",
      "GD iter. 93/499: loss=0.5998734342907366\n",
      "GD iter. 94/499: loss=0.595992058286318\n",
      "GD iter. 95/499: loss=0.5921220018043767\n",
      "GD iter. 96/499: loss=0.5882632279930132\n",
      "GD iter. 97/499: loss=0.5844157001308259\n",
      "GD iter. 98/499: loss=0.5805793816264241\n",
      "GD iter. 99/499: loss=0.576754236017941\n",
      "GD iter. 100/499: loss=0.5729402269725501\n",
      "GD iter. 101/499: loss=0.5691373182859829\n",
      "GD iter. 102/499: loss=0.5653454738820484\n",
      "GD iter. 103/499: loss=0.5615646578121538\n",
      "GD iter. 104/499: loss=0.5577948342548288\n",
      "GD iter. 105/499: loss=0.5540359675152501\n",
      "GD iter. 106/499: loss=0.5502880220247683\n",
      "GD iter. 107/499: loss=0.5465511236501791\n",
      "GD iter. 108/499: loss=0.5428604363301504\n",
      "GD iter. 109/499: loss=0.539418464698189\n",
      "GD iter. 110/499: loss=0.5369257526782498\n",
      "GD iter. 111/499: loss=0.5358380506985283\n",
      "GD iter. 112/499: loss=0.5353741710299927\n",
      "GD iter. 113/499: loss=0.5351861359715878\n",
      "GD iter. 114/499: loss=0.5350720167047628\n",
      "GD iter. 115/499: loss=0.5349804020805722\n",
      "GD iter. 116/499: loss=0.5349001895487289\n",
      "GD iter. 117/499: loss=0.5348358210309945\n",
      "GD iter. 118/499: loss=0.5347772923356703\n",
      "GD iter. 119/499: loss=0.5347227198430042\n",
      "GD iter. 120/499: loss=0.5346757061635625\n",
      "GD iter. 121/499: loss=0.5346330716834766\n",
      "GD iter. 122/499: loss=0.5345906889298769\n",
      "GD iter. 123/499: loss=0.5345505868633633\n",
      "GD iter. 124/499: loss=0.5345115839648368\n",
      "GD iter. 125/499: loss=0.5344727528659565\n",
      "GD iter. 126/499: loss=0.5344352561118512\n",
      "GD iter. 127/499: loss=0.5343979235861177\n",
      "GD iter. 128/499: loss=0.5343595297798023\n",
      "GD iter. 129/499: loss=0.5343224935094947\n",
      "GD iter. 130/499: loss=0.5342844013057547\n",
      "GD iter. 131/499: loss=0.5342476589271948\n",
      "GD iter. 132/499: loss=0.5342098659456995\n",
      "GD iter. 133/499: loss=0.5341734151140655\n",
      "GD iter. 134/499: loss=0.5341359189933691\n",
      "GD iter. 135/499: loss=0.5340997573825434\n",
      "GD iter. 136/499: loss=0.5340625557799346\n",
      "GD iter. 137/499: loss=0.5340266845263347\n",
      "GD iter. 138/499: loss=0.5339901768965655\n",
      "GD iter. 139/499: loss=0.5339551403901956\n",
      "GD iter. 140/499: loss=0.5339177226272235\n",
      "GD iter. 141/499: loss=0.5338837510996166\n",
      "GD iter. 142/499: loss=0.5338466135754849\n",
      "GD iter. 143/499: loss=0.5338121313123751\n",
      "GD iter. 144/499: loss=0.5337765869284472\n",
      "GD iter. 145/499: loss=0.5337411829103433\n",
      "GD iter. 146/499: loss=0.5337059187005012\n",
      "GD iter. 147/499: loss=0.5336707937435793\n",
      "GD iter. 148/499: loss=0.5336358115893048\n",
      "GD iter. 149/499: loss=0.5336021630190183\n",
      "GD iter. 150/499: loss=0.5335674513846199\n",
      "GD iter. 151/499: loss=0.53353392996908\n",
      "GD iter. 152/499: loss=0.5334957361424745\n",
      "GD iter. 153/499: loss=0.5334646485292318\n",
      "GD iter. 154/499: loss=0.5334295421113687\n",
      "GD iter. 155/499: loss=0.5333977934445884\n",
      "GD iter. 156/499: loss=0.5333618919485594\n",
      "GD iter. 157/499: loss=0.5333304377814956\n",
      "GD iter. 158/499: loss=0.5332947885275025\n",
      "GD iter. 159/499: loss=0.5332615432967808\n",
      "GD iter. 160/499: loss=0.5332274429332496\n",
      "GD iter. 161/499: loss=0.5331954623742279\n",
      "GD iter. 162/499: loss=0.5331604385320301\n",
      "GD iter. 163/499: loss=0.5331308413194301\n",
      "GD iter. 164/499: loss=0.5330959892186904\n",
      "GD iter. 165/499: loss=0.5330625416363443\n",
      "GD iter. 166/499: loss=0.533031207350664\n",
      "GD iter. 167/499: loss=0.5329980105721351\n",
      "GD iter. 168/499: loss=0.5329670547258009\n",
      "GD iter. 169/499: loss=0.5329341121843597\n",
      "GD iter. 170/499: loss=0.5329024081299448\n",
      "GD iter. 171/499: loss=0.5328708298435545\n",
      "GD iter. 172/499: loss=0.5328393880582619\n",
      "GD iter. 173/499: loss=0.5328062235252066\n",
      "GD iter. 174/499: loss=0.5327789980734978\n",
      "GD iter. 175/499: loss=0.5327448206018106\n",
      "GD iter. 176/499: loss=0.5327139775369942\n",
      "GD iter. 177/499: loss=0.5326832644247944\n",
      "GD iter. 178/499: loss=0.5326514708167045\n",
      "GD iter. 179/499: loss=0.5326209963160836\n",
      "GD iter. 180/499: loss=0.5325906429053725\n",
      "GD iter. 181/499: loss=0.532560419294549\n",
      "GD iter. 182/499: loss=0.5325291096601728\n",
      "GD iter. 183/499: loss=0.5324991189978175\n",
      "GD iter. 184/499: loss=0.5324692488759216\n",
      "GD iter. 185/499: loss=0.5324383069408289\n",
      "GD iter. 186/499: loss=0.53240867472195\n",
      "GD iter. 187/499: loss=0.5323791602462653\n",
      "GD iter. 188/499: loss=0.5323497662600308\n",
      "GD iter. 189/499: loss=0.5323192962501418\n",
      "GD iter. 190/499: loss=0.532291242421075\n",
      "GD iter. 191/499: loss=0.5322603000991469\n",
      "GD iter. 192/499: loss=0.532232473647271\n",
      "GD iter. 193/499: loss=0.5322017687286448\n",
      "GD iter. 194/499: loss=0.5321741683892595\n",
      "GD iter. 195/499: loss=0.5321448863397552\n",
      "GD iter. 196/499: loss=0.5321157222391547\n",
      "GD iter. 197/499: loss=0.5320873703469486\n",
      "GD iter. 198/499: loss=0.5320584309977693\n",
      "GD iter. 199/499: loss=0.5320303077837898\n",
      "GD iter. 200/499: loss=0.532001593233455\n",
      "GD iter. 201/499: loss=0.5319730018269224\n",
      "GD iter. 202/499: loss=0.5319473649050894\n",
      "GD iter. 203/499: loss=0.5319177335163197\n",
      "GD iter. 204/499: loss=0.5318883133625045\n",
      "GD iter. 205/499: loss=0.5318648407580763\n",
      "GD iter. 206/499: loss=0.531831472151092\n",
      "GD iter. 207/499: loss=0.5318098987641466\n",
      "GD iter. 208/499: loss=0.5317776796473216\n",
      "GD iter. 209/499: loss=0.5317547435559731\n",
      "GD iter. 210/499: loss=0.5317235372553788\n",
      "GD iter. 211/499: loss=0.5316992852513245\n",
      "GD iter. 212/499: loss=0.5316690914976737\n",
      "GD iter. 213/499: loss=0.5316458515112507\n",
      "GD iter. 214/499: loss=0.5316151631338247\n",
      "GD iter. 215/499: loss=0.5315921023121527\n",
      "GD iter. 216/499: loss=0.5315623135978984\n",
      "GD iter. 217/499: loss=0.5315373345076739\n",
      "GD iter. 218/499: loss=0.5315116865599913\n",
      "GD iter. 219/499: loss=0.531483363159136\n",
      "GD iter. 220/499: loss=0.5314586964527989\n",
      "GD iter. 221/499: loss=0.5314333856035103\n",
      "GD iter. 222/499: loss=0.5314046998029466\n",
      "GD iter. 223/499: loss=0.5313830533259862\n",
      "GD iter. 224/499: loss=0.5313525914809087\n",
      "GD iter. 225/499: loss=0.5313321430434922\n",
      "GD iter. 226/499: loss=0.531300981177957\n",
      "GD iter. 227/499: loss=0.5312806546856098\n",
      "GD iter. 228/499: loss=0.5312516220980319\n",
      "GD iter. 229/499: loss=0.5312275876804983\n",
      "GD iter. 230/499: loss=0.5312015669154978\n",
      "GD iter. 231/499: loss=0.5311772391079247\n",
      "GD iter. 232/499: loss=0.5311507792328477\n",
      "GD iter. 233/499: loss=0.5311301539706573\n",
      "GD iter. 234/499: loss=0.531101685901867\n",
      "GD iter. 235/499: loss=0.5310790917060484\n",
      "GD iter. 236/499: loss=0.5310510907237016\n",
      "GD iter. 237/499: loss=0.5310317532711101\n",
      "GD iter. 238/499: loss=0.5310036762567947\n",
      "GD iter. 239/499: loss=0.5309825433601476\n",
      "GD iter. 240/499: loss=0.5309547598844897\n",
      "GD iter. 241/499: loss=0.5309359953167525\n",
      "GD iter. 242/499: loss=0.5309063092914879\n",
      "GD iter. 243/499: loss=0.5308876399129043\n",
      "GD iter. 244/499: loss=0.5308592283245845\n",
      "GD iter. 245/499: loss=0.5308396769066722\n",
      "GD iter. 246/499: loss=0.530812367873339\n",
      "GD iter. 247/499: loss=0.5307887897248468\n",
      "GD iter. 248/499: loss=0.5307677556167734\n",
      "GD iter. 249/499: loss=0.5307420498948999\n",
      "GD iter. 250/499: loss=0.530724864680123\n",
      "GD iter. 251/499: loss=0.5306951613520758\n",
      "GD iter. 252/499: loss=0.5306791787279023\n",
      "GD iter. 253/499: loss=0.5306495880141217\n",
      "GD iter. 254/499: loss=0.5306298339987564\n",
      "GD iter. 255/499: loss=0.530603617566827\n",
      "GD iter. 256/499: loss=0.530587919339855\n",
      "GD iter. 257/499: loss=0.5305578174768603\n",
      "GD iter. 258/499: loss=0.5305421153125806\n",
      "GD iter. 259/499: loss=0.5305140525284023\n",
      "GD iter. 260/499: loss=0.5304933149314444\n",
      "GD iter. 261/499: loss=0.5304727059653249\n",
      "GD iter. 262/499: loss=0.5304487239045226\n",
      "GD iter. 263/499: loss=0.5304282726565483\n",
      "GD iter. 264/499: loss=0.5304066403880227\n",
      "GD iter. 265/499: loss=0.5303838929733256\n",
      "GD iter. 266/499: loss=0.5303629354610211\n",
      "GD iter. 267/499: loss=0.5303383810131711\n",
      "GD iter. 268/499: loss=0.5303214897485087\n",
      "GD iter. 269/499: loss=0.5302969059817757\n",
      "GD iter. 270/499: loss=0.5302770931365579\n",
      "GD iter. 271/499: loss=0.5302537727300005\n",
      "GD iter. 272/499: loss=0.5302358023168859\n",
      "GD iter. 273/499: loss=0.5302109058388913\n",
      "GD iter. 274/499: loss=0.5301930426828485\n",
      "GD iter. 275/499: loss=0.5301683795076096\n",
      "GD iter. 276/499: loss=0.5301522316446738\n",
      "GD iter. 277/499: loss=0.5301267379451573\n",
      "GD iter. 278/499: loss=0.5301099176529209\n",
      "GD iter. 279/499: loss=0.5300855209564589\n",
      "GD iter. 280/499: loss=0.5300681597447322\n",
      "GD iter. 281/499: loss=0.5300449333774634\n",
      "GD iter. 282/499: loss=0.5300250064131615\n",
      "GD iter. 283/499: loss=0.5300051724940325\n",
      "GD iter. 284/499: loss=0.5299839201477042\n",
      "GD iter. 285/499: loss=0.5299659043315812\n",
      "GD iter. 286/499: loss=0.5299422164430418\n",
      "GD iter. 287/499: loss=0.5299293068008575\n",
      "GD iter. 288/499: loss=0.5299016485287202\n",
      "GD iter. 289/499: loss=0.5298887333613481\n",
      "GD iter. 290/499: loss=0.5298623487715589\n",
      "GD iter. 291/499: loss=0.5298485051446613\n",
      "GD iter. 292/499: loss=0.5298232228870001\n",
      "GD iter. 293/499: loss=0.5298088316076666\n",
      "GD iter. 294/499: loss=0.5297847115627833\n",
      "GD iter. 295/499: loss=0.5297674650366481\n",
      "GD iter. 296/499: loss=0.5297461633348804\n",
      "GD iter. 297/499: loss=0.5297267802096486\n",
      "GD iter. 298/499: loss=0.529711986048387\n",
      "GD iter. 299/499: loss=0.5296875062718525\n",
      "GD iter. 300/499: loss=0.5296806010848027\n",
      "GD iter. 301/499: loss=0.5296496960572901\n",
      "GD iter. 302/499: loss=0.5296396097168572\n",
      "GD iter. 303/499: loss=0.5296122146517953\n",
      "GD iter. 304/499: loss=0.5296022741177707\n",
      "GD iter. 305/499: loss=0.5295750291669529\n",
      "GD iter. 306/499: loss=0.5295640952098479\n",
      "GD iter. 307/499: loss=0.5295389145760068\n",
      "GD iter. 308/499: loss=0.5295222978217989\n",
      "GD iter. 309/499: loss=0.5295038889067806\n",
      "GD iter. 310/499: loss=0.5294866897906744\n",
      "GD iter. 311/499: loss=0.5294667170964557\n",
      "GD iter. 312/499: loss=0.5294496144049153\n",
      "GD iter. 313/499: loss=0.5294298577763584\n",
      "GD iter. 314/499: loss=0.5294174917447365\n",
      "GD iter. 315/499: loss=0.5293949233920657\n",
      "GD iter. 316/499: loss=0.5293791316847999\n",
      "GD iter. 317/499: loss=0.5293595941745659\n",
      "GD iter. 318/499: loss=0.529343949947338\n",
      "GD iter. 319/499: loss=0.5293240699477074\n",
      "GD iter. 320/499: loss=0.5293093703302901\n",
      "GD iter. 321/499: loss=0.5292876558485274\n",
      "GD iter. 322/499: loss=0.529280098637983\n",
      "GD iter. 323/499: loss=0.5292541854995335\n",
      "GD iter. 324/499: loss=0.529241170134916\n",
      "GD iter. 325/499: loss=0.529219510699193\n",
      "GD iter. 326/499: loss=0.5292095012560685\n",
      "GD iter. 327/499: loss=0.5291843625434396\n",
      "GD iter. 328/499: loss=0.5291761790795548\n",
      "GD iter. 329/499: loss=0.5291498451099501\n",
      "GD iter. 330/499: loss=0.5291491320451669\n",
      "GD iter. 331/499: loss=0.529115397989824\n",
      "GD iter. 332/499: loss=0.5291165588730251\n",
      "GD iter. 333/499: loss=0.5290825595589119\n",
      "GD iter. 334/499: loss=0.5290839953415095\n",
      "GD iter. 335/499: loss=0.529049343223001\n",
      "GD iter. 336/499: loss=0.5290521100618094\n",
      "GD iter. 337/499: loss=0.529016271151051\n",
      "GD iter. 338/499: loss=0.5290210636665481\n",
      "GD iter. 339/499: loss=0.5289841112986182\n",
      "GD iter. 340/499: loss=0.5289832907760308\n",
      "GD iter. 341/499: loss=0.5289541953009488\n",
      "GD iter. 342/499: loss=0.5289405149412845\n",
      "GD iter. 343/499: loss=0.5289265085794245\n",
      "GD iter. 344/499: loss=0.5289079268490711\n",
      "GD iter. 345/499: loss=0.5288940108399165\n",
      "GD iter. 346/499: loss=0.5288767413476483\n",
      "GD iter. 347/499: loss=0.528863004775294\n",
      "GD iter. 348/499: loss=0.5288446669175841\n",
      "GD iter. 349/499: loss=0.5288324842022133\n",
      "GD iter. 350/499: loss=0.528811861129473\n",
      "GD iter. 351/499: loss=0.528806282178659\n",
      "GD iter. 352/499: loss=0.5287792858346947\n",
      "GD iter. 353/499: loss=0.5287813355025363\n",
      "GD iter. 354/499: loss=0.5287483740279819\n",
      "GD iter. 355/499: loss=0.5287528430257501\n",
      "GD iter. 356/499: loss=0.528717766545391\n",
      "GD iter. 357/499: loss=0.5287230868034206\n",
      "GD iter. 358/499: loss=0.5286878956647322\n",
      "GD iter. 359/499: loss=0.5286910212305643\n",
      "GD iter. 360/499: loss=0.5286594780620681\n",
      "GD iter. 361/499: loss=0.5286583206464545\n",
      "GD iter. 362/499: loss=0.5286296977648912\n",
      "GD iter. 363/499: loss=0.5286306703617856\n",
      "GD iter. 364/499: loss=0.5286014675252328\n",
      "GD iter. 365/499: loss=0.5285938121867528\n",
      "GD iter. 366/499: loss=0.5285728680533005\n",
      "GD iter. 367/499: loss=0.528565535244332\n",
      "GD iter. 368/499: loss=0.5285423600353579\n",
      "GD iter. 369/499: loss=0.528543761279457\n",
      "GD iter. 370/499: loss=0.5285124906742489\n",
      "GD iter. 371/499: loss=0.5285143432840312\n",
      "GD iter. 372/499: loss=0.5284850281627091\n",
      "GD iter. 373/499: loss=0.5284864788882031\n",
      "GD iter. 374/499: loss=0.5284551862743805\n",
      "GD iter. 375/499: loss=0.5284602443734426\n",
      "GD iter. 376/499: loss=0.5284272814994473\n",
      "GD iter. 377/499: loss=0.5284326147642159\n",
      "GD iter. 378/499: loss=0.5283996077506726\n",
      "GD iter. 379/499: loss=0.5284016073865138\n",
      "GD iter. 380/499: loss=0.5283724899045015\n",
      "GD iter. 381/499: loss=0.5283750820073272\n",
      "GD iter. 382/499: loss=0.5283443982221566\n",
      "GD iter. 383/499: loss=0.5283500689082659\n",
      "GD iter. 384/499: loss=0.5283166206393317\n",
      "GD iter. 385/499: loss=0.528321703906757\n",
      "GD iter. 386/499: loss=0.5282899272618612\n",
      "GD iter. 387/499: loss=0.5282954771803984\n",
      "GD iter. 388/499: loss=0.5282634697109493\n",
      "GD iter. 389/499: loss=0.5282653028975376\n",
      "GD iter. 390/499: loss=0.5282363814686647\n",
      "GD iter. 391/499: loss=0.5282427428802152\n",
      "GD iter. 392/499: loss=0.5282093712911915\n",
      "GD iter. 393/499: loss=0.5282174493416022\n",
      "GD iter. 394/499: loss=0.5281836180702617\n",
      "GD iter. 395/499: loss=0.528189853282153\n",
      "GD iter. 396/499: loss=0.5281578565116273\n",
      "GD iter. 397/499: loss=0.5281623883748394\n",
      "GD iter. 398/499: loss=0.5281330966660807\n",
      "GD iter. 399/499: loss=0.5281330410971821\n",
      "GD iter. 400/499: loss=0.5281114157351485\n",
      "GD iter. 401/499: loss=0.5281044868560657\n",
      "GD iter. 402/499: loss=0.5280904244141152\n",
      "GD iter. 403/499: loss=0.5280764195021309\n",
      "GD iter. 404/499: loss=0.5280633938463212\n",
      "GD iter. 405/499: loss=0.5280504254083296\n",
      "GD iter. 406/499: loss=0.5280391237951039\n",
      "GD iter. 407/499: loss=0.5280253672207821\n",
      "GD iter. 408/499: loss=0.5280158068972026\n",
      "GD iter. 409/499: loss=0.5280008845375387\n",
      "GD iter. 410/499: loss=0.52799092174984\n",
      "GD iter. 411/499: loss=0.5279746881686352\n",
      "GD iter. 412/499: loss=0.5279680208237317\n",
      "GD iter. 413/499: loss=0.5279488621692129\n",
      "GD iter. 414/499: loss=0.5279486839861026\n",
      "GD iter. 415/499: loss=0.5279197360719159\n",
      "GD iter. 416/499: loss=0.5279292891751939\n",
      "GD iter. 417/499: loss=0.5278956694592022\n",
      "GD iter. 418/499: loss=0.5279049637388895\n",
      "GD iter. 419/499: loss=0.5278723410164705\n",
      "GD iter. 420/499: loss=0.5278823052159901\n",
      "GD iter. 421/499: loss=0.5278485556335087\n",
      "GD iter. 422/499: loss=0.5278581027670547\n",
      "GD iter. 423/499: loss=0.5278249568386039\n",
      "GD iter. 424/499: loss=0.5278353971955998\n",
      "GD iter. 425/499: loss=0.5278020357456079\n",
      "GD iter. 426/499: loss=0.5278115713024993\n",
      "GD iter. 427/499: loss=0.527779294259919\n",
      "GD iter. 428/499: loss=0.5277878266030472\n",
      "GD iter. 429/499: loss=0.5277579462207225\n",
      "GD iter. 430/499: loss=0.5277653890269252\n",
      "GD iter. 431/499: loss=0.5277348712583847\n",
      "GD iter. 432/499: loss=0.527745045989031\n",
      "GD iter. 433/499: loss=0.5277109551814596\n",
      "GD iter. 434/499: loss=0.5277243263588156\n",
      "GD iter. 435/499: loss=0.527687895190744\n",
      "GD iter. 436/499: loss=0.5276996176536\n",
      "GD iter. 437/499: loss=0.5276671950382665\n",
      "GD iter. 438/499: loss=0.5276773442212372\n",
      "GD iter. 439/499: loss=0.5276459575112593\n",
      "GD iter. 440/499: loss=0.5276572943537919\n",
      "GD iter. 441/499: loss=0.5276232341594637\n",
      "GD iter. 442/499: loss=0.5276356353787027\n",
      "GD iter. 443/499: loss=0.5276016969044794\n",
      "GD iter. 444/499: loss=0.5276141461260828\n",
      "GD iter. 445/499: loss=0.5275803595753858\n",
      "GD iter. 446/499: loss=0.5275935859634726\n",
      "GD iter. 447/499: loss=0.5275594649432453\n",
      "GD iter. 448/499: loss=0.527571877335801\n",
      "GD iter. 449/499: loss=0.5275394375636839\n",
      "GD iter. 450/499: loss=0.5275484267547274\n",
      "GD iter. 451/499: loss=0.527521574344875\n",
      "GD iter. 452/499: loss=0.5275246710243079\n",
      "GD iter. 453/499: loss=0.5275046106478993\n",
      "GD iter. 454/499: loss=0.5274967664342665\n",
      "GD iter. 455/499: loss=0.5274853548850409\n",
      "GD iter. 456/499: loss=0.5274745212393865\n",
      "GD iter. 457/499: loss=0.5274663545915612\n",
      "GD iter. 458/499: loss=0.5274539993371996\n",
      "GD iter. 459/499: loss=0.5274488730388293\n",
      "GD iter. 460/499: loss=0.5274322687719216\n",
      "GD iter. 461/499: loss=0.5274304015479956\n",
      "GD iter. 462/499: loss=0.5274120873477752\n",
      "GD iter. 463/499: loss=0.5274102331524931\n",
      "GD iter. 464/499: loss=0.5273913420519831\n",
      "GD iter. 465/499: loss=0.5273968617709245\n",
      "GD iter. 466/499: loss=0.5273664800852688\n",
      "GD iter. 467/499: loss=0.5273897640791619\n",
      "GD iter. 468/499: loss=0.5273463376802574\n",
      "GD iter. 469/499: loss=0.5273708860267329\n",
      "GD iter. 470/499: loss=0.5273264298329061\n",
      "GD iter. 471/499: loss=0.5273515229756003\n",
      "GD iter. 472/499: loss=0.5273076785812035\n",
      "GD iter. 473/499: loss=0.5273300987470233\n",
      "GD iter. 474/499: loss=0.5272888920953971\n",
      "GD iter. 475/499: loss=0.5273116940199996\n",
      "GD iter. 476/499: loss=0.527269801031541\n",
      "GD iter. 477/499: loss=0.5272944945390873\n",
      "GD iter. 478/499: loss=0.5272514844172647\n",
      "GD iter. 479/499: loss=0.5272762391663538\n",
      "GD iter. 480/499: loss=0.5272331247486141\n",
      "GD iter. 481/499: loss=0.5272581281257892\n",
      "GD iter. 482/499: loss=0.5272149095436495\n",
      "GD iter. 483/499: loss=0.5272401602712834\n",
      "GD iter. 484/499: loss=0.5271968609411967\n",
      "GD iter. 485/499: loss=0.527220295301367\n",
      "GD iter. 486/499: loss=0.5271785905608007\n",
      "GD iter. 487/499: loss=0.5272005830226273\n",
      "GD iter. 488/499: loss=0.527160739259408\n",
      "GD iter. 489/499: loss=0.527182560942503\n",
      "GD iter. 490/499: loss=0.5271430281069578\n",
      "GD iter. 491/499: loss=0.5271646794187678\n",
      "GD iter. 492/499: loss=0.5271254559945431\n",
      "GD iter. 493/499: loss=0.527146947354107\n",
      "GD iter. 494/499: loss=0.5271086697582049\n",
      "GD iter. 495/499: loss=0.5271290283703962\n",
      "GD iter. 496/499: loss=0.5270913088588788\n",
      "GD iter. 497/499: loss=0.5271112486307418\n",
      "GD iter. 498/499: loss=0.5270745774304305\n",
      "GD iter. 499/499: loss=0.5270917336900561\n",
      "The Accuracy is: 0.9048\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "GD iter. 0/499: loss=1.020593948954359\n",
      "GD iter. 1/499: loss=1.0154629655633662\n",
      "GD iter. 2/499: loss=1.010347428324977\n",
      "GD iter. 3/499: loss=1.005247285630878\n",
      "GD iter. 4/499: loss=1.0001624860587561\n",
      "GD iter. 5/499: loss=0.9950929783715923\n",
      "GD iter. 6/499: loss=0.990038711516962\n",
      "GD iter. 7/499: loss=0.9849996346263392\n",
      "GD iter. 8/499: loss=0.9799756970144006\n",
      "GD iter. 9/499: loss=0.9749668481783331\n",
      "GD iter. 10/499: loss=0.9699730377971467\n",
      "GD iter. 11/499: loss=0.9649942157309852\n",
      "GD iter. 12/499: loss=0.9600303320204446\n",
      "GD iter. 13/499: loss=0.9550813368858907\n",
      "GD iter. 14/499: loss=0.9501471807267802\n",
      "GD iter. 15/499: loss=0.9452278141209853\n",
      "GD iter. 16/499: loss=0.9403231878241208\n",
      "GD iter. 17/499: loss=0.9354332527688721\n",
      "GD iter. 18/499: loss=0.9305579600643279\n",
      "GD iter. 19/499: loss=0.9256972609953141\n",
      "GD iter. 20/499: loss=0.9208511070217316\n",
      "GD iter. 21/499: loss=0.9160194497778947\n",
      "GD iter. 22/499: loss=0.9112022410718752\n",
      "GD iter. 23/499: loss=0.9063994328848444\n",
      "GD iter. 24/499: loss=0.9016109773704226\n",
      "GD iter. 25/499: loss=0.8968368268540279\n",
      "GD iter. 26/499: loss=0.8920769338322284\n",
      "GD iter. 27/499: loss=0.887331250972098\n",
      "GD iter. 28/499: loss=0.8825997311105716\n",
      "GD iter. 29/499: loss=0.8778823272538074\n",
      "GD iter. 30/499: loss=0.8731789925765479\n",
      "GD iter. 31/499: loss=0.8684896804214837\n",
      "GD iter. 32/499: loss=0.8638143442986228\n",
      "GD iter. 33/499: loss=0.8591529378846591\n",
      "GD iter. 34/499: loss=0.854505415022344\n",
      "GD iter. 35/499: loss=0.8498717297198619\n",
      "GD iter. 36/499: loss=0.845251836150207\n",
      "GD iter. 37/499: loss=0.8406456886505619\n",
      "GD iter. 38/499: loss=0.8360532417216805\n",
      "GD iter. 39/499: loss=0.8314744500272713\n",
      "GD iter. 40/499: loss=0.8269092683933839\n",
      "GD iter. 41/499: loss=0.8223576518077991\n",
      "GD iter. 42/499: loss=0.8178195554194185\n",
      "GD iter. 43/499: loss=0.8132949345376597\n",
      "GD iter. 44/499: loss=0.8087837446318511\n",
      "GD iter. 45/499: loss=0.8042859413306314\n",
      "GD iter. 46/499: loss=0.7998014804213487\n",
      "GD iter. 47/499: loss=0.7953303178494653\n",
      "GD iter. 48/499: loss=0.7908724097179616\n",
      "GD iter. 49/499: loss=0.7864277122867442\n",
      "GD iter. 50/499: loss=0.7819961819720564\n",
      "GD iter. 51/499: loss=0.777577775345889\n",
      "GD iter. 52/499: loss=0.7731724491353958\n",
      "GD iter. 53/499: loss=0.7687801602223104\n",
      "GD iter. 54/499: loss=0.7644008656423649\n",
      "GD iter. 55/499: loss=0.7600345225847103\n",
      "GD iter. 56/499: loss=0.7556810883913414\n",
      "GD iter. 57/499: loss=0.7513405205565217\n",
      "GD iter. 58/499: loss=0.7470127767262112\n",
      "GD iter. 59/499: loss=0.7426978146974973\n",
      "GD iter. 60/499: loss=0.7383955924180262\n",
      "GD iter. 61/499: loss=0.734106067985438\n",
      "GD iter. 62/499: loss=0.7298291996468033\n",
      "GD iter. 63/499: loss=0.7255649457980616\n",
      "GD iter. 64/499: loss=0.7213132649834639\n",
      "GD iter. 65/499: loss=0.7170741158950139\n",
      "GD iter. 66/499: loss=0.7128474573719147\n",
      "GD iter. 67/499: loss=0.708633248400016\n",
      "GD iter. 68/499: loss=0.7044314481112636\n",
      "GD iter. 69/499: loss=0.7002420157831516\n",
      "GD iter. 70/499: loss=0.6960649108381759\n",
      "GD iter. 71/499: loss=0.6919000928432903\n",
      "GD iter. 72/499: loss=0.6877475215093649\n",
      "GD iter. 73/499: loss=0.6836071566906459\n",
      "GD iter. 74/499: loss=0.6794789583842182\n",
      "GD iter. 75/499: loss=0.67536288672947\n",
      "GD iter. 76/499: loss=0.6712589020075583\n",
      "GD iter. 77/499: loss=0.6671669646408787\n",
      "GD iter. 78/499: loss=0.6630870351925353\n",
      "GD iter. 79/499: loss=0.6590190743658126\n",
      "GD iter. 80/499: loss=0.6549630430036522\n",
      "GD iter. 81/499: loss=0.650918902088127\n",
      "GD iter. 82/499: loss=0.6468866127399219\n",
      "GD iter. 83/499: loss=0.6428661362178134\n",
      "GD iter. 84/499: loss=0.6388574339181526\n",
      "GD iter. 85/499: loss=0.6348604673743503\n",
      "GD iter. 86/499: loss=0.6308751982563627\n",
      "GD iter. 87/499: loss=0.6269015883701816\n",
      "GD iter. 88/499: loss=0.622939599657324\n",
      "GD iter. 89/499: loss=0.6189891941943244\n",
      "GD iter. 90/499: loss=0.6150503341922309\n",
      "GD iter. 91/499: loss=0.6111229819961003\n",
      "GD iter. 92/499: loss=0.6072071000844967\n",
      "GD iter. 93/499: loss=0.6033026510689934\n",
      "GD iter. 94/499: loss=0.5994095976936732\n",
      "GD iter. 95/499: loss=0.5955279028346345\n",
      "GD iter. 96/499: loss=0.591657529499496\n",
      "GD iter. 97/499: loss=0.5877984408269058\n",
      "GD iter. 98/499: loss=0.5839506000860507\n",
      "GD iter. 99/499: loss=0.5801139706761692\n",
      "GD iter. 100/499: loss=0.5762885161260634\n",
      "GD iter. 101/499: loss=0.5724742000936169\n",
      "GD iter. 102/499: loss=0.56867098636531\n",
      "GD iter. 103/499: loss=0.5648788388557413\n",
      "GD iter. 104/499: loss=0.561097721607147\n",
      "GD iter. 105/499: loss=0.5573275987889256\n",
      "GD iter. 106/499: loss=0.5535684346971612\n",
      "GD iter. 107/499: loss=0.5498201937541518\n",
      "GD iter. 108/499: loss=0.5460828405079368\n",
      "GD iter. 109/499: loss=0.5424056982126849\n",
      "GD iter. 110/499: loss=0.5392964805171638\n",
      "GD iter. 111/499: loss=0.5374080994675795\n",
      "GD iter. 112/499: loss=0.5364804581686705\n",
      "GD iter. 113/499: loss=0.5360079282631127\n",
      "GD iter. 114/499: loss=0.5357292780222422\n",
      "GD iter. 115/499: loss=0.5355622393305072\n",
      "GD iter. 116/499: loss=0.5354354095868707\n",
      "GD iter. 117/499: loss=0.535353571181839\n",
      "GD iter. 118/499: loss=0.5352893497384946\n",
      "GD iter. 119/499: loss=0.5352387672880342\n",
      "GD iter. 120/499: loss=0.5351931398233104\n",
      "GD iter. 121/499: loss=0.5351493195973269\n",
      "GD iter. 122/499: loss=0.5351056668454005\n",
      "GD iter. 123/499: loss=0.5350622243993334\n",
      "GD iter. 124/499: loss=0.5350200002161323\n",
      "GD iter. 125/499: loss=0.5349779445956361\n",
      "GD iter. 126/499: loss=0.5349386649367951\n",
      "GD iter. 127/499: loss=0.5348970296586748\n",
      "GD iter. 128/499: loss=0.5348580489828898\n",
      "GD iter. 129/499: loss=0.5348192237014531\n",
      "GD iter. 130/499: loss=0.5347805531941539\n",
      "GD iter. 131/499: loss=0.5347420368432576\n",
      "GD iter. 132/499: loss=0.5347036878208227\n",
      "GD iter. 133/499: loss=0.5346629891293726\n",
      "GD iter. 134/499: loss=0.5346249364919813\n",
      "GD iter. 135/499: loss=0.5345870355548717\n",
      "GD iter. 136/499: loss=0.5345492857125661\n",
      "GD iter. 137/499: loss=0.5345116863620042\n",
      "GD iter. 138/499: loss=0.5344742459130103\n",
      "GD iter. 139/499: loss=0.5344344694893896\n",
      "GD iter. 140/499: loss=0.5343983108735628\n",
      "GD iter. 141/499: loss=0.5343613104361861\n",
      "GD iter. 142/499: loss=0.5343244639599726\n",
      "GD iter. 143/499: loss=0.534285291264134\n",
      "GD iter. 144/499: loss=0.5342497174138895\n",
      "GD iter. 145/499: loss=0.5342133074444984\n",
      "GD iter. 146/499: loss=0.5341760544778189\n",
      "GD iter. 147/499: loss=0.5341399384088296\n",
      "GD iter. 148/499: loss=0.5341029802710711\n",
      "GD iter. 149/499: loss=0.5340681925112231\n",
      "GD iter. 150/499: loss=0.5340290301394329\n",
      "GD iter. 151/499: loss=0.5339944540526106\n",
      "GD iter. 152/499: loss=0.5339600740022393\n",
      "GD iter. 153/499: loss=0.533921361702207\n",
      "GD iter. 154/499: loss=0.5338872150386768\n",
      "GD iter. 155/499: loss=0.5338522460030586\n",
      "GD iter. 156/499: loss=0.5338164271006878\n",
      "GD iter. 157/499: loss=0.533782764336693\n",
      "GD iter. 158/499: loss=0.5337464039417101\n",
      "GD iter. 159/499: loss=0.5337120074858356\n",
      "GD iter. 160/499: loss=0.5336769472839393\n",
      "GD iter. 161/499: loss=0.5336420274996746\n",
      "GD iter. 162/499: loss=0.5336082499123863\n",
      "GD iter. 163/499: loss=0.533572607494015\n",
      "GD iter. 164/499: loss=0.5335390916835163\n",
      "GD iter. 165/499: loss=0.5335047187552518\n",
      "GD iter. 166/499: loss=0.533471481532231\n",
      "GD iter. 167/499: loss=0.5334373734001562\n",
      "GD iter. 168/499: loss=0.5334034022561858\n",
      "GD iter. 169/499: loss=0.5333705778565173\n",
      "GD iter. 170/499: loss=0.5333363725597995\n",
      "GD iter. 171/499: loss=0.5333033119490086\n",
      "GD iter. 172/499: loss=0.5332693818940166\n",
      "GD iter. 173/499: loss=0.5332365810654576\n",
      "GD iter. 174/499: loss=0.5332039119866104\n",
      "GD iter. 175/499: loss=0.5331703773735282\n",
      "GD iter. 176/499: loss=0.5331379676870079\n",
      "GD iter. 177/499: loss=0.5331056867572569\n",
      "GD iter. 178/499: loss=0.5330747703714391\n",
      "GD iter. 179/499: loss=0.5330407357934603\n",
      "GD iter. 180/499: loss=0.5330114981275256\n",
      "GD iter. 181/499: loss=0.5329767805515084\n",
      "GD iter. 182/499: loss=0.5329463548500029\n",
      "GD iter. 183/499: loss=0.5329138282250181\n",
      "GD iter. 184/499: loss=0.532882648969423\n",
      "GD iter. 185/499: loss=0.5328503893820228\n",
      "GD iter. 186/499: loss=0.53282087223053\n",
      "GD iter. 187/499: loss=0.5327878564972026\n",
      "GD iter. 188/499: loss=0.5327595147932513\n",
      "GD iter. 189/499: loss=0.5327258389975476\n",
      "GD iter. 190/499: loss=0.5326981396415155\n",
      "GD iter. 191/499: loss=0.5326632051116501\n",
      "GD iter. 192/499: loss=0.5326366131985629\n",
      "GD iter. 193/499: loss=0.5326043236744684\n",
      "GD iter. 194/499: loss=0.5325756785418387\n",
      "GD iter. 195/499: loss=0.5325446408767813\n",
      "GD iter. 196/499: loss=0.5325150255433676\n",
      "GD iter. 197/499: loss=0.532485537589007\n",
      "GD iter. 198/499: loss=0.5324538974439053\n",
      "GD iter. 199/499: loss=0.5324258213141421\n",
      "GD iter. 200/499: loss=0.5323954009257956\n",
      "GD iter. 201/499: loss=0.5323676912022506\n",
      "GD iter. 202/499: loss=0.5323318418734962\n",
      "GD iter. 203/499: loss=0.5323085022573302\n",
      "GD iter. 204/499: loss=0.5322755873319608\n",
      "GD iter. 205/499: loss=0.5322520340506577\n",
      "GD iter. 206/499: loss=0.5322154573934424\n",
      "GD iter. 207/499: loss=0.5321932907838138\n",
      "GD iter. 208/499: loss=0.5321614721213125\n",
      "GD iter. 209/499: loss=0.5321345890578905\n",
      "GD iter. 210/499: loss=0.532106541745992\n",
      "GD iter. 211/499: loss=0.5320762204267573\n",
      "GD iter. 212/499: loss=0.5320511473985083\n",
      "GD iter. 213/499: loss=0.5320181847301736\n",
      "GD iter. 214/499: loss=0.5319965492106088\n",
      "GD iter. 215/499: loss=0.5319599670398757\n",
      "GD iter. 216/499: loss=0.5319386305373888\n",
      "GD iter. 217/499: loss=0.5319089806008915\n",
      "GD iter. 218/499: loss=0.5318830969133294\n",
      "GD iter. 219/499: loss=0.5318528539172582\n",
      "GD iter. 220/499: loss=0.531829513762946\n",
      "GD iter. 221/499: loss=0.531798110674867\n",
      "GD iter. 222/499: loss=0.5317749508151974\n",
      "GD iter. 223/499: loss=0.5317438016302025\n",
      "GD iter. 224/499: loss=0.5317219276554651\n",
      "GD iter. 225/499: loss=0.5316891387276894\n",
      "GD iter. 226/499: loss=0.5316695148468147\n",
      "GD iter. 227/499: loss=0.5316319119099919\n",
      "GD iter. 228/499: loss=0.5316127929017843\n",
      "GD iter. 229/499: loss=0.5315858074728768\n",
      "GD iter. 230/499: loss=0.5315598364901997\n",
      "GD iter. 231/499: loss=0.531533079413273\n",
      "GD iter. 232/499: loss=0.5315096997757495\n",
      "GD iter. 233/499: loss=0.5314807680373541\n",
      "GD iter. 234/499: loss=0.5314575698881958\n",
      "GD iter. 235/499: loss=0.5314281332943818\n",
      "GD iter. 236/499: loss=0.5314065270346208\n",
      "GD iter. 237/499: loss=0.5313753935529854\n",
      "GD iter. 238/499: loss=0.5313586754632222\n",
      "GD iter. 239/499: loss=0.5313186257347228\n",
      "GD iter. 240/499: loss=0.5313013488029692\n",
      "GD iter. 241/499: loss=0.5312776993748257\n",
      "GD iter. 242/499: loss=0.5312512614234244\n",
      "GD iter. 243/499: loss=0.5312283832930241\n",
      "GD iter. 244/499: loss=0.5311983490654681\n",
      "GD iter. 245/499: loss=0.5311836024017061\n",
      "GD iter. 246/499: loss=0.5311430456393232\n",
      "GD iter. 247/499: loss=0.5311274168546644\n",
      "GD iter. 248/499: loss=0.5311039894901933\n",
      "GD iter. 249/499: loss=0.5310781835989551\n",
      "GD iter. 250/499: loss=0.5310585163075481\n",
      "GD iter. 251/499: loss=0.5310257336775412\n",
      "GD iter. 252/499: loss=0.5310109803441843\n",
      "GD iter. 253/499: loss=0.5309773317296974\n",
      "GD iter. 254/499: loss=0.5309626362175618\n",
      "GD iter. 255/499: loss=0.5309301167838598\n",
      "GD iter. 256/499: loss=0.530914334263072\n",
      "GD iter. 257/499: loss=0.5308829707282804\n",
      "GD iter. 258/499: loss=0.530866535105492\n",
      "GD iter. 259/499: loss=0.5308365514224974\n",
      "GD iter. 260/499: loss=0.5308205016377655\n",
      "GD iter. 261/499: loss=0.5307894883818057\n",
      "GD iter. 262/499: loss=0.5307737944509013\n",
      "GD iter. 263/499: loss=0.5307423179567035\n",
      "GD iter. 264/499: loss=0.5307275825380128\n",
      "GD iter. 265/499: loss=0.5306973453831627\n",
      "GD iter. 266/499: loss=0.5306805134816879\n",
      "GD iter. 267/499: loss=0.5306556572352588\n",
      "GD iter. 268/499: loss=0.5306378664465532\n",
      "GD iter. 269/499: loss=0.5306036530562369\n",
      "GD iter. 270/499: loss=0.5305917975821746\n",
      "GD iter. 271/499: loss=0.5305606685972912\n",
      "GD iter. 272/499: loss=0.5305454680465989\n",
      "GD iter. 273/499: loss=0.5305192336851787\n",
      "GD iter. 274/499: loss=0.5305038025018032\n",
      "GD iter. 275/499: loss=0.530470170498922\n",
      "GD iter. 276/499: loss=0.5304588102678784\n",
      "GD iter. 277/499: loss=0.5304293301992901\n",
      "GD iter. 278/499: loss=0.5304144492288823\n",
      "GD iter. 279/499: loss=0.5303864477295812\n",
      "GD iter. 280/499: loss=0.5303719364988282\n",
      "GD iter. 281/499: loss=0.5303409615067308\n",
      "GD iter. 282/499: loss=0.5303284958076988\n",
      "GD iter. 283/499: loss=0.5303007905716048\n",
      "GD iter. 284/499: loss=0.530286678588676\n",
      "GD iter. 285/499: loss=0.5302563040274942\n",
      "GD iter. 286/499: loss=0.5302443805785507\n",
      "GD iter. 287/499: loss=0.5302132403833225\n",
      "GD iter. 288/499: loss=0.5302011085236477\n",
      "GD iter. 289/499: loss=0.5301779190471146\n",
      "GD iter. 290/499: loss=0.5301612991940515\n",
      "GD iter. 291/499: loss=0.5301293040854602\n",
      "GD iter. 292/499: loss=0.5301172575193435\n",
      "GD iter. 293/499: loss=0.5300956366462\n",
      "GD iter. 294/499: loss=0.5300793244447362\n",
      "GD iter. 295/499: loss=0.5300468949945134\n",
      "GD iter. 296/499: loss=0.5300388018633645\n",
      "GD iter. 297/499: loss=0.5300081473829892\n",
      "GD iter. 298/499: loss=0.529996955122609\n",
      "GD iter. 299/499: loss=0.5299713972077466\n",
      "GD iter. 300/499: loss=0.5299583714499734\n",
      "GD iter. 301/499: loss=0.529926522514579\n",
      "GD iter. 302/499: loss=0.5299176564650426\n",
      "GD iter. 303/499: loss=0.5298915694992089\n",
      "GD iter. 304/499: loss=0.5298786342984544\n",
      "GD iter. 305/499: loss=0.5298515929480422\n",
      "GD iter. 306/499: loss=0.5298386209889066\n",
      "GD iter. 307/499: loss=0.5298175618722748\n",
      "GD iter. 308/499: loss=0.5298014811301004\n",
      "GD iter. 309/499: loss=0.5297704448105376\n",
      "GD iter. 310/499: loss=0.5297621022075302\n",
      "GD iter. 311/499: loss=0.5297375805607529\n",
      "GD iter. 312/499: loss=0.5297246825990112\n",
      "GD iter. 313/499: loss=0.5296958785948656\n",
      "GD iter. 314/499: loss=0.5296889034682213\n",
      "GD iter. 315/499: loss=0.5296551375618895\n",
      "GD iter. 316/499: loss=0.5296489160483661\n",
      "GD iter. 317/499: loss=0.5296275914412253\n",
      "GD iter. 318/499: loss=0.5296121192157021\n",
      "GD iter. 319/499: loss=0.5295856025393546\n",
      "GD iter. 320/499: loss=0.5295769368383124\n",
      "GD iter. 321/499: loss=0.5295456581667317\n",
      "GD iter. 322/499: loss=0.5295396283259882\n",
      "GD iter. 323/499: loss=0.5295094803743632\n",
      "GD iter. 324/499: loss=0.529503758876876\n",
      "GD iter. 325/499: loss=0.5294735969279679\n",
      "GD iter. 326/499: loss=0.5294663207132031\n",
      "GD iter. 327/499: loss=0.5294466799577955\n",
      "GD iter. 328/499: loss=0.5294292454280535\n",
      "GD iter. 329/499: loss=0.5294104922031351\n",
      "GD iter. 330/499: loss=0.5293945352699437\n",
      "GD iter. 331/499: loss=0.5293723113243995\n",
      "GD iter. 332/499: loss=0.5293621467418319\n",
      "GD iter. 333/499: loss=0.5293282046408399\n",
      "GD iter. 334/499: loss=0.5293244517114625\n",
      "GD iter. 335/499: loss=0.5293021682663955\n",
      "GD iter. 336/499: loss=0.529292075197569\n",
      "GD iter. 337/499: loss=0.5292604101334059\n",
      "GD iter. 338/499: loss=0.5292568067142706\n",
      "GD iter. 339/499: loss=0.5292288170824343\n",
      "GD iter. 340/499: loss=0.529222986191991\n",
      "GD iter. 341/499: loss=0.529194096457587\n",
      "GD iter. 342/499: loss=0.5291891712022955\n",
      "GD iter. 343/499: loss=0.5291604541031756\n",
      "GD iter. 344/499: loss=0.5291556299927331\n",
      "GD iter. 345/499: loss=0.5291271446634964\n",
      "GD iter. 346/499: loss=0.5291224820884975\n",
      "GD iter. 347/499: loss=0.5290942466798132\n",
      "GD iter. 348/499: loss=0.5290898557686117\n",
      "GD iter. 349/499: loss=0.529058625919946\n",
      "GD iter. 350/499: loss=0.5290587605056079\n",
      "GD iter. 351/499: loss=0.5290258004351619\n",
      "GD iter. 352/499: loss=0.529025698257709\n",
      "GD iter. 353/499: loss=0.5289940370320219\n",
      "GD iter. 354/499: loss=0.528994705977736\n",
      "GD iter. 355/499: loss=0.5289609758012477\n",
      "GD iter. 356/499: loss=0.5289618926010816\n",
      "GD iter. 357/499: loss=0.5289336598028771\n",
      "GD iter. 358/499: loss=0.5289276912811716\n",
      "GD iter. 359/499: loss=0.5289091547882032\n",
      "GD iter. 360/499: loss=0.528895610688527\n",
      "GD iter. 361/499: loss=0.528876396277043\n",
      "GD iter. 362/499: loss=0.528862868586788\n",
      "GD iter. 363/499: loss=0.5288469531989791\n",
      "GD iter. 364/499: loss=0.5288311119474645\n",
      "GD iter. 365/499: loss=0.5288180107925156\n",
      "GD iter. 366/499: loss=0.5287937411018779\n",
      "GD iter. 367/499: loss=0.5287935056367835\n",
      "GD iter. 368/499: loss=0.528758585524159\n",
      "GD iter. 369/499: loss=0.5287600112519905\n",
      "GD iter. 370/499: loss=0.528728388701956\n",
      "GD iter. 371/499: loss=0.5287298600793819\n",
      "GD iter. 372/499: loss=0.5286992126668655\n",
      "GD iter. 373/499: loss=0.5287017585238585\n",
      "GD iter. 374/499: loss=0.5286694919232049\n",
      "GD iter. 375/499: loss=0.5286709626520671\n",
      "GD iter. 376/499: loss=0.528640320093292\n",
      "GD iter. 377/499: loss=0.5286401376816937\n",
      "GD iter. 378/499: loss=0.5286142172808004\n",
      "GD iter. 379/499: loss=0.5286134669827346\n",
      "GD iter. 380/499: loss=0.5285817288608347\n",
      "GD iter. 381/499: loss=0.5285833815195348\n",
      "GD iter. 382/499: loss=0.5285554923068454\n",
      "GD iter. 383/499: loss=0.5285567610632854\n",
      "GD iter. 384/499: loss=0.5285235364410464\n",
      "GD iter. 385/499: loss=0.5285265535925479\n",
      "GD iter. 386/499: loss=0.5284973194961959\n",
      "GD iter. 387/499: loss=0.528498411292018\n",
      "GD iter. 388/499: loss=0.528470032608929\n",
      "GD iter. 389/499: loss=0.5284710558054556\n",
      "GD iter. 390/499: loss=0.5284394051866266\n",
      "GD iter. 391/499: loss=0.5284409483935871\n",
      "GD iter. 392/499: loss=0.5284183861859466\n",
      "GD iter. 393/499: loss=0.528412835002931\n",
      "GD iter. 394/499: loss=0.5283924531783887\n",
      "GD iter. 395/499: loss=0.5283848679620895\n",
      "GD iter. 396/499: loss=0.528369324963646\n",
      "GD iter. 397/499: loss=0.5283558549664714\n",
      "GD iter. 398/499: loss=0.5283424432127267\n",
      "GD iter. 399/499: loss=0.5283282183879362\n",
      "GD iter. 400/499: loss=0.5283187833766809\n",
      "GD iter. 401/499: loss=0.5282911189754527\n",
      "GD iter. 402/499: loss=0.528292523013168\n",
      "GD iter. 403/499: loss=0.5282648120897959\n",
      "GD iter. 404/499: loss=0.5282664729024662\n",
      "GD iter. 405/499: loss=0.528237868420985\n",
      "GD iter. 406/499: loss=0.5282389576556139\n",
      "GD iter. 407/499: loss=0.5282148704068088\n",
      "GD iter. 408/499: loss=0.5282159319878783\n",
      "GD iter. 409/499: loss=0.5281843977571318\n",
      "GD iter. 410/499: loss=0.5281871440473742\n",
      "GD iter. 411/499: loss=0.5281637369355576\n",
      "GD iter. 412/499: loss=0.5281653744113263\n",
      "GD iter. 413/499: loss=0.5281322351252097\n",
      "GD iter. 414/499: loss=0.5281363974758199\n",
      "GD iter. 415/499: loss=0.528112020161668\n",
      "GD iter. 416/499: loss=0.5281127643887616\n",
      "GD iter. 417/499: loss=0.5280831246825318\n",
      "GD iter. 418/499: loss=0.5280864487808812\n",
      "GD iter. 419/499: loss=0.5280649700094042\n",
      "GD iter. 420/499: loss=0.5280635767932308\n",
      "GD iter. 421/499: loss=0.5280330074805006\n",
      "GD iter. 422/499: loss=0.5280370783188016\n",
      "GD iter. 423/499: loss=0.5280142520862214\n",
      "GD iter. 424/499: loss=0.5280154456994317\n",
      "GD iter. 425/499: loss=0.5279838773757928\n",
      "GD iter. 426/499: loss=0.5279878052960476\n",
      "GD iter. 427/499: loss=0.5279707981995974\n",
      "GD iter. 428/499: loss=0.527964463086334\n",
      "GD iter. 429/499: loss=0.5279416330425101\n",
      "GD iter. 430/499: loss=0.5279428469077272\n",
      "GD iter. 431/499: loss=0.5279119193471149\n",
      "GD iter. 432/499: loss=0.5279167598502121\n",
      "GD iter. 433/499: loss=0.5278974356705262\n",
      "GD iter. 434/499: loss=0.5278932984334307\n",
      "GD iter. 435/499: loss=0.5278697460543093\n",
      "GD iter. 436/499: loss=0.5278731419845313\n",
      "GD iter. 437/499: loss=0.5278408700291551\n",
      "GD iter. 438/499: loss=0.5278460961986362\n",
      "GD iter. 439/499: loss=0.52782970298989\n",
      "GD iter. 440/499: loss=0.5278238204161483\n",
      "GD iter. 441/499: loss=0.5278033568620771\n",
      "GD iter. 442/499: loss=0.5278027656838368\n",
      "GD iter. 443/499: loss=0.527773779465846\n",
      "GD iter. 444/499: loss=0.5277795052204619\n",
      "GD iter. 445/499: loss=0.5277525068860578\n",
      "GD iter. 446/499: loss=0.5277566254180778\n",
      "GD iter. 447/499: loss=0.5277334796552496\n",
      "GD iter. 448/499: loss=0.5277374938820846\n",
      "GD iter. 449/499: loss=0.527705752097929\n",
      "GD iter. 450/499: loss=0.5277127306478581\n",
      "GD iter. 451/499: loss=0.5276891552154601\n",
      "GD iter. 452/499: loss=0.5276929009043815\n",
      "GD iter. 453/499: loss=0.5276623828588553\n",
      "GD iter. 454/499: loss=0.5276687055797519\n",
      "GD iter. 455/499: loss=0.5276518125877141\n",
      "GD iter. 456/499: loss=0.5276490692407994\n",
      "GD iter. 457/499: loss=0.527620521932544\n",
      "GD iter. 458/499: loss=0.5276285974210874\n",
      "GD iter. 459/499: loss=0.527598264622588\n",
      "GD iter. 460/499: loss=0.5276048998224673\n",
      "GD iter. 461/499: loss=0.5275873048085042\n",
      "GD iter. 462/499: loss=0.5275842393479351\n",
      "GD iter. 463/499: loss=0.5275601590865076\n",
      "GD iter. 464/499: loss=0.5275642480565483\n",
      "GD iter. 465/499: loss=0.5275351049993781\n",
      "GD iter. 466/499: loss=0.5275453738472574\n",
      "GD iter. 467/499: loss=0.5275125443045722\n",
      "GD iter. 468/499: loss=0.5275254894653374\n",
      "GD iter. 469/499: loss=0.5274924035115036\n",
      "GD iter. 470/499: loss=0.5275045619829328\n",
      "GD iter. 471/499: loss=0.527474719729595\n",
      "GD iter. 472/499: loss=0.5274864297690911\n",
      "GD iter. 473/499: loss=0.5274521749528638\n",
      "GD iter. 474/499: loss=0.5274640325590286\n",
      "GD iter. 475/499: loss=0.5274398440561688\n",
      "GD iter. 476/499: loss=0.5274417766069933\n",
      "GD iter. 477/499: loss=0.527429578720894\n",
      "GD iter. 478/499: loss=0.5274194361092188\n",
      "GD iter. 479/499: loss=0.5274105259957731\n",
      "GD iter. 480/499: loss=0.5274004506737893\n",
      "GD iter. 481/499: loss=0.527390420291111\n",
      "GD iter. 482/499: loss=0.527381682031211\n",
      "GD iter. 483/499: loss=0.5273682030385797\n",
      "GD iter. 484/499: loss=0.5273656028408334\n",
      "GD iter. 485/499: loss=0.5273364882454784\n",
      "GD iter. 486/499: loss=0.5273535325473718\n",
      "GD iter. 487/499: loss=0.5273153031212156\n",
      "GD iter. 488/499: loss=0.5273295975371323\n",
      "GD iter. 489/499: loss=0.5272978525446913\n",
      "GD iter. 490/499: loss=0.5273112760623851\n",
      "GD iter. 491/499: loss=0.5272798720846593\n",
      "GD iter. 492/499: loss=0.5272952555128974\n",
      "GD iter. 493/499: loss=0.5272600993964894\n",
      "GD iter. 494/499: loss=0.5272738409183244\n",
      "GD iter. 495/499: loss=0.5272440458375756\n",
      "GD iter. 496/499: loss=0.5272577359467656\n",
      "GD iter. 497/499: loss=0.5272239472955378\n",
      "GD iter. 498/499: loss=0.5272373414373126\n",
      "GD iter. 499/499: loss=0.527210062937227\n",
      "The Accuracy is: 0.9195\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "GD iter. 0/499: loss=1.0255487579329692\n",
      "GD iter. 1/499: loss=1.0204018608383945\n",
      "GD iter. 2/499: loss=1.015270462774714\n",
      "GD iter. 3/499: loss=1.0101545119437396\n",
      "GD iter. 4/499: loss=1.0050539567339973\n",
      "GD iter. 5/499: loss=0.9999687457200218\n",
      "GD iter. 6/499: loss=0.994898827661652\n",
      "GD iter. 7/499: loss=0.9898441515033327\n",
      "GD iter. 8/499: loss=0.984804666373416\n",
      "GD iter. 9/499: loss=0.9797803215834681\n",
      "GD iter. 10/499: loss=0.9747710666275752\n",
      "GD iter. 11/499: loss=0.969776851181656\n",
      "GD iter. 12/499: loss=0.9647976251027742\n",
      "GD iter. 13/499: loss=0.959833338428455\n",
      "GD iter. 14/499: loss=0.9548839413760034\n",
      "GD iter. 15/499: loss=0.949949384341826\n",
      "GD iter. 16/499: loss=0.9450296179007542\n",
      "GD iter. 17/499: loss=0.9401245928053716\n",
      "GD iter. 18/499: loss=0.935234259985343\n",
      "GD iter. 19/499: loss=0.930358570546746\n",
      "GD iter. 20/499: loss=0.9254974757714052\n",
      "GD iter. 21/499: loss=0.9206509271162296\n",
      "GD iter. 22/499: loss=0.9158188762125525\n",
      "GD iter. 23/499: loss=0.9110012748654726\n",
      "GD iter. 24/499: loss=0.9061980750531993\n",
      "GD iter. 25/499: loss=0.9014092289264\n",
      "GD iter. 26/499: loss=0.8966346888075494\n",
      "GD iter. 27/499: loss=0.8918744071902819\n",
      "GD iter. 28/499: loss=0.8871283367387469\n",
      "GD iter. 29/499: loss=0.8823964302869653\n",
      "GD iter. 30/499: loss=0.8776786408381899\n",
      "GD iter. 31/499: loss=0.8729749215642668\n",
      "GD iter. 32/499: loss=0.8682852258050006\n",
      "GD iter. 33/499: loss=0.863609507067522\n",
      "GD iter. 34/499: loss=0.858947719025657\n",
      "GD iter. 35/499: loss=0.8542998155192989\n",
      "GD iter. 36/499: loss=0.8496657505537826\n",
      "GD iter. 37/499: loss=0.8450454782992622\n",
      "GD iter. 38/499: loss=0.84043895309009\n",
      "GD iter. 39/499: loss=0.8358461294241971\n",
      "GD iter. 40/499: loss=0.8312669619624805\n",
      "GD iter. 41/499: loss=0.8267014055281862\n",
      "GD iter. 42/499: loss=0.8221494151063\n",
      "GD iter. 43/499: loss=0.8176109458429388\n",
      "GD iter. 44/499: loss=0.8130859530447436\n",
      "GD iter. 45/499: loss=0.8085743921782753\n",
      "GD iter. 46/499: loss=0.8040762188694139\n",
      "GD iter. 47/499: loss=0.7995913889027586\n",
      "GD iter. 48/499: loss=0.7951198582210302\n",
      "GD iter. 49/499: loss=0.7906615829244776\n",
      "GD iter. 50/499: loss=0.7862165192702845\n",
      "GD iter. 51/499: loss=0.7817846236719794\n",
      "GD iter. 52/499: loss=0.7773658526988482\n",
      "GD iter. 53/499: loss=0.7729601630753477\n",
      "GD iter. 54/499: loss=0.7685675116805232\n",
      "GD iter. 55/499: loss=0.7641878555474269\n",
      "GD iter. 56/499: loss=0.7598211518625394\n",
      "GD iter. 57/499: loss=0.7554673579651935\n",
      "GD iter. 58/499: loss=0.7511264313469985\n",
      "GD iter. 59/499: loss=0.7467983296512697\n",
      "GD iter. 60/499: loss=0.7424830106724574\n",
      "GD iter. 61/499: loss=0.7381804323555798\n",
      "GD iter. 62/499: loss=0.7338905527956571\n",
      "GD iter. 63/499: loss=0.7296133302371486\n",
      "GD iter. 64/499: loss=0.7253487230733912\n",
      "GD iter. 65/499: loss=0.7210966898460407\n",
      "GD iter. 66/499: loss=0.7168571892445146\n",
      "GD iter. 67/499: loss=0.7126301801054387\n",
      "GD iter. 68/499: loss=0.7084156214120922\n",
      "GD iter. 69/499: loss=0.70421347229386\n",
      "GD iter. 70/499: loss=0.7000236920256827\n",
      "GD iter. 71/499: loss=0.6958462400275109\n",
      "GD iter. 72/499: loss=0.6916810758637619\n",
      "GD iter. 73/499: loss=0.6875281592427767\n",
      "GD iter. 74/499: loss=0.6833874500162807\n",
      "GD iter. 75/499: loss=0.6792589081788464\n",
      "GD iter. 76/499: loss=0.6751424938673571\n",
      "GD iter. 77/499: loss=0.6710381673604737\n",
      "GD iter. 78/499: loss=0.6669458890781028\n",
      "GD iter. 79/499: loss=0.6628656195808676\n",
      "GD iter. 80/499: loss=0.6587973195695807\n",
      "GD iter. 81/499: loss=0.6547409498847179\n",
      "GD iter. 82/499: loss=0.6506964715058959\n",
      "GD iter. 83/499: loss=0.6466638455513503\n",
      "GD iter. 84/499: loss=0.6426430332774158\n",
      "GD iter. 85/499: loss=0.6386339960780105\n",
      "GD iter. 86/499: loss=0.6346366954841192\n",
      "GD iter. 87/499: loss=0.6306510931632805\n",
      "GD iter. 88/499: loss=0.6266771509190756\n",
      "GD iter. 89/499: loss=0.6227148306906184\n",
      "GD iter. 90/499: loss=0.6187640945520492\n",
      "GD iter. 91/499: loss=0.6148249047120283\n",
      "GD iter. 92/499: loss=0.6108972235132328\n",
      "GD iter. 93/499: loss=0.6069810134318551\n",
      "GD iter. 94/499: loss=0.6030762370771031\n",
      "GD iter. 95/499: loss=0.5991828571907033\n",
      "GD iter. 96/499: loss=0.5953008366464038\n",
      "GD iter. 97/499: loss=0.5914301384494817\n",
      "GD iter. 98/499: loss=0.5875707257362498\n",
      "GD iter. 99/499: loss=0.5837225617735684\n",
      "GD iter. 100/499: loss=0.5798856099583553\n",
      "GD iter. 101/499: loss=0.5760598338171008\n",
      "GD iter. 102/499: loss=0.5722451970053828\n",
      "GD iter. 103/499: loss=0.5684416633073842\n",
      "GD iter. 104/499: loss=0.5646491966354127\n",
      "GD iter. 105/499: loss=0.5608677610294213\n",
      "GD iter. 106/499: loss=0.5570973206565322\n",
      "GD iter. 107/499: loss=0.5533378398105611\n",
      "GD iter. 108/499: loss=0.549589282911544\n",
      "GD iter. 109/499: loss=0.5458639438840787\n",
      "GD iter. 110/499: loss=0.5422412034286967\n",
      "GD iter. 111/499: loss=0.5390497440453139\n",
      "GD iter. 112/499: loss=0.5370688979080075\n",
      "GD iter. 113/499: loss=0.5361489719725172\n",
      "GD iter. 114/499: loss=0.5357215993814395\n",
      "GD iter. 115/499: loss=0.5354965174729218\n",
      "GD iter. 116/499: loss=0.5353777742376153\n",
      "GD iter. 117/499: loss=0.5352970920654048\n",
      "GD iter. 118/499: loss=0.5352337932607016\n",
      "GD iter. 119/499: loss=0.5351823016811258\n",
      "GD iter. 120/499: loss=0.535132311950418\n",
      "GD iter. 121/499: loss=0.5350843466051165\n",
      "GD iter. 122/499: loss=0.5350393608357985\n",
      "GD iter. 123/499: loss=0.5349959124692504\n",
      "GD iter. 124/499: loss=0.5349526747059848\n",
      "GD iter. 125/499: loss=0.5349106374912991\n",
      "GD iter. 126/499: loss=0.5348687835778778\n",
      "GD iter. 127/499: loss=0.5348292294170249\n",
      "GD iter. 128/499: loss=0.5347898636307344\n",
      "GD iter. 129/499: loss=0.5347492585411321\n",
      "GD iter. 130/499: loss=0.5347102080384433\n",
      "GD iter. 131/499: loss=0.5346711190014095\n",
      "GD iter. 132/499: loss=0.5346321847503983\n",
      "GD iter. 133/499: loss=0.5345934046697123\n",
      "GD iter. 134/499: loss=0.5345547781461084\n",
      "GD iter. 135/499: loss=0.5345163045687892\n",
      "GD iter. 136/499: loss=0.5344779833293917\n",
      "GD iter. 137/499: loss=0.5344398138219786\n",
      "GD iter. 138/499: loss=0.5344017954430286\n",
      "GD iter. 139/499: loss=0.5343639275914254\n",
      "GD iter. 140/499: loss=0.5343262096684506\n",
      "GD iter. 141/499: loss=0.5342886410777716\n",
      "GD iter. 142/499: loss=0.5342512273892096\n",
      "GD iter. 143/499: loss=0.5342131958136416\n",
      "GD iter. 144/499: loss=0.5341775937386384\n",
      "GD iter. 145/499: loss=0.5341408691335975\n",
      "GD iter. 146/499: loss=0.5341042903555077\n",
      "GD iter. 147/499: loss=0.5340678568234932\n",
      "GD iter. 148/499: loss=0.5340315679589968\n",
      "GD iter. 149/499: loss=0.5339954231857683\n",
      "GD iter. 150/499: loss=0.5339594219298571\n",
      "GD iter. 151/499: loss=0.5339235637428004\n",
      "GD iter. 152/499: loss=0.5338864548116954\n",
      "GD iter. 153/499: loss=0.533850883467176\n",
      "GD iter. 154/499: loss=0.5338154533614398\n",
      "GD iter. 155/499: loss=0.5337801639319051\n",
      "GD iter. 156/499: loss=0.533745014618235\n",
      "GD iter. 157/499: loss=0.5337100048623278\n",
      "GD iter. 158/499: loss=0.5336751441912861\n",
      "GD iter. 159/499: loss=0.5336394224525831\n",
      "GD iter. 160/499: loss=0.5336048481027444\n",
      "GD iter. 161/499: loss=0.5335694021713404\n",
      "GD iter. 162/499: loss=0.5335340997624577\n",
      "GD iter. 163/499: loss=0.5334999362827124\n",
      "GD iter. 164/499: loss=0.533464902986336\n",
      "GD iter. 165/499: loss=0.533430018820315\n",
      "GD iter. 166/499: loss=0.5333968689731186\n",
      "GD iter. 167/499: loss=0.5333622497505713\n",
      "GD iter. 168/499: loss=0.5333283730896387\n",
      "GD iter. 169/499: loss=0.5332946429181992\n",
      "GD iter. 170/499: loss=0.5332611700977721\n",
      "GD iter. 171/499: loss=0.5332278296470135\n",
      "GD iter. 172/499: loss=0.5331946210397499\n",
      "GD iter. 173/499: loss=0.5331615437519048\n",
      "GD iter. 174/499: loss=0.53312859726149\n",
      "GD iter. 175/499: loss=0.5330957821889818\n",
      "GD iter. 176/499: loss=0.5330639018773649\n",
      "GD iter. 177/499: loss=0.5330313430452861\n",
      "GD iter. 178/499: loss=0.5329989203610002\n",
      "GD iter. 179/499: loss=0.5329674183455954\n",
      "GD iter. 180/499: loss=0.5329352493387624\n",
      "GD iter. 181/499: loss=0.5329035144089929\n",
      "GD iter. 182/499: loss=0.5328719055402045\n",
      "GD iter. 183/499: loss=0.5328404237205454\n",
      "GD iter. 184/499: loss=0.5328082672403458\n",
      "GD iter. 185/499: loss=0.532777047642299\n",
      "GD iter. 186/499: loss=0.5327453566387752\n",
      "GD iter. 187/499: loss=0.5327138100135412\n",
      "GD iter. 188/499: loss=0.5326835964929777\n",
      "GD iter. 189/499: loss=0.53265143775438\n",
      "GD iter. 190/499: loss=0.5326214924982887\n",
      "GD iter. 191/499: loss=0.5325883933052484\n",
      "GD iter. 192/499: loss=0.532558663963931\n",
      "GD iter. 193/499: loss=0.5325281816081675\n",
      "GD iter. 194/499: loss=0.5324978371805726\n",
      "GD iter. 195/499: loss=0.5324652300280756\n",
      "GD iter. 196/499: loss=0.5324364791763293\n",
      "GD iter. 197/499: loss=0.5324052945350413\n",
      "GD iter. 198/499: loss=0.5323763189245939\n",
      "GD iter. 199/499: loss=0.5323434672692409\n",
      "GD iter. 200/499: loss=0.5323170926087251\n",
      "GD iter. 201/499: loss=0.5322844168065248\n",
      "GD iter. 202/499: loss=0.5322572850211136\n",
      "GD iter. 203/499: loss=0.5322257567757075\n",
      "GD iter. 204/499: loss=0.5321979834642954\n",
      "GD iter. 205/499: loss=0.5321690656848321\n",
      "GD iter. 206/499: loss=0.5321378791020304\n",
      "GD iter. 207/499: loss=0.5321114897092765\n",
      "GD iter. 208/499: loss=0.5320804906361991\n",
      "GD iter. 209/499: loss=0.5320520189588598\n",
      "GD iter. 210/499: loss=0.5320236600909296\n",
      "GD iter. 211/499: loss=0.5319954261625998\n",
      "GD iter. 212/499: loss=0.5319661230929713\n",
      "GD iter. 213/499: loss=0.5319406494237581\n",
      "GD iter. 214/499: loss=0.5319083713615731\n",
      "GD iter. 215/499: loss=0.531884285540334\n",
      "GD iter. 216/499: loss=0.5318522654445137\n",
      "GD iter. 217/499: loss=0.5318283987778049\n",
      "GD iter. 218/499: loss=0.531797927722947\n",
      "GD iter. 219/499: loss=0.5317720200018629\n",
      "GD iter. 220/499: loss=0.5317425317392453\n",
      "GD iter. 221/499: loss=0.5317178526610018\n",
      "GD iter. 222/499: loss=0.5316878494246523\n",
      "GD iter. 223/499: loss=0.5316633880752499\n",
      "GD iter. 224/499: loss=0.5316335973544926\n",
      "GD iter. 225/499: loss=0.5316093524622854\n",
      "GD iter. 226/499: loss=0.5315804402740446\n",
      "GD iter. 227/499: loss=0.531555422863391\n",
      "GD iter. 228/499: loss=0.5315267500482831\n",
      "GD iter. 229/499: loss=0.5315029727925388\n",
      "GD iter. 230/499: loss=0.5314737895538835\n",
      "GD iter. 231/499: loss=0.5314511670426172\n",
      "GD iter. 232/499: loss=0.5314221534156992\n",
      "GD iter. 233/499: loss=0.5313961530968327\n",
      "GD iter. 234/499: loss=0.5313714827802652\n",
      "GD iter. 235/499: loss=0.5313437960672748\n",
      "GD iter. 236/499: loss=0.5313217145869024\n",
      "GD iter. 237/499: loss=0.5312930402365716\n",
      "GD iter. 238/499: loss=0.5312713515349169\n",
      "GD iter. 239/499: loss=0.5312417133994282\n",
      "GD iter. 240/499: loss=0.5312225867577438\n",
      "GD iter. 241/499: loss=0.5311929436944264\n",
      "GD iter. 242/499: loss=0.5311665632514486\n",
      "GD iter. 243/499: loss=0.5311463598649313\n",
      "GD iter. 244/499: loss=0.5311166226230662\n",
      "GD iter. 245/499: loss=0.5310980032211523\n",
      "GD iter. 246/499: loss=0.5310682786041669\n",
      "GD iter. 247/499: loss=0.5310450417972318\n",
      "GD iter. 248/499: loss=0.5310188102719852\n",
      "GD iter. 249/499: loss=0.5309992801018987\n",
      "GD iter. 250/499: loss=0.5309700312007949\n",
      "GD iter. 251/499: loss=0.5309520953566028\n",
      "GD iter. 252/499: loss=0.5309220087024202\n",
      "GD iter. 253/499: loss=0.5309016525042475\n",
      "GD iter. 254/499: loss=0.5308731110951991\n",
      "GD iter. 255/499: loss=0.5308588817098271\n",
      "GD iter. 256/499: loss=0.5308266534560497\n",
      "GD iter. 257/499: loss=0.5308084445496757\n",
      "GD iter. 258/499: loss=0.5307783239013206\n",
      "GD iter. 259/499: loss=0.5307642282200721\n",
      "GD iter. 260/499: loss=0.5307325933171144\n",
      "GD iter. 261/499: loss=0.530718094787559\n",
      "GD iter. 262/499: loss=0.5306865782735894\n",
      "GD iter. 263/499: loss=0.5306695539846628\n",
      "GD iter. 264/499: loss=0.5306405214420591\n",
      "GD iter. 265/499: loss=0.5306263519288779\n",
      "GD iter. 266/499: loss=0.5305952352247005\n",
      "GD iter. 267/499: loss=0.5305785644215564\n",
      "GD iter. 268/499: loss=0.5305507791832637\n",
      "GD iter. 269/499: loss=0.5305323527638839\n",
      "GD iter. 270/499: loss=0.5305063355809192\n",
      "GD iter. 271/499: loss=0.5304846285907883\n",
      "GD iter. 272/499: loss=0.5304630084747264\n",
      "GD iter. 273/499: loss=0.5304414987315342\n",
      "GD iter. 274/499: loss=0.5304175933027704\n",
      "GD iter. 275/499: loss=0.5304031893065418\n",
      "GD iter. 276/499: loss=0.5303736794721758\n",
      "GD iter. 277/499: loss=0.5303609060088187\n",
      "GD iter. 278/499: loss=0.5303287271350388\n",
      "GD iter. 279/499: loss=0.5303173528885741\n",
      "GD iter. 280/499: loss=0.5302862402641788\n",
      "GD iter. 281/499: loss=0.5302743780706947\n",
      "GD iter. 282/499: loss=0.5302434545761134\n",
      "GD iter. 283/499: loss=0.5302327388035535\n",
      "GD iter. 284/499: loss=0.530199436694821\n",
      "GD iter. 285/499: loss=0.5301894798532372\n",
      "GD iter. 286/499: loss=0.5301584442850589\n",
      "GD iter. 287/499: loss=0.5301477921570495\n",
      "GD iter. 288/499: loss=0.5301151474125108\n",
      "GD iter. 289/499: loss=0.5301060670088938\n",
      "GD iter. 290/499: loss=0.5300744228238585\n",
      "GD iter. 291/499: loss=0.5300659062084597\n",
      "GD iter. 292/499: loss=0.5300332639463692\n",
      "GD iter. 293/499: loss=0.5300248251811659\n",
      "GD iter. 294/499: loss=0.5299924289149662\n",
      "GD iter. 295/499: loss=0.529984067052006\n",
      "GD iter. 296/499: loss=0.5299519168204813\n",
      "GD iter. 297/499: loss=0.5299426090931727\n",
      "GD iter. 298/499: loss=0.5299154264466082\n",
      "GD iter. 299/499: loss=0.5298973945500047\n",
      "GD iter. 300/499: loss=0.5298770440399487\n",
      "GD iter. 301/499: loss=0.529856774950984\n",
      "GD iter. 302/499: loss=0.529837293999913\n",
      "GD iter. 303/499: loss=0.5298179064403684\n",
      "GD iter. 304/499: loss=0.5297945409658218\n",
      "GD iter. 305/499: loss=0.5297865007790807\n",
      "GD iter. 306/499: loss=0.5297548992348095\n",
      "GD iter. 307/499: loss=0.5297465355241434\n",
      "GD iter. 308/499: loss=0.5297172769080098\n",
      "GD iter. 309/499: loss=0.529708231562881\n",
      "GD iter. 310/499: loss=0.5296799395320226\n",
      "GD iter. 311/499: loss=0.5296695230539721\n",
      "GD iter. 312/499: loss=0.5296415778311421\n",
      "GD iter. 313/499: loss=0.5296311182294537\n",
      "GD iter. 314/499: loss=0.5296045130982253\n",
      "GD iter. 315/499: loss=0.529592902702744\n",
      "GD iter. 316/499: loss=0.5295666637154586\n",
      "GD iter. 317/499: loss=0.5295587981377057\n",
      "GD iter. 318/499: loss=0.5295287565746177\n",
      "GD iter. 319/499: loss=0.5295237683421951\n",
      "GD iter. 320/499: loss=0.5294904684647911\n",
      "GD iter. 321/499: loss=0.5294873027793995\n",
      "GD iter. 322/499: loss=0.529454166536037\n",
      "GD iter. 323/499: loss=0.5294511246419824\n",
      "GD iter. 324/499: loss=0.5294181587393029\n",
      "GD iter. 325/499: loss=0.5294142073051967\n",
      "GD iter. 326/499: loss=0.529383443989994\n",
      "GD iter. 327/499: loss=0.5293784910660775\n",
      "GD iter. 328/499: loss=0.5293468528823267\n",
      "GD iter. 329/499: loss=0.5293443629381684\n",
      "GD iter. 330/499: loss=0.529311581599066\n",
      "GD iter. 331/499: loss=0.5293109035204346\n",
      "GD iter. 332/499: loss=0.5292770696691209\n",
      "GD iter. 333/499: loss=0.5292749462530281\n",
      "GD iter. 334/499: loss=0.5292423459989609\n",
      "GD iter. 335/499: loss=0.5292402067628836\n",
      "GD iter. 336/499: loss=0.5292089704296833\n",
      "GD iter. 337/499: loss=0.529205114561766\n",
      "GD iter. 338/499: loss=0.5291757491095122\n",
      "GD iter. 339/499: loss=0.52917130965305\n",
      "GD iter. 340/499: loss=0.5291412328301511\n",
      "GD iter. 341/499: loss=0.5291376415809044\n",
      "GD iter. 342/499: loss=0.529107733376703\n",
      "GD iter. 343/499: loss=0.5291042389148268\n",
      "GD iter. 344/499: loss=0.5290753344792487\n",
      "GD iter. 345/499: loss=0.5290684628222472\n",
      "GD iter. 346/499: loss=0.5290434274334734\n",
      "GD iter. 347/499: loss=0.5290342289928309\n",
      "GD iter. 348/499: loss=0.5290120491940268\n",
      "GD iter. 349/499: loss=0.5289999859573618\n",
      "GD iter. 350/499: loss=0.5289816127458831\n",
      "GD iter. 351/499: loss=0.5289660370548656\n",
      "GD iter. 352/499: loss=0.5289493265155998\n",
      "GD iter. 353/499: loss=0.5289338764019479\n",
      "GD iter. 354/499: loss=0.5289173048267316\n",
      "GD iter. 355/499: loss=0.5289034059004388\n",
      "GD iter. 356/499: loss=0.5288808083010778\n",
      "GD iter. 357/499: loss=0.5288784942299435\n",
      "GD iter. 358/499: loss=0.5288491569069531\n",
      "GD iter. 359/499: loss=0.5288467713875726\n",
      "GD iter. 360/499: loss=0.5288177607375933\n",
      "GD iter. 361/499: loss=0.5288170728797202\n",
      "GD iter. 362/499: loss=0.5287876515983747\n",
      "GD iter. 363/499: loss=0.5287844417908031\n",
      "GD iter. 364/499: loss=0.5287559302287156\n",
      "GD iter. 365/499: loss=0.5287567743327765\n",
      "GD iter. 366/499: loss=0.52872660412211\n",
      "GD iter. 367/499: loss=0.5287226077924418\n",
      "GD iter. 368/499: loss=0.5286958631378557\n",
      "GD iter. 369/499: loss=0.528692614675552\n",
      "GD iter. 370/499: loss=0.5286659683239222\n",
      "GD iter. 371/499: loss=0.528662858249586\n",
      "GD iter. 372/499: loss=0.5286363093412079\n",
      "GD iter. 373/499: loss=0.5286333366368201\n",
      "GD iter. 374/499: loss=0.528606886636924\n",
      "GD iter. 375/499: loss=0.5286016273159005\n",
      "GD iter. 376/499: loss=0.5285777785393568\n",
      "GD iter. 377/499: loss=0.528572696482307\n",
      "GD iter. 378/499: loss=0.5285489082068315\n",
      "GD iter. 379/499: loss=0.5285445993244396\n",
      "GD iter. 380/499: loss=0.5285198247567858\n",
      "GD iter. 381/499: loss=0.5285229674381842\n",
      "GD iter. 382/499: loss=0.5284909170704964\n",
      "GD iter. 383/499: loss=0.5284937958897324\n",
      "GD iter. 384/499: loss=0.5284622450294754\n",
      "GD iter. 385/499: loss=0.5284660563524257\n",
      "GD iter. 386/499: loss=0.5284344147616299\n",
      "GD iter. 387/499: loss=0.5284337756372276\n",
      "GD iter. 388/499: loss=0.5284052537518935\n",
      "GD iter. 389/499: loss=0.5284108668824741\n",
      "GD iter. 390/499: loss=0.5283793068694358\n",
      "GD iter. 391/499: loss=0.5283769803269946\n",
      "GD iter. 392/499: loss=0.5283520258042936\n",
      "GD iter. 393/499: loss=0.5283464599692747\n",
      "GD iter. 394/499: loss=0.5283309607533292\n",
      "GD iter. 395/499: loss=0.5283173667761901\n",
      "GD iter. 396/499: loss=0.5283030614212437\n",
      "GD iter. 397/499: loss=0.5282920006420998\n",
      "GD iter. 398/499: loss=0.5282761592104336\n",
      "GD iter. 399/499: loss=0.5282665390342486\n",
      "GD iter. 400/499: loss=0.5282433003157013\n",
      "GD iter. 401/499: loss=0.5282501770281974\n",
      "GD iter. 402/499: loss=0.5282181132884095\n",
      "GD iter. 403/499: loss=0.5282189012398885\n",
      "GD iter. 404/499: loss=0.528190674934427\n",
      "GD iter. 405/499: loss=0.528200194473149\n",
      "GD iter. 406/499: loss=0.528167462066873\n",
      "GD iter. 407/499: loss=0.5281665023866785\n",
      "GD iter. 408/499: loss=0.5281389523913971\n",
      "GD iter. 409/499: loss=0.5281515331648418\n",
      "GD iter. 410/499: loss=0.5281150587082382\n",
      "GD iter. 411/499: loss=0.5281166525893973\n",
      "GD iter. 412/499: loss=0.5280887853780126\n",
      "GD iter. 413/499: loss=0.5280964728756846\n",
      "GD iter. 414/499: loss=0.5280651234695756\n",
      "GD iter. 415/499: loss=0.528065365383394\n",
      "GD iter. 416/499: loss=0.5280384374269198\n",
      "GD iter. 417/499: loss=0.5280467710780355\n",
      "GD iter. 418/499: loss=0.5280157038211323\n",
      "GD iter. 419/499: loss=0.5280171099105466\n",
      "GD iter. 420/499: loss=0.5279898855273611\n",
      "GD iter. 421/499: loss=0.5279962283042546\n",
      "GD iter. 422/499: loss=0.5279662281503554\n",
      "GD iter. 423/499: loss=0.5279713765031591\n",
      "GD iter. 424/499: loss=0.5279422635556869\n",
      "GD iter. 425/499: loss=0.5279448618600618\n",
      "GD iter. 426/499: loss=0.5279179097778074\n",
      "GD iter. 427/499: loss=0.527926073903665\n",
      "GD iter. 428/499: loss=0.5278940855060508\n",
      "GD iter. 429/499: loss=0.527906536002182\n",
      "GD iter. 430/499: loss=0.5278726477557832\n",
      "GD iter. 431/499: loss=0.5278717500686729\n",
      "GD iter. 432/499: loss=0.5278512337842504\n",
      "GD iter. 433/499: loss=0.527845560373496\n",
      "GD iter. 434/499: loss=0.5278327172167862\n",
      "GD iter. 435/499: loss=0.527821526271675\n",
      "GD iter. 436/499: loss=0.5278088015767791\n",
      "GD iter. 437/499: loss=0.5277981924604281\n",
      "GD iter. 438/499: loss=0.5277871545945928\n",
      "GD iter. 439/499: loss=0.5277745581646128\n",
      "GD iter. 440/499: loss=0.5277654038785279\n",
      "GD iter. 441/499: loss=0.5277486353256672\n",
      "GD iter. 442/499: loss=0.5277461253951239\n",
      "GD iter. 443/499: loss=0.5277221764567128\n",
      "GD iter. 444/499: loss=0.5277349512293668\n",
      "GD iter. 445/499: loss=0.5277019449988295\n",
      "GD iter. 446/499: loss=0.527708431758237\n",
      "GD iter. 447/499: loss=0.5276789151305161\n",
      "GD iter. 448/499: loss=0.5276904789536795\n",
      "GD iter. 449/499: loss=0.5276574744655945\n",
      "GD iter. 450/499: loss=0.5276664138359367\n",
      "GD iter. 451/499: loss=0.5276354317747963\n",
      "GD iter. 452/499: loss=0.5276468651912126\n",
      "GD iter. 453/499: loss=0.5276143304235603\n",
      "GD iter. 454/499: loss=0.5276231369864863\n",
      "GD iter. 455/499: loss=0.5275934748776513\n",
      "GD iter. 456/499: loss=0.5276009947388748\n",
      "GD iter. 457/499: loss=0.5275726214059553\n",
      "GD iter. 458/499: loss=0.5275794490498088\n",
      "GD iter. 459/499: loss=0.527551498064413\n",
      "GD iter. 460/499: loss=0.5275608436132445\n",
      "GD iter. 461/499: loss=0.5275297806713126\n",
      "GD iter. 462/499: loss=0.5275424697121892\n",
      "GD iter. 463/499: loss=0.527509499716388\n",
      "GD iter. 464/499: loss=0.5275210873551925\n",
      "GD iter. 465/499: loss=0.5274887586112048\n",
      "GD iter. 466/499: loss=0.52750146101483\n",
      "GD iter. 467/499: loss=0.5274681226442041\n",
      "GD iter. 468/499: loss=0.5274815927931644\n",
      "GD iter. 469/499: loss=0.5274477071655344\n",
      "GD iter. 470/499: loss=0.5274607418279331\n",
      "GD iter. 471/499: loss=0.5274285434256973\n",
      "GD iter. 472/499: loss=0.5274380276602737\n",
      "GD iter. 473/499: loss=0.5274103485747336\n",
      "GD iter. 474/499: loss=0.5274177954715665\n",
      "GD iter. 475/499: loss=0.527389974226458\n",
      "GD iter. 476/499: loss=0.5273966756056443\n",
      "GD iter. 477/499: loss=0.5273709972820637\n",
      "GD iter. 478/499: loss=0.5273782452693999\n",
      "GD iter. 479/499: loss=0.5273516514927605\n",
      "GD iter. 480/499: loss=0.5273589017895792\n",
      "GD iter. 481/499: loss=0.5273324579690943\n",
      "GD iter. 482/499: loss=0.5273397105165942\n",
      "GD iter. 483/499: loss=0.527312741435929\n",
      "GD iter. 484/499: loss=0.527324080882482\n",
      "GD iter. 485/499: loss=0.527291407313185\n",
      "GD iter. 486/499: loss=0.5273113912638219\n",
      "GD iter. 487/499: loss=0.5272730929512841\n",
      "GD iter. 488/499: loss=0.5272899196734133\n",
      "GD iter. 489/499: loss=0.5272545931475338\n",
      "GD iter. 490/499: loss=0.5272715323916213\n",
      "GD iter. 491/499: loss=0.5272362391769224\n",
      "GD iter. 492/499: loss=0.5272532902059869\n",
      "GD iter. 493/499: loss=0.5272180298831616\n",
      "GD iter. 494/499: loss=0.5272351919654579\n",
      "GD iter. 495/499: loss=0.5271999831862013\n",
      "GD iter. 496/499: loss=0.52721648308325\n",
      "GD iter. 497/499: loss=0.5271825430665807\n",
      "GD iter. 498/499: loss=0.5271970181368968\n",
      "GD iter. 499/499: loss=0.5271642623725209\n",
      "The Accuracy is: 0.9130\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "GD iter. 0/499: loss=1.0101646942804183\n",
      "GD iter. 1/499: loss=1.0050645228151371\n",
      "GD iter. 2/499: loss=0.9999796940038813\n",
      "GD iter. 3/499: loss=0.9949101566126677\n",
      "GD iter. 4/499: loss=0.9898558595920933\n",
      "GD iter. 5/499: loss=0.984816752076639\n",
      "GD iter. 6/499: loss=0.9797927833839737\n",
      "GD iter. 7/499: loss=0.9747839030142632\n",
      "GD iter. 8/499: loss=0.9697900606494808\n",
      "GD iter. 9/499: loss=0.9648112061527211\n",
      "GD iter. 10/499: loss=0.9598472895675152\n",
      "GD iter. 11/499: loss=0.954898261117151\n",
      "GD iter. 12/499: loss=0.9499640712039933\n",
      "GD iter. 13/499: loss=0.945044670408808\n",
      "GD iter. 14/499: loss=0.94014000949009\n",
      "GD iter. 15/499: loss=0.9352500393833907\n",
      "GD iter. 16/499: loss=0.9303747112006517\n",
      "GD iter. 17/499: loss=0.9255139762295379\n",
      "GD iter. 18/499: loss=0.920667785932775\n",
      "GD iter. 19/499: loss=0.9158360919474897\n",
      "GD iter. 20/499: loss=0.9110188460845513\n",
      "GD iter. 21/499: loss=0.9062160003279164\n",
      "GD iter. 22/499: loss=0.9014275068339763\n",
      "GD iter. 23/499: loss=0.8966533179309077\n",
      "GD iter. 24/499: loss=0.8918933861180235\n",
      "GD iter. 25/499: loss=0.8871476640651286\n",
      "GD iter. 26/499: loss=0.8824161046118774\n",
      "GD iter. 27/499: loss=0.8776986607671328\n",
      "GD iter. 28/499: loss=0.8729952857083293\n",
      "GD iter. 29/499: loss=0.8683059327808378\n",
      "GD iter. 30/499: loss=0.8636305554973323\n",
      "GD iter. 31/499: loss=0.8589691075371603\n",
      "GD iter. 32/499: loss=0.8543215427457138\n",
      "GD iter. 33/499: loss=0.8496878151338059\n",
      "GD iter. 34/499: loss=0.8450678788770456\n",
      "GD iter. 35/499: loss=0.8404616883152183\n",
      "GD iter. 36/499: loss=0.8358691979516679\n",
      "GD iter. 37/499: loss=0.8312903624526806\n",
      "GD iter. 38/499: loss=0.8267251366468715\n",
      "GD iter. 39/499: loss=0.8221734755245733\n",
      "GD iter. 40/499: loss=0.8176353342372288\n",
      "GD iter. 41/499: loss=0.8131106680967829\n",
      "GD iter. 42/499: loss=0.80859943257508\n",
      "GD iter. 43/499: loss=0.8041015833032621\n",
      "GD iter. 44/499: loss=0.7996170760711693\n",
      "GD iter. 45/499: loss=0.7951458668267424\n",
      "GD iter. 46/499: loss=0.7906879116754291\n",
      "GD iter. 47/499: loss=0.7862431668795917\n",
      "GD iter. 48/499: loss=0.7818115888579162\n",
      "GD iter. 49/499: loss=0.7773931341848254\n",
      "GD iter. 50/499: loss=0.7729877595898924\n",
      "GD iter. 51/499: loss=0.7685954219572584\n",
      "GD iter. 52/499: loss=0.7642160783250511\n",
      "GD iter. 53/499: loss=0.7598496858848067\n",
      "GD iter. 54/499: loss=0.755496201980892\n",
      "GD iter. 55/499: loss=0.7511555841099318\n",
      "GD iter. 56/499: loss=0.7468277899202359\n",
      "GD iter. 57/499: loss=0.7425127772112294\n",
      "GD iter. 58/499: loss=0.7382105039328855\n",
      "GD iter. 59/499: loss=0.7339209281851591\n",
      "GD iter. 60/499: loss=0.7296440082174247\n",
      "GD iter. 61/499: loss=0.7253797024279152\n",
      "GD iter. 62/499: loss=0.7211279693631619\n",
      "GD iter. 63/499: loss=0.7168887677174394\n",
      "GD iter. 64/499: loss=0.7126620563322097\n",
      "GD iter. 65/499: loss=0.7084477941955704\n",
      "GD iter. 66/499: loss=0.7042459404417044\n",
      "GD iter. 67/499: loss=0.7000564543503315\n",
      "GD iter. 68/499: loss=0.6958792953461629\n",
      "GD iter. 69/499: loss=0.6917144229983561\n",
      "GD iter. 70/499: loss=0.6875617970199748\n",
      "GD iter. 71/499: loss=0.6834213772674471\n",
      "GD iter. 72/499: loss=0.6792931237400301\n",
      "GD iter. 73/499: loss=0.6751769965792728\n",
      "GD iter. 74/499: loss=0.6710729560684825\n",
      "GD iter. 75/499: loss=0.6669809626321943\n",
      "GD iter. 76/499: loss=0.6629009768356416\n",
      "GD iter. 77/499: loss=0.6588329593842279\n",
      "GD iter. 78/499: loss=0.6547768711230021\n",
      "GD iter. 79/499: loss=0.6507326730361354\n",
      "GD iter. 80/499: loss=0.6467003262463996\n",
      "GD iter. 81/499: loss=0.6426797920146486\n",
      "GD iter. 82/499: loss=0.6386710317393001\n",
      "GD iter. 83/499: loss=0.6346740069558208\n",
      "GD iter. 84/499: loss=0.6306886793362138\n",
      "GD iter. 85/499: loss=0.6267150106885067\n",
      "GD iter. 86/499: loss=0.622752962956242\n",
      "GD iter. 87/499: loss=0.6188024982179708\n",
      "GD iter. 88/499: loss=0.6148635786867467\n",
      "GD iter. 89/499: loss=0.610936166709622\n",
      "GD iter. 90/499: loss=0.6070202247671477\n",
      "GD iter. 91/499: loss=0.6031157154728722\n",
      "GD iter. 92/499: loss=0.5992226015728451\n",
      "GD iter. 93/499: loss=0.5953408459451207\n",
      "GD iter. 94/499: loss=0.5914704115992648\n",
      "GD iter. 95/499: loss=0.5876112616758615\n",
      "GD iter. 96/499: loss=0.5837633594460261\n",
      "GD iter. 97/499: loss=0.5799266683109137\n",
      "GD iter. 98/499: loss=0.5761011518012347\n",
      "GD iter. 99/499: loss=0.5722867735767709\n",
      "GD iter. 100/499: loss=0.5684834974258919\n",
      "GD iter. 101/499: loss=0.5646912872650754\n",
      "GD iter. 102/499: loss=0.560910107138428\n",
      "GD iter. 103/499: loss=0.5571399212172082\n",
      "GD iter. 104/499: loss=0.5533806937993522\n",
      "GD iter. 105/499: loss=0.5496363556026149\n",
      "GD iter. 106/499: loss=0.5459117379214803\n",
      "GD iter. 107/499: loss=0.5422314937473974\n",
      "GD iter. 108/499: loss=0.5390259095387648\n",
      "GD iter. 109/499: loss=0.5371114856489072\n",
      "GD iter. 110/499: loss=0.5362118046692024\n",
      "GD iter. 111/499: loss=0.5357833696466922\n",
      "GD iter. 112/499: loss=0.5355720769597988\n",
      "GD iter. 113/499: loss=0.5354594221402836\n",
      "GD iter. 114/499: loss=0.5353760609823622\n",
      "GD iter. 115/499: loss=0.5353031081623636\n",
      "GD iter. 116/499: loss=0.5352390188870937\n",
      "GD iter. 117/499: loss=0.5351851733730059\n",
      "GD iter. 118/499: loss=0.5351366660513162\n",
      "GD iter. 119/499: loss=0.5350894792909103\n",
      "GD iter. 120/499: loss=0.5350435847544585\n",
      "GD iter. 121/499: loss=0.5349979239767814\n",
      "GD iter. 122/499: loss=0.5349533151240611\n",
      "GD iter. 123/499: loss=0.5349102094747258\n",
      "GD iter. 124/499: loss=0.5348685273502137\n",
      "GD iter. 125/499: loss=0.5348270074036322\n",
      "GD iter. 126/499: loss=0.5347856489956855\n",
      "GD iter. 127/499: loss=0.5347444558147126\n",
      "GD iter. 128/499: loss=0.5347046561967332\n",
      "GD iter. 129/499: loss=0.5346626416821462\n",
      "GD iter. 130/499: loss=0.5346239912429278\n",
      "GD iter. 131/499: loss=0.5345831176140919\n",
      "GD iter. 132/499: loss=0.5345435987049966\n",
      "GD iter. 133/499: loss=0.534504235076903\n",
      "GD iter. 134/499: loss=0.534465032105704\n",
      "GD iter. 135/499: loss=0.5344248146554252\n",
      "GD iter. 136/499: loss=0.5343868562922564\n",
      "GD iter. 137/499: loss=0.5343478635228158\n",
      "GD iter. 138/499: loss=0.5343090285430274\n",
      "GD iter. 139/499: loss=0.5342716075205707\n",
      "GD iter. 140/499: loss=0.5342317062593706\n",
      "GD iter. 141/499: loss=0.5341931872514073\n",
      "GD iter. 142/499: loss=0.5341548187056433\n",
      "GD iter. 143/499: loss=0.5341166000277511\n",
      "GD iter. 144/499: loss=0.5340785306257639\n",
      "GD iter. 145/499: loss=0.5340406099100676\n",
      "GD iter. 146/499: loss=0.5340028372933895\n",
      "GD iter. 147/499: loss=0.5339652121907905\n",
      "GD iter. 148/499: loss=0.5339277340196547\n",
      "GD iter. 149/499: loss=0.5338904021996812\n",
      "GD iter. 150/499: loss=0.533853230881848\n",
      "GD iter. 151/499: loss=0.5338170275894804\n",
      "GD iter. 152/499: loss=0.5337809672089224\n",
      "GD iter. 153/499: loss=0.5337450585382307\n",
      "GD iter. 154/499: loss=0.5337080942657032\n",
      "GD iter. 155/499: loss=0.5336733002242922\n",
      "GD iter. 156/499: loss=0.5336374557160081\n",
      "GD iter. 157/499: loss=0.5336017569052073\n",
      "GD iter. 158/499: loss=0.533567013104832\n",
      "GD iter. 159/499: loss=0.5335324072805315\n",
      "GD iter. 160/499: loss=0.5334979388826734\n",
      "GD iter. 161/499: loss=0.5334636219335939\n",
      "GD iter. 162/499: loss=0.5334288668207752\n",
      "GD iter. 163/499: loss=0.5333942520232285\n",
      "GD iter. 164/499: loss=0.5333606816333558\n",
      "GD iter. 165/499: loss=0.5333263459083182\n",
      "GD iter. 166/499: loss=0.5332930711317951\n",
      "GD iter. 167/499: loss=0.5332585599440953\n",
      "GD iter. 168/499: loss=0.5332241837351914\n",
      "GD iter. 169/499: loss=0.5331899419715636\n",
      "GD iter. 170/499: loss=0.5331558341218126\n",
      "GD iter. 171/499: loss=0.5331218596566512\n",
      "GD iter. 172/499: loss=0.5330880180488953\n",
      "GD iter. 173/499: loss=0.5330543087734558\n",
      "GD iter. 174/499: loss=0.5330207313073301\n",
      "GD iter. 175/499: loss=0.5329872851295944\n",
      "GD iter. 176/499: loss=0.5329539728295174\n",
      "GD iter. 177/499: loss=0.5329221249014886\n",
      "GD iter. 178/499: loss=0.5328898308299219\n",
      "GD iter. 179/499: loss=0.5328590167164193\n",
      "GD iter. 180/499: loss=0.5328256687326623\n",
      "GD iter. 181/499: loss=0.5327937650847039\n",
      "GD iter. 182/499: loss=0.5327619874337678\n",
      "GD iter. 183/499: loss=0.5327303352793503\n",
      "GD iter. 184/499: loss=0.5326988081229411\n",
      "GD iter. 185/499: loss=0.5326674054680158\n",
      "GD iter. 186/499: loss=0.5326361297470257\n",
      "GD iter. 187/499: loss=0.5326060062282341\n",
      "GD iter. 188/499: loss=0.5325757216612169\n",
      "GD iter. 189/499: loss=0.532545558085099\n",
      "GD iter. 190/499: loss=0.5325155150164527\n",
      "GD iter. 191/499: loss=0.5324855919737824\n",
      "GD iter. 192/499: loss=0.5324557884775168\n",
      "GD iter. 193/499: loss=0.5324261040500009\n",
      "GD iter. 194/499: loss=0.5323965382154874\n",
      "GD iter. 195/499: loss=0.532367090500131\n",
      "GD iter. 196/499: loss=0.5323377604319793\n",
      "GD iter. 197/499: loss=0.5323085509865193\n",
      "GD iter. 198/499: loss=0.5322784769686153\n",
      "GD iter. 199/499: loss=0.532248521126511\n",
      "GD iter. 200/499: loss=0.5322186845793605\n",
      "GD iter. 201/499: loss=0.5321899641826261\n",
      "GD iter. 202/499: loss=0.5321603580601244\n",
      "GD iter. 203/499: loss=0.5321308682556674\n",
      "GD iter. 204/499: loss=0.5321015098971109\n",
      "GD iter. 205/499: loss=0.5320738033284355\n",
      "GD iter. 206/499: loss=0.5320462458842226\n",
      "GD iter. 207/499: loss=0.5320179312647796\n",
      "GD iter. 208/499: loss=0.5319889350709496\n",
      "GD iter. 209/499: loss=0.5319608461008953\n",
      "GD iter. 210/499: loss=0.5319320846604518\n",
      "GD iter. 211/499: loss=0.5319031600116532\n",
      "GD iter. 212/499: loss=0.5318743481389786\n",
      "GD iter. 213/499: loss=0.5318456485973877\n",
      "GD iter. 214/499: loss=0.5318170609436084\n",
      "GD iter. 215/499: loss=0.5317885847361274\n",
      "GD iter. 216/499: loss=0.531760219535186\n",
      "GD iter. 217/499: loss=0.5317319649027704\n",
      "GD iter. 218/499: loss=0.5317038204026063\n",
      "GD iter. 219/499: loss=0.5316757994775672\n",
      "GD iter. 220/499: loss=0.5316495927229578\n",
      "GD iter. 221/499: loss=0.5316217938335325\n",
      "GD iter. 222/499: loss=0.5315948124189872\n",
      "GD iter. 223/499: loss=0.5315679388394442\n",
      "GD iter. 224/499: loss=0.531540100535411\n",
      "GD iter. 225/499: loss=0.531513467120393\n",
      "GD iter. 226/499: loss=0.5314858499708348\n",
      "GD iter. 227/499: loss=0.5314593368093367\n",
      "GD iter. 228/499: loss=0.5314329403742516\n",
      "GD iter. 229/499: loss=0.5314047829492254\n",
      "GD iter. 230/499: loss=0.5313796333301205\n",
      "GD iter. 231/499: loss=0.5313513094295779\n",
      "GD iter. 232/499: loss=0.5313263852802614\n",
      "GD iter. 233/499: loss=0.5312983223947934\n",
      "GD iter. 234/499: loss=0.5312750795174742\n",
      "GD iter. 235/499: loss=0.5312476213405009\n",
      "GD iter. 236/499: loss=0.5312212856644047\n",
      "GD iter. 237/499: loss=0.5311950541613747\n",
      "GD iter. 238/499: loss=0.5311703360129074\n",
      "GD iter. 239/499: loss=0.5311443027670739\n",
      "GD iter. 240/499: loss=0.5311183704859306\n",
      "GD iter. 241/499: loss=0.5310925405793878\n",
      "GD iter. 242/499: loss=0.5310682674331425\n",
      "GD iter. 243/499: loss=0.5310411627322615\n",
      "GD iter. 244/499: loss=0.5310201699512146\n",
      "GD iter. 245/499: loss=0.5309921550835341\n",
      "GD iter. 246/499: loss=0.5309665060941557\n",
      "GD iter. 247/499: loss=0.5309438967485383\n",
      "GD iter. 248/499: loss=0.5309196900959064\n",
      "GD iter. 249/499: loss=0.530896297084658\n",
      "GD iter. 250/499: loss=0.5308718939916224\n",
      "GD iter. 251/499: loss=0.5308487096394595\n",
      "GD iter. 252/499: loss=0.5308234959141399\n",
      "GD iter. 253/499: loss=0.530800484268353\n",
      "GD iter. 254/499: loss=0.5307765678024886\n",
      "GD iter. 255/499: loss=0.5307527472803074\n",
      "GD iter. 256/499: loss=0.5307279455132256\n",
      "GD iter. 257/499: loss=0.5307053004217647\n",
      "GD iter. 258/499: loss=0.5306816748701767\n",
      "GD iter. 259/499: loss=0.5306586515622346\n",
      "GD iter. 260/499: loss=0.5306347239247703\n",
      "GD iter. 261/499: loss=0.5306129857808516\n",
      "GD iter. 262/499: loss=0.5305892209497205\n",
      "GD iter. 263/499: loss=0.5305655640184849\n",
      "GD iter. 264/499: loss=0.5305429914661304\n",
      "GD iter. 265/499: loss=0.5305205097836416\n",
      "GD iter. 266/499: loss=0.5304971220174356\n",
      "GD iter. 267/499: loss=0.5304748180073687\n",
      "GD iter. 268/499: loss=0.5304526042455664\n",
      "GD iter. 269/499: loss=0.5304294824574642\n",
      "GD iter. 270/499: loss=0.530407443812783\n",
      "GD iter. 271/499: loss=0.5303854948047559\n",
      "GD iter. 272/499: loss=0.5303626574979206\n",
      "GD iter. 273/499: loss=0.5303420776866538\n",
      "GD iter. 274/499: loss=0.5303194214138327\n",
      "GD iter. 275/499: loss=0.5302997456204771\n",
      "GD iter. 276/499: loss=0.5302772555139784\n",
      "GD iter. 277/499: loss=0.5302565553414187\n",
      "GD iter. 278/499: loss=0.5302359460881205\n",
      "GD iter. 279/499: loss=0.5302140388577247\n",
      "GD iter. 280/499: loss=0.5301936092760062\n",
      "GD iter. 281/499: loss=0.5301718578663048\n",
      "GD iter. 282/499: loss=0.5301516097523752\n",
      "GD iter. 283/499: loss=0.5301289291928047\n",
      "GD iter. 284/499: loss=0.5301088258910717\n",
      "GD iter. 285/499: loss=0.530087415531854\n",
      "GD iter. 286/499: loss=0.5300674879105747\n",
      "GD iter. 287/499: loss=0.5300462294250803\n",
      "GD iter. 288/499: loss=0.53002647613622\n",
      "GD iter. 289/499: loss=0.5300053941951726\n",
      "GD iter. 290/499: loss=0.5299853104343302\n",
      "GD iter. 291/499: loss=0.5299653101711687\n",
      "GD iter. 292/499: loss=0.5299443075091129\n",
      "GD iter. 293/499: loss=0.5299272448827694\n",
      "GD iter. 294/499: loss=0.5299026457772038\n",
      "GD iter. 295/499: loss=0.5298860911178737\n",
      "GD iter. 296/499: loss=0.5298617804974839\n",
      "GD iter. 297/499: loss=0.5298452778996915\n",
      "GD iter. 298/499: loss=0.529823311089369\n",
      "GD iter. 299/499: loss=0.529805642687062\n",
      "GD iter. 300/499: loss=0.5297829946563386\n",
      "GD iter. 301/499: loss=0.5297654330040517\n",
      "GD iter. 302/499: loss=0.5297481974880515\n",
      "GD iter. 303/499: loss=0.5297233746103521\n",
      "GD iter. 304/499: loss=0.5297060485193572\n",
      "GD iter. 305/499: loss=0.529687875701573\n",
      "GD iter. 306/499: loss=0.5296684075567943\n",
      "GD iter. 307/499: loss=0.5296493102034724\n",
      "GD iter. 308/499: loss=0.5296314612991296\n",
      "GD iter. 309/499: loss=0.529608735557209\n",
      "GD iter. 310/499: loss=0.5295929261288225\n",
      "GD iter. 311/499: loss=0.5295716056081954\n",
      "GD iter. 312/499: loss=0.5295549459016728\n",
      "GD iter. 313/499: loss=0.5295373865521366\n",
      "GD iter. 314/499: loss=0.5295160795799801\n",
      "GD iter. 315/499: loss=0.529499661526707\n",
      "GD iter. 316/499: loss=0.5294808720567012\n",
      "GD iter. 317/499: loss=0.5294636013638238\n",
      "GD iter. 318/499: loss=0.5294425903280464\n",
      "GD iter. 319/499: loss=0.5294264534024784\n",
      "GD iter. 320/499: loss=0.5294068033383957\n",
      "GD iter. 321/499: loss=0.529390866161311\n",
      "GD iter. 322/499: loss=0.5293701615557679\n",
      "GD iter. 323/499: loss=0.5293549249676948\n",
      "GD iter. 324/499: loss=0.5293343763231341\n",
      "GD iter. 325/499: loss=0.5293192657650582\n",
      "GD iter. 326/499: loss=0.5292988719053333\n",
      "GD iter. 327/499: loss=0.5292838913578549\n",
      "GD iter. 328/499: loss=0.5292631801673593\n",
      "GD iter. 329/499: loss=0.529248315752858\n",
      "GD iter. 330/499: loss=0.5292314304588027\n",
      "GD iter. 331/499: loss=0.529212051263543\n",
      "GD iter. 332/499: loss=0.529196978694715\n",
      "GD iter. 333/499: loss=0.5291765764940982\n",
      "GD iter. 334/499: loss=0.5291615909077012\n",
      "GD iter. 335/499: loss=0.5291456527615973\n",
      "GD iter. 336/499: loss=0.5291254470872075\n",
      "GD iter. 337/499: loss=0.5291106636804902\n",
      "GD iter. 338/499: loss=0.5290949304933564\n",
      "GD iter. 339/499: loss=0.5290749189842793\n",
      "GD iter. 340/499: loss=0.529060335367206\n",
      "GD iter. 341/499: loss=0.5290448239978806\n",
      "GD iter. 342/499: loss=0.529023132464324\n",
      "GD iter. 343/499: loss=0.52901179535998\n",
      "GD iter. 344/499: loss=0.5289913687163541\n",
      "GD iter. 345/499: loss=0.5289788244412161\n",
      "GD iter. 346/499: loss=0.5289601560911273\n",
      "GD iter. 347/499: loss=0.5289479842582384\n",
      "GD iter. 348/499: loss=0.528927542047815\n",
      "GD iter. 349/499: loss=0.5289154370839417\n",
      "GD iter. 350/499: loss=0.5288951832685492\n",
      "GD iter. 351/499: loss=0.528883153499957\n",
      "GD iter. 352/499: loss=0.5288637274999822\n",
      "GD iter. 353/499: loss=0.5288507057200402\n",
      "GD iter. 354/499: loss=0.5288326235148225\n",
      "GD iter. 355/499: loss=0.5288197849131514\n",
      "GD iter. 356/499: loss=0.5288006527777372\n",
      "GD iter. 357/499: loss=0.5287890099740347\n",
      "GD iter. 358/499: loss=0.5287699461329435\n",
      "GD iter. 359/499: loss=0.5287576229328134\n",
      "GD iter. 360/499: loss=0.5287412231684077\n",
      "GD iter. 361/499: loss=0.5287274491087867\n",
      "GD iter. 362/499: loss=0.5287086277306207\n",
      "GD iter. 363/499: loss=0.5286972978880792\n",
      "GD iter. 364/499: loss=0.5286785933331462\n",
      "GD iter. 365/499: loss=0.5286677530000122\n",
      "GD iter. 366/499: loss=0.5286478001098446\n",
      "GD iter. 367/499: loss=0.528640475095078\n",
      "GD iter. 368/499: loss=0.528619127552746\n",
      "GD iter. 369/499: loss=0.5286078474216643\n",
      "GD iter. 370/499: loss=0.5285882729326034\n",
      "GD iter. 371/499: loss=0.5285810248366283\n",
      "GD iter. 372/499: loss=0.5285600524711783\n",
      "GD iter. 373/499: loss=0.528550116428937\n",
      "GD iter. 374/499: loss=0.5285319071811982\n",
      "GD iter. 375/499: loss=0.5285227594855164\n",
      "GD iter. 376/499: loss=0.5285022702481915\n",
      "GD iter. 377/499: loss=0.5284969393701421\n",
      "GD iter. 378/499: loss=0.5284752686179417\n",
      "GD iter. 379/499: loss=0.5284675762936492\n",
      "GD iter. 380/499: loss=0.5284463296637081\n",
      "GD iter. 381/499: loss=0.5284400456595831\n",
      "GD iter. 382/499: loss=0.5284201187702927\n",
      "GD iter. 383/499: loss=0.5284083943718337\n",
      "GD iter. 384/499: loss=0.5283924173815021\n",
      "GD iter. 385/499: loss=0.5283821449319481\n",
      "GD iter. 386/499: loss=0.5283661730381981\n",
      "GD iter. 387/499: loss=0.5283540094223633\n",
      "GD iter. 388/499: loss=0.5283382313613834\n",
      "GD iter. 389/499: loss=0.5283281912807067\n",
      "GD iter. 390/499: loss=0.5283124225126041\n",
      "GD iter. 391/499: loss=0.5283004836333695\n",
      "GD iter. 392/499: loss=0.5282849012152383\n",
      "GD iter. 393/499: loss=0.528275121663516\n",
      "GD iter. 394/499: loss=0.5282577479513794\n",
      "GD iter. 395/499: loss=0.5282509368531658\n",
      "GD iter. 396/499: loss=0.5282287566091798\n",
      "GD iter. 397/499: loss=0.5282248661281506\n",
      "GD iter. 398/499: loss=0.528202501508656\n",
      "GD iter. 399/499: loss=0.5282001664047162\n",
      "GD iter. 400/499: loss=0.5281745582480643\n",
      "GD iter. 401/499: loss=0.5281735947541962\n",
      "GD iter. 402/499: loss=0.5281511555928184\n",
      "GD iter. 403/499: loss=0.5281468924027662\n",
      "GD iter. 404/499: loss=0.5281256734154347\n",
      "GD iter. 405/499: loss=0.5281203142149149\n",
      "GD iter. 406/499: loss=0.5281013269700465\n",
      "GD iter. 407/499: loss=0.5280948529983195\n",
      "GD iter. 408/499: loss=0.5280760147145497\n",
      "GD iter. 409/499: loss=0.5280712030128986\n",
      "GD iter. 410/499: loss=0.5280511466404741\n",
      "GD iter. 411/499: loss=0.5280479601832114\n",
      "GD iter. 412/499: loss=0.5280239751762067\n",
      "GD iter. 413/499: loss=0.5280253551975923\n",
      "GD iter. 414/499: loss=0.5280005969945254\n",
      "GD iter. 415/499: loss=0.5280018375497404\n",
      "GD iter. 416/499: loss=0.5279748195552761\n",
      "GD iter. 417/499: loss=0.5279763749729567\n",
      "GD iter. 418/499: loss=0.5279526780484591\n",
      "GD iter. 419/499: loss=0.5279474519952841\n",
      "GD iter. 420/499: loss=0.5279277529326188\n",
      "GD iter. 421/499: loss=0.5279303176753625\n",
      "GD iter. 422/499: loss=0.5279042598084926\n",
      "GD iter. 423/499: loss=0.5279061756870242\n",
      "GD iter. 424/499: loss=0.527881225215438\n",
      "GD iter. 425/499: loss=0.5278772731092283\n",
      "GD iter. 426/499: loss=0.5278574571956801\n",
      "GD iter. 427/499: loss=0.5278575073344842\n",
      "GD iter. 428/499: loss=0.5278334691506787\n",
      "GD iter. 429/499: loss=0.5278377223335072\n",
      "GD iter. 430/499: loss=0.5278088422601152\n",
      "GD iter. 431/499: loss=0.5278132497649146\n",
      "GD iter. 432/499: loss=0.5277877456409406\n",
      "GD iter. 433/499: loss=0.5277878233279407\n",
      "GD iter. 434/499: loss=0.5277656954842327\n",
      "GD iter. 435/499: loss=0.5277614494090851\n",
      "GD iter. 436/499: loss=0.5277458294500776\n",
      "GD iter. 437/499: loss=0.5277350387514098\n",
      "GD iter. 438/499: loss=0.5277234898239975\n",
      "GD iter. 439/499: loss=0.5277133122098415\n",
      "GD iter. 440/499: loss=0.527700949005911\n",
      "GD iter. 441/499: loss=0.5276908486556422\n",
      "GD iter. 442/499: loss=0.5276786102063031\n",
      "GD iter. 443/499: loss=0.5276715789942964\n",
      "GD iter. 444/499: loss=0.527656439219175\n",
      "GD iter. 445/499: loss=0.5276587115956523\n",
      "GD iter. 446/499: loss=0.5276302975983669\n",
      "GD iter. 447/499: loss=0.5276368245900432\n",
      "GD iter. 448/499: loss=0.5276086412700576\n",
      "GD iter. 449/499: loss=0.5276151084877947\n",
      "GD iter. 450/499: loss=0.5275871545898847\n",
      "GD iter. 451/499: loss=0.5275935619404337\n",
      "GD iter. 452/499: loss=0.5275658362170504\n",
      "GD iter. 453/499: loss=0.5275721836101845\n",
      "GD iter. 454/499: loss=0.5275446887634438\n",
      "GD iter. 455/499: loss=0.5275539847682336\n",
      "GD iter. 456/499: loss=0.5275243806026708\n",
      "GD iter. 457/499: loss=0.5275302250819095\n",
      "GD iter. 458/499: loss=0.5275037871436936\n",
      "GD iter. 459/499: loss=0.5275114110868209\n",
      "GD iter. 460/499: loss=0.5274826930530851\n",
      "GD iter. 461/499: loss=0.5274920942125266\n",
      "GD iter. 462/499: loss=0.5274628762838515\n",
      "GD iter. 463/499: loss=0.5274704562304128\n",
      "GD iter. 464/499: loss=0.5274427891303439\n",
      "GD iter. 465/499: loss=0.527450509664452\n",
      "GD iter. 466/499: loss=0.5274210136620784\n",
      "GD iter. 467/499: loss=0.5274339369084448\n",
      "GD iter. 468/499: loss=0.5274015608286671\n",
      "GD iter. 469/499: loss=0.5274150892843286\n",
      "GD iter. 470/499: loss=0.5273822622268955\n",
      "GD iter. 471/499: loss=0.5273964015330974\n",
      "GD iter. 472/499: loss=0.5273627496544385\n",
      "GD iter. 473/499: loss=0.5273756711427774\n",
      "GD iter. 474/499: loss=0.5273429673719365\n",
      "GD iter. 475/499: loss=0.5273562738650373\n",
      "GD iter. 476/499: loss=0.5273237804895513\n",
      "GD iter. 477/499: loss=0.5273386643689835\n",
      "GD iter. 478/499: loss=0.5273048670426664\n",
      "GD iter. 479/499: loss=0.5273198893373887\n",
      "GD iter. 480/499: loss=0.5272861089694072\n",
      "GD iter. 481/499: loss=0.5272986390024648\n",
      "GD iter. 482/499: loss=0.5272680583308839\n",
      "GD iter. 483/499: loss=0.5272816097354076\n",
      "GD iter. 484/499: loss=0.5272486342174408\n",
      "GD iter. 485/499: loss=0.5272632802517988\n",
      "GD iter. 486/499: loss=0.5272303144111176\n",
      "GD iter. 487/499: loss=0.5272459926248781\n",
      "GD iter. 488/499: loss=0.527212477837576\n",
      "GD iter. 489/499: loss=0.5272267842510444\n",
      "GD iter. 490/499: loss=0.5271946409993644\n",
      "GD iter. 491/499: loss=0.5272093431322279\n",
      "GD iter. 492/499: loss=0.5271769453433479\n",
      "GD iter. 493/499: loss=0.5271920498764111\n",
      "GD iter. 494/499: loss=0.5271583153203423\n",
      "GD iter. 495/499: loss=0.5271753590376943\n",
      "GD iter. 496/499: loss=0.5271410473231595\n",
      "GD iter. 497/499: loss=0.5271552572840481\n",
      "GD iter. 498/499: loss=0.527126169215061\n",
      "GD iter. 499/499: loss=0.5271355603408103\n",
      "The Accuracy is: 0.9261\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "GD iter. 0/499: loss=1.0069698588666818\n",
      "GD iter. 1/499: loss=1.0018825832354745\n",
      "GD iter. 2/499: loss=0.996810607814804\n",
      "GD iter. 3/499: loss=0.9917538815221149\n",
      "GD iter. 4/499: loss=0.9867123534588632\n",
      "GD iter. 5/499: loss=0.9816859729098217\n",
      "GD iter. 6/499: loss=0.9766746893423878\n",
      "GD iter. 7/499: loss=0.9716784524058927\n",
      "GD iter. 8/499: loss=0.9666972119309158\n",
      "GD iter. 9/499: loss=0.9617309179285994\n",
      "GD iter. 10/499: loss=0.9567795205899673\n",
      "GD iter. 11/499: loss=0.9518429702852457\n",
      "GD iter. 12/499: loss=0.9469212175631867\n",
      "GD iter. 13/499: loss=0.9420142131503949\n",
      "GD iter. 14/499: loss=0.9371219079506558\n",
      "GD iter. 15/499: loss=0.9322442530442666\n",
      "GD iter. 16/499: loss=0.9273811996873721\n",
      "GD iter. 17/499: loss=0.9225326993112992\n",
      "GD iter. 18/499: loss=0.9176987035218978\n",
      "GD iter. 19/499: loss=0.9128791640988808\n",
      "GD iter. 20/499: loss=0.9080740329951696\n",
      "GD iter. 21/499: loss=0.9032832623362413\n",
      "GD iter. 22/499: loss=0.8985068044194761\n",
      "GD iter. 23/499: loss=0.8937446117135115\n",
      "GD iter. 24/499: loss=0.888996636857595\n",
      "GD iter. 25/499: loss=0.8842628326609419\n",
      "GD iter. 26/499: loss=0.8795431521020945\n",
      "GD iter. 27/499: loss=0.8748375483282833\n",
      "GD iter. 28/499: loss=0.8701459746547925\n",
      "GD iter. 29/499: loss=0.865468384564326\n",
      "GD iter. 30/499: loss=0.8608047317063763\n",
      "GD iter. 31/499: loss=0.8561549698965978\n",
      "GD iter. 32/499: loss=0.8515190531161793\n",
      "GD iter. 33/499: loss=0.8468969355112206\n",
      "GD iter. 34/499: loss=0.8422885713921129\n",
      "GD iter. 35/499: loss=0.8376939152329189\n",
      "GD iter. 36/499: loss=0.8331129216707572\n",
      "GD iter. 37/499: loss=0.8285455455051877\n",
      "GD iter. 38/499: loss=0.8239917416976013\n",
      "GD iter. 39/499: loss=0.8194514653706095\n",
      "GD iter. 40/499: loss=0.8149246718074388\n",
      "GD iter. 41/499: loss=0.8104113164513259\n",
      "GD iter. 42/499: loss=0.8059113549049154\n",
      "GD iter. 43/499: loss=0.80142474292966\n",
      "GD iter. 44/499: loss=0.7969514364452241\n",
      "GD iter. 45/499: loss=0.7924913915288876\n",
      "GD iter. 46/499: loss=0.7880445644149534\n",
      "GD iter. 47/499: loss=0.7836109114941575\n",
      "GD iter. 48/499: loss=0.77919038931308\n",
      "GD iter. 49/499: loss=0.7747829545735598\n",
      "GD iter. 50/499: loss=0.7703885641321104\n",
      "GD iter. 51/499: loss=0.7660071749993395\n",
      "GD iter. 52/499: loss=0.7616387443393678\n",
      "GD iter. 53/499: loss=0.7572832294692552\n",
      "GD iter. 54/499: loss=0.7529405878584231\n",
      "GD iter. 55/499: loss=0.7486107771280835\n",
      "GD iter. 56/499: loss=0.7442937550506694\n",
      "GD iter. 57/499: loss=0.7399894795492648\n",
      "GD iter. 58/499: loss=0.7356979086970402\n",
      "GD iter. 59/499: loss=0.7314190007166891\n",
      "GD iter. 60/499: loss=0.7271527139798661\n",
      "GD iter. 61/499: loss=0.7228990070066276\n",
      "GD iter. 62/499: loss=0.7186578384648746\n",
      "GD iter. 63/499: loss=0.7144291671697982\n",
      "GD iter. 64/499: loss=0.710212952083326\n",
      "GD iter. 65/499: loss=0.7060091523135729\n",
      "GD iter. 66/499: loss=0.7018177271142898\n",
      "GD iter. 67/499: loss=0.6976386358843207\n",
      "GD iter. 68/499: loss=0.6934718381670555\n",
      "GD iter. 69/499: loss=0.6893172936498893\n",
      "GD iter. 70/499: loss=0.6851749621636813\n",
      "GD iter. 71/499: loss=0.6810448036822181\n",
      "GD iter. 72/499: loss=0.6769267783216764\n",
      "GD iter. 73/499: loss=0.6728208463400893\n",
      "GD iter. 74/499: loss=0.6687269681368159\n",
      "GD iter. 75/499: loss=0.6646451042520094\n",
      "GD iter. 76/499: loss=0.660575215366091\n",
      "GD iter. 77/499: loss=0.6565172622992236\n",
      "GD iter. 78/499: loss=0.6524712060107879\n",
      "GD iter. 79/499: loss=0.6484370075988617\n",
      "GD iter. 80/499: loss=0.6444146282996991\n",
      "GD iter. 81/499: loss=0.6404040294872138\n",
      "GD iter. 82/499: loss=0.6364051726724628\n",
      "GD iter. 83/499: loss=0.6324180195031336\n",
      "GD iter. 84/499: loss=0.6284425317630319\n",
      "GD iter. 85/499: loss=0.624478671371572\n",
      "GD iter. 86/499: loss=0.6205264003832698\n",
      "GD iter. 87/499: loss=0.6165856809872358\n",
      "GD iter. 88/499: loss=0.6126564755066728\n",
      "GD iter. 89/499: loss=0.6087387463983729\n",
      "GD iter. 90/499: loss=0.6048324562522184\n",
      "GD iter. 91/499: loss=0.600937567790683\n",
      "GD iter. 92/499: loss=0.5970540438683369\n",
      "GD iter. 93/499: loss=0.5931818474713514\n",
      "GD iter. 94/499: loss=0.589320941717008\n",
      "GD iter. 95/499: loss=0.5854712898532066\n",
      "GD iter. 96/499: loss=0.5816328552579787\n",
      "GD iter. 97/499: loss=0.5778056014389992\n",
      "GD iter. 98/499: loss=0.5739894920331035\n",
      "GD iter. 99/499: loss=0.5701844908058027\n",
      "GD iter. 100/499: loss=0.5663905616508048\n",
      "GD iter. 101/499: loss=0.5626076685895339\n",
      "GD iter. 102/499: loss=0.5588357757706538\n",
      "GD iter. 103/499: loss=0.5550748474695925\n",
      "GD iter. 104/499: loss=0.5513248480880686\n",
      "GD iter. 105/499: loss=0.5475881887839342\n",
      "GD iter. 106/499: loss=0.5438769615089365\n",
      "GD iter. 107/499: loss=0.540314273809893\n",
      "GD iter. 108/499: loss=0.5377307805581063\n",
      "GD iter. 109/499: loss=0.5364203400086626\n",
      "GD iter. 110/499: loss=0.5358214306227077\n",
      "GD iter. 111/499: loss=0.5355079798012761\n",
      "GD iter. 112/499: loss=0.5353348909279645\n",
      "GD iter. 113/499: loss=0.5352185980553205\n",
      "GD iter. 114/499: loss=0.535129664981146\n",
      "GD iter. 115/499: loss=0.535063284487679\n",
      "GD iter. 116/499: loss=0.5350031044402916\n",
      "GD iter. 117/499: loss=0.5349541145673389\n",
      "GD iter. 118/499: loss=0.5349109110764045\n",
      "GD iter. 119/499: loss=0.5348679294203802\n",
      "GD iter. 120/499: loss=0.5348266708444941\n",
      "GD iter. 121/499: loss=0.5347872462885763\n",
      "GD iter. 122/499: loss=0.5347479784475292\n",
      "GD iter. 123/499: loss=0.5347088666967716\n",
      "GD iter. 124/499: loss=0.5346699315564445\n",
      "GD iter. 125/499: loss=0.5346302582108989\n",
      "GD iter. 126/499: loss=0.5345920679987153\n",
      "GD iter. 127/499: loss=0.534552765087821\n",
      "GD iter. 128/499: loss=0.5345160741659761\n",
      "GD iter. 129/499: loss=0.5344782563005716\n",
      "GD iter. 130/499: loss=0.5344405894110691\n",
      "GD iter. 131/499: loss=0.5344030728944575\n",
      "GD iter. 132/499: loss=0.534365706150135\n",
      "GD iter. 133/499: loss=0.5343284899294768\n",
      "GD iter. 134/499: loss=0.5342907473839678\n",
      "GD iter. 135/499: loss=0.5342544969880434\n",
      "GD iter. 136/499: loss=0.5342154084618557\n",
      "GD iter. 137/499: loss=0.5341785689108458\n",
      "GD iter. 138/499: loss=0.5341410976463413\n",
      "GD iter. 139/499: loss=0.5341037738968681\n",
      "GD iter. 140/499: loss=0.5340665970773961\n",
      "GD iter. 141/499: loss=0.5340295666052244\n",
      "GD iter. 142/499: loss=0.533992687003686\n",
      "GD iter. 143/499: loss=0.5339556618498809\n",
      "GD iter. 144/499: loss=0.5339187821140866\n",
      "GD iter. 145/499: loss=0.533882750704944\n",
      "GD iter. 146/499: loss=0.5338461731834883\n",
      "GD iter. 147/499: loss=0.533808961711628\n",
      "GD iter. 148/499: loss=0.5337733834902743\n",
      "GD iter. 149/499: loss=0.5337371522864884\n",
      "GD iter. 150/499: loss=0.5337018654828751\n",
      "GD iter. 151/499: loss=0.5336659340308203\n",
      "GD iter. 152/499: loss=0.5336314480462321\n",
      "GD iter. 153/499: loss=0.5335964109418273\n",
      "GD iter. 154/499: loss=0.5335615121734809\n",
      "GD iter. 155/499: loss=0.5335267511917466\n",
      "GD iter. 156/499: loss=0.5334921274493669\n",
      "GD iter. 157/499: loss=0.5334576512530205\n",
      "GD iter. 158/499: loss=0.5334218178632535\n",
      "GD iter. 159/499: loss=0.5333893295924413\n",
      "GD iter. 160/499: loss=0.5333537758711153\n",
      "GD iter. 161/499: loss=0.5333218799917809\n",
      "GD iter. 162/499: loss=0.5332866148776555\n",
      "GD iter. 163/499: loss=0.5332535039151983\n",
      "GD iter. 164/499: loss=0.5332205312002463\n",
      "GD iter. 165/499: loss=0.5331863154534128\n",
      "GD iter. 166/499: loss=0.533155132099179\n",
      "GD iter. 167/499: loss=0.5331211503037966\n",
      "GD iter. 168/499: loss=0.5330873033103651\n",
      "GD iter. 169/499: loss=0.5330550073972279\n",
      "GD iter. 170/499: loss=0.5330214340702822\n",
      "GD iter. 171/499: loss=0.5329894884334915\n",
      "GD iter. 172/499: loss=0.5329561837741478\n",
      "GD iter. 173/499: loss=0.5329259086936595\n",
      "GD iter. 174/499: loss=0.5328928311649074\n",
      "GD iter. 175/499: loss=0.532859886205548\n",
      "GD iter. 176/499: loss=0.5328285901581129\n",
      "GD iter. 177/499: loss=0.532795958266626\n",
      "GD iter. 178/499: loss=0.5327689594795417\n",
      "GD iter. 179/499: loss=0.532736514505089\n",
      "GD iter. 180/499: loss=0.5327055663954297\n",
      "GD iter. 181/499: loss=0.5326734488758877\n",
      "GD iter. 182/499: loss=0.5326427623356285\n",
      "GD iter. 183/499: loss=0.5326108878165949\n",
      "GD iter. 184/499: loss=0.5325791448036146\n",
      "GD iter. 185/499: loss=0.5325488247748327\n",
      "GD iter. 186/499: loss=0.5325173327091224\n",
      "GD iter. 187/499: loss=0.5324872612911659\n",
      "GD iter. 188/499: loss=0.5324560363524358\n",
      "GD iter. 189/499: loss=0.5324288316555786\n",
      "GD iter. 190/499: loss=0.5323978025808853\n",
      "GD iter. 191/499: loss=0.5323668949575008\n",
      "GD iter. 192/499: loss=0.5323374333735561\n",
      "GD iter. 193/499: loss=0.5323081488897666\n",
      "GD iter. 194/499: loss=0.5322789809088527\n",
      "GD iter. 195/499: loss=0.5322499465889584\n",
      "GD iter. 196/499: loss=0.5322199484833542\n",
      "GD iter. 197/499: loss=0.5321900970025153\n",
      "GD iter. 198/499: loss=0.5321627417963771\n",
      "GD iter. 199/499: loss=0.5321330889910354\n",
      "GD iter. 200/499: loss=0.5321035784812858\n",
      "GD iter. 201/499: loss=0.5320765687925696\n",
      "GD iter. 202/499: loss=0.5320472571834449\n",
      "GD iter. 203/499: loss=0.5320180881605157\n",
      "GD iter. 204/499: loss=0.5319929417646069\n",
      "GD iter. 205/499: loss=0.5319635596671409\n",
      "GD iter. 206/499: loss=0.5319346960682996\n",
      "GD iter. 207/499: loss=0.5319059577344264\n",
      "GD iter. 208/499: loss=0.5318784256298886\n",
      "GD iter. 209/499: loss=0.5318510163557236\n",
      "GD iter. 210/499: loss=0.5318222493390077\n",
      "GD iter. 211/499: loss=0.531797948139947\n",
      "GD iter. 212/499: loss=0.5317671120427827\n",
      "GD iter. 213/499: loss=0.5317415069579658\n",
      "GD iter. 214/499: loss=0.5317120904468234\n",
      "GD iter. 215/499: loss=0.53168891472038\n",
      "GD iter. 216/499: loss=0.5316585310652829\n",
      "GD iter. 217/499: loss=0.5316355899605638\n",
      "GD iter. 218/499: loss=0.5316054079096862\n",
      "GD iter. 219/499: loss=0.5315810844460125\n",
      "GD iter. 220/499: loss=0.5315513656633614\n",
      "GD iter. 221/499: loss=0.5315313081494502\n",
      "GD iter. 222/499: loss=0.5314989241519295\n",
      "GD iter. 223/499: loss=0.5314775343852143\n",
      "GD iter. 224/499: loss=0.5314475787312656\n",
      "GD iter. 225/499: loss=0.5314235985607683\n",
      "GD iter. 226/499: loss=0.5313962144892507\n",
      "GD iter. 227/499: loss=0.5313711558398001\n",
      "GD iter. 228/499: loss=0.5313450894463847\n",
      "GD iter. 229/499: loss=0.5313191321775684\n",
      "GD iter. 230/499: loss=0.5312959244146118\n",
      "GD iter. 231/499: loss=0.5312690303080316\n",
      "GD iter. 232/499: loss=0.5312432166887983\n",
      "GD iter. 233/499: loss=0.5312214263396451\n",
      "GD iter. 234/499: loss=0.531193532462008\n",
      "GD iter. 235/499: loss=0.5311691062907661\n",
      "GD iter. 236/499: loss=0.5311436692337052\n",
      "GD iter. 237/499: loss=0.5311209588330293\n",
      "GD iter. 238/499: loss=0.5310957104441183\n",
      "GD iter. 239/499: loss=0.5310705607391553\n",
      "GD iter. 240/499: loss=0.5310455262917912\n",
      "GD iter. 241/499: loss=0.5310256823950664\n",
      "GD iter. 242/499: loss=0.5309984392730542\n",
      "GD iter. 243/499: loss=0.5309736748864163\n",
      "GD iter. 244/499: loss=0.5309490167229597\n",
      "GD iter. 245/499: loss=0.5309270857611247\n",
      "GD iter. 246/499: loss=0.5309011317452175\n",
      "GD iter. 247/499: loss=0.5308838475302546\n",
      "GD iter. 248/499: loss=0.5308538125948176\n",
      "GD iter. 249/499: loss=0.5308356851723866\n",
      "GD iter. 250/499: loss=0.5308075045556131\n",
      "GD iter. 251/499: loss=0.5307822154088859\n",
      "GD iter. 252/499: loss=0.530765603270794\n",
      "GD iter. 253/499: loss=0.5307361829661524\n",
      "GD iter. 254/499: loss=0.530719912228364\n",
      "GD iter. 255/499: loss=0.5306905359487184\n",
      "GD iter. 256/499: loss=0.5306721545614985\n",
      "GD iter. 257/499: loss=0.5306447295494929\n",
      "GD iter. 258/499: loss=0.5306296647219423\n",
      "GD iter. 259/499: loss=0.5305994422438208\n",
      "GD iter. 260/499: loss=0.5305817269385887\n",
      "GD iter. 261/499: loss=0.5305533253909503\n",
      "GD iter. 262/499: loss=0.5305381848518523\n",
      "GD iter. 263/499: loss=0.5305097036923279\n",
      "GD iter. 264/499: loss=0.5304923484488102\n",
      "GD iter. 265/499: loss=0.5304685234359732\n",
      "GD iter. 266/499: loss=0.5304447946879788\n",
      "GD iter. 267/499: loss=0.5304226028464926\n",
      "GD iter. 268/499: loss=0.5304005111513048\n",
      "GD iter. 269/499: loss=0.5303811714354001\n",
      "GD iter. 270/499: loss=0.5303563665240322\n",
      "GD iter. 271/499: loss=0.5303435737346829\n",
      "GD iter. 272/499: loss=0.5303126774333327\n",
      "GD iter. 273/499: loss=0.530302304080478\n",
      "GD iter. 274/499: loss=0.5302711176238663\n",
      "GD iter. 275/499: loss=0.5302601863407906\n",
      "GD iter. 276/499: loss=0.5302290944694177\n",
      "GD iter. 277/499: loss=0.5302175803408073\n",
      "GD iter. 278/499: loss=0.5301868504155854\n",
      "GD iter. 279/499: loss=0.5301753086726796\n",
      "GD iter. 280/499: loss=0.5301449404446744\n",
      "GD iter. 281/499: loss=0.5301325707013664\n",
      "GD iter. 282/499: loss=0.5301037180226347\n",
      "GD iter. 283/499: loss=0.5300926833729811\n",
      "GD iter. 284/499: loss=0.5300611834017771\n",
      "GD iter. 285/499: loss=0.5300523466731056\n",
      "GD iter. 286/499: loss=0.5300225764634412\n",
      "GD iter. 287/499: loss=0.5300120201259664\n",
      "GD iter. 288/499: loss=0.5299826287320847\n",
      "GD iter. 289/499: loss=0.5299698133649806\n",
      "GD iter. 290/499: loss=0.5299427434742268\n",
      "GD iter. 291/499: loss=0.5299310435611775\n",
      "GD iter. 292/499: loss=0.5299022891742422\n",
      "GD iter. 293/499: loss=0.5298920279316169\n",
      "GD iter. 294/499: loss=0.5298632849987154\n",
      "GD iter. 295/499: loss=0.5298533262994419\n",
      "GD iter. 296/499: loss=0.5298227543365732\n",
      "GD iter. 297/499: loss=0.5298134139604944\n",
      "GD iter. 298/499: loss=0.5297860983867851\n",
      "GD iter. 299/499: loss=0.5297756291454079\n",
      "GD iter. 300/499: loss=0.5297455198789682\n",
      "GD iter. 301/499: loss=0.52973540621021\n",
      "GD iter. 302/499: loss=0.5297090930080516\n",
      "GD iter. 303/499: loss=0.5296987090844069\n",
      "GD iter. 304/499: loss=0.5296691315908825\n",
      "GD iter. 305/499: loss=0.5296591201431874\n",
      "GD iter. 306/499: loss=0.5296341283944116\n",
      "GD iter. 307/499: loss=0.5296242201649702\n",
      "GD iter. 308/499: loss=0.529593963999445\n",
      "GD iter. 309/499: loss=0.5295855550049959\n",
      "GD iter. 310/499: loss=0.5295595260153831\n",
      "GD iter. 311/499: loss=0.5295497270206634\n",
      "GD iter. 312/499: loss=0.5295201585634723\n",
      "GD iter. 313/499: loss=0.5295119427009859\n",
      "GD iter. 314/499: loss=0.5294855583148337\n",
      "GD iter. 315/499: loss=0.5294768419422657\n",
      "GD iter. 316/499: loss=0.5294484616755276\n",
      "GD iter. 317/499: loss=0.5294409496088608\n",
      "GD iter. 318/499: loss=0.5294110402869165\n",
      "GD iter. 319/499: loss=0.5294029208954811\n",
      "GD iter. 320/499: loss=0.5293790408409835\n",
      "GD iter. 321/499: loss=0.5293675985906736\n",
      "GD iter. 322/499: loss=0.5293438413978976\n",
      "GD iter. 323/499: loss=0.5293325629411443\n",
      "GD iter. 324/499: loss=0.52930857672095\n",
      "GD iter. 325/499: loss=0.5292984931868667\n",
      "GD iter. 326/499: loss=0.5292734501237619\n",
      "GD iter. 327/499: loss=0.5292661947943268\n",
      "GD iter. 328/499: loss=0.5292366889088425\n",
      "GD iter. 329/499: loss=0.5292308539083876\n",
      "GD iter. 330/499: loss=0.5292038663869018\n",
      "GD iter. 331/499: loss=0.5291963260468678\n",
      "GD iter. 332/499: loss=0.5291715437167592\n",
      "GD iter. 333/499: loss=0.5291598999701261\n",
      "GD iter. 334/499: loss=0.5291395918138617\n",
      "GD iter. 335/499: loss=0.5291284659722234\n",
      "GD iter. 336/499: loss=0.529106569186248\n",
      "GD iter. 337/499: loss=0.5290945358301015\n",
      "GD iter. 338/499: loss=0.529072919980522\n",
      "GD iter. 339/499: loss=0.5290619736033463\n",
      "GD iter. 340/499: loss=0.5290404225262206\n",
      "GD iter. 341/499: loss=0.5290286798065008\n",
      "GD iter. 342/499: loss=0.5290073988800805\n",
      "GD iter. 343/499: loss=0.528998216387566\n",
      "GD iter. 344/499: loss=0.5289705227536242\n",
      "GD iter. 345/499: loss=0.5289678612207371\n",
      "GD iter. 346/499: loss=0.5289391410839683\n",
      "GD iter. 347/499: loss=0.5289347525878891\n",
      "GD iter. 348/499: loss=0.5289081871166995\n",
      "GD iter. 349/499: loss=0.5289044064563373\n",
      "GD iter. 350/499: loss=0.5288759775505241\n",
      "GD iter. 351/499: loss=0.528871811932179\n",
      "GD iter. 352/499: loss=0.5288459028746176\n",
      "GD iter. 353/499: loss=0.5288409526216399\n",
      "GD iter. 354/499: loss=0.5288151684983611\n",
      "GD iter. 355/499: loss=0.5288104685998198\n",
      "GD iter. 356/499: loss=0.5287828212149732\n",
      "GD iter. 357/499: loss=0.5287807459251558\n",
      "GD iter. 358/499: loss=0.5287528729765205\n",
      "GD iter. 359/499: loss=0.5287512606643244\n",
      "GD iter. 360/499: loss=0.5287231625791597\n",
      "GD iter. 361/499: loss=0.5287220510017075\n",
      "GD iter. 362/499: loss=0.5286926933552584\n",
      "GD iter. 363/499: loss=0.528691286025454\n",
      "GD iter. 364/499: loss=0.5286643009339697\n",
      "GD iter. 365/499: loss=0.5286598694505947\n",
      "GD iter. 366/499: loss=0.528636964264215\n",
      "GD iter. 367/499: loss=0.5286256482391167\n",
      "GD iter. 368/499: loss=0.5286100621956827\n",
      "GD iter. 369/499: loss=0.5285969477838505\n",
      "GD iter. 370/499: loss=0.5285803043207272\n",
      "GD iter. 371/499: loss=0.5285694329493553\n",
      "GD iter. 372/499: loss=0.5285514409068338\n",
      "GD iter. 373/499: loss=0.5285406991030003\n",
      "GD iter. 374/499: loss=0.5285208017293651\n",
      "GD iter. 375/499: loss=0.5285180077921342\n",
      "GD iter. 376/499: loss=0.528488995172484\n",
      "GD iter. 377/499: loss=0.5284879620974815\n",
      "GD iter. 378/499: loss=0.5284608754592249\n",
      "GD iter. 379/499: loss=0.5284599557665158\n",
      "GD iter. 380/499: loss=0.5284346388502494\n",
      "GD iter. 381/499: loss=0.5284340239137914\n",
      "GD iter. 382/499: loss=0.528404415060288\n",
      "GD iter. 383/499: loss=0.5284050378206562\n",
      "GD iter. 384/499: loss=0.5283779431131194\n",
      "GD iter. 385/499: loss=0.5283774683888768\n",
      "GD iter. 386/499: loss=0.5283518749455763\n",
      "GD iter. 387/499: loss=0.5283512881409236\n",
      "GD iter. 388/499: loss=0.5283228176184449\n",
      "GD iter. 389/499: loss=0.5283238926619209\n",
      "GD iter. 390/499: loss=0.5282963760843279\n",
      "GD iter. 391/499: loss=0.5282968574502162\n",
      "GD iter. 392/499: loss=0.528271699184247\n",
      "GD iter. 393/499: loss=0.5282685953677491\n",
      "GD iter. 394/499: loss=0.5282464263595044\n",
      "GD iter. 395/499: loss=0.5282428238424883\n",
      "GD iter. 396/499: loss=0.5282193945271747\n",
      "GD iter. 397/499: loss=0.5282166686916325\n",
      "GD iter. 398/499: loss=0.5281934428869559\n",
      "GD iter. 399/499: loss=0.5281908484523757\n",
      "GD iter. 400/499: loss=0.5281666078119873\n",
      "GD iter. 401/499: loss=0.5281687351298289\n",
      "GD iter. 402/499: loss=0.5281377587932237\n",
      "GD iter. 403/499: loss=0.5281392353776662\n",
      "GD iter. 404/499: loss=0.5281164604962365\n",
      "GD iter. 405/499: loss=0.5281154216466646\n",
      "GD iter. 406/499: loss=0.528088173277252\n",
      "GD iter. 407/499: loss=0.5280895075596713\n",
      "GD iter. 408/499: loss=0.528065588019794\n",
      "GD iter. 409/499: loss=0.528066405107611\n",
      "GD iter. 410/499: loss=0.5280376082896339\n",
      "GD iter. 411/499: loss=0.5280399946570385\n",
      "GD iter. 412/499: loss=0.5280200085822074\n",
      "GD iter. 413/499: loss=0.5280117879372065\n",
      "GD iter. 414/499: loss=0.527996334403451\n",
      "GD iter. 415/499: loss=0.527988386171604\n",
      "GD iter. 416/499: loss=0.5279714039322436\n",
      "GD iter. 417/499: loss=0.5279648554270715\n",
      "GD iter. 418/499: loss=0.5279478425170199\n",
      "GD iter. 419/499: loss=0.5279415544840618\n",
      "GD iter. 420/499: loss=0.5279211601626909\n",
      "GD iter. 421/499: loss=0.5279227696849519\n",
      "GD iter. 422/499: loss=0.5278917635350258\n",
      "GD iter. 423/499: loss=0.5278977704838979\n",
      "GD iter. 424/499: loss=0.5278694870156149\n",
      "GD iter. 425/499: loss=0.527875031530677\n",
      "GD iter. 426/499: loss=0.5278454609088117\n",
      "GD iter. 427/499: loss=0.5278538296590609\n",
      "GD iter. 428/499: loss=0.527821821893129\n",
      "GD iter. 429/499: loss=0.5278317942692254\n",
      "GD iter. 430/499: loss=0.5277987633506537\n",
      "GD iter. 431/499: loss=0.5278083188994583\n",
      "GD iter. 432/499: loss=0.5277764678465996\n",
      "GD iter. 433/499: loss=0.5277840838784982\n",
      "GD iter. 434/499: loss=0.5277547709270747\n",
      "GD iter. 435/499: loss=0.5277625618399654\n",
      "GD iter. 436/499: loss=0.5277328578881787\n",
      "GD iter. 437/499: loss=0.5277412200837807\n",
      "GD iter. 438/499: loss=0.5277103102260224\n",
      "GD iter. 439/499: loss=0.5277180869436071\n",
      "GD iter. 440/499: loss=0.5276896279716735\n",
      "GD iter. 441/499: loss=0.5276979571740537\n",
      "GD iter. 442/499: loss=0.527666268626638\n",
      "GD iter. 443/499: loss=0.5276761281678376\n",
      "GD iter. 444/499: loss=0.5276452670922311\n",
      "GD iter. 445/499: loss=0.5276544872687282\n",
      "GD iter. 446/499: loss=0.5276246425071318\n",
      "GD iter. 447/499: loss=0.5276310628386567\n",
      "GD iter. 448/499: loss=0.5276080439010523\n",
      "GD iter. 449/499: loss=0.5276060295259783\n",
      "GD iter. 450/499: loss=0.5275924511017548\n",
      "GD iter. 451/499: loss=0.5275790579464472\n",
      "GD iter. 452/499: loss=0.5275723156619079\n",
      "GD iter. 453/499: loss=0.5275589256477494\n",
      "GD iter. 454/499: loss=0.5275473533334959\n",
      "GD iter. 455/499: loss=0.5275419548738458\n",
      "GD iter. 456/499: loss=0.5275281391408366\n",
      "GD iter. 457/499: loss=0.5275166832097001\n",
      "GD iter. 458/499: loss=0.5275103724295994\n",
      "GD iter. 459/499: loss=0.5274967418634615\n",
      "GD iter. 460/499: loss=0.5274905887720477\n",
      "GD iter. 461/499: loss=0.5274769869935068\n",
      "GD iter. 462/499: loss=0.5274681959738876\n",
      "GD iter. 463/499: loss=0.5274563447763779\n",
      "GD iter. 464/499: loss=0.5274519090844947\n",
      "GD iter. 465/499: loss=0.5274311799568795\n",
      "GD iter. 466/499: loss=0.5274397576096208\n",
      "GD iter. 467/499: loss=0.527406216139246\n",
      "GD iter. 468/499: loss=0.5274182303007268\n",
      "GD iter. 469/499: loss=0.5273883764251374\n",
      "GD iter. 470/499: loss=0.5274025532659365\n",
      "GD iter. 471/499: loss=0.527367018618937\n",
      "GD iter. 472/499: loss=0.5273782428203679\n",
      "GD iter. 473/499: loss=0.5273498374518534\n",
      "GD iter. 474/499: loss=0.5273640675736684\n",
      "GD iter. 475/499: loss=0.527328797215819\n",
      "GD iter. 476/499: loss=0.527340081222062\n",
      "GD iter. 477/499: loss=0.5273123839041389\n",
      "GD iter. 478/499: loss=0.5273261141352639\n",
      "GD iter. 479/499: loss=0.5272895463084657\n",
      "GD iter. 480/499: loss=0.5273007389149424\n",
      "GD iter. 481/499: loss=0.5272794949796781\n",
      "GD iter. 482/499: loss=0.527289301707403\n",
      "GD iter. 483/499: loss=0.527253055931624\n",
      "GD iter. 484/499: loss=0.527264391492677\n",
      "GD iter. 485/499: loss=0.5272398345368288\n",
      "GD iter. 486/499: loss=0.5272513099102938\n",
      "GD iter. 487/499: loss=0.5272183397973472\n",
      "GD iter. 488/499: loss=0.5272315549020323\n",
      "GD iter. 489/499: loss=0.5271998712159074\n",
      "GD iter. 490/499: loss=0.5272132105039886\n",
      "GD iter. 491/499: loss=0.5271833967581685\n",
      "GD iter. 492/499: loss=0.5271990084412965\n",
      "GD iter. 493/499: loss=0.5271626812543816\n",
      "GD iter. 494/499: loss=0.5271779517375685\n",
      "GD iter. 495/499: loss=0.5271484838699653\n",
      "GD iter. 496/499: loss=0.5271633598906824\n",
      "GD iter. 497/499: loss=0.5271284366403426\n",
      "GD iter. 498/499: loss=0.5271433806016121\n",
      "GD iter. 499/499: loss=0.5271138478895461\n",
      "The Accuracy is: 0.9064\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "GD iter. 0/499: loss=0.9956866822070668\n",
      "GD iter. 1/499: loss=0.9906285837079478\n",
      "GD iter. 2/499: loss=0.9855856872599488\n",
      "GD iter. 3/499: loss=0.9805579421358941\n",
      "GD iter. 4/499: loss=0.9755452977912737\n",
      "GD iter. 5/499: loss=0.9705477038635537\n",
      "GD iter. 6/499: loss=0.9655651101714875\n",
      "GD iter. 7/499: loss=0.9605974667144328\n",
      "GD iter. 8/499: loss=0.9556447236716694\n",
      "GD iter. 9/499: loss=0.9507068314017195\n",
      "GD iter. 10/499: loss=0.9457837404416722\n",
      "GD iter. 11/499: loss=0.9408754015065078\n",
      "GD iter. 12/499: loss=0.9359817654884284\n",
      "GD iter. 13/499: loss=0.9311027834561878\n",
      "GD iter. 14/499: loss=0.9262384066544259\n",
      "GD iter. 15/499: loss=0.9213885865030051\n",
      "GD iter. 16/499: loss=0.9165532745963498\n",
      "GD iter. 17/499: loss=0.9117324227027864\n",
      "GD iter. 18/499: loss=0.9069259827638898\n",
      "GD iter. 19/499: loss=0.9021339068938279\n",
      "GD iter. 20/499: loss=0.8973561473787115\n",
      "GD iter. 21/499: loss=0.8925926566759471\n",
      "GD iter. 22/499: loss=0.8878433874135901\n",
      "GD iter. 23/499: loss=0.8831082923897011\n",
      "GD iter. 24/499: loss=0.8783873245717065\n",
      "GD iter. 25/499: loss=0.8736804370957592\n",
      "GD iter. 26/499: loss=0.8689875832661028\n",
      "GD iter. 27/499: loss=0.8643087165544389\n",
      "GD iter. 28/499: loss=0.8596437905992961\n",
      "GD iter. 29/499: loss=0.8549927592054009\n",
      "GD iter. 30/499: loss=0.8503555763430519\n",
      "GD iter. 31/499: loss=0.8457321961474975\n",
      "GD iter. 32/499: loss=0.8411225729183128\n",
      "GD iter. 33/499: loss=0.836526661118782\n",
      "GD iter. 34/499: loss=0.8319444153752814\n",
      "GD iter. 35/499: loss=0.8273757904766665\n",
      "GD iter. 36/499: loss=0.8228207413736595\n",
      "GD iter. 37/499: loss=0.8182792231782396\n",
      "GD iter. 38/499: loss=0.8137511911630377\n",
      "GD iter. 39/499: loss=0.8092366007607306\n",
      "GD iter. 40/499: loss=0.804735407563439\n",
      "GD iter. 41/499: loss=0.8002475673221285\n",
      "GD iter. 42/499: loss=0.795773035946011\n",
      "GD iter. 43/499: loss=0.7913117695019497\n",
      "GD iter. 44/499: loss=0.7868637242138672\n",
      "GD iter. 45/499: loss=0.7824288564621532\n",
      "GD iter. 46/499: loss=0.7780071227830779\n",
      "GD iter. 47/499: loss=0.7735984798682048\n",
      "GD iter. 48/499: loss=0.7692028845638071\n",
      "GD iter. 49/499: loss=0.7648202938702867\n",
      "GD iter. 50/499: loss=0.7604506649415946\n",
      "GD iter. 51/499: loss=0.7560939550846535\n",
      "GD iter. 52/499: loss=0.7517501217587838\n",
      "GD iter. 53/499: loss=0.7474191225751301\n",
      "GD iter. 54/499: loss=0.7431009152960917\n",
      "GD iter. 55/499: loss=0.738795457834754\n",
      "GD iter. 56/499: loss=0.7345027082543228\n",
      "GD iter. 57/499: loss=0.73022262476756\n",
      "GD iter. 58/499: loss=0.7259551657362228\n",
      "GD iter. 59/499: loss=0.7217002896705034\n",
      "GD iter. 60/499: loss=0.7174579552284729\n",
      "GD iter. 61/499: loss=0.7132281212155249\n",
      "GD iter. 62/499: loss=0.7090107465838237\n",
      "GD iter. 63/499: loss=0.7048057904317527\n",
      "GD iter. 64/499: loss=0.700613212003366\n",
      "GD iter. 65/499: loss=0.696432970687842\n",
      "GD iter. 66/499: loss=0.6922650260189389\n",
      "GD iter. 67/499: loss=0.6881093376744519\n",
      "GD iter. 68/499: loss=0.6839658654756738\n",
      "GD iter. 69/499: loss=0.6798345693868564\n",
      "GD iter. 70/499: loss=0.6757154095146736\n",
      "GD iter. 71/499: loss=0.6716083461076892\n",
      "GD iter. 72/499: loss=0.6675133395558235\n",
      "GD iter. 73/499: loss=0.6634303503898239\n",
      "GD iter. 74/499: loss=0.6593593392807371\n",
      "GD iter. 75/499: loss=0.6553002670393834\n",
      "GD iter. 76/499: loss=0.6512530946158326\n",
      "GD iter. 77/499: loss=0.6472177830988823\n",
      "GD iter. 78/499: loss=0.643194293715539\n",
      "GD iter. 79/499: loss=0.6391825878304985\n",
      "GD iter. 80/499: loss=0.6351826269456329\n",
      "GD iter. 81/499: loss=0.6311943726994742\n",
      "GD iter. 82/499: loss=0.6272177868667043\n",
      "GD iter. 83/499: loss=0.6232528313576441\n",
      "GD iter. 84/499: loss=0.619299468217747\n",
      "GD iter. 85/499: loss=0.6153576596270917\n",
      "GD iter. 86/499: loss=0.6114273678998791\n",
      "GD iter. 87/499: loss=0.6075085554839306\n",
      "GD iter. 88/499: loss=0.6036011849601868\n",
      "GD iter. 89/499: loss=0.5997052190422114\n",
      "GD iter. 90/499: loss=0.5958206205756933\n",
      "GD iter. 91/499: loss=0.591947352537953\n",
      "GD iter. 92/499: loss=0.5880853780374505\n",
      "GD iter. 93/499: loss=0.5842346603132949\n",
      "GD iter. 94/499: loss=0.5803951627347553\n",
      "GD iter. 95/499: loss=0.5765668488007746\n",
      "GD iter. 96/499: loss=0.5727496821394847\n",
      "GD iter. 97/499: loss=0.5689436265077237\n",
      "GD iter. 98/499: loss=0.5651486457905541\n",
      "GD iter. 99/499: loss=0.5613647040007851\n",
      "GD iter. 100/499: loss=0.5575917652784934\n",
      "GD iter. 101/499: loss=0.5538297938905494\n",
      "GD iter. 102/499: loss=0.5500787542301429\n",
      "GD iter. 103/499: loss=0.5463386108163104\n",
      "GD iter. 104/499: loss=0.5426690161788142\n",
      "GD iter. 105/499: loss=0.5395433624942737\n",
      "GD iter. 106/499: loss=0.5375973936539298\n",
      "GD iter. 107/499: loss=0.5366814051613923\n",
      "GD iter. 108/499: loss=0.5362049809459817\n",
      "GD iter. 109/499: loss=0.5359246696901904\n",
      "GD iter. 110/499: loss=0.535743614695473\n",
      "GD iter. 111/499: loss=0.5356234033599377\n",
      "GD iter. 112/499: loss=0.5355239878488337\n",
      "GD iter. 113/499: loss=0.5354507819939673\n",
      "GD iter. 114/499: loss=0.5353930469659797\n",
      "GD iter. 115/499: loss=0.5353399708892823\n",
      "GD iter. 116/499: loss=0.5352884971176958\n",
      "GD iter. 117/499: loss=0.5352418891883951\n",
      "GD iter. 118/499: loss=0.5351978248151195\n",
      "GD iter. 119/499: loss=0.5351539292375594\n",
      "GD iter. 120/499: loss=0.5351116394597151\n",
      "GD iter. 121/499: loss=0.5350695136198171\n",
      "GD iter. 122/499: loss=0.5350275510728738\n",
      "GD iter. 123/499: loss=0.53498575117645\n",
      "GD iter. 124/499: loss=0.5349441132906579\n",
      "GD iter. 125/499: loss=0.5349026367781466\n",
      "GD iter. 126/499: loss=0.534861321004092\n",
      "GD iter. 127/499: loss=0.5348201653361864\n",
      "GD iter. 128/499: loss=0.534779169144629\n",
      "GD iter. 129/499: loss=0.5347383318021157\n",
      "GD iter. 130/499: loss=0.5346976526838294\n",
      "GD iter. 131/499: loss=0.5346571311674295\n",
      "GD iter. 132/499: loss=0.5346167908415846\n",
      "GD iter. 133/499: loss=0.5345775418071699\n",
      "GD iter. 134/499: loss=0.5345384465462276\n",
      "GD iter. 135/499: loss=0.5344995044504103\n",
      "GD iter. 136/499: loss=0.5344607149137893\n",
      "GD iter. 137/499: loss=0.534422077332845\n",
      "GD iter. 138/499: loss=0.534383591106457\n",
      "GD iter. 139/499: loss=0.5343452556358949\n",
      "GD iter. 140/499: loss=0.5343070703248086\n",
      "GD iter. 141/499: loss=0.5342690345792191\n",
      "GD iter. 142/499: loss=0.534231147807509\n",
      "GD iter. 143/499: loss=0.534193409420412\n",
      "GD iter. 144/499: loss=0.5341558297874688\n",
      "GD iter. 145/499: loss=0.5341194202551047\n",
      "GD iter. 146/499: loss=0.5340831552533376\n",
      "GD iter. 147/499: loss=0.534047034206547\n",
      "GD iter. 148/499: loss=0.5340110565414085\n",
      "GD iter. 149/499: loss=0.5339752216868856\n",
      "GD iter. 150/499: loss=0.5339395290742189\n",
      "GD iter. 151/499: loss=0.5339039781369185\n",
      "GD iter. 152/499: loss=0.5338685683107545\n",
      "GD iter. 153/499: loss=0.5338332990337472\n",
      "GD iter. 154/499: loss=0.5337981697461599\n",
      "GD iter. 155/499: loss=0.5337631798904872\n",
      "GD iter. 156/499: loss=0.533728328911449\n",
      "GD iter. 157/499: loss=0.5336936162559802\n",
      "GD iter. 158/499: loss=0.5336590413732215\n",
      "GD iter. 159/499: loss=0.533624603714511\n",
      "GD iter. 160/499: loss=0.5335903027333762\n",
      "GD iter. 161/499: loss=0.5335561378855238\n",
      "GD iter. 162/499: loss=0.5335221086288323\n",
      "GD iter. 163/499: loss=0.5334882144233425\n",
      "GD iter. 164/499: loss=0.5334544547312491\n",
      "GD iter. 165/499: loss=0.5334208290168924\n",
      "GD iter. 166/499: loss=0.5333873367467493\n",
      "GD iter. 167/499: loss=0.5333539773894249\n",
      "GD iter. 168/499: loss=0.5333207504156445\n",
      "GD iter. 169/499: loss=0.5332876617719939\n",
      "GD iter. 170/499: loss=0.5332534261423088\n",
      "GD iter. 171/499: loss=0.5332206262288307\n",
      "GD iter. 172/499: loss=0.5331866484603024\n",
      "GD iter. 173/499: loss=0.5331528065510749\n",
      "GD iter. 174/499: loss=0.5331195102289341\n",
      "GD iter. 175/499: loss=0.5330863443089415\n",
      "GD iter. 176/499: loss=0.5330533082753104\n",
      "GD iter. 177/499: loss=0.5330204033768107\n",
      "GD iter. 178/499: loss=0.5329889273920684\n",
      "GD iter. 179/499: loss=0.5329562753340742\n",
      "GD iter. 180/499: loss=0.532923751124943\n",
      "GD iter. 181/499: loss=0.5328913542590478\n",
      "GD iter. 182/499: loss=0.532859084232772\n",
      "GD iter. 183/499: loss=0.5328269405445013\n",
      "GD iter. 184/499: loss=0.532794922694615\n",
      "GD iter. 185/499: loss=0.5327630301854787\n",
      "GD iter. 186/499: loss=0.5327312625214354\n",
      "GD iter. 187/499: loss=0.5326996192087992\n",
      "GD iter. 188/499: loss=0.5326680997558463\n",
      "GD iter. 189/499: loss=0.5326367036728071\n",
      "GD iter. 190/499: loss=0.5326054304718593\n",
      "GD iter. 191/499: loss=0.5325742796671192\n",
      "GD iter. 192/499: loss=0.5325432507746347\n",
      "GD iter. 193/499: loss=0.5325123433123772\n",
      "GD iter. 194/499: loss=0.5324815568002339\n",
      "GD iter. 195/499: loss=0.5324508907600002\n",
      "GD iter. 196/499: loss=0.5324203447153726\n",
      "GD iter. 197/499: loss=0.5323899181919404\n",
      "GD iter. 198/499: loss=0.5323596107171783\n",
      "GD iter. 199/499: loss=0.5323294218204392\n",
      "GD iter. 200/499: loss=0.53229935137687\n",
      "GD iter. 201/499: loss=0.5322707014670448\n",
      "GD iter. 202/499: loss=0.5322408636652298\n",
      "GD iter. 203/499: loss=0.5322118528527713\n",
      "GD iter. 204/499: loss=0.532182255650908\n",
      "GD iter. 205/499: loss=0.5321534742813083\n",
      "GD iter. 206/499: loss=0.5321248068265474\n",
      "GD iter. 207/499: loss=0.5320962528336127\n",
      "GD iter. 208/499: loss=0.5320678118512975\n",
      "GD iter. 209/499: loss=0.5320394834301935\n",
      "GD iter. 210/499: loss=0.532011268919593\n",
      "GD iter. 211/499: loss=0.5319824761106008\n",
      "GD iter. 212/499: loss=0.5319557899843101\n",
      "GD iter. 213/499: loss=0.531927196196033\n",
      "GD iter. 214/499: loss=0.5318987452864735\n",
      "GD iter. 215/499: loss=0.5318724283707706\n",
      "GD iter. 216/499: loss=0.5318433036219946\n",
      "GD iter. 217/499: loss=0.5318163108613138\n",
      "GD iter. 218/499: loss=0.5317888386931379\n",
      "GD iter. 219/499: loss=0.5317614984809409\n",
      "GD iter. 220/499: loss=0.5317343868421269\n",
      "GD iter. 221/499: loss=0.5317073835141626\n",
      "GD iter. 222/499: loss=0.5316792077990956\n",
      "GD iter. 223/499: loss=0.5316535135122767\n",
      "GD iter. 224/499: loss=0.5316279772237391\n",
      "GD iter. 225/499: loss=0.5315994010484538\n",
      "GD iter. 226/499: loss=0.5315740226893612\n",
      "GD iter. 227/499: loss=0.5315487944434772\n",
      "GD iter. 228/499: loss=0.5315216229723307\n",
      "GD iter. 229/499: loss=0.5314946006324668\n",
      "GD iter. 230/499: loss=0.5314696523145307\n",
      "GD iter. 231/499: loss=0.531441952195944\n",
      "GD iter. 232/499: loss=0.5314172044867459\n",
      "GD iter. 233/499: loss=0.5313916847684371\n",
      "GD iter. 234/499: loss=0.5313662691198608\n",
      "GD iter. 235/499: loss=0.5313396572594042\n",
      "GD iter. 236/499: loss=0.5313144427610309\n",
      "GD iter. 237/499: loss=0.5312893283374365\n",
      "GD iter. 238/499: loss=0.5312643135900869\n",
      "GD iter. 239/499: loss=0.5312394048569433\n",
      "GD iter. 240/499: loss=0.5312132900095621\n",
      "GD iter. 241/499: loss=0.531188574488646\n",
      "GD iter. 242/499: loss=0.5311639570605672\n",
      "GD iter. 243/499: loss=0.5311394406827747\n",
      "GD iter. 244/499: loss=0.5311148379248545\n",
      "GD iter. 245/499: loss=0.5310916616328616\n",
      "GD iter. 246/499: loss=0.531065428054317\n",
      "GD iter. 247/499: loss=0.5310424374590647\n",
      "GD iter. 248/499: loss=0.5310163496268333\n",
      "GD iter. 249/499: loss=0.5309931848130734\n",
      "GD iter. 250/499: loss=0.5309693065105112\n",
      "GD iter. 251/499: loss=0.5309463702515285\n",
      "GD iter. 252/499: loss=0.5309206811956345\n",
      "GD iter. 253/499: loss=0.530897890579692\n",
      "GD iter. 254/499: loss=0.5308729522490627\n",
      "GD iter. 255/499: loss=0.5308503674722528\n",
      "GD iter. 256/499: loss=0.5308268986039726\n",
      "GD iter. 257/499: loss=0.5308035233167118\n",
      "GD iter. 258/499: loss=0.5307789724688581\n",
      "GD iter. 259/499: loss=0.5307568001514373\n",
      "GD iter. 260/499: loss=0.5307324098611531\n",
      "GD iter. 261/499: loss=0.5307104500970605\n",
      "GD iter. 262/499: loss=0.5306853468587438\n",
      "GD iter. 263/499: loss=0.5306635585335246\n",
      "GD iter. 264/499: loss=0.5306396571439304\n",
      "GD iter. 265/499: loss=0.5306178685265242\n",
      "GD iter. 266/499: loss=0.5305931605117207\n",
      "GD iter. 267/499: loss=0.5305715355926481\n",
      "GD iter. 268/499: loss=0.5305491101516352\n",
      "GD iter. 269/499: loss=0.5305267730852037\n",
      "GD iter. 270/499: loss=0.5305045240426837\n",
      "GD iter. 271/499: loss=0.5304823626748014\n",
      "GD iter. 272/499: loss=0.5304602886336746\n",
      "GD iter. 273/499: loss=0.5304383058060006\n",
      "GD iter. 274/499: loss=0.5304154044863632\n",
      "GD iter. 275/499: loss=0.5303935923408135\n",
      "GD iter. 276/499: loss=0.5303718661403254\n",
      "GD iter. 277/499: loss=0.5303502255438949\n",
      "GD iter. 278/499: loss=0.5303286702118758\n",
      "GD iter. 279/499: loss=0.5303072023295812\n",
      "GD iter. 280/499: loss=0.5302848293977437\n",
      "GD iter. 281/499: loss=0.5302644369369496\n",
      "GD iter. 282/499: loss=0.5302408548258557\n",
      "GD iter. 283/499: loss=0.5302221317409693\n",
      "GD iter. 284/499: loss=0.5301986909911823\n",
      "GD iter. 285/499: loss=0.5301786085662163\n",
      "GD iter. 286/499: loss=0.5301572419295361\n",
      "GD iter. 287/499: loss=0.5301373641671826\n",
      "GD iter. 288/499: loss=0.5301152599168086\n",
      "GD iter. 289/499: loss=0.5300970968527647\n",
      "GD iter. 290/499: loss=0.5300740781163464\n",
      "GD iter. 291/499: loss=0.530056110563339\n",
      "GD iter. 292/499: loss=0.5300321746110552\n",
      "GD iter. 293/499: loss=0.5300142901905193\n",
      "GD iter. 294/499: loss=0.5299916419565834\n",
      "GD iter. 295/499: loss=0.5299739728610924\n",
      "GD iter. 296/499: loss=0.5299502062153731\n",
      "GD iter. 297/499: loss=0.5299339786612758\n",
      "GD iter. 298/499: loss=0.5299103211389113\n",
      "GD iter. 299/499: loss=0.5298929196543176\n",
      "GD iter. 300/499: loss=0.529870465591163\n",
      "GD iter. 301/499: loss=0.5298532355667716\n",
      "GD iter. 302/499: loss=0.5298309222309129\n",
      "GD iter. 303/499: loss=0.5298138623704811\n",
      "GD iter. 304/499: loss=0.5297916907387263\n",
      "GD iter. 305/499: loss=0.5297762049298649\n",
      "GD iter. 306/499: loss=0.5297530894392076\n",
      "GD iter. 307/499: loss=0.529736267249837\n",
      "GD iter. 308/499: loss=0.5297134901243615\n",
      "GD iter. 309/499: loss=0.5296981453731977\n",
      "GD iter. 310/499: loss=0.5296754942978583\n",
      "GD iter. 311/499: loss=0.5296619483061041\n",
      "GD iter. 312/499: loss=0.5296375354170548\n",
      "GD iter. 313/499: loss=0.5296241123281337\n",
      "GD iter. 314/499: loss=0.5296003394853904\n",
      "GD iter. 315/499: loss=0.5295842585834046\n",
      "GD iter. 316/499: loss=0.5295643340497385\n",
      "GD iter. 317/499: loss=0.5295459952599317\n",
      "GD iter. 318/499: loss=0.5295277357057654\n",
      "GD iter. 319/499: loss=0.529508119500786\n",
      "GD iter. 320/499: loss=0.5294939255370048\n",
      "GD iter. 321/499: loss=0.5294704335965026\n",
      "GD iter. 322/499: loss=0.5294575454779958\n",
      "GD iter. 323/499: loss=0.5294341939030125\n",
      "GD iter. 324/499: loss=0.5294214537521574\n",
      "GD iter. 325/499: loss=0.5293980716425496\n",
      "GD iter. 326/499: loss=0.5293866834742161\n",
      "GD iter. 327/499: loss=0.5293610180290207\n",
      "GD iter. 328/499: loss=0.5293528222239339\n",
      "GD iter. 329/499: loss=0.5293247543217444\n",
      "GD iter. 330/499: loss=0.5293174668100152\n",
      "GD iter. 331/499: loss=0.5292885710756757\n",
      "GD iter. 332/499: loss=0.5292829000480452\n",
      "GD iter. 333/499: loss=0.5292534655161166\n",
      "GD iter. 334/499: loss=0.5292477338554138\n",
      "GD iter. 335/499: loss=0.5292196300706598\n",
      "GD iter. 336/499: loss=0.52921322556862\n",
      "GD iter. 337/499: loss=0.5291863606344852\n",
      "GD iter. 338/499: loss=0.5291770836646282\n",
      "GD iter. 339/499: loss=0.5291560050319952\n",
      "GD iter. 340/499: loss=0.5291417343172804\n",
      "GD iter. 341/499: loss=0.5291241928861664\n",
      "GD iter. 342/499: loss=0.5291090919888058\n",
      "GD iter. 343/499: loss=0.5290883450224211\n",
      "GD iter. 344/499: loss=0.5290767950393244\n",
      "GD iter. 345/499: loss=0.5290521506324953\n",
      "GD iter. 346/499: loss=0.5290442379960059\n",
      "GD iter. 347/499: loss=0.5290182373869148\n",
      "GD iter. 348/499: loss=0.5290159731934414\n",
      "GD iter. 349/499: loss=0.528984803357582\n",
      "GD iter. 350/499: loss=0.5289820259553898\n",
      "GD iter. 351/499: loss=0.5289516441640213\n",
      "GD iter. 352/499: loss=0.5289495104386046\n",
      "GD iter. 353/499: loss=0.5289193877618705\n",
      "GD iter. 354/499: loss=0.5289188137643883\n",
      "GD iter. 355/499: loss=0.5288882271100861\n",
      "GD iter. 356/499: loss=0.5288840336543068\n",
      "GD iter. 357/499: loss=0.528858322234984\n",
      "GD iter. 358/499: loss=0.5288516264058508\n",
      "GD iter. 359/499: loss=0.5288303559065392\n",
      "GD iter. 360/499: loss=0.5288170851981981\n",
      "GD iter. 361/499: loss=0.5288012250945581\n",
      "GD iter. 362/499: loss=0.5287861426034403\n",
      "GD iter. 363/499: loss=0.528769434186339\n",
      "GD iter. 364/499: loss=0.5287555614516007\n",
      "GD iter. 365/499: loss=0.5287384943770731\n",
      "GD iter. 366/499: loss=0.5287272481689371\n",
      "GD iter. 367/499: loss=0.5287044484675908\n",
      "GD iter. 368/499: loss=0.5287013294747911\n",
      "GD iter. 369/499: loss=0.5286719799238727\n",
      "GD iter. 370/499: loss=0.5286725802804725\n",
      "GD iter. 371/499: loss=0.5286427587675807\n",
      "GD iter. 372/499: loss=0.5286412219406483\n",
      "GD iter. 373/499: loss=0.528613769709191\n",
      "GD iter. 374/499: loss=0.528610290833587\n",
      "GD iter. 375/499: loss=0.5285842688398017\n",
      "GD iter. 376/499: loss=0.5285808467335796\n",
      "GD iter. 377/499: loss=0.5285557775583826\n",
      "GD iter. 378/499: loss=0.5285509148744912\n",
      "GD iter. 379/499: loss=0.5285272944907801\n",
      "GD iter. 380/499: loss=0.528522776915211\n",
      "GD iter. 381/499: loss=0.5284980035409657\n",
      "GD iter. 382/499: loss=0.5284933063461834\n",
      "GD iter. 383/499: loss=0.5284712459799302\n",
      "GD iter. 384/499: loss=0.5284646574544398\n",
      "GD iter. 385/499: loss=0.5284433938797879\n",
      "GD iter. 386/499: loss=0.528435676310083\n",
      "GD iter. 387/499: loss=0.5284168314468547\n",
      "GD iter. 388/499: loss=0.5284067282661689\n",
      "GD iter. 389/499: loss=0.5283903890093918\n",
      "GD iter. 390/499: loss=0.5283773137319608\n",
      "GD iter. 391/499: loss=0.5283631984269961\n",
      "GD iter. 392/499: loss=0.5283502488666834\n",
      "GD iter. 393/499: loss=0.5283356364054455\n",
      "GD iter. 394/499: loss=0.5283236443215245\n",
      "GD iter. 395/499: loss=0.5283064717874202\n",
      "GD iter. 396/499: loss=0.5283047762294627\n",
      "GD iter. 397/499: loss=0.5282760463030581\n",
      "GD iter. 398/499: loss=0.5282862458372558\n",
      "GD iter. 399/499: loss=0.5282517495988592\n",
      "GD iter. 400/499: loss=0.5282534963893497\n",
      "GD iter. 401/499: loss=0.5282239857299154\n",
      "GD iter. 402/499: loss=0.5282314132073427\n",
      "GD iter. 403/499: loss=0.5281991190978802\n",
      "GD iter. 404/499: loss=0.5281990969909873\n",
      "GD iter. 405/499: loss=0.5281714959733976\n",
      "GD iter. 406/499: loss=0.5281811983034926\n",
      "GD iter. 407/499: loss=0.5281484994891777\n",
      "GD iter. 408/499: loss=0.5281463758509111\n",
      "GD iter. 409/499: loss=0.52812139929918\n",
      "GD iter. 410/499: loss=0.5281249376595584\n",
      "GD iter. 411/499: loss=0.5280950113026572\n",
      "GD iter. 412/499: loss=0.5281049352620557\n",
      "GD iter. 413/499: loss=0.5280717969885034\n",
      "GD iter. 414/499: loss=0.5280703229319657\n",
      "GD iter. 415/499: loss=0.5280468007620445\n",
      "GD iter. 416/499: loss=0.5280468502128205\n",
      "GD iter. 417/499: loss=0.5280219395064406\n",
      "GD iter. 418/499: loss=0.5280233843427983\n",
      "GD iter. 419/499: loss=0.527996516252181\n",
      "GD iter. 420/499: loss=0.5280027529091308\n",
      "GD iter. 421/499: loss=0.5279736780870836\n",
      "GD iter. 422/499: loss=0.5279719404673305\n",
      "GD iter. 423/499: loss=0.5279514925266303\n",
      "GD iter. 424/499: loss=0.5279455659030037\n",
      "GD iter. 425/499: loss=0.5279281340196745\n",
      "GD iter. 426/499: loss=0.5279224733878138\n",
      "GD iter. 427/499: loss=0.5279029798238387\n",
      "GD iter. 428/499: loss=0.5278997488313852\n",
      "GD iter. 429/499: loss=0.5278787624945427\n",
      "GD iter. 430/499: loss=0.5278823559363869\n",
      "GD iter. 431/499: loss=0.5278537809204056\n",
      "GD iter. 432/499: loss=0.5278651073927896\n",
      "GD iter. 433/499: loss=0.5278327246670685\n",
      "GD iter. 434/499: loss=0.5278335441252464\n",
      "GD iter. 435/499: loss=0.5278082519125139\n",
      "GD iter. 436/499: loss=0.5278150054079043\n",
      "GD iter. 437/499: loss=0.5277856916251522\n",
      "GD iter. 438/499: loss=0.5277935909068112\n",
      "GD iter. 439/499: loss=0.527763741972534\n",
      "GD iter. 440/499: loss=0.5277659551469598\n",
      "GD iter. 441/499: loss=0.5277400594607029\n",
      "GD iter. 442/499: loss=0.5277532436642532\n",
      "GD iter. 443/499: loss=0.5277208136784143\n",
      "GD iter. 444/499: loss=0.5277150219732422\n",
      "GD iter. 445/499: loss=0.5276999894964335\n",
      "GD iter. 446/499: loss=0.5276888191305353\n",
      "GD iter. 447/499: loss=0.5276787960295743\n",
      "GD iter. 448/499: loss=0.5276671048450162\n",
      "GD iter. 449/499: loss=0.5276560724747917\n",
      "GD iter. 450/499: loss=0.5276490539990214\n",
      "GD iter. 451/499: loss=0.5276330535939975\n",
      "GD iter. 452/499: loss=0.5276381171929247\n",
      "GD iter. 453/499: loss=0.527609964959677\n",
      "GD iter. 454/499: loss=0.5276272423603431\n",
      "GD iter. 455/499: loss=0.5275919695896101\n",
      "GD iter. 456/499: loss=0.5275857270119283\n",
      "GD iter. 457/499: loss=0.5275698505178201\n",
      "GD iter. 458/499: loss=0.527570893746775\n",
      "GD iter. 459/499: loss=0.5275469787577031\n",
      "GD iter. 460/499: loss=0.5275657605626931\n",
      "GD iter. 461/499: loss=0.5275274711611485\n",
      "GD iter. 462/499: loss=0.5275397948883241\n",
      "GD iter. 463/499: loss=0.5275067214255338\n",
      "GD iter. 464/499: loss=0.5275188457499831\n",
      "GD iter. 465/499: loss=0.5274861485979709\n",
      "GD iter. 466/499: loss=0.5275000337817234\n",
      "GD iter. 467/499: loss=0.527465745139705\n",
      "GD iter. 468/499: loss=0.5274794216321783\n",
      "GD iter. 469/499: loss=0.5274448061594436\n",
      "GD iter. 470/499: loss=0.5274634164818863\n",
      "GD iter. 471/499: loss=0.5274246522282204\n",
      "GD iter. 472/499: loss=0.5274428047573748\n",
      "GD iter. 473/499: loss=0.527404657715297\n",
      "GD iter. 474/499: loss=0.5274229665366923\n",
      "GD iter. 475/499: loss=0.5273858871229936\n",
      "GD iter. 476/499: loss=0.5274013498102638\n",
      "GD iter. 477/499: loss=0.5273657165393985\n",
      "GD iter. 478/499: loss=0.5273818264889427\n",
      "GD iter. 479/499: loss=0.5273463691870138\n",
      "GD iter. 480/499: loss=0.5273624718608403\n",
      "GD iter. 481/499: loss=0.5273268542150812\n",
      "GD iter. 482/499: loss=0.5273447494962639\n",
      "GD iter. 483/499: loss=0.5273083464892809\n",
      "GD iter. 484/499: loss=0.5273246562248428\n",
      "GD iter. 485/499: loss=0.5272918680054722\n",
      "GD iter. 486/499: loss=0.5272942640866702\n",
      "GD iter. 487/499: loss=0.5272725991129444\n",
      "GD iter. 488/499: loss=0.5272822638695261\n",
      "GD iter. 489/499: loss=0.5272543861801628\n",
      "GD iter. 490/499: loss=0.5272628289166849\n",
      "GD iter. 491/499: loss=0.5272364183769365\n",
      "GD iter. 492/499: loss=0.5272375779044481\n",
      "GD iter. 493/499: loss=0.527218285711387\n",
      "GD iter. 494/499: loss=0.5272282513052369\n",
      "GD iter. 495/499: loss=0.5271987587400054\n",
      "GD iter. 496/499: loss=0.5272171900707566\n",
      "GD iter. 497/499: loss=0.5271804730401095\n",
      "GD iter. 498/499: loss=0.5272001911858893\n",
      "GD iter. 499/499: loss=0.5271629786922398\n",
      "The Accuracy is: 0.9206\n",
      "The F1 score is: nan\n",
      "The precision is: nan\n",
      "The recall is: 0.0000\n",
      "Average accuracy score is:  0.912091187562042\n",
      "Average f1 score is:  nan\n"
     ]
    }
   ],
   "source": [
    "x_pca_t = add_bias(x_pca)\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_pca_t), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(10):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = hinge_regression(y_t, x_t, initial_w, lambda_=0.1, max_iters=500, gamma=0.01)\n",
    "    y_pred = ((x_v @ w) > 1.5).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    lambda_ = trial.suggest_loguniform('lambda_', 5e-3, 5)\n",
    "    thres = trial.suggest_float('thres', 0.5, 2)\n",
    "    n_com = trial.suggest_int('n_com', 10, 200)\n",
    "    x_pca, eig_vec, eig_val,weight = pca(pre_train_data, n_com)\n",
    "    x_pca_t = add_bias(x_pca)\n",
    "    sub_x, sub_y = split_cross_validation(add_bias(x_pca_t), y_train_processed, 5)\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    # cross-validation\n",
    "    for i in range(5):\n",
    "        sub_cur_x = sub_x.copy()\n",
    "        sub_cur_y = sub_y.copy()\n",
    "        x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "        x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "        x_t, y_t = data_augmentation(x_t, y_t)\n",
    "        initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "        w, loss = hinge_regression(y_t, x_t, initial_w, lambda_=lambda_, max_iters=250, gamma=0.01)\n",
    "        y_pred = ((x_v @ w) > thres).astype(int)\n",
    "        accs.append(predict_acc_pure(y_pred, y_v))\n",
    "        f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "    print(\"Average accuracy score is: \", np.mean(accs))\n",
    "    print(\"Average f1 score is: \", np.mean(f1s))\n",
    "    return np.mean(f1s)\n",
    "\n",
    "sampler = TPESampler(seed=10)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Kernel PCA to classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_data(num_samples=500, radius=0.5):\n",
    "    # Generate random 2D points\n",
    "    x = np.random.rand(num_samples, 2) * 2 - 1 # Rescale to [-1, 1]\n",
    "    \n",
    "    # Create labels based on distance to the origin\n",
    "    labels = (np.sum(x**2, axis=1) < radius**2).astype(int)\n",
    "    \n",
    "    return x, labels\n",
    "\n",
    "x, y = generate_data()\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(x[y == 0][:, 0], x[y == 0][:, 1], color='blue', label='Class 0')\n",
    "plt.scatter(x[y == 1][:, 0], x[y == 1][:, 1], color='red', label='Class 1')\n",
    "plt.legend()\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Toy Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.554140901035659\n",
      "Epoch: 2 Loss: 0.6355115822676142\n",
      "Epoch: 4 Loss: 0.5703803972660615\n",
      "Epoch: 6 Loss: 0.538148358572958\n",
      "Epoch: 8 Loss: 0.6037274295874099\n",
      "Epoch: 10 Loss: 0.5935929089893998\n",
      "Epoch: 12 Loss: 0.5377727988257001\n",
      "Epoch: 14 Loss: 0.5528197762511157\n",
      "Epoch: 16 Loss: 0.5266685407389036\n",
      "Epoch: 18 Loss: 0.6114912648196452\n",
      "Epoch: 20 Loss: 0.6243905797773407\n",
      "Epoch: 22 Loss: 0.5553445897213269\n",
      "Epoch: 24 Loss: 0.5752018005867288\n",
      "Epoch: 26 Loss: 0.5991425648936147\n",
      "Epoch: 28 Loss: 0.5570613615321734\n",
      "Epoch: 30 Loss: 0.5951387318402769\n",
      "Epoch: 32 Loss: 0.5971782551430359\n",
      "Epoch: 34 Loss: 0.5107926032784098\n",
      "Epoch: 36 Loss: 0.5418308481425692\n",
      "Epoch: 38 Loss: 0.5648235199545646\n",
      "The Accuracy is: 0.0020\n",
      "The F1 score is: 0.0959\n",
      "The precision is: 0.1346\n",
      "The recall is: 0.0745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0958904109589041"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = initialize_network(input_size=np.shape(x)[1], hidden_size1=10, hidden_size2=10, hidden_size3=4, output_size=1)\n",
    "nn_trained = train(nn, x, y, learning_rate=0.0001, epochs=40, batch_size=200)\n",
    "y_pred = predict(nn_trained,x)\n",
    "predict_acc_pure(y_pred, y)\n",
    "predict_f1_pure(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as svm\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbda558e750>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel_pca = KernelPCA(\n",
    "    n_components=3000, kernel=\"rbf\", gamma=None, fit_inverse_transform=True, alpha=0.1\n",
    ")\n",
    "pca = PCA(n_components=None)\n",
    "kernel_pca.fit(x_train_processed)\n",
    "cummulation = np.cumsum(kernel_pca.eigenvalues_/np.sum(kernel_pca.eigenvalues_))\n",
    "# cummulation = np.cumsum(pca.explained_variance_ratio_)\n",
    "# cummulation\n",
    "plt.plot(cummulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/2999: loss=0.6933423583843883\n",
      "GD iter. 1/2999: loss=0.6768741700334122\n",
      "GD iter. 2/2999: loss=0.6696720436679526\n",
      "GD iter. 3/2999: loss=0.7761250250344813\n",
      "GD iter. 4/2999: loss=1.8203439818504852\n",
      "GD iter. 5/2999: loss=2.8398157584161847\n",
      "GD iter. 6/2999: loss=1.964438470507169\n",
      "GD iter. 7/2999: loss=2.690771834908934\n",
      "GD iter. 8/2999: loss=2.0337363005400553\n",
      "GD iter. 9/2999: loss=2.578452658377237\n",
      "GD iter. 10/2999: loss=2.067705865665283\n",
      "GD iter. 11/2999: loss=2.488087092559182\n",
      "GD iter. 12/2999: loss=2.0800525770907647\n",
      "GD iter. 13/2999: loss=2.412525546581288\n",
      "GD iter. 14/2999: loss=2.077349955298133\n",
      "GD iter. 15/2999: loss=2.3476947126344174\n",
      "GD iter. 16/2999: loss=2.0632700901582224\n",
      "GD iter. 17/2999: loss=2.2909998914186107\n",
      "GD iter. 18/2999: loss=2.040088054497279\n",
      "GD iter. 19/2999: loss=2.240604670502651\n",
      "GD iter. 20/2999: loss=2.009365243412015\n",
      "GD iter. 21/2999: loss=2.195048360244732\n",
      "GD iter. 22/2999: loss=1.972310902411984\n",
      "GD iter. 23/2999: loss=2.153028271306601\n",
      "GD iter. 24/2999: loss=1.9299824957006826\n",
      "GD iter. 25/2999: loss=2.1132890057147478\n",
      "GD iter. 26/2999: loss=1.8833820182138459\n",
      "GD iter. 27/2999: loss=2.0745982949633723\n",
      "GD iter. 28/2999: loss=1.8334760315843583\n",
      "GD iter. 29/2999: loss=2.0357920424091356\n",
      "GD iter. 30/2999: loss=1.7811678273965286\n",
      "GD iter. 31/2999: loss=1.9958588846095502\n",
      "GD iter. 32/2999: loss=1.7272577456071159\n",
      "GD iter. 33/2999: loss=1.9540248947356338\n",
      "GD iter. 34/2999: loss=1.672424199229215\n",
      "GD iter. 35/2999: loss=1.9098065154397277\n",
      "GD iter. 36/2999: loss=1.61723750204674\n",
      "GD iter. 37/2999: loss=1.8630223922836484\n",
      "GD iter. 38/2999: loss=1.5621942488547806\n",
      "GD iter. 39/2999: loss=1.8137741065122202\n",
      "GD iter. 40/2999: loss=1.507750734211819\n",
      "GD iter. 41/2999: loss=1.7624086652029594\n",
      "GD iter. 42/2999: loss=1.4543422596250557\n",
      "GD iter. 43/2999: loss=1.7094688150797788\n",
      "GD iter. 44/2999: loss=1.4023871376438954\n",
      "GD iter. 45/2999: loss=1.655633808876346\n",
      "GD iter. 46/2999: loss=1.3522795724980121\n",
      "GD iter. 47/2999: loss=1.6016550643572625\n",
      "GD iter. 48/2999: loss=1.3043760376425744\n",
      "GD iter. 49/2999: loss=1.5482930863321516\n",
      "GD iter. 50/2999: loss=1.258979441064021\n",
      "GD iter. 51/2999: loss=1.4962617823213635\n",
      "GD iter. 52/2999: loss=1.216325041519686\n",
      "GD iter. 53/2999: loss=1.4461847072070348\n",
      "GD iter. 54/2999: loss=1.1765711470111475\n",
      "GD iter. 55/2999: loss=1.398565726220151\n",
      "GD iter. 56/2999: loss=1.1397961386996718\n",
      "GD iter. 57/2999: loss=1.3537744999865342\n",
      "GD iter. 58/2999: loss=1.106001855134615\n",
      "GD iter. 59/2999: loss=1.312045389779844\n",
      "GD iter. 60/2999: loss=1.0751222505715223\n",
      "GD iter. 61/2999: loss=1.2734871169868855\n",
      "GD iter. 62/2999: loss=1.047035622236703\n",
      "GD iter. 63/2999: loss=1.238099877745449\n",
      "GD iter. 64/2999: loss=1.021578535179753\n",
      "GD iter. 65/2999: loss=1.2057965524484477\n",
      "GD iter. 66/2999: loss=0.9985597573794264\n",
      "GD iter. 67/2999: loss=1.1764250335360322\n",
      "GD iter. 68/2999: loss=0.9777729260810243\n",
      "GD iter. 69/2999: loss=1.1497893682460767\n",
      "GD iter. 70/2999: loss=0.9590071624142866\n",
      "GD iter. 71/2999: loss=1.1256682058988936\n",
      "GD iter. 72/2999: loss=0.9420553160460341\n",
      "GD iter. 73/2999: loss=1.1038297949809805\n",
      "GD iter. 74/2999: loss=0.9267198796423327\n",
      "GD iter. 75/2999: loss=1.0840433836290548\n",
      "GD iter. 76/2999: loss=0.9128168392367325\n",
      "GD iter. 77/2999: loss=1.066087291100056\n",
      "GD iter. 78/2999: loss=0.9001778342281713\n",
      "GD iter. 79/2999: loss=1.0497541452826642\n",
      "GD iter. 80/2999: loss=0.8886510213131473\n",
      "GD iter. 81/2999: loss=1.0348538623504295\n",
      "GD iter. 82/2999: loss=0.8781010043331992\n",
      "GD iter. 83/2999: loss=1.0212149287826306\n",
      "GD iter. 84/2999: loss=0.8684081345075203\n",
      "GD iter. 85/2999: loss=1.0086844771022334\n",
      "GD iter. 86/2999: loss=0.8594674214527123\n",
      "GD iter. 87/2999: loss=0.9971275568637351\n",
      "GD iter. 88/2999: loss=0.8511872353421419\n",
      "GD iter. 89/2999: loss=0.9864259116721489\n",
      "GD iter. 90/2999: loss=0.8434879293309752\n",
      "GD iter. 91/2999: loss=0.9764764918538582\n",
      "GD iter. 92/2999: loss=0.836300470280217\n",
      "GD iter. 93/2999: loss=0.9671898650506082\n",
      "GD iter. 94/2999: loss=0.8295651342887412\n",
      "GD iter. 95/2999: loss=0.9584886339658922\n",
      "GD iter. 96/2999: loss=0.8232303002559945\n",
      "GD iter. 97/2999: loss=0.9503059304229035\n",
      "GD iter. 98/2999: loss=0.8172513580868073\n",
      "GD iter. 99/2999: loss=0.9425840256967308\n",
      "GD iter. 100/2999: loss=0.8115897367450791\n",
      "GD iter. 101/2999: loss=0.9352730765622964\n",
      "GD iter. 102/2999: loss=0.8062120498891511\n",
      "GD iter. 103/2999: loss=0.9283300126352906\n",
      "GD iter. 104/2999: loss=0.8010893522305597\n",
      "GD iter. 105/2999: loss=0.9217175616669402\n",
      "GD iter. 106/2999: loss=0.7961964972205854\n",
      "GD iter. 107/2999: loss=0.9154034041059632\n",
      "GD iter. 108/2999: loss=0.7915115855505978\n",
      "GD iter. 109/2999: loss=0.9093594453912776\n",
      "GD iter. 110/2999: loss=0.7870154937717554\n",
      "GD iter. 111/2999: loss=0.9035611932770252\n",
      "GD iter. 112/2999: loss=0.7826914727508115\n",
      "GD iter. 113/2999: loss=0.8979872274238007\n",
      "GD iter. 114/2999: loss=0.7785248064318149\n",
      "GD iter. 115/2999: loss=0.8926187490900975\n",
      "GD iter. 116/2999: loss=0.7745025222967229\n",
      "GD iter. 117/2999: loss=0.8874391997320613\n",
      "GD iter. 118/2999: loss=0.7706131458990984\n",
      "GD iter. 119/2999: loss=0.8824339384662575\n",
      "GD iter. 120/2999: loss=0.7668464928092148\n",
      "GD iter. 121/2999: loss=0.877589969541323\n",
      "GD iter. 122/2999: loss=0.7631934922155392\n",
      "GD iter. 123/2999: loss=0.8728957121196704\n",
      "GD iter. 124/2999: loss=0.7596460372517982\n",
      "GD iter. 125/2999: loss=0.8683408057435004\n",
      "GD iter. 126/2999: loss=0.7561968578526997\n",
      "GD iter. 127/2999: loss=0.863915945828695\n",
      "GD iter. 128/2999: loss=0.7528394125849317\n",
      "GD iter. 129/2999: loss=0.8596127443883921\n",
      "GD iter. 130/2999: loss=0.7495677964538827\n",
      "GD iter. 131/2999: loss=0.8554236119328855\n",
      "GD iter. 132/2999: loss=0.7463766621646545\n",
      "GD iter. 133/2999: loss=0.8513416571383822\n",
      "GD iter. 134/2999: loss=0.7432611527181698\n",
      "GD iter. 135/2999: loss=0.8473606014230839\n",
      "GD iter. 136/2999: loss=0.7402168435674285\n",
      "GD iter. 137/2999: loss=0.8434747060370504\n",
      "GD iter. 138/2999: loss=0.7372396928457015\n",
      "GD iter. 139/2999: loss=0.8396787096635538\n",
      "GD iter. 140/2999: loss=0.7343259984203062\n",
      "GD iter. 141/2999: loss=0.8359677748592367\n",
      "GD iter. 142/2999: loss=0.7314723607283871\n",
      "GD iter. 143/2999: loss=0.832337441937063\n",
      "GD iter. 144/2999: loss=0.72867565051974\n",
      "GD iter. 145/2999: loss=0.8287835891264277\n",
      "GD iter. 146/2999: loss=0.7259329807730757\n",
      "GD iter. 147/2999: loss=0.8253023980368623\n",
      "GD iter. 148/2999: loss=0.7232416821700621\n",
      "GD iter. 149/2999: loss=0.8218903236126324\n",
      "GD iter. 150/2999: loss=0.7205992816093755\n",
      "GD iter. 151/2999: loss=0.8185440678982017\n",
      "GD iter. 152/2999: loss=0.7180034833251152\n",
      "GD iter. 153/2999: loss=0.815260557045967\n",
      "GD iter. 154/2999: loss=0.7154521522425922\n",
      "GD iter. 155/2999: loss=0.8120369210897413\n",
      "GD iter. 156/2999: loss=0.712943299260673\n",
      "GD iter. 157/2999: loss=0.8088704760841142\n",
      "GD iter. 158/2999: loss=0.7104750681992332\n",
      "GD iter. 159/2999: loss=0.8057587082743719\n",
      "GD iter. 160/2999: loss=0.7080457241884017\n",
      "GD iter. 161/2999: loss=0.8026992600140794\n",
      "GD iter. 162/2999: loss=0.7056536433106412\n",
      "GD iter. 163/2999: loss=0.7996899171917469\n",
      "GD iter. 164/2999: loss=0.7032973033352377\n",
      "GD iter. 165/2999: loss=0.79672859796625\n",
      "GD iter. 166/2999: loss=0.7009752754069617\n",
      "GD iter. 167/2999: loss=0.7938133426395702\n",
      "GD iter. 168/2999: loss=0.6986862165718617\n",
      "GD iter. 169/2999: loss=0.790942304522932\n",
      "GD iter. 170/2999: loss=0.6964288630388955\n",
      "GD iter. 171/2999: loss=0.7881137416729791\n",
      "GD iter. 172/2999: loss=0.6942020240908185\n",
      "GD iter. 173/2999: loss=0.785326009393084\n",
      "GD iter. 174/2999: loss=0.6920045765691847\n",
      "GD iter. 175/2999: loss=0.7825775534099269\n",
      "GD iter. 176/2999: loss=0.6898354598687461\n",
      "GD iter. 177/2999: loss=0.7798669036482324\n",
      "GD iter. 178/2999: loss=0.687693671385194\n",
      "GD iter. 179/2999: loss=0.777192668537553\n",
      "GD iter. 180/2999: loss=0.6855782623667401\n",
      "GD iter. 181/2999: loss=0.7745535297931403\n",
      "GD iter. 182/2999: loss=0.6834883341277339\n",
      "GD iter. 183/2999: loss=0.7719482376222062\n",
      "GD iter. 184/2999: loss=0.6814230345860064\n",
      "GD iter. 185/2999: loss=0.7693756063112881\n",
      "GD iter. 186/2999: loss=0.6793815550918801\n",
      "GD iter. 187/2999: loss=0.7668345101579916\n",
      "GD iter. 188/2999: loss=0.6773631275193482\n",
      "GD iter. 189/2999: loss=0.7643238797130915\n",
      "GD iter. 190/2999: loss=0.6753670215944254\n",
      "GD iter. 191/2999: loss=0.7618426983047756\n",
      "GD iter. 192/2999: loss=0.673392542437722\n",
      "GD iter. 193/2999: loss=0.759389998818831\n",
      "GD iter. 194/2999: loss=0.6714390283012766\n",
      "GD iter. 195/2999: loss=0.7569648607121641\n",
      "GD iter. 196/2999: loss=0.6695058484818568\n",
      "GD iter. 197/2999: loss=0.7545664072395198\n",
      "GD iter. 198/2999: loss=0.6675924013948954\n",
      "GD iter. 199/2999: loss=0.7521938028755073\n",
      "GD iter. 200/2999: loss=0.6656981127944062\n",
      "GD iter. 201/2999: loss=0.7498462509153296\n",
      "GD iter. 202/2999: loss=0.6638224341263617\n",
      "GD iter. 203/2999: loss=0.7475229912399808\n",
      "GD iter. 204/2999: loss=0.6619648410038805\n",
      "GD iter. 205/2999: loss=0.7452232982328237\n",
      "GD iter. 206/2999: loss=0.6601248317941595\n",
      "GD iter. 207/2999: loss=0.7429464788358306\n",
      "GD iter. 208/2999: loss=0.6583019263072101\n",
      "GD iter. 209/2999: loss=0.7406918707343745\n",
      "GD iter. 210/2999: loss=0.6564956645785369\n",
      "GD iter. 211/2999: loss=0.7384588406612776\n",
      "GD iter. 212/2999: loss=0.65470560573738\n",
      "GD iter. 213/2999: loss=0.7362467828107111\n",
      "GD iter. 214/2999: loss=0.6529313269544258\n",
      "GD iter. 215/2999: loss=0.7340551173545365\n",
      "GD iter. 216/2999: loss=0.6511724224615785\n",
      "GD iter. 217/2999: loss=0.7318832890527128\n",
      "GD iter. 218/2999: loss=0.6494285026390086\n",
      "GD iter. 219/2999: loss=0.7297307659519218\n",
      "GD iter. 220/2999: loss=0.6476991931634702\n",
      "GD iter. 221/2999: loss=0.7275970381654088\n",
      "GD iter. 222/2999: loss=0.6459841342134776\n",
      "GD iter. 223/2999: loss=0.7254816167286423\n",
      "GD iter. 224/2999: loss=0.6442829797267902\n",
      "GD iter. 225/2999: loss=0.7233840325254253\n",
      "GD iter. 226/2999: loss=0.6425953967062428\n",
      "GD iter. 227/2999: loss=0.721303835279531\n",
      "GD iter. 228/2999: loss=0.6409210645701167\n",
      "GD iter. 229/2999: loss=0.7192405926072312\n",
      "GD iter. 230/2999: loss=0.6392596745436444\n",
      "GD iter. 231/2999: loss=0.7171938891265593\n",
      "GD iter. 232/2999: loss=0.6376109290889695\n",
      "GD iter. 233/2999: loss=0.7151633256198141\n",
      "GD iter. 234/2999: loss=0.6359745413700911\n",
      "GD iter. 235/2999: loss=0.7131485182451383\n",
      "GD iter. 236/2999: loss=0.6343502347505999\n",
      "GD iter. 237/2999: loss=0.7111490977941418\n",
      "GD iter. 238/2999: loss=0.6327377423217266\n",
      "GD iter. 239/2999: loss=0.7091647089925804\n",
      "GD iter. 240/2999: loss=0.6311368064582705\n",
      "GD iter. 241/2999: loss=0.7071950098409453\n",
      "GD iter. 242/2999: loss=0.6295471784010139\n",
      "GD iter. 243/2999: loss=0.7052396709928733\n",
      "GD iter. 244/2999: loss=0.6279686178626779\n",
      "GD iter. 245/2999: loss=0.7032983751679206\n",
      "GD iter. 246/2999: loss=0.6264008926567992\n",
      "GD iter. 247/2999: loss=0.7013708165975213\n",
      "GD iter. 248/2999: loss=0.6248437783472955\n",
      "GD iter. 249/2999: loss=0.6994567005012785\n",
      "GD iter. 250/2999: loss=0.623297057917641\n",
      "GD iter. 251/2999: loss=0.6975557425921263\n",
      "GD iter. 252/2999: loss=0.6217605214577158\n",
      "GD iter. 253/2999: loss=0.6956676686077585\n",
      "GD iter. 254/2999: loss=0.6202339658678813\n",
      "GD iter. 255/2999: loss=0.6937922138675885\n",
      "GD iter. 256/2999: loss=0.6187171945786395\n",
      "GD iter. 257/2999: loss=0.6919291228530492\n",
      "GD iter. 258/2999: loss=0.6172100172847396\n",
      "GD iter. 259/2999: loss=0.6900781488096726\n",
      "GD iter. 260/2999: loss=0.6157122496933183\n",
      "GD iter. 261/2999: loss=0.6882390533702589\n",
      "GD iter. 262/2999: loss=0.6142237132844425\n",
      "GD iter. 263/2999: loss=0.6864116061969279\n",
      "GD iter. 264/2999: loss=0.6127442350839502\n",
      "GD iter. 265/2999: loss=0.6845955846417623\n",
      "GD iter. 266/2999: loss=0.611273647447131\n",
      "GD iter. 267/2999: loss=0.6827907734241877\n",
      "GD iter. 268/2999: loss=0.6098117878532006\n",
      "GD iter. 269/2999: loss=0.6809969643246737\n",
      "GD iter. 270/2999: loss=0.6083584987094982\n",
      "GD iter. 271/2999: loss=0.6792139558934671\n",
      "GD iter. 272/2999: loss=0.606913627164733\n",
      "GD iter. 273/2999: loss=0.6774415531733102\n",
      "GD iter. 274/2999: loss=0.6054770249312327\n",
      "GD iter. 275/2999: loss=0.6756795674359252\n",
      "GD iter. 276/2999: loss=0.6040485481151284\n",
      "GD iter. 277/2999: loss=0.6739278159308489\n",
      "GD iter. 278/2999: loss=0.6026280570541394\n",
      "GD iter. 279/2999: loss=0.6721861216460485\n",
      "GD iter. 280/2999: loss=0.6012154161628812\n",
      "GD iter. 281/2999: loss=0.6704543130801102\n",
      "GD iter. 282/2999: loss=0.5998104937847007\n",
      "GD iter. 283/2999: loss=0.6687322240246873\n",
      "GD iter. 284/2999: loss=0.5984131620502289\n",
      "GD iter. 285/2999: loss=0.667019693357194\n",
      "GD iter. 286/2999: loss=0.5970232967416653\n",
      "GD iter. 287/2999: loss=0.665316564842664\n",
      "GD iter. 288/2999: loss=0.5956407771631276\n",
      "GD iter. 289/2999: loss=0.6636226869447662\n",
      "GD iter. 290/2999: loss=0.5942654860163524\n",
      "GD iter. 291/2999: loss=0.6619379126453282\n",
      "GD iter. 292/2999: loss=0.5928973092815419\n",
      "GD iter. 293/2999: loss=0.6602620992717185\n",
      "GD iter. 294/2999: loss=0.591536136103039\n",
      "GD iter. 295/2999: loss=0.6585951083318162\n",
      "GD iter. 296/2999: loss=0.5901818586797906\n",
      "GD iter. 297/2999: loss=0.6569368053563625\n",
      "GD iter. 298/2999: loss=0.588834372160066\n",
      "GD iter. 299/2999: loss=0.6552870597479015\n",
      "GD iter. 300/2999: loss=0.587493574540619\n",
      "GD iter. 301/2999: loss=0.6536457446365489\n",
      "GD iter. 302/2999: loss=0.5861593665695173\n",
      "GD iter. 303/2999: loss=0.6520127367415608\n",
      "GD iter. 304/2999: loss=0.5848316516530544\n",
      "GD iter. 305/2999: loss=0.6503879162389815\n",
      "GD iter. 306/2999: loss=0.5835103357661134\n",
      "GD iter. 307/2999: loss=0.6487711666348458\n",
      "GD iter. 308/2999: loss=0.5821953273660777\n",
      "GD iter. 309/2999: loss=0.6471623746435927\n",
      "GD iter. 310/2999: loss=0.5808865373100721\n",
      "GD iter. 311/2999: loss=0.645561430071749\n",
      "GD iter. 312/2999: loss=0.5795838787752147\n",
      "GD iter. 313/2999: loss=0.6439682257061\n",
      "GD iter. 314/2999: loss=0.578287267182069\n",
      "GD iter. 315/2999: loss=0.6423826572067897\n",
      "GD iter. 316/2999: loss=0.5769966201206729\n",
      "GD iter. 317/2999: loss=0.6408046230044045\n",
      "GD iter. 318/2999: loss=0.575711857279847\n",
      "GD iter. 319/2999: loss=0.6392340242018142\n",
      "GD iter. 320/2999: loss=0.5744329003786252\n",
      "GD iter. 321/2999: loss=0.6376707644794292\n",
      "GD iter. 322/2999: loss=0.57315967310038\n",
      "GD iter. 323/2999: loss=0.6361147500044386\n",
      "GD iter. 324/2999: loss=0.5718921010295344\n",
      "GD iter. 325/2999: loss=0.6345658893439208\n",
      "GD iter. 326/2999: loss=0.570630111590367\n",
      "GD iter. 327/2999: loss=0.6330240933810894\n",
      "GD iter. 328/2999: loss=0.569373633988298\n",
      "GD iter. 329/2999: loss=0.6314892752352205\n",
      "GD iter. 330/2999: loss=0.568122599152918\n",
      "GD iter. 331/2999: loss=0.6299613501842597\n",
      "GD iter. 332/2999: loss=0.5668769396835477\n",
      "GD iter. 333/2999: loss=0.6284402355910647\n",
      "GD iter. 334/2999: loss=0.5656365897963512\n",
      "GD iter. 335/2999: loss=0.6269258508320463\n",
      "GD iter. 336/2999: loss=0.5644014852737684\n",
      "GD iter. 337/2999: loss=0.6254181172290871\n",
      "GD iter. 338/2999: loss=0.5631715634152448\n",
      "GD iter. 339/2999: loss=0.6239169579835253\n",
      "GD iter. 340/2999: loss=0.5619467629901683\n",
      "GD iter. 341/2999: loss=0.6224222981132447\n",
      "GD iter. 342/2999: loss=0.5607270241922966\n",
      "GD iter. 343/2999: loss=0.6209340643920259\n",
      "GD iter. 344/2999: loss=0.5595122885958067\n",
      "GD iter. 345/2999: loss=0.6194521852911313\n",
      "GD iter. 346/2999: loss=0.5583024991128804\n",
      "GD iter. 347/2999: loss=0.6179765909232349\n",
      "GD iter. 348/2999: loss=0.5570975999528347\n",
      "GD iter. 349/2999: loss=0.6165072129885399\n",
      "GD iter. 350/2999: loss=0.555897536582692\n",
      "GD iter. 351/2999: loss=0.6150439847229224\n",
      "GD iter. 352/2999: loss=0.554702255689147\n",
      "GD iter. 353/2999: loss=0.6135868408481293\n",
      "GD iter. 354/2999: loss=0.5535117051417778\n",
      "GD iter. 355/2999: loss=0.6121357175237235\n",
      "GD iter. 356/2999: loss=0.5523258339576377\n",
      "GD iter. 357/2999: loss=0.6106905523010299\n",
      "GD iter. 358/2999: loss=0.5511445922671248\n",
      "GD iter. 359/2999: loss=0.6092512840787823\n",
      "GD iter. 360/2999: loss=0.5499679312808257\n",
      "GD iter. 361/2999: loss=0.6078178530603364\n",
      "GD iter. 362/2999: loss=0.5487958032577853\n",
      "GD iter. 363/2999: loss=0.6063902007127063\n",
      "GD iter. 364/2999: loss=0.5476281614746549\n",
      "GD iter. 365/2999: loss=0.6049682697269361\n",
      "GD iter. 366/2999: loss=0.546464960196094\n",
      "GD iter. 367/2999: loss=0.6035520039801839\n",
      "GD iter. 368/2999: loss=0.5453061546460742\n",
      "GD iter. 369/2999: loss=0.6021413484990906\n",
      "GD iter. 370/2999: loss=0.5441517009800811\n",
      "GD iter. 371/2999: loss=0.6007362494243759\n",
      "GD iter. 372/2999: loss=0.5430015562586488\n",
      "GD iter. 373/2999: loss=0.5993366539772769\n",
      "GD iter. 374/2999: loss=0.5418556784212601\n",
      "GD iter. 375/2999: loss=0.5979425104265215\n",
      "GD iter. 376/2999: loss=0.5407140262616189\n",
      "GD iter. 377/2999: loss=0.5965537680571481\n",
      "GD iter. 378/2999: loss=0.5395765594034927\n",
      "GD iter. 379/2999: loss=0.5951703771401222\n",
      "GD iter. 380/2999: loss=0.5384432382774608\n",
      "GD iter. 381/2999: loss=0.5937922889032323\n",
      "GD iter. 382/2999: loss=0.5373140240985214\n",
      "GD iter. 383/2999: loss=0.5924194555030793\n",
      "GD iter. 384/2999: loss=0.5361888788443252\n",
      "GD iter. 385/2999: loss=0.5910518299979528\n",
      "GD iter. 386/2999: loss=0.5350677652341805\n",
      "GD iter. 387/2999: loss=0.5896893663218069\n",
      "GD iter. 388/2999: loss=0.5339506467089455\n",
      "GD iter. 389/2999: loss=0.5883320192593187\n",
      "GD iter. 390/2999: loss=0.5328374874112313\n",
      "GD iter. 391/2999: loss=0.5869797444214757\n",
      "GD iter. 392/2999: loss=0.5317282521666628\n",
      "GD iter. 393/2999: loss=0.5856324982224822\n",
      "GD iter. 394/2999: loss=0.5306229064654858\n",
      "GD iter. 395/2999: loss=0.5842902378572513\n",
      "GD iter. 396/2999: loss=0.5295214164450364\n",
      "GD iter. 397/2999: loss=0.5829529212798926\n",
      "GD iter. 398/2999: loss=0.5284237488724591\n",
      "GD iter. 399/2999: loss=0.5816205071826721\n",
      "GD iter. 400/2999: loss=0.5273298711283021\n",
      "GD iter. 401/2999: loss=0.5802929549760447\n",
      "GD iter. 402/2999: loss=0.5262397511905516\n",
      "GD iter. 403/2999: loss=0.5789702247693124\n",
      "GD iter. 404/2999: loss=0.5251533576191982\n",
      "GD iter. 405/2999: loss=0.5776522773519229\n",
      "GD iter. 406/2999: loss=0.5240706595411637\n",
      "GD iter. 407/2999: loss=0.576339074175339\n",
      "GD iter. 408/2999: loss=0.5229916266360374\n",
      "GD iter. 409/2999: loss=0.5750305773358585\n",
      "GD iter. 410/2999: loss=0.5219162291220172\n",
      "GD iter. 411/2999: loss=0.5737267495577956\n",
      "GD iter. 412/2999: loss=0.5208444377423196\n",
      "GD iter. 413/2999: loss=0.5724275541771827\n",
      "GD iter. 414/2999: loss=0.5197762237521769\n",
      "GD iter. 415/2999: loss=0.571132955126328\n",
      "GD iter. 416/2999: loss=0.5187115589061955\n",
      "GD iter. 417/2999: loss=0.5698429169187452\n",
      "GD iter. 418/2999: loss=0.517650415446006\n",
      "GD iter. 419/2999: loss=0.5685574046345223\n",
      "GD iter. 420/2999: loss=0.5165927660884541\n",
      "GD iter. 421/2999: loss=0.5672763839063867\n",
      "GD iter. 422/2999: loss=0.5155385840141604\n",
      "GD iter. 423/2999: loss=0.5659998209061646\n",
      "GD iter. 424/2999: loss=0.5144878428563299\n",
      "GD iter. 425/2999: loss=0.5647276823316784\n",
      "GD iter. 426/2999: loss=0.513440516689969\n",
      "GD iter. 427/2999: loss=0.563459935394044\n",
      "GD iter. 428/2999: loss=0.5123965800215227\n",
      "GD iter. 429/2999: loss=0.5621965478055976\n",
      "GD iter. 430/2999: loss=0.5113560077786955\n",
      "GD iter. 431/2999: loss=0.5609374877679983\n",
      "GD iter. 432/2999: loss=0.5103187753007378\n",
      "GD iter. 433/2999: loss=0.5596827239609187\n",
      "GD iter. 434/2999: loss=0.5092848583288359\n",
      "GD iter. 435/2999: loss=0.5584322255309015\n",
      "GD iter. 436/2999: loss=0.5082542329970673\n",
      "GD iter. 437/2999: loss=0.5571859620808611\n",
      "GD iter. 438/2999: loss=0.507226875823358\n",
      "GD iter. 439/2999: loss=0.5559439036596933\n",
      "GD iter. 440/2999: loss=0.5062027637008957\n",
      "GD iter. 441/2999: loss=0.5547060207523091\n",
      "GD iter. 442/2999: loss=0.5051818738897356\n",
      "GD iter. 443/2999: loss=0.5534722842700415\n",
      "GD iter. 444/2999: loss=0.5041641840086857\n",
      "GD iter. 445/2999: loss=0.5522426655413329\n",
      "GD iter. 446/2999: loss=0.5031496720273514\n",
      "GD iter. 447/2999: loss=0.5510171363025828\n",
      "GD iter. 448/2999: loss=0.5021383162585485\n",
      "GD iter. 449/2999: loss=0.5497956686895465\n",
      "GD iter. 450/2999: loss=0.5011300953508627\n",
      "GD iter. 451/2999: loss=0.5485782352288503\n",
      "GD iter. 452/2999: loss=0.5001249882814135\n",
      "GD iter. 453/2999: loss=0.5473648088297097\n",
      "GD iter. 454/2999: loss=0.49912297434886344\n",
      "GD iter. 455/2999: loss=0.5461553627760912\n",
      "GD iter. 456/2999: loss=0.49812403316662596\n",
      "GD iter. 457/2999: loss=0.544949870718952\n",
      "GD iter. 458/2999: loss=0.4971281446562413\n",
      "GD iter. 459/2999: loss=0.5437483066687925\n",
      "GD iter. 460/2999: loss=0.49613528904098014\n",
      "GD iter. 461/2999: loss=0.5425506449884459\n",
      "GD iter. 462/2999: loss=0.49514544683957995\n",
      "GD iter. 463/2999: loss=0.5413568603860093\n",
      "GD iter. 464/2999: loss=0.49415859886022706\n",
      "GD iter. 465/2999: loss=0.5401669279081126\n",
      "GD iter. 466/2999: loss=0.4931747261945886\n",
      "GD iter. 467/2999: loss=0.5389808229332198\n",
      "GD iter. 468/2999: loss=0.4921938102121684\n",
      "GD iter. 469/2999: loss=0.5377985211653087\n",
      "GD iter. 470/2999: loss=0.4912158325546819\n",
      "GD iter. 471/2999: loss=0.5366199986276102\n",
      "GD iter. 472/2999: loss=0.49024077513063646\n",
      "GD iter. 473/2999: loss=0.5354452316565245\n",
      "GD iter. 474/2999: loss=0.4892686201100566\n",
      "GD iter. 475/2999: loss=0.5342741968957953\n",
      "GD iter. 476/2999: loss=0.4882993499193608\n",
      "GD iter. 477/2999: loss=0.5331068712908024\n",
      "GD iter. 478/2999: loss=0.4873329472363543\n",
      "GD iter. 479/2999: loss=0.5319432320829903\n",
      "GD iter. 480/2999: loss=0.4863693949853429\n",
      "GD iter. 481/2999: loss=0.5307832568044784\n",
      "GD iter. 482/2999: loss=0.4854086763323937\n",
      "GD iter. 483/2999: loss=0.5296269232727884\n",
      "GD iter. 484/2999: loss=0.48445077468070263\n",
      "GD iter. 485/2999: loss=0.5284742095857619\n",
      "GD iter. 486/2999: loss=0.4834956736661491\n",
      "GD iter. 487/2999: loss=0.5273250941166441\n",
      "GD iter. 488/2999: loss=0.48254335715283525\n",
      "GD iter. 489/2999: loss=0.5261795555091469\n",
      "GD iter. 490/2999: loss=0.4815938092288204\n",
      "GD iter. 491/2999: loss=0.5250375726727883\n",
      "GD iter. 492/2999: loss=0.48064701420197137\n",
      "GD iter. 493/2999: loss=0.5238991247783118\n",
      "GD iter. 494/2999: loss=0.479702956595818\n",
      "GD iter. 495/2999: loss=0.5227641912531399\n",
      "GD iter. 496/2999: loss=0.478761621145689\n",
      "GD iter. 497/2999: loss=0.5216327517771289\n",
      "GD iter. 498/2999: loss=0.4778229927947148\n",
      "GD iter. 499/2999: loss=0.5205047862782147\n",
      "GD iter. 500/2999: loss=0.4768870566901318\n",
      "GD iter. 501/2999: loss=0.5193802749283383\n",
      "GD iter. 502/2999: loss=0.4759537981795107\n",
      "GD iter. 503/2999: loss=0.5182591981393321\n",
      "GD iter. 504/2999: loss=0.47502320280718957\n",
      "GD iter. 505/2999: loss=0.5171415365590415\n",
      "GD iter. 506/2999: loss=0.47409525631072064\n",
      "GD iter. 507/2999: loss=0.516027271067407\n",
      "GD iter. 508/2999: loss=0.4731699446174008\n",
      "GD iter. 509/2999: loss=0.5149163827727072\n",
      "GD iter. 510/2999: loss=0.47224725384093563\n",
      "GD iter. 511/2999: loss=0.5138088530079248\n",
      "GD iter. 512/2999: loss=0.47132717027812465\n",
      "GD iter. 513/2999: loss=0.5127046633271183\n",
      "GD iter. 514/2999: loss=0.47040968040563164\n",
      "GD iter. 515/2999: loss=0.5116037955019199\n",
      "GD iter. 516/2999: loss=0.46949477087680563\n",
      "GD iter. 517/2999: loss=0.5105062315180632\n",
      "GD iter. 518/2999: loss=0.46858242851862486\n",
      "GD iter. 519/2999: loss=0.5094119535720701\n",
      "GD iter. 520/2999: loss=0.46767264032866074\n",
      "GD iter. 521/2999: loss=0.5083209440679456\n",
      "GD iter. 522/2999: loss=0.46676539347210044\n",
      "GD iter. 523/2999: loss=0.5072331856139474\n",
      "GD iter. 524/2999: loss=0.46586067527887254\n",
      "GD iter. 525/2999: loss=0.5061486610194527\n",
      "GD iter. 526/2999: loss=0.46495847324080275\n",
      "GD iter. 527/2999: loss=0.5050673532918799\n",
      "GD iter. 528/2999: loss=0.4640587750087942\n",
      "GD iter. 529/2999: loss=0.5039892456336342\n",
      "GD iter. 530/2999: loss=0.4631615683901587\n",
      "GD iter. 531/2999: loss=0.5029143214391791\n",
      "GD iter. 532/2999: loss=0.4622668413458949\n",
      "GD iter. 533/2999: loss=0.5018425642921261\n",
      "GD iter. 534/2999: loss=0.4613745819880888\n",
      "GD iter. 535/2999: loss=0.500773957962379\n",
      "GD iter. 536/2999: loss=0.4604847785773509\n",
      "GD iter. 537/2999: loss=0.4997084864033725\n",
      "GD iter. 538/2999: loss=0.4595974195202755\n",
      "GD iter. 539/2999: loss=0.49864613374930844\n",
      "GD iter. 540/2999: loss=0.45871249336697895\n",
      "GD iter. 541/2999: loss=0.49758688431249315\n",
      "GD iter. 542/2999: loss=0.45782998880867254\n",
      "GD iter. 543/2999: loss=0.49653072258068226\n",
      "GD iter. 544/2999: loss=0.4569498946753013\n",
      "GD iter. 545/2999: loss=0.49547763321455907\n",
      "GD iter. 546/2999: loss=0.4560721999331437\n",
      "GD iter. 547/2999: loss=0.4944276010450809\n",
      "GD iter. 548/2999: loss=0.45519689368257393\n",
      "GD iter. 549/2999: loss=0.4933806110710976\n",
      "GD iter. 550/2999: loss=0.45432396515580703\n",
      "GD iter. 551/2999: loss=0.4923366484568657\n",
      "GD iter. 552/2999: loss=0.4534534037146495\n",
      "GD iter. 553/2999: loss=0.4912956985296303\n",
      "GD iter. 554/2999: loss=0.45258519884832854\n",
      "GD iter. 555/2999: loss=0.4902577467772532\n",
      "GD iter. 556/2999: loss=0.45171934017140214\n",
      "GD iter. 557/2999: loss=0.48922277884597154\n",
      "GD iter. 558/2999: loss=0.4508558174215992\n",
      "GD iter. 559/2999: loss=0.48819078053799286\n",
      "GD iter. 560/2999: loss=0.4499946204577579\n",
      "GD iter. 561/2999: loss=0.48716173780931216\n",
      "GD iter. 562/2999: loss=0.44913573925786066\n",
      "GD iter. 563/2999: loss=0.4861356367675499\n",
      "GD iter. 564/2999: loss=0.44827916391696904\n",
      "GD iter. 565/2999: loss=0.48511246366971167\n",
      "GD iter. 566/2999: loss=0.4474248846452942\n",
      "GD iter. 567/2999: loss=0.4840922049200681\n",
      "GD iter. 568/2999: loss=0.44657289176625686\n",
      "GD iter. 569/2999: loss=0.4830748470680789\n",
      "GD iter. 570/2999: loss=0.445723175714575\n",
      "GD iter. 571/2999: loss=0.48206037680626723\n",
      "GD iter. 572/2999: loss=0.44487572703442085\n",
      "GD iter. 573/2999: loss=0.4810487809682392\n",
      "GD iter. 574/2999: loss=0.44403053637758433\n",
      "GD iter. 575/2999: loss=0.4800400465266749\n",
      "GD iter. 576/2999: loss=0.4431875945016168\n",
      "GD iter. 577/2999: loss=0.4790341605912995\n",
      "GD iter. 578/2999: loss=0.4423468922681371\n",
      "GD iter. 579/2999: loss=0.47803111040702234\n",
      "GD iter. 580/2999: loss=0.44150842064098683\n",
      "GD iter. 581/2999: loss=0.47703088335193994\n",
      "GD iter. 582/2999: loss=0.4406721706845874\n",
      "GD iter. 583/2999: loss=0.47603346693552157\n",
      "GD iter. 584/2999: loss=0.43983813356217943\n",
      "GD iter. 585/2999: loss=0.4750388487966866\n",
      "GD iter. 586/2999: loss=0.4390063005341576\n",
      "GD iter. 587/2999: loss=0.4740470167019892\n",
      "GD iter. 588/2999: loss=0.43817666295647967\n",
      "GD iter. 589/2999: loss=0.47305795854385746\n",
      "GD iter. 590/2999: loss=0.43734921227899853\n",
      "GD iter. 591/2999: loss=0.47207166233877146\n",
      "GD iter. 592/2999: loss=0.43652394004386147\n",
      "GD iter. 593/2999: loss=0.47108811622549596\n",
      "GD iter. 594/2999: loss=0.43570083788395025\n",
      "GD iter. 595/2999: loss=0.4701073084633893\n",
      "GD iter. 596/2999: loss=0.4348798975213533\n",
      "GD iter. 597/2999: loss=0.46912922743068414\n",
      "GD iter. 598/2999: loss=0.43406111076582493\n",
      "GD iter. 599/2999: loss=0.46815386162281325\n",
      "GD iter. 600/2999: loss=0.4332444695132767\n",
      "GD iter. 601/2999: loss=0.4671811996507511\n",
      "GD iter. 602/2999: loss=0.4324299657442889\n",
      "GD iter. 603/2999: loss=0.46621123023933575\n",
      "GD iter. 604/2999: loss=0.4316175915226851\n",
      "GD iter. 605/2999: loss=0.4652439422257343\n",
      "GD iter. 606/2999: loss=0.43080733899406826\n",
      "GD iter. 607/2999: loss=0.46427932455779697\n",
      "GD iter. 608/2999: loss=0.429999200384391\n",
      "GD iter. 609/2999: loss=0.46331736629246195\n",
      "GD iter. 610/2999: loss=0.4291931679986287\n",
      "GD iter. 611/2999: loss=0.46235805659432583\n",
      "GD iter. 612/2999: loss=0.428389234219324\n",
      "GD iter. 613/2999: loss=0.4614013847339734\n",
      "GD iter. 614/2999: loss=0.4275873915052895\n",
      "GD iter. 615/2999: loss=0.4604473400865752\n",
      "GD iter. 616/2999: loss=0.4267876323902656\n",
      "GD iter. 617/2999: loss=0.45949591213036334\n",
      "GD iter. 618/2999: loss=0.42598994948156405\n",
      "GD iter. 619/2999: loss=0.45854709044512776\n",
      "GD iter. 620/2999: loss=0.42519433545882146\n",
      "GD iter. 621/2999: loss=0.45760086471083405\n",
      "GD iter. 622/2999: loss=0.42440078307272033\n",
      "GD iter. 623/2999: loss=0.4566572247061764\n",
      "GD iter. 624/2999: loss=0.42360928514368795\n",
      "GD iter. 625/2999: loss=0.4557161603071165\n",
      "GD iter. 626/2999: loss=0.422819834560711\n",
      "GD iter. 627/2999: loss=0.4547776614855718\n",
      "GD iter. 628/2999: loss=0.4220324242800823\n",
      "GD iter. 629/2999: loss=0.45384171830798814\n",
      "GD iter. 630/2999: loss=0.4212470473242064\n",
      "GD iter. 631/2999: loss=0.452908320933997\n",
      "GD iter. 632/2999: loss=0.42046369678041634\n",
      "GD iter. 633/2999: loss=0.45197745961508284\n",
      "GD iter. 634/2999: loss=0.4196823657997802\n",
      "GD iter. 635/2999: loss=0.45104912469323405\n",
      "GD iter. 636/2999: loss=0.41890304759601166\n",
      "GD iter. 637/2999: loss=0.4501233065997109\n",
      "GD iter. 638/2999: loss=0.4181257354442954\n",
      "GD iter. 639/2999: loss=0.4491999958537103\n",
      "GD iter. 640/2999: loss=0.41735042268014927\n",
      "GD iter. 641/2999: loss=0.4482791830610578\n",
      "GD iter. 642/2999: loss=0.41657710269835496\n",
      "GD iter. 643/2999: loss=0.4473608589130176\n",
      "GD iter. 644/2999: loss=0.41580576895186805\n",
      "GD iter. 645/2999: loss=0.4464450141850319\n",
      "GD iter. 646/2999: loss=0.4150364149507536\n",
      "GD iter. 647/2999: loss=0.44553163973549387\n",
      "GD iter. 648/2999: loss=0.4142690342611443\n",
      "GD iter. 649/2999: loss=0.4446207265045823\n",
      "GD iter. 650/2999: loss=0.41350362050416134\n",
      "GD iter. 651/2999: loss=0.44371226551299037\n",
      "GD iter. 652/2999: loss=0.4127401673549269\n",
      "GD iter. 653/2999: loss=0.44280624786079886\n",
      "GD iter. 654/2999: loss=0.41197866854155635\n",
      "GD iter. 655/2999: loss=0.4419026647263383\n",
      "GD iter. 656/2999: loss=0.4112191178441781\n",
      "GD iter. 657/2999: loss=0.441001507365015\n",
      "GD iter. 658/2999: loss=0.41046150909393275\n",
      "GD iter. 659/2999: loss=0.4401027671081731\n",
      "GD iter. 660/2999: loss=0.409705836172053\n",
      "GD iter. 661/2999: loss=0.43920643536203646\n",
      "GD iter. 662/2999: loss=0.4089520930088326\n",
      "GD iter. 663/2999: loss=0.4383125036064692\n",
      "GD iter. 664/2999: loss=0.4082002735828234\n",
      "GD iter. 665/2999: loss=0.4374209633940904\n",
      "GD iter. 666/2999: loss=0.40745037191980193\n",
      "GD iter. 667/2999: loss=0.43653180634901984\n",
      "GD iter. 668/2999: loss=0.4067023820919506\n",
      "GD iter. 669/2999: loss=0.4356450241659389\n",
      "GD iter. 670/2999: loss=0.4059562982169216\n",
      "GD iter. 671/2999: loss=0.434760608608984\n",
      "GD iter. 672/2999: loss=0.40521211445698074\n",
      "GD iter. 673/2999: loss=0.43387855151073446\n",
      "GD iter. 674/2999: loss=0.4044698250181214\n",
      "GD iter. 675/2999: loss=0.43299884477117434\n",
      "GD iter. 676/2999: loss=0.4037294241492855\n",
      "GD iter. 677/2999: loss=0.43212148035676445\n",
      "GD iter. 678/2999: loss=0.40299090614145205\n",
      "GD iter. 679/2999: loss=0.4312464502993238\n",
      "GD iter. 680/2999: loss=0.40225426532687186\n",
      "GD iter. 681/2999: loss=0.43037374669518597\n",
      "GD iter. 682/2999: loss=0.40151949607822546\n",
      "GD iter. 683/2999: loss=0.4295033617041308\n",
      "GD iter. 684/2999: loss=0.40078659280787265\n",
      "GD iter. 685/2999: loss=0.42863528754851504\n",
      "GD iter. 686/2999: loss=0.40005554996703035\n",
      "GD iter. 687/2999: loss=0.4277695165122478\n",
      "GD iter. 688/2999: loss=0.3993263620450402\n",
      "GD iter. 689/2999: loss=0.42690604093996265\n",
      "GD iter. 690/2999: loss=0.39859902356857374\n",
      "GD iter. 691/2999: loss=0.42604485323601066\n",
      "GD iter. 692/2999: loss=0.3978735291009347\n",
      "GD iter. 693/2999: loss=0.4251859458636106\n",
      "GD iter. 694/2999: loss=0.3971498732413103\n",
      "GD iter. 695/2999: loss=0.4243293113439712\n",
      "GD iter. 696/2999: loss=0.3964280506240604\n",
      "GD iter. 697/2999: loss=0.42347494225537596\n",
      "GD iter. 698/2999: loss=0.39570805591800406\n",
      "GD iter. 699/2999: loss=0.42262283123235717\n",
      "GD iter. 700/2999: loss=0.3949898838257374\n",
      "GD iter. 701/2999: loss=0.4217729709647953\n",
      "GD iter. 702/2999: loss=0.3942735290829575\n",
      "GD iter. 703/2999: loss=0.42092535419715316\n",
      "GD iter. 704/2999: loss=0.3935589864577846\n",
      "GD iter. 705/2999: loss=0.4200799737275724\n",
      "GD iter. 706/2999: loss=0.39284625075010543\n",
      "GD iter. 707/2999: loss=0.4192368224070876\n",
      "GD iter. 708/2999: loss=0.3921353167909513\n",
      "GD iter. 709/2999: loss=0.4183958931388418\n",
      "GD iter. 710/2999: loss=0.39142617944184493\n",
      "GD iter. 711/2999: loss=0.41755717887727156\n",
      "GD iter. 712/2999: loss=0.3907188335941786\n",
      "GD iter. 713/2999: loss=0.4167206726273164\n",
      "GD iter. 714/2999: loss=0.3900132741686249\n",
      "GD iter. 715/2999: loss=0.4158863674436808\n",
      "GD iter. 716/2999: loss=0.3893094961145236\n",
      "GD iter. 717/2999: loss=0.4150542564300424\n",
      "GD iter. 718/2999: loss=0.3886074944092773\n",
      "GD iter. 719/2999: loss=0.4142243327383025\n",
      "GD iter. 720/2999: loss=0.3879072640578143\n",
      "GD iter. 721/2999: loss=0.41339658956787606\n",
      "GD iter. 722/2999: loss=0.3872088000920146\n",
      "GD iter. 723/2999: loss=0.4125710201649672\n",
      "GD iter. 724/2999: loss=0.3865120975701421\n",
      "GD iter. 725/2999: loss=0.41174761782184427\n",
      "GD iter. 726/2999: loss=0.3858171515763132\n",
      "GD iter. 727/2999: loss=0.4109263758761258\n",
      "GD iter. 728/2999: loss=0.38512395721993953\n",
      "GD iter. 729/2999: loss=0.4101072877100712\n",
      "GD iter. 730/2999: loss=0.3844325096352498\n",
      "GD iter. 731/2999: loss=0.4092903467499666\n",
      "GD iter. 732/2999: loss=0.3837428039807449\n",
      "GD iter. 733/2999: loss=0.40847554646537837\n",
      "GD iter. 734/2999: loss=0.3830548354387216\n",
      "GD iter. 735/2999: loss=0.4076628803685489\n",
      "GD iter. 736/2999: loss=0.38236859921474653\n",
      "GD iter. 737/2999: loss=0.40685234201369697\n",
      "GD iter. 738/2999: loss=0.3816840905372205\n",
      "GD iter. 739/2999: loss=0.4060439249964254\n",
      "GD iter. 740/2999: loss=0.38100130465689686\n",
      "GD iter. 741/2999: loss=0.40523762295307486\n",
      "GD iter. 742/2999: loss=0.3803202368463641\n",
      "GD iter. 743/2999: loss=0.40443342956004724\n",
      "GD iter. 744/2999: loss=0.37964088239971383\n",
      "GD iter. 745/2999: loss=0.4036313385333408\n",
      "GD iter. 746/2999: loss=0.3789632366319826\n",
      "GD iter. 747/2999: loss=0.40283134362778805\n",
      "GD iter. 748/2999: loss=0.37828729487877005\n",
      "GD iter. 749/2999: loss=0.40203343863654617\n",
      "GD iter. 750/2999: loss=0.37761305249581995\n",
      "GD iter. 751/2999: loss=0.401237617390513\n",
      "GD iter. 752/2999: loss=0.37694050485862285\n",
      "GD iter. 753/2999: loss=0.40044387375778157\n",
      "GD iter. 754/2999: loss=0.37626964736198876\n",
      "GD iter. 755/2999: loss=0.3996522016430354\n",
      "GD iter. 756/2999: loss=0.37560047541966296\n",
      "GD iter. 757/2999: loss=0.3988625949870305\n",
      "GD iter. 758/2999: loss=0.3749329844639221\n",
      "GD iter. 759/2999: loss=0.39807504776601815\n",
      "GD iter. 760/2999: loss=0.37426716994524195\n",
      "GD iter. 761/2999: loss=0.3972895539912643\n",
      "GD iter. 762/2999: loss=0.37360302733188233\n",
      "GD iter. 763/2999: loss=0.3965061077084821\n",
      "GD iter. 764/2999: loss=0.3729405521095896\n",
      "GD iter. 765/2999: loss=0.3957247029973874\n",
      "GD iter. 766/2999: loss=0.37227973978119977\n",
      "GD iter. 767/2999: loss=0.3949453339711272\n",
      "GD iter. 768/2999: loss=0.3716205858662873\n",
      "GD iter. 769/2999: loss=0.3941679947757877\n",
      "GD iter. 770/2999: loss=0.3709630859008985\n",
      "GD iter. 771/2999: loss=0.3933926795899847\n",
      "GD iter. 772/2999: loss=0.3703072354371744\n",
      "GD iter. 773/2999: loss=0.3926193826243193\n",
      "GD iter. 774/2999: loss=0.3696530300430489\n",
      "GD iter. 775/2999: loss=0.39184809812094046\n",
      "GD iter. 776/2999: loss=0.3690004653019609\n",
      "GD iter. 777/2999: loss=0.39107882035308944\n",
      "GD iter. 778/2999: loss=0.3683495368125221\n",
      "GD iter. 779/2999: loss=0.390311543624633\n",
      "GD iter. 780/2999: loss=0.36770024018827596\n",
      "GD iter. 781/2999: loss=0.38954626226967565\n",
      "GD iter. 782/2999: loss=0.3670525710573463\n",
      "GD iter. 783/2999: loss=0.38878297065203793\n",
      "GD iter. 784/2999: loss=0.3664065250622476\n",
      "GD iter. 785/2999: loss=0.3880216631649583\n",
      "GD iter. 786/2999: loss=0.36576209785956304\n",
      "GD iter. 787/2999: loss=0.38726233423058914\n",
      "GD iter. 788/2999: loss=0.36511928511968705\n",
      "GD iter. 789/2999: loss=0.3865049782996067\n",
      "GD iter. 790/2999: loss=0.364478082526598\n",
      "GD iter. 791/2999: loss=0.3857495898508528\n",
      "GD iter. 792/2999: loss=0.36383848577758787\n",
      "GD iter. 793/2999: loss=0.3849961633909043\n",
      "GD iter. 794/2999: loss=0.36320049058306414\n",
      "GD iter. 795/2999: loss=0.38424469345373874\n",
      "GD iter. 796/2999: loss=0.3625640926662866\n",
      "GD iter. 797/2999: loss=0.38349517460033583\n",
      "GD iter. 798/2999: loss=0.36192928776314465\n",
      "GD iter. 799/2999: loss=0.38274760141829817\n",
      "GD iter. 800/2999: loss=0.36129607162198574\n",
      "GD iter. 801/2999: loss=0.3820019685215678\n",
      "GD iter. 802/2999: loss=0.36066444000335574\n",
      "GD iter. 803/2999: loss=0.3812582705499988\n",
      "GD iter. 804/2999: loss=0.3600343886798117\n",
      "GD iter. 805/2999: loss=0.38051650216904814\n",
      "GD iter. 806/2999: loss=0.3594059134357474\n",
      "GD iter. 807/2999: loss=0.37977665806946376\n",
      "GD iter. 808/2999: loss=0.35877901006719465\n",
      "GD iter. 809/2999: loss=0.37903873296695123\n",
      "GD iter. 810/2999: loss=0.3581536743816469\n",
      "GD iter. 811/2999: loss=0.3783027216018661\n",
      "GD iter. 812/2999: loss=0.35752990219787173\n",
      "GD iter. 813/2999: loss=0.3775686187388811\n",
      "GD iter. 814/2999: loss=0.3569076893457358\n",
      "GD iter. 815/2999: loss=0.37683641916669236\n",
      "GD iter. 816/2999: loss=0.35628703166608555\n",
      "GD iter. 817/2999: loss=0.37610611769776797\n",
      "GD iter. 818/2999: loss=0.3556679250105503\n",
      "GD iter. 819/2999: loss=0.37537770916800856\n",
      "GD iter. 820/2999: loss=0.3550503652414031\n",
      "GD iter. 821/2999: loss=0.37465118843650075\n",
      "GD iter. 822/2999: loss=0.35443434823142367\n",
      "GD iter. 823/2999: loss=0.3739265503852339\n",
      "GD iter. 824/2999: loss=0.3538198698637485\n",
      "GD iter. 825/2999: loss=0.37320378991883074\n",
      "GD iter. 826/2999: loss=0.35320692603175113\n",
      "GD iter. 827/2999: loss=0.3724829019643023\n",
      "GD iter. 828/2999: loss=0.3525955126389081\n",
      "GD iter. 829/2999: loss=0.37176388147079964\n",
      "GD iter. 830/2999: loss=0.3519856255986738\n",
      "GD iter. 831/2999: loss=0.3710467234093401\n",
      "GD iter. 832/2999: loss=0.351377260834387\n",
      "GD iter. 833/2999: loss=0.3703314227726274\n",
      "GD iter. 834/2999: loss=0.3507704142791288\n",
      "GD iter. 835/2999: loss=0.36961797457476125\n",
      "GD iter. 836/2999: loss=0.3501650818756387\n",
      "GD iter. 837/2999: loss=0.3689063738510391\n",
      "GD iter. 838/2999: loss=0.34956125957621537\n",
      "GD iter. 839/2999: loss=0.36819661565775486\n",
      "GD iter. 840/2999: loss=0.34895894334260175\n",
      "GD iter. 841/2999: loss=0.36748869507194776\n",
      "GD iter. 842/2999: loss=0.3483581291459151\n",
      "GD iter. 843/2999: loss=0.3667826071912224\n",
      "GD iter. 844/2999: loss=0.3477588129665486\n",
      "GD iter. 845/2999: loss=0.3660783471335433\n",
      "GD iter. 846/2999: loss=0.34716099079411245\n",
      "GD iter. 847/2999: loss=0.3653759100370553\n",
      "GD iter. 848/2999: loss=0.34656465862731767\n",
      "GD iter. 849/2999: loss=0.3646752910598515\n",
      "GD iter. 850/2999: loss=0.34596981247394915\n",
      "GD iter. 851/2999: loss=0.3639764853798482\n",
      "GD iter. 852/2999: loss=0.34537644835077175\n",
      "GD iter. 853/2999: loss=0.3632794881945699\n",
      "GD iter. 854/2999: loss=0.34478456228346843\n",
      "GD iter. 855/2999: loss=0.3625842947209765\n",
      "GD iter. 856/2999: loss=0.3441941503066057\n",
      "GD iter. 857/2999: loss=0.3618909001953367\n",
      "GD iter. 858/2999: loss=0.34360520846354115\n",
      "GD iter. 859/2999: loss=0.36119929987301364\n",
      "GD iter. 860/2999: loss=0.34301773280640946\n",
      "GD iter. 861/2999: loss=0.36050948902835084\n",
      "GD iter. 862/2999: loss=0.34243171939603767\n",
      "GD iter. 863/2999: loss=0.35982146295448164\n",
      "GD iter. 864/2999: loss=0.34184716430194817\n",
      "GD iter. 865/2999: loss=0.35913521696323536\n",
      "GD iter. 866/2999: loss=0.3412640636022844\n",
      "GD iter. 867/2999: loss=0.3584507463849554\n",
      "GD iter. 868/2999: loss=0.34068241338379057\n",
      "GD iter. 869/2999: loss=0.357768046568378\n",
      "GD iter. 870/2999: loss=0.34010220974177247\n",
      "GD iter. 871/2999: loss=0.35708711288049727\n",
      "GD iter. 872/2999: loss=0.33952344878008794\n",
      "GD iter. 873/2999: loss=0.3564079407064535\n",
      "GD iter. 874/2999: loss=0.33894612661109197\n",
      "GD iter. 875/2999: loss=0.35573052544938016\n",
      "GD iter. 876/2999: loss=0.33837023935565363\n",
      "GD iter. 877/2999: loss=0.3550548625303375\n",
      "GD iter. 878/2999: loss=0.33779578314309033\n",
      "GD iter. 879/2999: loss=0.3543809473881271\n",
      "GD iter. 880/2999: loss=0.33722275411119457\n",
      "GD iter. 881/2999: loss=0.3537087754792451\n",
      "GD iter. 882/2999: loss=0.3366511484062025\n",
      "GD iter. 883/2999: loss=0.3530383422777536\n",
      "GD iter. 884/2999: loss=0.3360809621828012\n",
      "GD iter. 885/2999: loss=0.3523696432751928\n",
      "GD iter. 886/2999: loss=0.33551219160409507\n",
      "GD iter. 887/2999: loss=0.3517026739804524\n",
      "GD iter. 888/2999: loss=0.3349448328416206\n",
      "GD iter. 889/2999: loss=0.35103742991970327\n",
      "GD iter. 890/2999: loss=0.33437888207533434\n",
      "GD iter. 891/2999: loss=0.3503739066362988\n",
      "GD iter. 892/2999: loss=0.333814335493645\n",
      "GD iter. 893/2999: loss=0.3497120996907095\n",
      "GD iter. 894/2999: loss=0.3332511892933742\n",
      "GD iter. 895/2999: loss=0.3490520046604084\n",
      "GD iter. 896/2999: loss=0.33268943967979775\n",
      "GD iter. 897/2999: loss=0.3483936171398153\n",
      "GD iter. 898/2999: loss=0.33212908286664544\n",
      "GD iter. 899/2999: loss=0.34773693274022316\n",
      "GD iter. 900/2999: loss=0.33157011507609724\n",
      "GD iter. 901/2999: loss=0.3470819470896991\n",
      "GD iter. 902/2999: loss=0.3310125325388256\n",
      "GD iter. 903/2999: loss=0.3464286558330592\n",
      "GD iter. 904/2999: loss=0.3304563314939828\n",
      "GD iter. 905/2999: loss=0.3457770546317507\n",
      "GD iter. 906/2999: loss=0.3299015081892475\n",
      "GD iter. 907/2999: loss=0.3451271391638439\n",
      "GD iter. 908/2999: loss=0.32934805888080587\n",
      "GD iter. 909/2999: loss=0.34447890512391566\n",
      "GD iter. 910/2999: loss=0.32879597983342596\n",
      "GD iter. 911/2999: loss=0.3438323482230497\n",
      "GD iter. 912/2999: loss=0.3282452673204364\n",
      "GD iter. 913/2999: loss=0.34318746418873947\n",
      "GD iter. 914/2999: loss=0.3276959176237797\n",
      "GD iter. 915/2999: loss=0.34254424876486367\n",
      "GD iter. 916/2999: loss=0.32714792703401746\n",
      "GD iter. 917/2999: loss=0.3419026977116139\n",
      "GD iter. 918/2999: loss=0.32660129185039055\n",
      "GD iter. 919/2999: loss=0.341262806805478\n",
      "GD iter. 920/2999: loss=0.3260560083808248\n",
      "GD iter. 921/2999: loss=0.3406245718391761\n",
      "GD iter. 922/2999: loss=0.32551207294197987\n",
      "GD iter. 923/2999: loss=0.3399879886216375\n",
      "GD iter. 924/2999: loss=0.32496948185926994\n",
      "GD iter. 925/2999: loss=0.33935305297793666\n",
      "GD iter. 926/2999: loss=0.32442823146689953\n",
      "GD iter. 927/2999: loss=0.33871976074926674\n",
      "GD iter. 928/2999: loss=0.3238883181079091\n",
      "GD iter. 929/2999: loss=0.3380881077929151\n",
      "GD iter. 930/2999: loss=0.32334973813422385\n",
      "GD iter. 931/2999: loss=0.3374580899822392\n",
      "GD iter. 932/2999: loss=0.32281248790666617\n",
      "GD iter. 933/2999: loss=0.33682970320660893\n",
      "GD iter. 934/2999: loss=0.32227656379503133\n",
      "GD iter. 935/2999: loss=0.3362029433714184\n",
      "GD iter. 936/2999: loss=0.3217419621780998\n",
      "GD iter. 937/2999: loss=0.3355778063980259\n",
      "GD iter. 938/2999: loss=0.32120867944370285\n",
      "GD iter. 939/2999: loss=0.3349542882237559\n",
      "GD iter. 940/2999: loss=0.32067671198875025\n",
      "GD iter. 941/2999: loss=0.33433238480185545\n",
      "GD iter. 942/2999: loss=0.32014605621929443\n",
      "GD iter. 943/2999: loss=0.33371209210150493\n",
      "GD iter. 944/2999: loss=0.3196167085505649\n",
      "GD iter. 945/2999: loss=0.33309340610777266\n",
      "GD iter. 946/2999: loss=0.31908866540701564\n",
      "GD iter. 947/2999: loss=0.33247632282161116\n",
      "GD iter. 948/2999: loss=0.3185619232224\n",
      "GD iter. 949/2999: loss=0.3318608382598614\n",
      "GD iter. 950/2999: loss=0.3180364784397815\n",
      "GD iter. 951/2999: loss=0.3312469484552097\n",
      "GD iter. 952/2999: loss=0.3175123275116205\n",
      "GD iter. 953/2999: loss=0.3306346494562076\n",
      "GD iter. 954/2999: loss=0.3169894668998217\n",
      "GD iter. 955/2999: loss=0.33002393732725877\n",
      "GD iter. 956/2999: loss=0.31646789307576567\n",
      "GD iter. 957/2999: loss=0.3294148081485931\n",
      "GD iter. 958/2999: loss=0.31594760252039056\n",
      "GD iter. 959/2999: loss=0.32880725801629185\n",
      "GD iter. 960/2999: loss=0.31542859172422505\n",
      "GD iter. 961/2999: loss=0.3282012830422541\n",
      "GD iter. 962/2999: loss=0.31491085718745976\n",
      "GD iter. 963/2999: loss=0.32759687935421533\n",
      "GD iter. 964/2999: loss=0.31439439542001507\n",
      "GD iter. 965/2999: loss=0.32699404309576735\n",
      "GD iter. 966/2999: loss=0.313879202941575\n",
      "GD iter. 967/2999: loss=0.32639277042632386\n",
      "GD iter. 968/2999: loss=0.3133652762816882\n",
      "GD iter. 969/2999: loss=0.3257930575211808\n",
      "GD iter. 970/2999: loss=0.31285261197976044\n",
      "GD iter. 971/2999: loss=0.32519490057143285\n",
      "GD iter. 972/2999: loss=0.3123412065851868\n",
      "GD iter. 973/2999: loss=0.32459829578407645\n",
      "GD iter. 974/2999: loss=0.31183105665736904\n",
      "GD iter. 975/2999: loss=0.32400323938194914\n",
      "GD iter. 976/2999: loss=0.3113221587658183\n",
      "GD iter. 977/2999: loss=0.3234097276038123\n",
      "GD iter. 978/2999: loss=0.31081450949017936\n",
      "GD iter. 979/2999: loss=0.3228177567042971\n",
      "GD iter. 980/2999: loss=0.31030810542031556\n",
      "GD iter. 981/2999: loss=0.3222273229539467\n",
      "GD iter. 982/2999: loss=0.30980294315639\n",
      "GD iter. 983/2999: loss=0.32163842263925735\n",
      "GD iter. 984/2999: loss=0.3092990193088881\n",
      "GD iter. 985/2999: loss=0.3210510520626358\n",
      "GD iter. 986/2999: loss=0.3087963304987521\n",
      "GD iter. 987/2999: loss=0.32046520754250646\n",
      "GD iter. 988/2999: loss=0.308294873357408\n",
      "GD iter. 989/2999: loss=0.3198808854132731\n",
      "GD iter. 990/2999: loss=0.30779464452682226\n",
      "GD iter. 991/2999: loss=0.31929808202533627\n",
      "GD iter. 992/2999: loss=0.3072956406596287\n",
      "GD iter. 993/2999: loss=0.318716793745178\n",
      "GD iter. 994/2999: loss=0.3067978584191498\n",
      "GD iter. 995/2999: loss=0.31813701695533064\n",
      "GD iter. 996/2999: loss=0.306301294479494\n",
      "GD iter. 997/2999: loss=0.31755874805444356\n",
      "GD iter. 998/2999: loss=0.30580594552565\n",
      "GD iter. 999/2999: loss=0.31698198345731615\n",
      "GD iter. 1000/2999: loss=0.30531180825351156\n",
      "GD iter. 1001/2999: loss=0.31640671959489536\n",
      "GD iter. 1002/2999: loss=0.30481887937000474\n",
      "GD iter. 1003/2999: loss=0.3158329529143564\n",
      "GD iter. 1004/2999: loss=0.3043271555931586\n",
      "GD iter. 1005/2999: loss=0.31526067987913314\n",
      "GD iter. 1006/2999: loss=0.30383663365217134\n",
      "GD iter. 1007/2999: loss=0.31468989696894656\n",
      "GD iter. 1008/2999: loss=0.30334731028748846\n",
      "GD iter. 1009/2999: loss=0.3141206006798353\n",
      "GD iter. 1010/2999: loss=0.3028591822509184\n",
      "GD iter. 1011/2999: loss=0.3135527875242522\n",
      "GD iter. 1012/2999: loss=0.30237224630569537\n",
      "GD iter. 1013/2999: loss=0.31298645403106806\n",
      "GD iter. 1014/2999: loss=0.3018864992265462\n",
      "GD iter. 1015/2999: loss=0.3124215967456188\n",
      "GD iter. 1016/2999: loss=0.30140193779983154\n",
      "GD iter. 1017/2999: loss=0.3118582122298047\n",
      "GD iter. 1018/2999: loss=0.3009185588235974\n",
      "GD iter. 1019/2999: loss=0.3112962970621089\n",
      "GD iter. 1020/2999: loss=0.3004363591076695\n",
      "GD iter. 1021/2999: loss=0.31073584783765895\n",
      "GD iter. 1022/2999: loss=0.29995533547376296\n",
      "GD iter. 1023/2999: loss=0.31017686116830046\n",
      "GD iter. 1024/2999: loss=0.2994754847555745\n",
      "GD iter. 1025/2999: loss=0.3096193336826701\n",
      "GD iter. 1026/2999: loss=0.2989968037988694\n",
      "GD iter. 1027/2999: loss=0.3090632620262345\n",
      "GD iter. 1028/2999: loss=0.29851928946158895\n",
      "GD iter. 1029/2999: loss=0.3085086428613789\n",
      "GD iter. 1030/2999: loss=0.2980429386139489\n",
      "GD iter. 1031/2999: loss=0.3079554728674766\n",
      "GD iter. 1032/2999: loss=0.2975677481385668\n",
      "GD iter. 1033/2999: loss=0.3074037487409905\n",
      "GD iter. 1034/2999: loss=0.2970937149305258\n",
      "GD iter. 1035/2999: loss=0.30685346719550766\n",
      "GD iter. 1036/2999: loss=0.2966208358975202\n",
      "GD iter. 1037/2999: loss=0.30630462496185584\n",
      "GD iter. 1038/2999: loss=0.29614910795995564\n",
      "GD iter. 1039/2999: loss=0.3057572187881866\n",
      "GD iter. 1040/2999: loss=0.29567852805103834\n",
      "GD iter. 1041/2999: loss=0.30521124544003375\n",
      "GD iter. 1042/2999: loss=0.2952090931169416\n",
      "GD iter. 1043/2999: loss=0.3046667017004665\n",
      "GD iter. 1044/2999: loss=0.29474080011688647\n",
      "GD iter. 1045/2999: loss=0.30412358437014303\n",
      "GD iter. 1046/2999: loss=0.2942736460232718\n",
      "GD iter. 1047/2999: loss=0.3035818902674219\n",
      "GD iter. 1048/2999: loss=0.2938076278218007\n",
      "GD iter. 1049/2999: loss=0.30304161622846865\n",
      "GD iter. 1050/2999: loss=0.29334274251162534\n",
      "GD iter. 1051/2999: loss=0.3025027591073862\n",
      "GD iter. 1052/2999: loss=0.2928789871054366\n",
      "GD iter. 1053/2999: loss=0.3019653157762823\n",
      "GD iter. 1054/2999: loss=0.2924163586296447\n",
      "GD iter. 1055/2999: loss=0.30142928312544176\n",
      "GD iter. 1056/2999: loss=0.291954854124484\n",
      "GD iter. 1057/2999: loss=0.3008946580634099\n",
      "GD iter. 1058/2999: loss=0.29149447064417755\n",
      "GD iter. 1059/2999: loss=0.30036143751714994\n",
      "GD iter. 1060/2999: loss=0.2910352052570557\n",
      "GD iter. 1061/2999: loss=0.29982961843214406\n",
      "GD iter. 1062/2999: loss=0.2905770550457366\n",
      "GD iter. 1063/2999: loss=0.29929919777257197\n",
      "GD iter. 1064/2999: loss=0.2901200171072395\n",
      "GD iter. 1065/2999: loss=0.29877017252140003\n",
      "GD iter. 1066/2999: loss=0.2896640885531731\n",
      "GD iter. 1067/2999: loss=0.29824253968057757\n",
      "GD iter. 1068/2999: loss=0.2892092665098837\n",
      "GD iter. 1069/2999: loss=0.2977162962711694\n",
      "GD iter. 1070/2999: loss=0.2887555481186028\n",
      "GD iter. 1071/2999: loss=0.2971914393334924\n",
      "GD iter. 1072/2999: loss=0.28830293053565204\n",
      "GD iter. 1073/2999: loss=0.2966679659273319\n",
      "GD iter. 1074/2999: loss=0.28785141093258565\n",
      "GD iter. 1075/2999: loss=0.2961458731320676\n",
      "GD iter. 1076/2999: loss=0.28740098649637535\n",
      "GD iter. 1077/2999: loss=0.29562515804685796\n",
      "GD iter. 1078/2999: loss=0.2869516544295892\n",
      "GD iter. 1079/2999: loss=0.29510581779082573\n",
      "GD iter. 1080/2999: loss=0.2865034119505984\n",
      "GD iter. 1081/2999: loss=0.2945878495032586\n",
      "GD iter. 1082/2999: loss=0.2860562562937404\n",
      "GD iter. 1083/2999: loss=0.2940712503437789\n",
      "GD iter. 1084/2999: loss=0.2856101847095365\n",
      "GD iter. 1085/2999: loss=0.2935560174925604\n",
      "GD iter. 1086/2999: loss=0.2851651944648785\n",
      "GD iter. 1087/2999: loss=0.2930421481505201\n",
      "GD iter. 1088/2999: loss=0.28472128284324494\n",
      "GD iter. 1089/2999: loss=0.2925296395395479\n",
      "GD iter. 1090/2999: loss=0.28427844714492495\n",
      "GD iter. 1091/2999: loss=0.29201848890272913\n",
      "GD iter. 1092/2999: loss=0.28383668468721496\n",
      "GD iter. 1093/2999: loss=0.2915086935045551\n",
      "GD iter. 1094/2999: loss=0.28339599280465694\n",
      "GD iter. 1095/2999: loss=0.2910002506311724\n",
      "GD iter. 1096/2999: loss=0.28295636884926817\n",
      "GD iter. 1097/2999: loss=0.2904931575906188\n",
      "GD iter. 1098/2999: loss=0.2825178101907755\n",
      "GD iter. 1099/2999: loss=0.289987411713076\n",
      "GD iter. 1100/2999: loss=0.28208031421687135\n",
      "GD iter. 1101/2999: loss=0.289483010351146\n",
      "GD iter. 1102/2999: loss=0.28164387833344323\n",
      "GD iter. 1103/2999: loss=0.28897995088008094\n",
      "GD iter. 1104/2999: loss=0.2812084999648433\n",
      "GD iter. 1105/2999: loss=0.28847823069808354\n",
      "GD iter. 1106/2999: loss=0.2807741765541624\n",
      "GD iter. 1107/2999: loss=0.28797784722660014\n",
      "GD iter. 1108/2999: loss=0.28034090556348057\n",
      "GD iter. 1109/2999: loss=0.2874787979105815\n",
      "GD iter. 1110/2999: loss=0.2799086844741598\n",
      "GD iter. 1111/2999: loss=0.28698108021880986\n",
      "GD iter. 1112/2999: loss=0.27947751078712546\n",
      "GD iter. 1113/2999: loss=0.2864846916441958\n",
      "GD iter. 1114/2999: loss=0.2790473820231755\n",
      "GD iter. 1115/2999: loss=0.2859896297041179\n",
      "GD iter. 1116/2999: loss=0.27861829572325475\n",
      "GD iter. 1117/2999: loss=0.2854958919407177\n",
      "GD iter. 1118/2999: loss=0.2781902494487982\n",
      "GD iter. 1119/2999: loss=0.28500347592128\n",
      "GD iter. 1120/2999: loss=0.27776324078202397\n",
      "GD iter. 1121/2999: loss=0.2845123792385483\n",
      "GD iter. 1122/2999: loss=0.2773372673262828\n",
      "GD iter. 1123/2999: loss=0.28402259951110886\n",
      "GD iter. 1124/2999: loss=0.27691232670637667\n",
      "GD iter. 1125/2999: loss=0.2835341343837413\n",
      "GD iter. 1126/2999: loss=0.276488416568927\n",
      "GD iter. 1127/2999: loss=0.28304698152782265\n",
      "GD iter. 1128/2999: loss=0.2760655345827062\n",
      "GD iter. 1129/2999: loss=0.28256113864169247\n",
      "GD iter. 1130/2999: loss=0.27564367843902227\n",
      "GD iter. 1131/2999: loss=0.28207660345107594\n",
      "GD iter. 1132/2999: loss=0.2752228458520858\n",
      "GD iter. 1133/2999: loss=0.28159337370949356\n",
      "GD iter. 1134/2999: loss=0.2748030345594012\n",
      "GD iter. 1135/2999: loss=0.281111447198686\n",
      "GD iter. 1136/2999: loss=0.27438424232216013\n",
      "GD iter. 1137/2999: loss=0.2806308217290576\n",
      "GD iter. 1138/2999: loss=0.27396646692565774\n",
      "GD iter. 1139/2999: loss=0.2801514951401323\n",
      "GD iter. 1140/2999: loss=0.2735497061796961\n",
      "GD iter. 1141/2999: loss=0.27967346530100623\n",
      "GD iter. 1142/2999: loss=0.2731339579190231\n",
      "GD iter. 1143/2999: loss=0.2791967301108226\n",
      "GD iter. 1144/2999: loss=0.2727192200037791\n",
      "GD iter. 1145/2999: loss=0.2787212874992828\n",
      "GD iter. 1146/2999: loss=0.2723054903199499\n",
      "GD iter. 1147/2999: loss=0.27824713542714075\n",
      "GD iter. 1148/2999: loss=0.27189276677981894\n",
      "GD iter. 1149/2999: loss=0.27777427188670506\n",
      "GD iter. 1150/2999: loss=0.2714810473224603\n",
      "GD iter. 1151/2999: loss=0.27730269490238996\n",
      "GD iter. 1152/2999: loss=0.2710703299142327\n",
      "GD iter. 1153/2999: loss=0.276832402531264\n",
      "GD iter. 1154/2999: loss=0.27066061254927104\n",
      "GD iter. 1155/2999: loss=0.2763633928635943\n",
      "GD iter. 1156/2999: loss=0.27025189325002197\n",
      "GD iter. 1157/2999: loss=0.27589566402344634\n",
      "GD iter. 1158/2999: loss=0.2698441700677602\n",
      "GD iter. 1159/2999: loss=0.27542921416924865\n",
      "GD iter. 1160/2999: loss=0.26943744108315526\n",
      "GD iter. 1161/2999: loss=0.2749640414944374\n",
      "GD iter. 1162/2999: loss=0.2690317044068198\n",
      "GD iter. 1163/2999: loss=0.27450014422805247\n",
      "GD iter. 1164/2999: loss=0.26862695817987847\n",
      "GD iter. 1165/2999: loss=0.2740375206353757\n",
      "GD iter. 1166/2999: loss=0.26822320057458976\n",
      "GD iter. 1167/2999: loss=0.27357616901861787\n",
      "GD iter. 1168/2999: loss=0.2678204297949264\n",
      "GD iter. 1169/2999: loss=0.2731160877175688\n",
      "GD iter. 1170/2999: loss=0.26741864407721133\n",
      "GD iter. 1171/2999: loss=0.27265727511029825\n",
      "GD iter. 1172/2999: loss=0.2670178416907668\n",
      "GD iter. 1173/2999: loss=0.27219972961387523\n",
      "GD iter. 1174/2999: loss=0.2666180209385602\n",
      "GD iter. 1175/2999: loss=0.2717434496850877\n",
      "GD iter. 1176/2999: loss=0.26621918015788737\n",
      "GD iter. 1177/2999: loss=0.27128843382119605\n",
      "GD iter. 1178/2999: loss=0.26582131772106954\n",
      "GD iter. 1179/2999: loss=0.27083468056070403\n",
      "GD iter. 1180/2999: loss=0.26542443203616206\n",
      "GD iter. 1181/2999: loss=0.270382188484146\n",
      "GD iter. 1182/2999: loss=0.26502852154768236\n",
      "GD iter. 1183/2999: loss=0.26993095621488644\n",
      "GD iter. 1184/2999: loss=0.26463358473737003\n",
      "GD iter. 1185/2999: loss=0.269480982419962\n",
      "GD iter. 1186/2999: loss=0.2642396201249518\n",
      "GD iter. 1187/2999: loss=0.269032265810924\n",
      "GD iter. 1188/2999: loss=0.26384662626893485\n",
      "GD iter. 1189/2999: loss=0.2685848051447114\n",
      "GD iter. 1190/2999: loss=0.2634546017674226\n",
      "GD iter. 1191/2999: loss=0.2681385992245502\n",
      "GD iter. 1192/2999: loss=0.26306354525894465\n",
      "GD iter. 1193/2999: loss=0.26769364690086495\n",
      "GD iter. 1194/2999: loss=0.26267345542330717\n",
      "GD iter. 1195/2999: loss=0.26724994707221056\n",
      "GD iter. 1196/2999: loss=0.26228433098248133\n",
      "GD iter. 1197/2999: loss=0.2668074986862494\n",
      "GD iter. 1198/2999: loss=0.26189617070149745\n",
      "GD iter. 1199/2999: loss=0.26636630074072704\n",
      "GD iter. 1200/2999: loss=0.2615089733893673\n",
      "GD iter. 1201/2999: loss=0.2659263522844832\n",
      "GD iter. 1202/2999: loss=0.26112273790004586\n",
      "GD iter. 1203/2999: loss=0.26548765241850214\n",
      "GD iter. 1204/2999: loss=0.2607374631333833\n",
      "GD iter. 1205/2999: loss=0.265050200296949\n",
      "GD iter. 1206/2999: loss=0.2603531480361425\n",
      "GD iter. 1207/2999: loss=0.2646139951282795\n",
      "GD iter. 1208/2999: loss=0.25996979160301137\n",
      "GD iter. 1209/2999: loss=0.26417903617634103\n",
      "GD iter. 1210/2999: loss=0.25958739287765464\n",
      "GD iter. 1211/2999: loss=0.2637453227615097\n",
      "GD iter. 1212/2999: loss=0.2592059509537979\n",
      "GD iter. 1213/2999: loss=0.2633128542618747\n",
      "GD iter. 1214/2999: loss=0.2588254649763224\n",
      "GD iter. 1215/2999: loss=0.2628816301144121\n",
      "GD iter. 1216/2999: loss=0.2584459341424009\n",
      "GD iter. 1217/2999: loss=0.26245164981622104\n",
      "GD iter. 1218/2999: loss=0.25806735770266626\n",
      "GD iter. 1219/2999: loss=0.26202291292577884\n",
      "GD iter. 1220/2999: loss=0.25768973496239544\n",
      "GD iter. 1221/2999: loss=0.2615954190642176\n",
      "GD iter. 1222/2999: loss=0.2573130652827342\n",
      "GD iter. 1223/2999: loss=0.2611691679166411\n",
      "GD iter. 1224/2999: loss=0.2569373480819439\n",
      "GD iter. 1225/2999: loss=0.26074415923346017\n",
      "GD iter. 1226/2999: loss=0.25656258283669237\n",
      "GD iter. 1227/2999: loss=0.26032039283177555\n",
      "GD iter. 1228/2999: loss=0.2561887690833681\n",
      "GD iter. 1229/2999: loss=0.25989786859678665\n",
      "GD iter. 1230/2999: loss=0.25581590641942187\n",
      "GD iter. 1231/2999: loss=0.2594765864832135\n",
      "GD iter. 1232/2999: loss=0.2554439945047546\n",
      "GD iter. 1233/2999: loss=0.25905654651678345\n",
      "GD iter. 1234/2999: loss=0.2550730330631326\n",
      "GD iter. 1235/2999: loss=0.2586377487957294\n",
      "GD iter. 1236/2999: loss=0.25470302188363864\n",
      "GD iter. 1237/2999: loss=0.2582201934923309\n",
      "GD iter. 1238/2999: loss=0.2543339608221505\n",
      "GD iter. 1239/2999: loss=0.2578038808544742\n",
      "GD iter. 1240/2999: loss=0.25396584980287396\n",
      "GD iter. 1241/2999: loss=0.25738881120727536\n",
      "GD iter. 1242/2999: loss=0.25359868881989783\n",
      "GD iter. 1243/2999: loss=0.25697498495471466\n",
      "GD iter. 1244/2999: loss=0.2532324779387863\n",
      "GD iter. 1245/2999: loss=0.2565624025813123\n",
      "GD iter. 1246/2999: loss=0.2528672172982168\n",
      "GD iter. 1247/2999: loss=0.256151064653845\n",
      "GD iter. 1248/2999: loss=0.25250290711166085\n",
      "GD iter. 1249/2999: loss=0.25574097182310357\n",
      "GD iter. 1250/2999: loss=0.2521395476690826\n",
      "GD iter. 1251/2999: loss=0.2553321248256658\n",
      "GD iter. 1252/2999: loss=0.2517771393387037\n",
      "GD iter. 1253/2999: loss=0.2549245244857338\n",
      "GD iter. 1254/2999: loss=0.25141568256879276\n",
      "GD iter. 1255/2999: loss=0.25451817171699265\n",
      "GD iter. 1256/2999: loss=0.2510551778895037\n",
      "GD iter. 1257/2999: loss=0.254113067524513\n",
      "GD iter. 1258/2999: loss=0.250695625914751\n",
      "GD iter. 1259/2999: loss=0.25370921300669014\n",
      "GD iter. 1260/2999: loss=0.2503370273441285\n",
      "GD iter. 1261/2999: loss=0.2533066093572192\n",
      "GD iter. 1262/2999: loss=0.2499793829648828\n",
      "GD iter. 1263/2999: loss=0.25290525786712426\n",
      "GD iter. 1264/2999: loss=0.24962269365391015\n",
      "GD iter. 1265/2999: loss=0.25250515992680483\n",
      "GD iter. 1266/2999: loss=0.24926696037981977\n",
      "GD iter. 1267/2999: loss=0.252106317028144\n",
      "GD iter. 1268/2999: loss=0.24891218420501668\n",
      "GD iter. 1269/2999: loss=0.25170873076662553\n",
      "GD iter. 1270/2999: loss=0.24855836628785818\n",
      "GD iter. 1271/2999: loss=0.25131240284353196\n",
      "GD iter. 1272/2999: loss=0.24820550788483778\n",
      "GD iter. 1273/2999: loss=0.25091733506815306\n",
      "GD iter. 1274/2999: loss=0.24785361035282064\n",
      "GD iter. 1275/2999: loss=0.2505235293600411\n",
      "GD iter. 1276/2999: loss=0.247502675151324\n",
      "GD iter. 1277/2999: loss=0.25013098775130554\n",
      "GD iter. 1278/2999: loss=0.24715270384484825\n",
      "GD iter. 1279/2999: loss=0.24973971238895362\n",
      "GD iter. 1280/2999: loss=0.24680369810525507\n",
      "GD iter. 1281/2999: loss=0.24934970553726632\n",
      "GD iter. 1282/2999: loss=0.24645565971418978\n",
      "GD iter. 1283/2999: loss=0.2489609695802113\n",
      "GD iter. 1284/2999: loss=0.2461085905655539\n",
      "GD iter. 1285/2999: loss=0.24857350702389688\n",
      "GD iter. 1286/2999: loss=0.24576249266802408\n",
      "GD iter. 1287/2999: loss=0.2481873204990634\n",
      "GD iter. 1288/2999: loss=0.24541736814762\n",
      "GD iter. 1289/2999: loss=0.2478024127636137\n",
      "GD iter. 1290/2999: loss=0.24507321925032316\n",
      "GD iter. 1291/2999: loss=0.24741878670518302\n",
      "GD iter. 1292/2999: loss=0.24473004834473897\n",
      "GD iter. 1293/2999: loss=0.2470364453437412\n",
      "GD iter. 1294/2999: loss=0.24438785792480863\n",
      "GD iter. 1295/2999: loss=0.24665539183423402\n",
      "GD iter. 1296/2999: loss=0.24404665061256675\n",
      "GD iter. 1297/2999: loss=0.24627562946925458\n",
      "GD iter. 1298/2999: loss=0.24370642916094803\n",
      "GD iter. 1299/2999: loss=0.245897161681753\n",
      "GD iter. 1300/2999: loss=0.24336719645663848\n",
      "GD iter. 1301/2999: loss=0.24551999204777572\n",
      "GD iter. 1302/2999: loss=0.24302895552297651\n",
      "GD iter. 1303/2999: loss=0.24514412428923993\n",
      "GD iter. 1304/2999: loss=0.24269170952289004\n",
      "GD iter. 1305/2999: loss=0.24476956227672694\n",
      "GD iter. 1306/2999: loss=0.24235546176188802\n",
      "GD iter. 1307/2999: loss=0.2443963100323156\n",
      "GD iter. 1308/2999: loss=0.2420202156910896\n",
      "GD iter. 1309/2999: loss=0.24402437173243513\n",
      "GD iter. 1310/2999: loss=0.2416859749102965\n",
      "GD iter. 1311/2999: loss=0.24365375171074086\n",
      "GD iter. 1312/2999: loss=0.2413527431711003\n",
      "GD iter. 1313/2999: loss=0.24328445446100805\n",
      "GD iter. 1314/2999: loss=0.24102052438004143\n",
      "GD iter. 1315/2999: loss=0.24291648464005808\n",
      "GD iter. 1316/2999: loss=0.2406893226017888\n",
      "GD iter. 1317/2999: loss=0.24254984707067861\n",
      "GD iter. 1318/2999: loss=0.24035914206237094\n",
      "GD iter. 1319/2999: loss=0.24218454674457984\n",
      "GD iter. 1320/2999: loss=0.2400299871524249\n",
      "GD iter. 1321/2999: loss=0.2418205888253407\n",
      "GD iter. 1322/2999: loss=0.2397018624304902\n",
      "GD iter. 1323/2999: loss=0.24145797865137617\n",
      "GD iter. 1324/2999: loss=0.23937477262631554\n",
      "GD iter. 1325/2999: loss=0.24109672173889402\n",
      "GD iter. 1326/2999: loss=0.23904872264420635\n",
      "GD iter. 1327/2999: loss=0.2407368237848628\n",
      "GD iter. 1328/2999: loss=0.2387237175663757\n",
      "GD iter. 1329/2999: loss=0.24037829066995434\n",
      "GD iter. 1330/2999: loss=0.23839976265633603\n",
      "GD iter. 1331/2999: loss=0.24002112846150328\n",
      "GD iter. 1332/2999: loss=0.23807686336228662\n",
      "GD iter. 1333/2999: loss=0.23966534341642823\n",
      "GD iter. 1334/2999: loss=0.23775502532051584\n",
      "GD iter. 1335/2999: loss=0.23931094198413133\n",
      "GD iter. 1336/2999: loss=0.23743425435881518\n",
      "GD iter. 1337/2999: loss=0.23895793080938424\n",
      "GD iter. 1338/2999: loss=0.23711455649988894\n",
      "GD iter. 1339/2999: loss=0.23860631673517158\n",
      "GD iter. 1340/2999: loss=0.2367959379647596\n",
      "GD iter. 1341/2999: loss=0.2382561068054935\n",
      "GD iter. 1342/2999: loss=0.23647840517616803\n",
      "GD iter. 1343/2999: loss=0.2379073082681253\n",
      "GD iter. 1344/2999: loss=0.23616196476195306\n",
      "GD iter. 1345/2999: loss=0.2375599285773203\n",
      "GD iter. 1346/2999: loss=0.23584662355840733\n",
      "GD iter. 1347/2999: loss=0.23721397539644387\n",
      "GD iter. 1348/2999: loss=0.23553238861361195\n",
      "GD iter. 1349/2999: loss=0.2368694566005504\n",
      "GD iter. 1350/2999: loss=0.23521926719073102\n",
      "GD iter. 1351/2999: loss=0.23652638027886982\n",
      "GD iter. 1352/2999: loss=0.23490726677125215\n",
      "GD iter. 1353/2999: loss=0.23618475473720152\n",
      "GD iter. 1354/2999: loss=0.23459639505819063\n",
      "GD iter. 1355/2999: loss=0.23584458850021806\n",
      "GD iter. 1356/2999: loss=0.23428665997921183\n",
      "GD iter. 1357/2999: loss=0.23550589031364666\n",
      "GD iter. 1358/2999: loss=0.23397806968970947\n",
      "GD iter. 1359/2999: loss=0.23516866914635043\n",
      "GD iter. 1360/2999: loss=0.23367063257576967\n",
      "GD iter. 1361/2999: loss=0.2348329341922496\n",
      "GD iter. 1362/2999: loss=0.2333643572570655\n",
      "GD iter. 1363/2999: loss=0.23449869487211147\n",
      "GD iter. 1364/2999: loss=0.23305925258964688\n",
      "GD iter. 1365/2999: loss=0.2341659608351913\n",
      "GD iter. 1366/2999: loss=0.23275532766859694\n",
      "GD iter. 1367/2999: loss=0.23383474196067788\n",
      "GD iter. 1368/2999: loss=0.23245259183057868\n",
      "GD iter. 1369/2999: loss=0.2335050483589702\n",
      "GD iter. 1370/2999: loss=0.23215105465622687\n",
      "GD iter. 1371/2999: loss=0.23317689037274353\n",
      "GD iter. 1372/2999: loss=0.23185072597239464\n",
      "GD iter. 1373/2999: loss=0.23285027857780863\n",
      "GD iter. 1374/2999: loss=0.23155161585422007\n",
      "GD iter. 1375/2999: loss=0.2325252237837265\n",
      "GD iter. 1376/2999: loss=0.23125373462700977\n",
      "GD iter. 1377/2999: loss=0.23220173703418062\n",
      "GD iter. 1378/2999: loss=0.23095709286791613\n",
      "GD iter. 1379/2999: loss=0.23187982960707174\n",
      "GD iter. 1380/2999: loss=0.2306617014073987\n",
      "GD iter. 1381/2999: loss=0.23155951301433583\n",
      "GD iter. 1382/2999: loss=0.2303675713304418\n",
      "GD iter. 1383/2999: loss=0.23124079900144867\n",
      "GD iter. 1384/2999: loss=0.23007471397752283\n",
      "GD iter. 1385/2999: loss=0.2309236995466131\n",
      "GD iter. 1386/2999: loss=0.2297831409452906\n",
      "GD iter. 1387/2999: loss=0.23060822685958463\n",
      "GD iter. 1388/2999: loss=0.2294928640869567\n",
      "GD iter. 1389/2999: loss=0.23029439338014274\n",
      "GD iter. 1390/2999: loss=0.22920389551236814\n",
      "GD iter. 1391/2999: loss=0.22998221177617006\n",
      "GD iter. 1392/2999: loss=0.22891624758773055\n",
      "GD iter. 1393/2999: loss=0.22967169494131137\n",
      "GD iter. 1394/2999: loss=0.22862993293497422\n",
      "GD iter. 1395/2999: loss=0.22936285599220294\n",
      "GD iter. 1396/2999: loss=0.2283449644307309\n",
      "GD iter. 1397/2999: loss=0.22905570826523827\n",
      "GD iter. 1398/2999: loss=0.22806135520489623\n",
      "GD iter. 1399/2999: loss=0.22875026531285075\n",
      "GD iter. 1400/2999: loss=0.2277791186387524\n",
      "GD iter. 1401/2999: loss=0.22844654089928149\n",
      "GD iter. 1402/2999: loss=0.2274982683626339\n",
      "GD iter. 1403/2999: loss=0.22814454899581843\n",
      "GD iter. 1404/2999: loss=0.22721881825309345\n",
      "GD iter. 1405/2999: loss=0.2278443037754644\n",
      "GD iter. 1406/2999: loss=0.22694078242955448\n",
      "GD iter. 1407/2999: loss=0.227545819607024\n",
      "GD iter. 1408/2999: loss=0.22666417525041063\n",
      "GD iter. 1409/2999: loss=0.22724911104856255\n",
      "GD iter. 1410/2999: loss=0.22638901130855896\n",
      "GD iter. 1411/2999: loss=0.22695419284023827\n",
      "GD iter. 1412/2999: loss=0.22611530542631583\n",
      "GD iter. 1413/2999: loss=0.2266610798964456\n",
      "GD iter. 1414/2999: loss=0.2258430726497046\n",
      "GD iter. 1415/2999: loss=0.2263697872972695\n",
      "GD iter. 1416/2999: loss=0.22557232824207565\n",
      "GD iter. 1417/2999: loss=0.2260803302792124\n",
      "GD iter. 1418/2999: loss=0.22530308767702448\n",
      "GD iter. 1419/2999: loss=0.2257927242251563\n",
      "GD iter. 1420/2999: loss=0.22503536663059004\n",
      "GD iter. 1421/2999: loss=0.2255069846535597\n",
      "GD iter. 1422/2999: loss=0.22476918097268636\n",
      "GD iter. 1423/2999: loss=0.2252231272068278\n",
      "GD iter. 1424/2999: loss=0.22450454675775208\n",
      "GD iter. 1425/2999: loss=0.22494116763886038\n",
      "GD iter. 1426/2999: loss=0.22424148021456503\n",
      "GD iter. 1427/2999: loss=0.22466112180172207\n",
      "GD iter. 1428/2999: loss=0.22397999773522057\n",
      "GD iter. 1429/2999: loss=0.22438300563144042\n",
      "GD iter. 1430/2999: loss=0.22372011586322071\n",
      "GD iter. 1431/2999: loss=0.2241068351328866\n",
      "GD iter. 1432/2999: loss=0.22346185128064536\n",
      "GD iter. 1433/2999: loss=0.22383262636371196\n",
      "GD iter. 1434/2999: loss=0.2232052207943937\n",
      "GD iter. 1435/2999: loss=0.22356039541734052\n",
      "GD iter. 1436/2999: loss=0.2229502413214466\n",
      "GD iter. 1437/2999: loss=0.22329015840497354\n",
      "GD iter. 1438/2999: loss=0.22269692987313786\n",
      "GD iter. 1439/2999: loss=0.22302193143660448\n",
      "GD iter. 1440/2999: loss=0.22244530353840214\n",
      "GD iter. 1441/2999: loss=0.22275573060101575\n",
      "GD iter. 1442/2999: loss=0.22219537946598428\n",
      "GD iter. 1443/2999: loss=0.22249157194475658\n",
      "GD iter. 1444/2999: loss=0.22194717484556958\n",
      "GD iter. 1445/2999: loss=0.2222294714500721\n",
      "GD iter. 1446/2999: loss=0.22170070688783805\n",
      "GD iter. 1447/2999: loss=0.22196944501179394\n",
      "GD iter. 1448/2999: loss=0.22145599280341247\n",
      "GD iter. 1449/2999: loss=0.22171150841317375\n",
      "GD iter. 1450/2999: loss=0.22121304978068165\n",
      "GD iter. 1451/2999: loss=0.2214556773006569\n",
      "GD iter. 1452/2999: loss=0.22097189496250125\n",
      "GD iter. 1453/2999: loss=0.22120196715760843\n",
      "GD iter. 1454/2999: loss=0.22073254542175763\n",
      "GD iter. 1455/2999: loss=0.22095039327699106\n",
      "GD iter. 1456/2999: loss=0.22049501813578057\n",
      "GD iter. 1457/2999: loss=0.22070097073299247\n",
      "GD iter. 1458/2999: loss=0.22025932995962505\n",
      "GD iter. 1459/2999: loss=0.2204537143516418\n",
      "GD iter. 1460/2999: loss=0.22002549759820444\n",
      "GD iter. 1461/2999: loss=0.22020863868040683\n",
      "GD iter. 1462/2999: loss=0.21979353757730352\n",
      "GD iter. 1463/2999: loss=0.2199657579568218\n",
      "GD iter. 1464/2999: loss=0.21956346621346953\n",
      "GD iter. 1465/2999: loss=0.219725086076151\n",
      "GD iter. 1466/2999: loss=0.21933529958281134\n",
      "GD iter. 1467/2999: loss=0.21948663655814696\n",
      "GD iter. 1468/2999: loss=0.21910905348872478\n",
      "GD iter. 1469/2999: loss=0.21925042251292554\n",
      "GD iter. 1470/2999: loss=0.218884743428582\n",
      "GD iter. 1471/2999: loss=0.21901645660602334\n",
      "GD iter. 1472/2999: loss=0.21866238455941853\n",
      "GD iter. 1473/2999: loss=0.21878475102267855\n",
      "GD iter. 1474/2999: loss=0.2184419916626639\n",
      "GD iter. 1475/2999: loss=0.2185553174314051\n",
      "GD iter. 1476/2999: loss=0.2182235791079669\n",
      "GD iter. 1477/2999: loss=0.21832816694692803\n",
      "GD iter. 1478/2999: loss=0.21800716081617916\n",
      "GD iter. 1479/2999: loss=0.21810331009255685\n",
      "GD iter. 1480/2999: loss=0.2177927502215573\n",
      "GD iter. 1481/2999: loss=0.21788075676207722\n",
      "GD iter. 1482/2999: loss=0.21758036023326663\n",
      "GD iter. 1483/2999: loss=0.21766051618126062\n",
      "GD iter. 1484/2999: loss=0.2173700031962611\n",
      "GD iter. 1485/2999: loss=0.21744259686908132\n",
      "GD iter. 1486/2999: loss=0.21716169085163886\n",
      "GD iter. 1487/2999: loss=0.21722700659875763\n",
      "GD iter. 1488/2999: loss=0.21695543429657108\n",
      "GD iter. 1489/2999: loss=0.21701375235872788\n",
      "GD iter. 1490/2999: loss=0.21675124394391002\n",
      "GD iter. 1491/2999: loss=0.21680284031368344\n",
      "GD iter. 1492/2999: loss=0.21654912948159485\n",
      "GD iter. 1493/2999: loss=0.21659427576578882\n",
      "GD iter. 1494/2999: loss=0.21634909983198708\n",
      "GD iter. 1495/2999: loss=0.21638806311623326\n",
      "GD iter. 1496/2999: loss=0.21615116311125718\n",
      "GD iter. 1497/2999: loss=0.2161842058272467\n",
      "GD iter. 1498/2999: loss=0.2159553265889768\n",
      "GD iter. 1499/2999: loss=0.21598270638474196\n",
      "GD iter. 1500/2999: loss=0.21576159664806413\n",
      "GD iter. 1501/2999: loss=0.21578356626174075\n",
      "GD iter. 1502/2999: loss=0.2155699787452344\n",
      "GD iter. 1503/2999: loss=0.2155867858827397\n",
      "GD iter. 1504/2999: loss=0.2153804773721264\n",
      "GD iter. 1505/2999: loss=0.21539236458919075\n",
      "GD iter. 1506/2999: loss=0.21519309601727904\n",
      "GD iter. 1507/2999: loss=0.2152003006062734\n",
      "GD iter. 1508/2999: loss=0.21500783712912772\n",
      "GD iter. 1509/2999: loss=0.21501059101112752\n",
      "GD iter. 1510/2999: loss=0.21482470208021387\n",
      "GD iter. 1511/2999: loss=0.2148232317027364\n",
      "GD iter. 1512/2999: loss=0.21464369113278736\n",
      "GD iter. 1513/2999: loss=0.21463821737363586\n",
      "GD iter. 1514/2999: loss=0.21446480340599958\n",
      "GD iter. 1515/2999: loss=0.21445554148363782\n",
      "GD iter. 1516/2999: loss=0.21428803684487832\n",
      "GD iter. 1517/2999: loss=0.21427519623575225\n",
      "GD iter. 1518/2999: loss=0.214113388191281\n",
      "GD iter. 1519/2999: loss=0.21409717255448565\n",
      "GD iter. 1520/2999: loss=0.21394085295702497\n",
      "GD iter. 1521/2999: loss=0.21392146006670348\n",
      "GD iter. 1522/2999: loss=0.21377042539938848\n",
      "GD iter. 1523/2999: loss=0.21374804708522752\n",
      "GD iter. 1524/2999: loss=0.21360209849917458\n",
      "GD iter. 1525/2999: loss=0.21357692059534267\n",
      "GD iter. 1526/2999: loss=0.21343586394153344\n",
      "GD iter. 1527/2999: loss=0.21340806624437905\n",
      "GD iter. 1528/2999: loss=0.21327171209972062\n",
      "GD iter. 1529/2999: loss=0.21324146833452298\n",
      "GD iter. 1530/2999: loss=0.2131096320219752\n",
      "GD iter. 1531/2999: loss=0.21307710981901068\n",
      "GD iter. 1532/2999: loss=0.21294961142168548\n",
      "GD iter. 1533/2999: loss=0.21291497230183484\n",
      "GD iter. 1534/2999: loss=0.21279163667100223\n",
      "GD iter. 1535/2999: loss=0.21275503604109555\n",
      "GD iter. 1536/2999: loss=0.21263569279804723\n",
      "GD iter. 1537/2999: loss=0.21259727995609906\n",
      "GD iter. 1538/2999: loss=0.21248176348785533\n",
      "GD iter. 1539/2999: loss=0.21244168163830615\n",
      "GD iter. 1540/2999: loss=0.21232983108716633\n",
      "GD iter. 1541/2999: loss=0.21228821736620435\n",
      "GD iter. 1542/2999: loss=0.2121798766131758\n",
      "GD iter. 1543/2999: loss=0.21213686212416585\n",
      "GD iter. 1544/2999: loss=0.21203187976632618\n",
      "GD iter. 1545/2999: loss=0.21198758962533187\n",
      "GD iter. 1546/2999: loss=0.21188581894720773\n",
      "GD iter. 1547/2999: loss=0.21184037233854408\n",
      "GD iter. 1548/2999: loss=0.21174167127761476\n",
      "GD iter. 1549/2999: loss=0.2116951815193209\n",
      "GD iter. 1550/2999: loss=0.2115994126257815\n",
      "GD iter. 1551/2999: loss=0.211551987244856\n",
      "GD iter. 1552/2999: loss=0.21145901763579628\n",
      "GD iter. 1553/2999: loss=0.21141075845299148\n",
      "GD iter. 1554/2999: loss=0.21132045976117425\n",
      "GD iter. 1555/2999: loss=0.21127146298509517\n",
      "GD iter. 1556/2999: loss=0.21118371130253658\n",
      "GD iter. 1557/2999: loss=0.21113406763274548\n",
      "GD iter. 1558/2999: loss=0.21104874344932534\n",
      "GD iter. 1559/2999: loss=0.2109985381881073\n",
      "GD iter. 1560/2999: loss=0.2109155263254525\n",
      "GD iter. 1561/2999: loss=0.21086483949785131\n",
      "GD iter. 1562/2999: loss=0.21078402903875895\n",
      "GD iter. 1563/2999: loss=0.21073293552045227\n",
      "GD iter. 1564/2999: loss=0.21065421973412934\n",
      "GD iter. 1565/2999: loss=0.21060278938667004\n",
      "GD iter. 1566/2999: loss=0.21052606565008916\n",
      "GD iter. 1567/2999: loss=0.2104743634630054\n",
      "GD iter. 1568/2999: loss=0.21039953317867838\n",
      "GD iter. 1569/2999: loss=0.21034761941788555\n",
      "GD iter. 1570/2999: loss=0.2102745879283767\n",
      "GD iter. 1571/2999: loss=0.21022251829032965\n",
      "GD iter. 1572/2999: loss=0.21015119478983085\n",
      "GD iter. 1573/2999: loss=0.21009902056081073\n",
      "GD iter. 1574/2999: loss=0.21002931800411162\n",
      "GD iter. 1575/2999: loss=0.2099770862240259\n",
      "GD iter. 1576/2999: loss=0.20990892123321112\n",
      "GD iter. 1577/2999: loss=0.20985667486325804\n",
      "GD iter. 1578/2999: loss=0.20978996763246874\n",
      "GD iter. 1579/2999: loss=0.2097377457260094\n",
      "GD iter. 1580/2999: loss=0.20967241992460015\n",
      "GD iter. 1581/2999: loss=0.2096202578005683\n",
      "GD iter. 1582/2999: loss=0.20955624047499208\n",
      "GD iter. 1583/2999: loss=0.20950416989316614\n",
      "GD iter. 1584/2999: loss=0.20944139136790849\n",
      "GD iter. 1585/2999: loss=0.20938944070537044\n",
      "GD iter. 1586/2999: loss=0.20932783448325107\n",
      "GD iter. 1587/2999: loss=0.2092760289113608\n",
      "GD iter. 1588/2999: loss=0.2092155315735073\n",
      "GD iter. 1589/2999: loss=0.2091638932347291\n",
      "GD iter. 1590/2999: loss=0.20910444434051811\n",
      "GD iter. 1591/2999: loss=0.20905299252445084\n",
      "GD iter. 1592/2999: loss=0.20899453451169667\n",
      "GD iter. 1593/2999: loss=0.20894328582967417\n",
      "GD iter. 1594/2999: loss=0.20888576391533062\n",
      "GD iter. 1595/2999: loss=0.20883473247298534\n",
      "GD iter. 1596/2999: loss=0.20877809455461255\n",
      "GD iter. 1597/2999: loss=0.20872729212181648\n",
      "GD iter. 1598/2999: loss=0.2086714886800433\n",
      "GD iter. 1599/2999: loss=0.20862092485767406\n",
      "GD iter. 1600/2999: loss=0.20856590885987183\n",
      "GD iter. 1601/2999: loss=0.20851559124288396\n",
      "GD iter. 1602/2999: loss=0.20846131804824886\n",
      "GD iter. 1603/2999: loss=0.20841125238456454\n",
      "GD iter. 1604/2999: loss=0.20835767965078383\n",
      "GD iter. 1605/2999: loss=0.20830786999556056\n",
      "GD iter. 1606/2999: loss=0.20825495758722273\n",
      "GD iter. 1607/2999: loss=0.20820540645209176\n",
      "GD iter. 1608/2999: loss=0.2081531163509785\n",
      "GD iter. 1609/2999: loss=0.20810382484789497\n",
      "GD iter. 1610/2999: loss=0.20805212106527673\n",
      "GD iter. 1611/2999: loss=0.20800308904466436\n",
      "GD iter. 1612/2999: loss=0.20795193753569968\n",
      "GD iter. 1613/2999: loss=0.20790316371861908\n",
      "GD iter. 1614/2999: loss=0.20785253229894354\n",
      "GD iter. 1615/2999: loss=0.2078040144030586\n",
      "GD iter. 1616/2999: loss=0.20775387266762943\n",
      "GD iter. 1617/2999: loss=0.2077056075267926\n",
      "GD iter. 1618/2999: loss=0.20765592677104114\n",
      "GD iter. 1619/2999: loss=0.207607910448363\n",
      "GD iter. 1620/2999: loss=0.20755866359168954\n",
      "GD iter. 1621/2999: loss=0.20751089148600252\n",
      "GD iter. 1622/2999: loss=0.20746205299763718\n",
      "GD iter. 1623/2999: loss=0.20741451994330806\n",
      "GD iter. 1624/2999: loss=0.20736606577054523\n",
      "GD iter. 1625/2999: loss=0.20731876613062955\n",
      "GD iter. 1626/2999: loss=0.20727067362943488\n",
      "GD iter. 1627/2999: loss=0.20722360138221\n",
      "GD iter. 1628/2999: loss=0.20717584925018576\n",
      "GD iter. 1629/2999: loss=0.20712899806913326\n",
      "GD iter. 1630/2999: loss=0.2070815662808215\n",
      "GD iter. 1631/2999: loss=0.2070349296081676\n",
      "GD iter. 1632/2999: loss=0.2069877993526599\n",
      "GD iter. 1633/2999: loss=0.20694137046661185\n",
      "GD iter. 1634/2999: loss=0.20689452408742995\n",
      "GD iter. 1635/2999: loss=0.20684829616327888\n",
      "GD iter. 1636/2999: loss=0.20680171710048365\n",
      "GD iter. 1637/2999: loss=0.20675568326576818\n",
      "GD iter. 1638/2999: loss=0.2067093560002513\n",
      "GD iter. 1639/2999: loss=0.206663509384201\n",
      "GD iter. 1640/2999: loss=0.20661741938410816\n",
      "GD iter. 1641/2999: loss=0.2065717531616055\n",
      "GD iter. 1642/2999: loss=0.20652588683083933\n",
      "GD iter. 1643/2999: loss=0.20648039426115572\n",
      "GD iter. 1644/2999: loss=0.20643473888990463\n",
      "GD iter. 1645/2999: loss=0.20638941335048056\n",
      "GD iter. 1646/2999: loss=0.20634395706771802\n",
      "GD iter. 1647/2999: loss=0.20629879208326632\n",
      "GD iter. 1648/2999: loss=0.20625352381116577\n",
      "GD iter. 1649/2999: loss=0.2062085130783878\n",
      "GD iter. 1650/2999: loss=0.2061634224885971\n",
      "GD iter. 1651/2999: loss=0.2061185598968033\n",
      "GD iter. 1652/2999: loss=0.20607363736852613\n",
      "GD iter. 1653/2999: loss=0.2060289170164556\n",
      "GD iter. 1654/2999: loss=0.20598415359628575\n",
      "GD iter. 1655/2999: loss=0.20593956980542136\n",
      "GD iter. 1656/2999: loss=0.20589495716887757\n",
      "GD iter. 1657/2999: loss=0.20585050449354675\n",
      "GD iter. 1658/2999: loss=0.20580603490825886\n",
      "GD iter. 1659/2999: loss=0.20576170814280728\n",
      "GD iter. 1660/2999: loss=0.20571737443330435\n",
      "GD iter. 1661/2999: loss=0.20567316861662327\n",
      "GD iter. 1662/2999: loss=0.20562896413067683\n",
      "GD iter. 1663/2999: loss=0.20558487454835558\n",
      "GD iter. 1664/2999: loss=0.20554079312483287\n",
      "GD iter. 1665/2999: loss=0.20549681530919697\n",
      "GD iter. 1666/2999: loss=0.20545285124738058\n",
      "GD iter. 1667/2999: loss=0.2054089809756671\n",
      "GD iter. 1668/2999: loss=0.2053651290059977\n",
      "GD iter. 1669/2999: loss=0.20532136229690526\n",
      "GD iter. 1670/2999: loss=0.20527761755310825\n",
      "GD iter. 1671/2999: loss=0.20523395066194533\n",
      "GD iter. 1672/2999: loss=0.2051903086545006\n",
      "GD iter. 1673/2999: loss=0.20514673806714503\n",
      "GD iter. 1674/2999: loss=0.20510319465806096\n",
      "GD iter. 1675/2999: loss=0.20505971708392512\n",
      "GD iter. 1676/2999: loss=0.2050162684627803\n",
      "GD iter. 1677/2999: loss=0.20497288082696521\n",
      "GD iter. 1678/2999: loss=0.2049295234881794\n",
      "GD iter. 1679/2999: loss=0.20488622292298558\n",
      "GD iter. 1680/2999: loss=0.20484295364428434\n",
      "GD iter. 1681/2999: loss=0.20479973748023092\n",
      "GD iter. 1682/2999: loss=0.20475655330226702\n",
      "GD iter. 1683/2999: loss=0.20471341905875948\n",
      "GD iter. 1684/2999: loss=0.20467031726585572\n",
      "GD iter. 1685/2999: loss=0.2046272626416259\n",
      "GD iter. 1686/2999: loss=0.20458424074360318\n",
      "GD iter. 1687/2999: loss=0.20454126360703329\n",
      "GD iter. 1688/2999: loss=0.2044983193220876\n",
      "GD iter. 1689/2999: loss=0.20445541770151662\n",
      "GD iter. 1690/2999: loss=0.2044125489401105\n",
      "GD iter. 1691/2999: loss=0.20436972101420747\n",
      "GD iter. 1692/2999: loss=0.20432692586393844\n",
      "GD iter. 1693/2999: loss=0.20428416995221943\n",
      "GD iter. 1694/2999: loss=0.20424144666363006\n",
      "GD iter. 1695/2999: loss=0.2041987612171793\n",
      "GD iter. 1696/2999: loss=0.20415610819047317\n",
      "GD iter. 1697/2999: loss=0.2041134917829226\n",
      "GD iter. 1698/2999: loss=0.20407090755554866\n",
      "GD iter. 1699/2999: loss=0.2040283588743597\n",
      "GD iter. 1700/2999: loss=0.20398584210942985\n",
      "GD iter. 1701/2999: loss=0.20394335994751128\n",
      "GD iter. 1702/2999: loss=0.20390090942301403\n",
      "GD iter. 1703/2999: loss=0.2038584926707033\n",
      "GD iter. 1704/2999: loss=0.20381610726947771\n",
      "GD iter. 1705/2999: loss=0.20377375490690544\n",
      "GD iter. 1706/2999: loss=0.20373143360733914\n",
      "GD iter. 1707/2999: loss=0.20368914469718977\n",
      "GD iter. 1708/2999: loss=0.2036468865646049\n",
      "GD iter. 1709/2999: loss=0.20360466024528184\n",
      "GD iter. 1710/2999: loss=0.20356246442397352\n",
      "GD iter. 1711/2999: loss=0.20352029990317053\n",
      "GD iter. 1712/2999: loss=0.20347816560906293\n",
      "GD iter. 1713/2999: loss=0.20343606215774046\n",
      "GD iter. 1714/2999: loss=0.20339398867162586\n",
      "GD iter. 1715/2999: loss=0.20335194561838735\n",
      "GD iter. 1716/2999: loss=0.20330993227971372\n",
      "GD iter. 1717/2999: loss=0.20326794900557252\n",
      "GD iter. 1718/2999: loss=0.2032259952067477\n",
      "GD iter. 1719/2999: loss=0.20318407114027412\n",
      "GD iter. 1720/2999: loss=0.2031421763214527\n",
      "GD iter. 1721/2999: loss=0.2031003109342871\n",
      "GD iter. 1722/2999: loss=0.20305847457860998\n",
      "GD iter. 1723/2999: loss=0.20301666738132862\n",
      "GD iter. 1724/2999: loss=0.2029748890105839\n",
      "GD iter. 1725/2999: loss=0.20293313954889894\n",
      "GD iter. 1726/2999: loss=0.20289141871957564\n",
      "GD iter. 1727/2999: loss=0.20284972657085434\n",
      "GD iter. 1728/2999: loss=0.20280806287055983\n",
      "GD iter. 1729/2999: loss=0.20276642764064426\n",
      "GD iter. 1730/2999: loss=0.20272482068485956\n",
      "GD iter. 1731/2999: loss=0.20268324200516832\n",
      "GD iter. 1732/2999: loss=0.20264169143431401\n",
      "GD iter. 1733/2999: loss=0.20260016895920957\n",
      "GD iter. 1734/2999: loss=0.2025586744359997\n",
      "GD iter. 1735/2999: loss=0.20251720784040148\n",
      "GD iter. 1736/2999: loss=0.20247576904746004\n",
      "GD iter. 1737/2999: loss=0.20243435802468698\n",
      "GD iter. 1738/2999: loss=0.20239297466240708\n",
      "GD iter. 1739/2999: loss=0.202351618922232\n",
      "GD iter. 1740/2999: loss=0.20231029070685538\n",
      "GD iter. 1741/2999: loss=0.20226898997375467\n",
      "GD iter. 1742/2999: loss=0.2022277166356534\n",
      "GD iter. 1743/2999: loss=0.20218647064723594\n",
      "GD iter. 1744/2999: loss=0.20214525192937716\n",
      "GD iter. 1745/2999: loss=0.20210406043497778\n",
      "GD iter. 1746/2999: loss=0.20206289609155517\n",
      "GD iter. 1747/2999: loss=0.20202175885097798\n",
      "GD iter. 1748/2999: loss=0.20198064864619394\n",
      "GD iter. 1749/2999: loss=0.201939565428592\n",
      "GD iter. 1750/2999: loss=0.20189850913557603\n",
      "GD iter. 1751/2999: loss=0.20185747971845452\n",
      "GD iter. 1752/2999: loss=0.20181647711830406\n",
      "GD iter. 1753/2999: loss=0.20177550128663568\n",
      "GD iter. 1754/2999: loss=0.20173455216756783\n",
      "GD iter. 1755/2999: loss=0.20169362971300855\n",
      "GD iter. 1756/2999: loss=0.20165273386961005\n",
      "GD iter. 1757/2999: loss=0.2016118645898059\n",
      "GD iter. 1758/2999: loss=0.20157102182237185\n",
      "GD iter. 1759/2999: loss=0.20153020552034678\n",
      "GD iter. 1760/2999: loss=0.2014894156342983\n",
      "GD iter. 1761/2999: loss=0.20144865211791474\n",
      "GD iter. 1762/2999: loss=0.20140791492328694\n",
      "GD iter. 1763/2999: loss=0.2013672040047711\n",
      "GD iter. 1764/2999: loss=0.20132651931576337\n",
      "GD iter. 1765/2999: loss=0.20128586081128827\n",
      "GD iter. 1766/2999: loss=0.2012452284458698\n",
      "GD iter. 1767/2999: loss=0.20120462217518953\n",
      "GD iter. 1768/2999: loss=0.20116404195475412\n",
      "GD iter. 1769/2999: loss=0.20112348774088262\n",
      "GD iter. 1770/2999: loss=0.20108295948994623\n",
      "GD iter. 1771/2999: loss=0.20104245715887725\n",
      "GD iter. 1772/2999: loss=0.20100198070481368\n",
      "GD iter. 1773/2999: loss=0.20096153008527468\n",
      "GD iter. 1774/2999: loss=0.20092110525808485\n",
      "GD iter. 1775/2999: loss=0.20088070618132245\n",
      "GD iter. 1776/2999: loss=0.20084033281343233\n",
      "GD iter. 1777/2999: loss=0.2007999851130247\n",
      "GD iter. 1778/2999: loss=0.20075966303910966\n",
      "GD iter. 1779/2999: loss=0.20071936655080252\n",
      "GD iter. 1780/2999: loss=0.20067909560763292\n",
      "GD iter. 1781/2999: loss=0.20063885016919647\n",
      "GD iter. 1782/2999: loss=0.20059863019550359\n",
      "GD iter. 1783/2999: loss=0.20055843564660697\n",
      "GD iter. 1784/2999: loss=0.20051826648296556\n",
      "GD iter. 1785/2999: loss=0.20047812266506734\n",
      "GD iter. 1786/2999: loss=0.20043800415379256\n",
      "GD iter. 1787/2999: loss=0.2003979109100447\n",
      "GD iter. 1788/2999: loss=0.20035784289510203\n",
      "GD iter. 1789/2999: loss=0.2003178000702655\n",
      "GD iter. 1790/2999: loss=0.2002777823971917\n",
      "GD iter. 1791/2999: loss=0.20023778983756252\n",
      "GD iter. 1792/2999: loss=0.20019782235339575\n",
      "GD iter. 1793/2999: loss=0.20015787990673933\n",
      "GD iter. 1794/2999: loss=0.20011796245995753\n",
      "GD iter. 1795/2999: loss=0.2000780699754511\n",
      "GD iter. 1796/2999: loss=0.20003820241591788\n",
      "GD iter. 1797/2999: loss=0.19999835974409896\n",
      "GD iter. 1798/2999: loss=0.19995854192301465\n",
      "GD iter. 1799/2999: loss=0.1999187489157355\n",
      "GD iter. 1800/2999: loss=0.19987898068559426\n",
      "GD iter. 1801/2999: loss=0.19983923719598068\n",
      "GD iter. 1802/2999: loss=0.1997995184105315\n",
      "GD iter. 1803/2999: loss=0.19975982429294648\n",
      "GD iter. 1804/2999: loss=0.19972015480715813\n",
      "GD iter. 1805/2999: loss=0.19968050991716807\n",
      "GD iter. 1806/2999: loss=0.19964088958719753\n",
      "GD iter. 1807/2999: loss=0.1996012937815423\n",
      "GD iter. 1808/2999: loss=0.19956172246470588\n",
      "GD iter. 1809/2999: loss=0.19952217560127106\n",
      "GD iter. 1810/2999: loss=0.1994826531560177\n",
      "GD iter. 1811/2999: loss=0.19944315509380922\n",
      "GD iter. 1812/2999: loss=0.19940368137969638\n",
      "GD iter. 1813/2999: loss=0.199364231978817\n",
      "GD iter. 1814/2999: loss=0.19932480685648774\n",
      "GD iter. 1815/2999: loss=0.1992854059781153\n",
      "GD iter. 1816/2999: loss=0.19924602930927732\n",
      "GD iter. 1817/2999: loss=0.1992066768156442\n",
      "GD iter. 1818/2999: loss=0.19916734846305004\n",
      "GD iter. 1819/2999: loss=0.19912804421742406\n",
      "GD iter. 1820/2999: loss=0.19908876404485248\n",
      "GD iter. 1821/2999: loss=0.19904950791151876\n",
      "GD iter. 1822/2999: loss=0.19901027578375716\n",
      "GD iter. 1823/2999: loss=0.19897106762800087\n",
      "GD iter. 1824/2999: loss=0.19893188341082851\n",
      "GD iter. 1825/2999: loss=0.1988927230989186\n",
      "GD iter. 1826/2999: loss=0.19885358665909048\n",
      "GD iter. 1827/2999: loss=0.19881447405826402\n",
      "GD iter. 1828/2999: loss=0.19877538526349559\n",
      "GD iter. 1829/2999: loss=0.1987363202419428\n",
      "GD iter. 1830/2999: loss=0.19869727896089526\n",
      "GD iter. 1831/2999: loss=0.19865826138774478\n",
      "GD iter. 1832/2999: loss=0.1986192674900111\n",
      "GD iter. 1833/2999: loss=0.1985802972353162\n",
      "GD iter. 1834/2999: loss=0.19854135059140698\n",
      "GD iter. 1835/2999: loss=0.19850242752613223\n",
      "GD iter. 1836/2999: loss=0.1984635280074625\n",
      "GD iter. 1837/2999: loss=0.1984246520034703\n",
      "GD iter. 1838/2999: loss=0.19838579948234686\n",
      "GD iter. 1839/2999: loss=0.19834697041238492\n",
      "GD iter. 1840/2999: loss=0.19830816476199353\n",
      "GD iter. 1841/2999: loss=0.1982693824996825\n",
      "GD iter. 1842/2999: loss=0.19823062359407556\n",
      "GD iter. 1843/2999: loss=0.19819188801389687\n",
      "GD iter. 1844/2999: loss=0.1981531757279819\n",
      "GD iter. 1845/2999: loss=0.19811448670526594\n",
      "GD iter. 1846/2999: loss=0.1980758209147935\n",
      "GD iter. 1847/2999: loss=0.19803717832570825\n",
      "GD iter. 1848/2999: loss=0.19799855890726084\n",
      "GD iter. 1849/2999: loss=0.1979599626288005\n",
      "GD iter. 1850/2999: loss=0.19792138945978136\n",
      "GD iter. 1851/2999: loss=0.19788283936975537\n",
      "GD iter. 1852/2999: loss=0.19784431232837751\n",
      "GD iter. 1853/2999: loss=0.19780580830539982\n",
      "GD iter. 1854/2999: loss=0.19776732727067556\n",
      "GD iter. 1855/2999: loss=0.19772886919415408\n",
      "GD iter. 1856/2999: loss=0.19769043404588443\n",
      "GD iter. 1857/2999: loss=0.19765202179601063\n",
      "GD iter. 1858/2999: loss=0.19761363241477503\n",
      "GD iter. 1859/2999: loss=0.19757526587251398\n",
      "GD iter. 1860/2999: loss=0.1975369221396605\n",
      "GD iter. 1861/2999: loss=0.19749860118674073\n",
      "GD iter. 1862/2999: loss=0.19746030298437606\n",
      "GD iter. 1863/2999: loss=0.19742202750327995\n",
      "GD iter. 1864/2999: loss=0.19738377471425972\n",
      "GD iter. 1865/2999: loss=0.1973455445882139\n",
      "GD iter. 1866/2999: loss=0.19730733709613335\n",
      "GD iter. 1867/2999: loss=0.19726915220909927\n",
      "GD iter. 1868/2999: loss=0.19723098989828394\n",
      "GD iter. 1869/2999: loss=0.1971928501349488\n",
      "GD iter. 1870/2999: loss=0.19715473289044536\n",
      "GD iter. 1871/2999: loss=0.19711663813621302\n",
      "GD iter. 1872/2999: loss=0.19707856584378014\n",
      "GD iter. 1873/2999: loss=0.19704051598476224\n",
      "GD iter. 1874/2999: loss=0.1970024885308623\n",
      "GD iter. 1875/2999: loss=0.19696448345386944\n",
      "GD iter. 1876/2999: loss=0.19692650072565934\n",
      "GD iter. 1877/2999: loss=0.19688854031819278\n",
      "GD iter. 1878/2999: loss=0.19685060220351577\n",
      "GD iter. 1879/2999: loss=0.1968126863537587\n",
      "GD iter. 1880/2999: loss=0.19677479274113616\n",
      "GD iter. 1881/2999: loss=0.19673692133794574\n",
      "GD iter. 1882/2999: loss=0.19669907211656865\n",
      "GD iter. 1883/2999: loss=0.19666124504946791\n",
      "GD iter. 1884/2999: loss=0.19662344010918895\n",
      "GD iter. 1885/2999: loss=0.19658565726835847\n",
      "GD iter. 1886/2999: loss=0.19654789649968435\n",
      "GD iter. 1887/2999: loss=0.19651015777595487\n",
      "GD iter. 1888/2999: loss=0.1964724410700383\n",
      "GD iter. 1889/2999: loss=0.19643474635488264\n",
      "GD iter. 1890/2999: loss=0.1963970736035149\n",
      "GD iter. 1891/2999: loss=0.1963594227890407\n",
      "GD iter. 1892/2999: loss=0.1963217938846439\n",
      "GD iter. 1893/2999: loss=0.19628418686358604\n",
      "GD iter. 1894/2999: loss=0.19624660169920596\n",
      "GD iter. 1895/2999: loss=0.1962090383649194\n",
      "GD iter. 1896/2999: loss=0.19617149683421828\n",
      "GD iter. 1897/2999: loss=0.19613397708067057\n",
      "GD iter. 1898/2999: loss=0.19609647907791972\n",
      "GD iter. 1899/2999: loss=0.19605900279968436\n",
      "GD iter. 1900/2999: loss=0.19602154821975754\n",
      "GD iter. 1901/2999: loss=0.19598411531200657\n",
      "GD iter. 1902/2999: loss=0.1959467040503728\n",
      "GD iter. 1903/2999: loss=0.19590931440887052\n",
      "GD iter. 1904/2999: loss=0.1958719463615874\n",
      "GD iter. 1905/2999: loss=0.19583459988268323\n",
      "GD iter. 1906/2999: loss=0.19579727494639032\n",
      "GD iter. 1907/2999: loss=0.19575997152701236\n",
      "GD iter. 1908/2999: loss=0.19572268959892464\n",
      "GD iter. 1909/2999: loss=0.19568542913657322\n",
      "GD iter. 1910/2999: loss=0.19564819011447465\n",
      "GD iter. 1911/2999: loss=0.1956109725072158\n",
      "GD iter. 1912/2999: loss=0.19557377628945305\n",
      "GD iter. 1913/2999: loss=0.19553660143591234\n",
      "GD iter. 1914/2999: loss=0.19549944792138846\n",
      "GD iter. 1915/2999: loss=0.19546231572074474\n",
      "GD iter. 1916/2999: loss=0.1954252048089129\n",
      "GD iter. 1917/2999: loss=0.1953881151608925\n",
      "GD iter. 1918/2999: loss=0.1953510467517504\n",
      "GD iter. 1919/2999: loss=0.19531399955662065\n",
      "GD iter. 1920/2999: loss=0.1952769735507041\n",
      "GD iter. 1921/2999: loss=0.19523996870926788\n",
      "GD iter. 1922/2999: loss=0.19520298500764535\n",
      "GD iter. 1923/2999: loss=0.19516602242123526\n",
      "GD iter. 1924/2999: loss=0.1951290809255018\n",
      "GD iter. 1925/2999: loss=0.19509216049597417\n",
      "GD iter. 1926/2999: loss=0.1950552611082461\n",
      "GD iter. 1927/2999: loss=0.19501838273797562\n",
      "GD iter. 1928/2999: loss=0.19498152536088473\n",
      "GD iter. 1929/2999: loss=0.19494468895275893\n",
      "GD iter. 1930/2999: loss=0.194907873489447\n",
      "GD iter. 1931/2999: loss=0.19487107894686076\n",
      "GD iter. 1932/2999: loss=0.19483430530097448\n",
      "GD iter. 1933/2999: loss=0.19479755252782463\n",
      "GD iter. 1934/2999: loss=0.19476082060350988\n",
      "GD iter. 1935/2999: loss=0.1947241095041902\n",
      "GD iter. 1936/2999: loss=0.19468741920608704\n",
      "GD iter. 1937/2999: loss=0.1946507496854827\n",
      "GD iter. 1938/2999: loss=0.1946141009187203\n",
      "GD iter. 1939/2999: loss=0.19457747288220306\n",
      "GD iter. 1940/2999: loss=0.19454086555239444\n",
      "GD iter. 1941/2999: loss=0.1945042789058175\n",
      "GD iter. 1942/2999: loss=0.1944677129190548\n",
      "GD iter. 1943/2999: loss=0.19443116756874784\n",
      "GD iter. 1944/2999: loss=0.19439464283159708\n",
      "GD iter. 1945/2999: loss=0.19435813868436147\n",
      "GD iter. 1946/2999: loss=0.1943216551038581\n",
      "GD iter. 1947/2999: loss=0.19428519206696207\n",
      "GD iter. 1948/2999: loss=0.19424874955060606\n",
      "GD iter. 1949/2999: loss=0.19421232753177997\n",
      "GD iter. 1950/2999: loss=0.19417592598753086\n",
      "GD iter. 1951/2999: loss=0.19413954489496255\n",
      "GD iter. 1952/2999: loss=0.19410318423123524\n",
      "GD iter. 1953/2999: loss=0.19406684397356544\n",
      "GD iter. 1954/2999: loss=0.19403052409922542\n",
      "GD iter. 1955/2999: loss=0.19399422458554327\n",
      "GD iter. 1956/2999: loss=0.19395794540990227\n",
      "GD iter. 1957/2999: loss=0.19392168654974087\n",
      "GD iter. 1958/2999: loss=0.19388544798255228\n",
      "GD iter. 1959/2999: loss=0.1938492296858843\n",
      "GD iter. 1960/2999: loss=0.19381303163733898\n",
      "GD iter. 1961/2999: loss=0.19377685381457233\n",
      "GD iter. 1962/2999: loss=0.1937406961952942\n",
      "GD iter. 1963/2999: loss=0.19370455875726783\n",
      "GD iter. 1964/2999: loss=0.19366844147830978\n",
      "GD iter. 1965/2999: loss=0.19363234433628945\n",
      "GD iter. 1966/2999: loss=0.19359626730912913\n",
      "GD iter. 1967/2999: loss=0.1935602103748033\n",
      "GD iter. 1968/2999: loss=0.19352417351133888\n",
      "GD iter. 1969/2999: loss=0.1934881566968147\n",
      "GD iter. 1970/2999: loss=0.19345215990936118\n",
      "GD iter. 1971/2999: loss=0.19341618312716025\n",
      "GD iter. 1972/2999: loss=0.1933802263284451\n",
      "GD iter. 1973/2999: loss=0.19334428949149995\n",
      "GD iter. 1974/2999: loss=0.19330837259465947\n",
      "GD iter. 1975/2999: loss=0.19327247561630906\n",
      "GD iter. 1976/2999: loss=0.19323659853488442\n",
      "GD iter. 1977/2999: loss=0.193200741328871\n",
      "GD iter. 1978/2999: loss=0.19316490397680428\n",
      "GD iter. 1979/2999: loss=0.1931290864572691\n",
      "GD iter. 1980/2999: loss=0.19309328874889972\n",
      "GD iter. 1981/2999: loss=0.19305751083037953\n",
      "GD iter. 1982/2999: loss=0.1930217526804406\n",
      "GD iter. 1983/2999: loss=0.1929860142778638\n",
      "GD iter. 1984/2999: loss=0.19295029560147825\n",
      "GD iter. 1985/2999: loss=0.19291459663016144\n",
      "GD iter. 1986/2999: loss=0.19287891734283868\n",
      "GD iter. 1987/2999: loss=0.19284325771848307\n",
      "GD iter. 1988/2999: loss=0.19280761773611518\n",
      "GD iter. 1989/2999: loss=0.192771997374803\n",
      "GD iter. 1990/2999: loss=0.19273639661366138\n",
      "GD iter. 1991/2999: loss=0.1927008154318523\n",
      "GD iter. 1992/2999: loss=0.19266525380858424\n",
      "GD iter. 1993/2999: loss=0.1926297117231122\n",
      "GD iter. 1994/2999: loss=0.19259418915473744\n",
      "GD iter. 1995/2999: loss=0.19255868608280716\n",
      "GD iter. 1996/2999: loss=0.19252320248671442\n",
      "GD iter. 1997/2999: loss=0.19248773834589805\n",
      "GD iter. 1998/2999: loss=0.19245229363984193\n",
      "GD iter. 1999/2999: loss=0.19241686834807561\n",
      "GD iter. 2000/2999: loss=0.1923814624501733\n",
      "GD iter. 2001/2999: loss=0.1923460759257542\n",
      "GD iter. 2002/2999: loss=0.19231070875448208\n",
      "GD iter. 2003/2999: loss=0.19227536091606512\n",
      "GD iter. 2004/2999: loss=0.19224003239025564\n",
      "GD iter. 2005/2999: loss=0.1922047231568502\n",
      "GD iter. 2006/2999: loss=0.1921694331956889\n",
      "GD iter. 2007/2999: loss=0.1921341624866559\n",
      "GD iter. 2008/2999: loss=0.19209891100967824\n",
      "GD iter. 2009/2999: loss=0.19206367874472682\n",
      "GD iter. 2010/2999: loss=0.19202846567181517\n",
      "GD iter. 2011/2999: loss=0.19199327177099984\n",
      "GD iter. 2012/2999: loss=0.19195809702238023\n",
      "GD iter. 2013/2999: loss=0.1919229414060981\n",
      "GD iter. 2014/2999: loss=0.19188780490233756\n",
      "GD iter. 2015/2999: loss=0.1918526874913249\n",
      "GD iter. 2016/2999: loss=0.19181758915332833\n",
      "GD iter. 2017/2999: loss=0.191782509868658\n",
      "GD iter. 2018/2999: loss=0.19174744961766546\n",
      "GD iter. 2019/2999: loss=0.19171240838074394\n",
      "GD iter. 2020/2999: loss=0.19167738613832772\n",
      "GD iter. 2021/2999: loss=0.19164238287089227\n",
      "GD iter. 2022/2999: loss=0.1916073985589539\n",
      "GD iter. 2023/2999: loss=0.1915724331830699\n",
      "GD iter. 2024/2999: loss=0.19153748672383775\n",
      "GD iter. 2025/2999: loss=0.19150255916189557\n",
      "GD iter. 2026/2999: loss=0.1914676504779217\n",
      "GD iter. 2027/2999: loss=0.1914327606526344\n",
      "GD iter. 2028/2999: loss=0.1913978896667921\n",
      "GD iter. 2029/2999: loss=0.19136303750119274\n",
      "GD iter. 2030/2999: loss=0.19132820413667384\n",
      "GD iter. 2031/2999: loss=0.19129338955411235\n",
      "GD iter. 2032/2999: loss=0.19125859373442453\n",
      "GD iter. 2033/2999: loss=0.19122381665856553\n",
      "GD iter. 2034/2999: loss=0.19118905830752958\n",
      "GD iter. 2035/2999: loss=0.19115431866234986\n",
      "GD iter. 2036/2999: loss=0.1911195977040976\n",
      "GD iter. 2037/2999: loss=0.191084895413883\n",
      "GD iter. 2038/2999: loss=0.19105021177285408\n",
      "GD iter. 2039/2999: loss=0.19101554676219756\n",
      "GD iter. 2040/2999: loss=0.1909809003631376\n",
      "GD iter. 2041/2999: loss=0.19094627255693647\n",
      "GD iter. 2042/2999: loss=0.19091166332489412\n",
      "GD iter. 2043/2999: loss=0.1908770726483477\n",
      "GD iter. 2044/2999: loss=0.19084250050867219\n",
      "GD iter. 2045/2999: loss=0.19080794688727942\n",
      "GD iter. 2046/2999: loss=0.19077341176561846\n",
      "GD iter. 2047/2999: loss=0.19073889512517525\n",
      "GD iter. 2048/2999: loss=0.19070439694747254\n",
      "GD iter. 2049/2999: loss=0.19066991721406962\n",
      "GD iter. 2050/2999: loss=0.1906354559065625\n",
      "GD iter. 2051/2999: loss=0.1906010130065831\n",
      "GD iter. 2052/2999: loss=0.19056658849580002\n",
      "GD iter. 2053/2999: loss=0.19053218235591743\n",
      "GD iter. 2054/2999: loss=0.19049779456867585\n",
      "GD iter. 2055/2999: loss=0.19046342511585132\n",
      "GD iter. 2056/2999: loss=0.19042907397925557\n",
      "GD iter. 2057/2999: loss=0.1903947411407358\n",
      "GD iter. 2058/2999: loss=0.19036042658217456\n",
      "GD iter. 2059/2999: loss=0.19032613028548973\n",
      "GD iter. 2060/2999: loss=0.19029185223263406\n",
      "GD iter. 2061/2999: loss=0.19025759240559542\n",
      "GD iter. 2062/2999: loss=0.19022335078639638\n",
      "GD iter. 2063/2999: loss=0.19018912735709434\n",
      "GD iter. 2064/2999: loss=0.19015492209978102\n",
      "GD iter. 2065/2999: loss=0.19012073499658272\n",
      "GD iter. 2066/2999: loss=0.19008656602965995\n",
      "GD iter. 2067/2999: loss=0.19005241518120738\n",
      "GD iter. 2068/2999: loss=0.19001828243345362\n",
      "GD iter. 2069/2999: loss=0.1899841677686614\n",
      "GD iter. 2070/2999: loss=0.18995007116912688\n",
      "GD iter. 2071/2999: loss=0.18991599261718015\n",
      "GD iter. 2072/2999: loss=0.18988193209518467\n",
      "GD iter. 2073/2999: loss=0.18984788958553725\n",
      "GD iter. 2074/2999: loss=0.18981386507066794\n",
      "GD iter. 2075/2999: loss=0.1897798585330401\n",
      "GD iter. 2076/2999: loss=0.18974586995514992\n",
      "GD iter. 2077/2999: loss=0.18971189931952642\n",
      "GD iter. 2078/2999: loss=0.18967794660873166\n",
      "GD iter. 2079/2999: loss=0.18964401180535997\n",
      "GD iter. 2080/2999: loss=0.18961009489203853\n",
      "GD iter. 2081/2999: loss=0.18957619585142674\n",
      "GD iter. 2082/2999: loss=0.1895423146662162\n",
      "GD iter. 2083/2999: loss=0.18950845131913097\n",
      "GD iter. 2084/2999: loss=0.18947460579292671\n",
      "GD iter. 2085/2999: loss=0.18944077807039153\n",
      "GD iter. 2086/2999: loss=0.18940696813434485\n",
      "GD iter. 2087/2999: loss=0.18937317596763809\n",
      "GD iter. 2088/2999: loss=0.18933940155315415\n",
      "GD iter. 2089/2999: loss=0.18930564487380738\n",
      "GD iter. 2090/2999: loss=0.18927190591254345\n",
      "GD iter. 2091/2999: loss=0.18923818465233933\n",
      "GD iter. 2092/2999: loss=0.1892044810762031\n",
      "GD iter. 2093/2999: loss=0.1891707951671738\n",
      "GD iter. 2094/2999: loss=0.18913712690832132\n",
      "GD iter. 2095/2999: loss=0.18910347628274657\n",
      "GD iter. 2096/2999: loss=0.1890698432735808\n",
      "GD iter. 2097/2999: loss=0.18903622786398613\n",
      "GD iter. 2098/2999: loss=0.18900263003715495\n",
      "GD iter. 2099/2999: loss=0.18896904977631002\n",
      "GD iter. 2100/2999: loss=0.18893548706470434\n",
      "GD iter. 2101/2999: loss=0.1889019418856212\n",
      "GD iter. 2102/2999: loss=0.18886841422237366\n",
      "GD iter. 2103/2999: loss=0.1888349040583049\n",
      "GD iter. 2104/2999: loss=0.18880141137678783\n",
      "GD iter. 2105/2999: loss=0.1887679361612251\n",
      "GD iter. 2106/2999: loss=0.18873447839504892\n",
      "GD iter. 2107/2999: loss=0.18870103806172103\n",
      "GD iter. 2108/2999: loss=0.18866761514473251\n",
      "GD iter. 2109/2999: loss=0.18863420962760402\n",
      "GD iter. 2110/2999: loss=0.18860082149388496\n",
      "GD iter. 2111/2999: loss=0.1885674507271541\n",
      "GD iter. 2112/2999: loss=0.18853409731101933\n",
      "GD iter. 2113/2999: loss=0.18850076122911713\n",
      "GD iter. 2114/2999: loss=0.18846744246511293\n",
      "GD iter. 2115/2999: loss=0.18843414100270087\n",
      "GD iter. 2116/2999: loss=0.18840085682560354\n",
      "GD iter. 2117/2999: loss=0.18836758991757235\n",
      "GD iter. 2118/2999: loss=0.18833434026238666\n",
      "GD iter. 2119/2999: loss=0.18830110784385456\n",
      "GD iter. 2120/2999: loss=0.18826789264581203\n",
      "GD iter. 2121/2999: loss=0.18823469465212347\n",
      "GD iter. 2122/2999: loss=0.1882015138466809\n",
      "GD iter. 2123/2999: loss=0.18816835021340472\n",
      "GD iter. 2124/2999: loss=0.18813520373624273\n",
      "GD iter. 2125/2999: loss=0.18810207439917073\n",
      "GD iter. 2126/2999: loss=0.1880689621861921\n",
      "GD iter. 2127/2999: loss=0.18803586708133774\n",
      "GD iter. 2128/2999: loss=0.18800278906866605\n",
      "GD iter. 2129/2999: loss=0.1879697281322627\n",
      "GD iter. 2130/2999: loss=0.18793668425624083\n",
      "GD iter. 2131/2999: loss=0.1879036574247404\n",
      "GD iter. 2132/2999: loss=0.18787064762192893\n",
      "GD iter. 2133/2999: loss=0.1878376548320006\n",
      "GD iter. 2134/2999: loss=0.18780467903917658\n",
      "GD iter. 2135/2999: loss=0.1877717202277049\n",
      "GD iter. 2136/2999: loss=0.1877387783818604\n",
      "GD iter. 2137/2999: loss=0.18770585348594437\n",
      "GD iter. 2138/2999: loss=0.18767294552428487\n",
      "GD iter. 2139/2999: loss=0.18764005448123616\n",
      "GD iter. 2140/2999: loss=0.18760718034117937\n",
      "GD iter. 2141/2999: loss=0.1875743230885214\n",
      "GD iter. 2142/2999: loss=0.18754148270769566\n",
      "GD iter. 2143/2999: loss=0.18750865918316165\n",
      "GD iter. 2144/2999: loss=0.18747585249940477\n",
      "GD iter. 2145/2999: loss=0.18744306264093655\n",
      "GD iter. 2146/2999: loss=0.18741028959229447\n",
      "GD iter. 2147/2999: loss=0.1873775333380416\n",
      "GD iter. 2148/2999: loss=0.18734479386276665\n",
      "GD iter. 2149/2999: loss=0.18731207115108436\n",
      "GD iter. 2150/2999: loss=0.18727936518763463\n",
      "GD iter. 2151/2999: loss=0.18724667595708297\n",
      "GD iter. 2152/2999: loss=0.1872140034441203\n",
      "GD iter. 2153/2999: loss=0.18718134763346284\n",
      "GD iter. 2154/2999: loss=0.18714870850985194\n",
      "GD iter. 2155/2999: loss=0.18711608605805413\n",
      "GD iter. 2156/2999: loss=0.18708348026286115\n",
      "GD iter. 2157/2999: loss=0.18705089110908954\n",
      "GD iter. 2158/2999: loss=0.18701831858158074\n",
      "GD iter. 2159/2999: loss=0.18698576266520117\n",
      "GD iter. 2160/2999: loss=0.18695322334484188\n",
      "GD iter. 2161/2999: loss=0.18692070060541865\n",
      "GD iter. 2162/2999: loss=0.18688819443187185\n",
      "GD iter. 2163/2999: loss=0.18685570480916636\n",
      "GD iter. 2164/2999: loss=0.18682323172229143\n",
      "GD iter. 2165/2999: loss=0.18679077515626075\n",
      "GD iter. 2166/2999: loss=0.18675833509611237\n",
      "GD iter. 2167/2999: loss=0.18672591152690832\n",
      "GD iter. 2168/2999: loss=0.18669350443373517\n",
      "GD iter. 2169/2999: loss=0.18666111380170305\n",
      "GD iter. 2170/2999: loss=0.18662873961594653\n",
      "GD iter. 2171/2999: loss=0.18659638186162378\n",
      "GD iter. 2172/2999: loss=0.18656404052391704\n",
      "GD iter. 2173/2999: loss=0.18653171558803214\n",
      "GD iter. 2174/2999: loss=0.1864994070391988\n",
      "GD iter. 2175/2999: loss=0.1864671148626702\n",
      "GD iter. 2176/2999: loss=0.1864348390437231\n",
      "GD iter. 2177/2999: loss=0.18640257956765785\n",
      "GD iter. 2178/2999: loss=0.18637033641979814\n",
      "GD iter. 2179/2999: loss=0.186338109585491\n",
      "GD iter. 2180/2999: loss=0.18630589905010672\n",
      "GD iter. 2181/2999: loss=0.18627370479903882\n",
      "GD iter. 2182/2999: loss=0.18624152681770398\n",
      "GD iter. 2183/2999: loss=0.18620936509154182\n",
      "GD iter. 2184/2999: loss=0.1861772196060151\n",
      "GD iter. 2185/2999: loss=0.18614509034660937\n",
      "GD iter. 2186/2999: loss=0.18611297729883328\n",
      "GD iter. 2187/2999: loss=0.18608088044821788\n",
      "GD iter. 2188/2999: loss=0.18604879978031724\n",
      "GD iter. 2189/2999: loss=0.18601673528070808\n",
      "GD iter. 2190/2999: loss=0.1859846869349895\n",
      "GD iter. 2191/2999: loss=0.18595265472878333\n",
      "GD iter. 2192/2999: loss=0.18592063864773373\n",
      "GD iter. 2193/2999: loss=0.18588863867750727\n",
      "GD iter. 2194/2999: loss=0.18585665480379285\n",
      "GD iter. 2195/2999: loss=0.1858246870123017\n",
      "GD iter. 2196/2999: loss=0.1857927352887671\n",
      "GD iter. 2197/2999: loss=0.18576079961894454\n",
      "GD iter. 2198/2999: loss=0.1857288799886115\n",
      "GD iter. 2199/2999: loss=0.18569697638356764\n",
      "GD iter. 2200/2999: loss=0.18566508878963428\n",
      "GD iter. 2201/2999: loss=0.1856332171926549\n",
      "GD iter. 2202/2999: loss=0.18560136157849455\n",
      "GD iter. 2203/2999: loss=0.1855695219330401\n",
      "GD iter. 2204/2999: loss=0.18553769824220012\n",
      "GD iter. 2205/2999: loss=0.18550589049190483\n",
      "GD iter. 2206/2999: loss=0.18547409866810602\n",
      "GD iter. 2207/2999: loss=0.18544232275677677\n",
      "GD iter. 2208/2999: loss=0.18541056274391193\n",
      "GD iter. 2209/2999: loss=0.18537881861552732\n",
      "GD iter. 2210/2999: loss=0.1853470903576604\n",
      "GD iter. 2211/2999: loss=0.18531537795636982\n",
      "GD iter. 2212/2999: loss=0.18528368139773527\n",
      "GD iter. 2213/2999: loss=0.18525200066785774\n",
      "GD iter. 2214/2999: loss=0.1852203357528591\n",
      "GD iter. 2215/2999: loss=0.18518868663888244\n",
      "GD iter. 2216/2999: loss=0.1851570533120917\n",
      "GD iter. 2217/2999: loss=0.18512543575867169\n",
      "GD iter. 2218/2999: loss=0.18509383396482812\n",
      "GD iter. 2219/2999: loss=0.18506224791678735\n",
      "GD iter. 2220/2999: loss=0.1850306776007966\n",
      "GD iter. 2221/2999: loss=0.18499912300312377\n",
      "GD iter. 2222/2999: loss=0.18496758411005707\n",
      "GD iter. 2223/2999: loss=0.18493606090790565\n",
      "GD iter. 2224/2999: loss=0.18490455338299888\n",
      "GD iter. 2225/2999: loss=0.18487306152168664\n",
      "GD iter. 2226/2999: loss=0.18484158531033928\n",
      "GD iter. 2227/2999: loss=0.18481012473534725\n",
      "GD iter. 2228/2999: loss=0.1847786797831214\n",
      "GD iter. 2229/2999: loss=0.18474725044009285\n",
      "GD iter. 2230/2999: loss=0.18471583669271271\n",
      "GD iter. 2231/2999: loss=0.1846844385274523\n",
      "GD iter. 2232/2999: loss=0.18465305593080292\n",
      "GD iter. 2233/2999: loss=0.18462168888927596\n",
      "GD iter. 2234/2999: loss=0.1845903373894025\n",
      "GD iter. 2235/2999: loss=0.18455900141773374\n",
      "GD iter. 2236/2999: loss=0.18452768096084066\n",
      "GD iter. 2237/2999: loss=0.18449637600531385\n",
      "GD iter. 2238/2999: loss=0.1844650865377639\n",
      "GD iter. 2239/2999: loss=0.18443381254482066\n",
      "GD iter. 2240/2999: loss=0.18440255401313402\n",
      "GD iter. 2241/2999: loss=0.18437131092937314\n",
      "GD iter. 2242/2999: loss=0.1843400832802267\n",
      "GD iter. 2243/2999: loss=0.184308871052403\n",
      "GD iter. 2244/2999: loss=0.1842776742326296\n",
      "GD iter. 2245/2999: loss=0.18424649280765348\n",
      "GD iter. 2246/2999: loss=0.1842153267642408\n",
      "GD iter. 2247/2999: loss=0.18418417608917706\n",
      "GD iter. 2248/2999: loss=0.18415304076926695\n",
      "GD iter. 2249/2999: loss=0.18412192079133424\n",
      "GD iter. 2250/2999: loss=0.18409081614222192\n",
      "GD iter. 2251/2999: loss=0.18405972680879182\n",
      "GD iter. 2252/2999: loss=0.184028652777925\n",
      "GD iter. 2253/2999: loss=0.18399759403652113\n",
      "GD iter. 2254/2999: loss=0.18396655057149922\n",
      "GD iter. 2255/2999: loss=0.18393552236979668\n",
      "GD iter. 2256/2999: loss=0.18390450941837\n",
      "GD iter. 2257/2999: loss=0.1838735117041943\n",
      "GD iter. 2258/2999: loss=0.18384252921426336\n",
      "GD iter. 2259/2999: loss=0.18381156193558967\n",
      "GD iter. 2260/2999: loss=0.1837806098552043\n",
      "GD iter. 2261/2999: loss=0.1837496729601569\n",
      "GD iter. 2262/2999: loss=0.18371875123751544\n",
      "GD iter. 2263/2999: loss=0.18368784467436658\n",
      "GD iter. 2264/2999: loss=0.1836569532578153\n",
      "GD iter. 2265/2999: loss=0.18362607697498495\n",
      "GD iter. 2266/2999: loss=0.18359521581301705\n",
      "GD iter. 2267/2999: loss=0.1835643697590715\n",
      "GD iter. 2268/2999: loss=0.18353353880032647\n",
      "GD iter. 2269/2999: loss=0.18350272292397812\n",
      "GD iter. 2270/2999: loss=0.18347192211724092\n",
      "GD iter. 2271/2999: loss=0.18344113636734732\n",
      "GD iter. 2272/2999: loss=0.18341036566154784\n",
      "GD iter. 2273/2999: loss=0.18337960998711084\n",
      "GD iter. 2274/2999: loss=0.18334886933132286\n",
      "GD iter. 2275/2999: loss=0.18331814368148805\n",
      "GD iter. 2276/2999: loss=0.18328743302492873\n",
      "GD iter. 2277/2999: loss=0.18325673734898468\n",
      "GD iter. 2278/2999: loss=0.18322605664101368\n",
      "GD iter. 2279/2999: loss=0.18319539088839118\n",
      "GD iter. 2280/2999: loss=0.1831647400785101\n",
      "GD iter. 2281/2999: loss=0.18313410419878134\n",
      "GD iter. 2282/2999: loss=0.18310348323663309\n",
      "GD iter. 2283/2999: loss=0.18307287717951107\n",
      "GD iter. 2284/2999: loss=0.1830422860148787\n",
      "GD iter. 2285/2999: loss=0.18301170973021672\n",
      "GD iter. 2286/2999: loss=0.1829811483130232\n",
      "GD iter. 2287/2999: loss=0.18295060175081365\n",
      "GD iter. 2288/2999: loss=0.18292007003112098\n",
      "GD iter. 2289/2999: loss=0.18288955314149524\n",
      "GD iter. 2290/2999: loss=0.1828590510695037\n",
      "GD iter. 2291/2999: loss=0.18282856380273094\n",
      "GD iter. 2292/2999: loss=0.1827980913287786\n",
      "GD iter. 2293/2999: loss=0.18276763363526544\n",
      "GD iter. 2294/2999: loss=0.18273719070982727\n",
      "GD iter. 2295/2999: loss=0.18270676254011692\n",
      "GD iter. 2296/2999: loss=0.18267634911380426\n",
      "GD iter. 2297/2999: loss=0.18264595041857598\n",
      "GD iter. 2298/2999: loss=0.18261556644213578\n",
      "GD iter. 2299/2999: loss=0.18258519717220417\n",
      "GD iter. 2300/2999: loss=0.18255484259651844\n",
      "GD iter. 2301/2999: loss=0.18252450270283263\n",
      "GD iter. 2302/2999: loss=0.18249417747891764\n",
      "GD iter. 2303/2999: loss=0.18246386691256095\n",
      "GD iter. 2304/2999: loss=0.18243357099156665\n",
      "GD iter. 2305/2999: loss=0.1824032897037556\n",
      "GD iter. 2306/2999: loss=0.1823730230369651\n",
      "GD iter. 2307/2999: loss=0.182342770979049\n",
      "GD iter. 2308/2999: loss=0.18231253351787766\n",
      "GD iter. 2309/2999: loss=0.1822823106413379\n",
      "GD iter. 2310/2999: loss=0.1822521023373329\n",
      "GD iter. 2311/2999: loss=0.1822219085937823\n",
      "GD iter. 2312/2999: loss=0.18219172939862202\n",
      "GD iter. 2313/2999: loss=0.1821615647398042\n",
      "GD iter. 2314/2999: loss=0.18213141460529733\n",
      "GD iter. 2315/2999: loss=0.18210127898308617\n",
      "GD iter. 2316/2999: loss=0.1820711578611715\n",
      "GD iter. 2317/2999: loss=0.1820410512275704\n",
      "GD iter. 2318/2999: loss=0.18201095907031586\n",
      "GD iter. 2319/2999: loss=0.1819808813774572\n",
      "GD iter. 2320/2999: loss=0.18195081813705946\n",
      "GD iter. 2321/2999: loss=0.18192076933720383\n",
      "GD iter. 2322/2999: loss=0.18189073496598754\n",
      "GD iter. 2323/2999: loss=0.18186071501152354\n",
      "GD iter. 2324/2999: loss=0.18183070946194085\n",
      "GD iter. 2325/2999: loss=0.18180071830538413\n",
      "GD iter. 2326/2999: loss=0.18177074153001394\n",
      "GD iter. 2327/2999: loss=0.18174077912400666\n",
      "GD iter. 2328/2999: loss=0.1817108310755543\n",
      "GD iter. 2329/2999: loss=0.18168089737286458\n",
      "GD iter. 2330/2999: loss=0.18165097800416094\n",
      "GD iter. 2331/2999: loss=0.18162107295768232\n",
      "GD iter. 2332/2999: loss=0.18159118222168338\n",
      "GD iter. 2333/2999: loss=0.18156130578443427\n",
      "GD iter. 2334/2999: loss=0.18153144363422058\n",
      "GD iter. 2335/2999: loss=0.1815015957593435\n",
      "GD iter. 2336/2999: loss=0.18147176214811953\n",
      "GD iter. 2337/2999: loss=0.18144194278888065\n",
      "GD iter. 2338/2999: loss=0.18141213766997424\n",
      "GD iter. 2339/2999: loss=0.18138234677976298\n",
      "GD iter. 2340/2999: loss=0.18135257010662478\n",
      "GD iter. 2341/2999: loss=0.1813228076389529\n",
      "GD iter. 2342/2999: loss=0.18129305936515575\n",
      "GD iter. 2343/2999: loss=0.18126332527365707\n",
      "GD iter. 2344/2999: loss=0.18123360535289565\n",
      "GD iter. 2345/2999: loss=0.18120389959132535\n",
      "GD iter. 2346/2999: loss=0.1811742079774153\n",
      "GD iter. 2347/2999: loss=0.18114453049964957\n",
      "GD iter. 2348/2999: loss=0.18111486714652725\n",
      "GD iter. 2349/2999: loss=0.18108521790656237\n",
      "GD iter. 2350/2999: loss=0.1810555827682841\n",
      "GD iter. 2351/2999: loss=0.1810259617202363\n",
      "GD iter. 2352/2999: loss=0.18099635475097797\n",
      "GD iter. 2353/2999: loss=0.18096676184908267\n",
      "GD iter. 2354/2999: loss=0.18093718300313905\n",
      "GD iter. 2355/2999: loss=0.18090761820175036\n",
      "GD iter. 2356/2999: loss=0.1808780674335348\n",
      "GD iter. 2357/2999: loss=0.18084853068712503\n",
      "GD iter. 2358/2999: loss=0.18081900795116868\n",
      "GD iter. 2359/2999: loss=0.18078949921432783\n",
      "GD iter. 2360/2999: loss=0.18076000446527926\n",
      "GD iter. 2361/2999: loss=0.1807305236927143\n",
      "GD iter. 2362/2999: loss=0.1807010568853389\n",
      "GD iter. 2363/2999: loss=0.18067160403187363\n",
      "GD iter. 2364/2999: loss=0.18064216512105327\n",
      "GD iter. 2365/2999: loss=0.18061274014162737\n",
      "GD iter. 2366/2999: loss=0.18058332908235966\n",
      "GD iter. 2367/2999: loss=0.1805539319320286\n",
      "GD iter. 2368/2999: loss=0.18052454867942655\n",
      "GD iter. 2369/2999: loss=0.18049517931336062\n",
      "GD iter. 2370/2999: loss=0.18046582382265203\n",
      "GD iter. 2371/2999: loss=0.18043648219613628\n",
      "GD iter. 2372/2999: loss=0.1804071544226634\n",
      "GD iter. 2373/2999: loss=0.180377840491097\n",
      "GD iter. 2374/2999: loss=0.1803485403903155\n",
      "GD iter. 2375/2999: loss=0.18031925410921124\n",
      "GD iter. 2376/2999: loss=0.18028998163669055\n",
      "GD iter. 2377/2999: loss=0.18026072296167409\n",
      "GD iter. 2378/2999: loss=0.1802314780730964\n",
      "GD iter. 2379/2999: loss=0.1802022469599061\n",
      "GD iter. 2380/2999: loss=0.18017302961106588\n",
      "GD iter. 2381/2999: loss=0.18014382601555223\n",
      "GD iter. 2382/2999: loss=0.18011463616235587\n",
      "GD iter. 2383/2999: loss=0.18008546004048112\n",
      "GD iter. 2384/2999: loss=0.18005629763894634\n",
      "GD iter. 2385/2999: loss=0.18002714894678365\n",
      "GD iter. 2386/2999: loss=0.1799980139530392\n",
      "GD iter. 2387/2999: loss=0.17996889264677265\n",
      "GD iter. 2388/2999: loss=0.17993978501705762\n",
      "GD iter. 2389/2999: loss=0.1799106910529814\n",
      "GD iter. 2390/2999: loss=0.17988161074364492\n",
      "GD iter. 2391/2999: loss=0.1798525440781629\n",
      "GD iter. 2392/2999: loss=0.1798234910456637\n",
      "GD iter. 2393/2999: loss=0.17979445163528915\n",
      "GD iter. 2394/2999: loss=0.17976542583619484\n",
      "GD iter. 2395/2999: loss=0.17973641363754975\n",
      "GD iter. 2396/2999: loss=0.17970741502853665\n",
      "GD iter. 2397/2999: loss=0.17967842999835146\n",
      "GD iter. 2398/2999: loss=0.1796494585362039\n",
      "GD iter. 2399/2999: loss=0.17962050063131696\n",
      "GD iter. 2400/2999: loss=0.1795915562729271\n",
      "GD iter. 2401/2999: loss=0.17956262545028417\n",
      "GD iter. 2402/2999: loss=0.17953370815265132\n",
      "GD iter. 2403/2999: loss=0.1795048043693051\n",
      "GD iter. 2404/2999: loss=0.1794759140895354\n",
      "GD iter. 2405/2999: loss=0.17944703730264527\n",
      "GD iter. 2406/2999: loss=0.17941817399795115\n",
      "GD iter. 2407/2999: loss=0.17938932416478265\n",
      "GD iter. 2408/2999: loss=0.17936048779248248\n",
      "GD iter. 2409/2999: loss=0.17933166487040664\n",
      "GD iter. 2410/2999: loss=0.17930285538792418\n",
      "GD iter. 2411/2999: loss=0.17927405933441742\n",
      "GD iter. 2412/2999: loss=0.1792452766992816\n",
      "GD iter. 2413/2999: loss=0.1792165074719251\n",
      "GD iter. 2414/2999: loss=0.1791877516417694\n",
      "GD iter. 2415/2999: loss=0.1791590091982488\n",
      "GD iter. 2416/2999: loss=0.1791302801308107\n",
      "GD iter. 2417/2999: loss=0.1791015644289155\n",
      "GD iter. 2418/2999: loss=0.17907286208203643\n",
      "GD iter. 2419/2999: loss=0.17904417307965972\n",
      "GD iter. 2420/2999: loss=0.17901549741128442\n",
      "GD iter. 2421/2999: loss=0.17898683506642235\n",
      "GD iter. 2422/2999: loss=0.17895818603459834\n",
      "GD iter. 2423/2999: loss=0.17892955030534988\n",
      "GD iter. 2424/2999: loss=0.17890092786822717\n",
      "GD iter. 2425/2999: loss=0.17887231871279338\n",
      "GD iter. 2426/2999: loss=0.17884372282862415\n",
      "GD iter. 2427/2999: loss=0.17881514020530803\n",
      "GD iter. 2428/2999: loss=0.1787865708324462\n",
      "GD iter. 2429/2999: loss=0.17875801469965222\n",
      "GD iter. 2430/2999: loss=0.17872947179655269\n",
      "GD iter. 2431/2999: loss=0.17870094211278642\n",
      "GD iter. 2432/2999: loss=0.17867242563800512\n",
      "GD iter. 2433/2999: loss=0.1786439223618727\n",
      "GD iter. 2434/2999: loss=0.1786154322740659\n",
      "GD iter. 2435/2999: loss=0.17858695536427374\n",
      "GD iter. 2436/2999: loss=0.17855849162219778\n",
      "GD iter. 2437/2999: loss=0.17853004103755218\n",
      "GD iter. 2438/2999: loss=0.17850160360006317\n",
      "GD iter. 2439/2999: loss=0.17847317929946968\n",
      "GD iter. 2440/2999: loss=0.1784447681255229\n",
      "GD iter. 2441/2999: loss=0.17841637006798622\n",
      "GD iter. 2442/2999: loss=0.17838798511663567\n",
      "GD iter. 2443/2999: loss=0.17835961326125935\n",
      "GD iter. 2444/2999: loss=0.17833125449165768\n",
      "GD iter. 2445/2999: loss=0.1783029087976432\n",
      "GD iter. 2446/2999: loss=0.17827457616904102\n",
      "GD iter. 2447/2999: loss=0.17824625659568813\n",
      "GD iter. 2448/2999: loss=0.1782179500674337\n",
      "GD iter. 2449/2999: loss=0.17818965657413932\n",
      "GD iter. 2450/2999: loss=0.17816137610567845\n",
      "GD iter. 2451/2999: loss=0.17813310865193677\n",
      "GD iter. 2452/2999: loss=0.17810485420281189\n",
      "GD iter. 2453/2999: loss=0.1780766127482138\n",
      "GD iter. 2454/2999: loss=0.17804838427806416\n",
      "GD iter. 2455/2999: loss=0.1780201687822969\n",
      "GD iter. 2456/2999: loss=0.1779919662508578\n",
      "GD iter. 2457/2999: loss=0.17796377667370458\n",
      "GD iter. 2458/2999: loss=0.17793560004080708\n",
      "GD iter. 2459/2999: loss=0.17790743634214692\n",
      "GD iter. 2460/2999: loss=0.17787928556771754\n",
      "GD iter. 2461/2999: loss=0.1778511477075244\n",
      "GD iter. 2462/2999: loss=0.1778230227515848\n",
      "GD iter. 2463/2999: loss=0.17779491068992778\n",
      "GD iter. 2464/2999: loss=0.1777668115125942\n",
      "GD iter. 2465/2999: loss=0.17773872520963682\n",
      "GD iter. 2466/2999: loss=0.17771065177111997\n",
      "GD iter. 2467/2999: loss=0.1776825911871199\n",
      "GD iter. 2468/2999: loss=0.17765454344772436\n",
      "GD iter. 2469/2999: loss=0.17762650854303308\n",
      "GD iter. 2470/2999: loss=0.17759848646315707\n",
      "GD iter. 2471/2999: loss=0.17757047719821947\n",
      "GD iter. 2472/2999: loss=0.17754248073835455\n",
      "GD iter. 2473/2999: loss=0.17751449707370856\n",
      "GD iter. 2474/2999: loss=0.17748652619443914\n",
      "GD iter. 2475/2999: loss=0.17745856809071556\n",
      "GD iter. 2476/2999: loss=0.17743062275271856\n",
      "GD iter. 2477/2999: loss=0.17740269017064042\n",
      "GD iter. 2478/2999: loss=0.177374770334685\n",
      "GD iter. 2479/2999: loss=0.17734686323506751\n",
      "GD iter. 2480/2999: loss=0.17731896886201468\n",
      "GD iter. 2481/2999: loss=0.1772910872057646\n",
      "GD iter. 2482/2999: loss=0.17726321825656696\n",
      "GD iter. 2483/2999: loss=0.1772353620046825\n",
      "GD iter. 2484/2999: loss=0.17720751844038368\n",
      "GD iter. 2485/2999: loss=0.17717968755395397\n",
      "GD iter. 2486/2999: loss=0.17715186933568844\n",
      "GD iter. 2487/2999: loss=0.17712406377589332\n",
      "GD iter. 2488/2999: loss=0.17709627086488616\n",
      "GD iter. 2489/2999: loss=0.1770684905929958\n",
      "GD iter. 2490/2999: loss=0.17704072295056206\n",
      "GD iter. 2491/2999: loss=0.17701296792793644\n",
      "GD iter. 2492/2999: loss=0.1769852255154813\n",
      "GD iter. 2493/2999: loss=0.1769574957035702\n",
      "GD iter. 2494/2999: loss=0.17692977848258806\n",
      "GD iter. 2495/2999: loss=0.17690207384293063\n",
      "GD iter. 2496/2999: loss=0.17687438177500503\n",
      "GD iter. 2497/2999: loss=0.17684670226922936\n",
      "GD iter. 2498/2999: loss=0.17681903531603277\n",
      "GD iter. 2499/2999: loss=0.1767913809058555\n",
      "GD iter. 2500/2999: loss=0.17676373902914888\n",
      "GD iter. 2501/2999: loss=0.17673610967637515\n",
      "GD iter. 2502/2999: loss=0.1767084928380076\n",
      "GD iter. 2503/2999: loss=0.17668088850453043\n",
      "GD iter. 2504/2999: loss=0.1766532966664389\n",
      "GD iter. 2505/2999: loss=0.176625717314239\n",
      "GD iter. 2506/2999: loss=0.17659815043844795\n",
      "GD iter. 2507/2999: loss=0.17657059602959363\n",
      "GD iter. 2508/2999: loss=0.17654305407821477\n",
      "GD iter. 2509/2999: loss=0.17651552457486105\n",
      "GD iter. 2510/2999: loss=0.17648800751009294\n",
      "GD iter. 2511/2999: loss=0.17646050287448178\n",
      "GD iter. 2512/2999: loss=0.17643301065860967\n",
      "GD iter. 2513/2999: loss=0.1764055308530694\n",
      "GD iter. 2514/2999: loss=0.17637806344846457\n",
      "GD iter. 2515/2999: loss=0.17635060843540967\n",
      "GD iter. 2516/2999: loss=0.1763231658045296\n",
      "GD iter. 2517/2999: loss=0.17629573554646025\n",
      "GD iter. 2518/2999: loss=0.17626831765184803\n",
      "GD iter. 2519/2999: loss=0.17624091211134996\n",
      "GD iter. 2520/2999: loss=0.1762135189156338\n",
      "GD iter. 2521/2999: loss=0.17618613805537794\n",
      "GD iter. 2522/2999: loss=0.17615876952127127\n",
      "GD iter. 2523/2999: loss=0.17613141330401338\n",
      "GD iter. 2524/2999: loss=0.1761040693943143\n",
      "GD iter. 2525/2999: loss=0.1760767377828948\n",
      "GD iter. 2526/2999: loss=0.17604941846048586\n",
      "GD iter. 2527/2999: loss=0.17602211141782925\n",
      "GD iter. 2528/2999: loss=0.17599481664567712\n",
      "GD iter. 2529/2999: loss=0.17596753413479213\n",
      "GD iter. 2530/2999: loss=0.17594026387594722\n",
      "GD iter. 2531/2999: loss=0.175913005859926\n",
      "GD iter. 2532/2999: loss=0.17588576007752235\n",
      "GD iter. 2533/2999: loss=0.17585852651954062\n",
      "GD iter. 2534/2999: loss=0.1758313051767954\n",
      "GD iter. 2535/2999: loss=0.17580409604011188\n",
      "GD iter. 2536/2999: loss=0.17577689910032537\n",
      "GD iter. 2537/2999: loss=0.17574971434828154\n",
      "GD iter. 2538/2999: loss=0.17572254177483643\n",
      "GD iter. 2539/2999: loss=0.17569538137085636\n",
      "GD iter. 2540/2999: loss=0.17566823312721783\n",
      "GD iter. 2541/2999: loss=0.1756410970348077\n",
      "GD iter. 2542/2999: loss=0.17561397308452295\n",
      "GD iter. 2543/2999: loss=0.17558686126727097\n",
      "GD iter. 2544/2999: loss=0.17555976157396902\n",
      "GD iter. 2545/2999: loss=0.17553267399554487\n",
      "GD iter. 2546/2999: loss=0.1755055985229363\n",
      "GD iter. 2547/2999: loss=0.17547853514709114\n",
      "GD iter. 2548/2999: loss=0.1754514838589675\n",
      "GD iter. 2549/2999: loss=0.1754244446495336\n",
      "GD iter. 2550/2999: loss=0.17539741750976756\n",
      "GD iter. 2551/2999: loss=0.1753704024306578\n",
      "GD iter. 2552/2999: loss=0.17534339940320262\n",
      "GD iter. 2553/2999: loss=0.17531640841841056\n",
      "GD iter. 2554/2999: loss=0.1752894294672999\n",
      "GD iter. 2555/2999: loss=0.1752624625408992\n",
      "GD iter. 2556/2999: loss=0.17523550763024678\n",
      "GD iter. 2557/2999: loss=0.17520856472639107\n",
      "GD iter. 2558/2999: loss=0.17518163382039045\n",
      "GD iter. 2559/2999: loss=0.1751547149033131\n",
      "GD iter. 2560/2999: loss=0.17512780796623725\n",
      "GD iter. 2561/2999: loss=0.175100913000251\n",
      "GD iter. 2562/2999: loss=0.17507402999645233\n",
      "GD iter. 2563/2999: loss=0.17504715894594908\n",
      "GD iter. 2564/2999: loss=0.1750202998398588\n",
      "GD iter. 2565/2999: loss=0.17499345266930927\n",
      "GD iter. 2566/2999: loss=0.17496661742543757\n",
      "GD iter. 2567/2999: loss=0.17493979409939098\n",
      "GD iter. 2568/2999: loss=0.17491298268232636\n",
      "GD iter. 2569/2999: loss=0.17488618316541038\n",
      "GD iter. 2570/2999: loss=0.17485939553981947\n",
      "GD iter. 2571/2999: loss=0.1748326197967398\n",
      "GD iter. 2572/2999: loss=0.17480585592736722\n",
      "GD iter. 2573/2999: loss=0.17477910392290733\n",
      "GD iter. 2574/2999: loss=0.1747523637745754\n",
      "GD iter. 2575/2999: loss=0.17472563547359626\n",
      "GD iter. 2576/2999: loss=0.17469891901120466\n",
      "GD iter. 2577/2999: loss=0.17467221437864458\n",
      "GD iter. 2578/2999: loss=0.17464552156716995\n",
      "GD iter. 2579/2999: loss=0.17461884056804425\n",
      "GD iter. 2580/2999: loss=0.17459217137254043\n",
      "GD iter. 2581/2999: loss=0.17456551397194103\n",
      "GD iter. 2582/2999: loss=0.1745388683575383\n",
      "GD iter. 2583/2999: loss=0.17451223452063375\n",
      "GD iter. 2584/2999: loss=0.17448561245253866\n",
      "GD iter. 2585/2999: loss=0.17445900214457374\n",
      "GD iter. 2586/2999: loss=0.17443240358806913\n",
      "GD iter. 2587/2999: loss=0.17440581677436448\n",
      "GD iter. 2588/2999: loss=0.1743792416948089\n",
      "GD iter. 2589/2999: loss=0.174352678340761\n",
      "GD iter. 2590/2999: loss=0.17432612670358874\n",
      "GD iter. 2591/2999: loss=0.17429958677466953\n",
      "GD iter. 2592/2999: loss=0.17427305854539005\n",
      "GD iter. 2593/2999: loss=0.17424654200714657\n",
      "GD iter. 2594/2999: loss=0.17422003715134463\n",
      "GD iter. 2595/2999: loss=0.17419354396939904\n",
      "GD iter. 2596/2999: loss=0.17416706245273403\n",
      "GD iter. 2597/2999: loss=0.17414059259278306\n",
      "GD iter. 2598/2999: loss=0.17411413438098908\n",
      "GD iter. 2599/2999: loss=0.17408768780880418\n",
      "GD iter. 2600/2999: loss=0.1740612528676897\n",
      "GD iter. 2601/2999: loss=0.17403482954911628\n",
      "GD iter. 2602/2999: loss=0.1740084178445639\n",
      "GD iter. 2603/2999: loss=0.17398201774552147\n",
      "GD iter. 2604/2999: loss=0.1739556292434875\n",
      "GD iter. 2605/2999: loss=0.17392925232996947\n",
      "GD iter. 2606/2999: loss=0.17390288699648396\n",
      "GD iter. 2607/2999: loss=0.1738765332345569\n",
      "GD iter. 2608/2999: loss=0.17385019103572336\n",
      "GD iter. 2609/2999: loss=0.17382386039152742\n",
      "GD iter. 2610/2999: loss=0.17379754129352232\n",
      "GD iter. 2611/2999: loss=0.17377123373327044\n",
      "GD iter. 2612/2999: loss=0.1737449377023433\n",
      "GD iter. 2613/2999: loss=0.17371865319232138\n",
      "GD iter. 2614/2999: loss=0.1736923801947943\n",
      "GD iter. 2615/2999: loss=0.17366611870136064\n",
      "GD iter. 2616/2999: loss=0.1736398687036282\n",
      "GD iter. 2617/2999: loss=0.17361363019321355\n",
      "GD iter. 2618/2999: loss=0.17358740316174243\n",
      "GD iter. 2619/2999: loss=0.17356118760084951\n",
      "GD iter. 2620/2999: loss=0.1735349835021785\n",
      "GD iter. 2621/2999: loss=0.173508790857382\n",
      "GD iter. 2622/2999: loss=0.17348260965812148\n",
      "GD iter. 2623/2999: loss=0.1734564398960675\n",
      "GD iter. 2624/2999: loss=0.1734302815628995\n",
      "GD iter. 2625/2999: loss=0.1734041346503057\n",
      "GD iter. 2626/2999: loss=0.17337799914998328\n",
      "GD iter. 2627/2999: loss=0.17335187505363847\n",
      "GD iter. 2628/2999: loss=0.17332576235298597\n",
      "GD iter. 2629/2999: loss=0.1732996610397497\n",
      "GD iter. 2630/2999: loss=0.17327357110566216\n",
      "GD iter. 2631/2999: loss=0.17324749254246488\n",
      "GD iter. 2632/2999: loss=0.17322142534190801\n",
      "GD iter. 2633/2999: loss=0.17319536949575048\n",
      "GD iter. 2634/2999: loss=0.17316932499576018\n",
      "GD iter. 2635/2999: loss=0.1731432918337136\n",
      "GD iter. 2636/2999: loss=0.173117270001396\n",
      "GD iter. 2637/2999: loss=0.17309125949060142\n",
      "GD iter. 2638/2999: loss=0.1730652602931326\n",
      "GD iter. 2639/2999: loss=0.17303927240080094\n",
      "GD iter. 2640/2999: loss=0.17301329580542665\n",
      "GD iter. 2641/2999: loss=0.1729873304988384\n",
      "GD iter. 2642/2999: loss=0.17296137647287377\n",
      "GD iter. 2643/2999: loss=0.17293543371937875\n",
      "GD iter. 2644/2999: loss=0.17290950223020823\n",
      "GD iter. 2645/2999: loss=0.1728835819972256\n",
      "GD iter. 2646/2999: loss=0.17285767301230262\n",
      "GD iter. 2647/2999: loss=0.17283177526732008\n",
      "GD iter. 2648/2999: loss=0.17280588875416705\n",
      "GD iter. 2649/2999: loss=0.1727800134647413\n",
      "GD iter. 2650/2999: loss=0.17275414939094902\n",
      "GD iter. 2651/2999: loss=0.1727282965247051\n",
      "GD iter. 2652/2999: loss=0.17270245485793287\n",
      "GD iter. 2653/2999: loss=0.17267662438256423\n",
      "GD iter. 2654/2999: loss=0.17265080509053943\n",
      "GD iter. 2655/2999: loss=0.17262499697380745\n",
      "GD iter. 2656/2999: loss=0.1725992000243255\n",
      "GD iter. 2657/2999: loss=0.1725734142340594\n",
      "GD iter. 2658/2999: loss=0.17254763959498345\n",
      "GD iter. 2659/2999: loss=0.17252187609908023\n",
      "GD iter. 2660/2999: loss=0.17249612373834083\n",
      "GD iter. 2661/2999: loss=0.1724703825047648\n",
      "GD iter. 2662/2999: loss=0.1724446523903599\n",
      "GD iter. 2663/2999: loss=0.1724189333871426\n",
      "GD iter. 2664/2999: loss=0.17239322548713737\n",
      "GD iter. 2665/2999: loss=0.17236752868237729\n",
      "GD iter. 2666/2999: loss=0.17234184296490368\n",
      "GD iter. 2667/2999: loss=0.17231616832676622\n",
      "GD iter. 2668/2999: loss=0.17229050476002286\n",
      "GD iter. 2669/2999: loss=0.17226485225673993\n",
      "GD iter. 2670/2999: loss=0.172239210808992\n",
      "GD iter. 2671/2999: loss=0.17221358040886192\n",
      "GD iter. 2672/2999: loss=0.1721879610484409\n",
      "GD iter. 2673/2999: loss=0.17216235271982813\n",
      "GD iter. 2674/2999: loss=0.1721367554151314\n",
      "GD iter. 2675/2999: loss=0.17211116912646654\n",
      "GD iter. 2676/2999: loss=0.17208559384595754\n",
      "GD iter. 2677/2999: loss=0.17206002956573674\n",
      "GD iter. 2678/2999: loss=0.1720344762779446\n",
      "GD iter. 2679/2999: loss=0.1720089339747296\n",
      "GD iter. 2680/2999: loss=0.1719834026482488\n",
      "GD iter. 2681/2999: loss=0.171957882290667\n",
      "GD iter. 2682/2999: loss=0.17193237289415725\n",
      "GD iter. 2683/2999: loss=0.17190687445090092\n",
      "GD iter. 2684/2999: loss=0.17188138695308713\n",
      "GD iter. 2685/2999: loss=0.17185591039291356\n",
      "GD iter. 2686/2999: loss=0.17183044476258558\n",
      "GD iter. 2687/2999: loss=0.17180499005431682\n",
      "GD iter. 2688/2999: loss=0.17177954626032899\n",
      "GD iter. 2689/2999: loss=0.17175411337285182\n",
      "GD iter. 2690/2999: loss=0.17172869138412303\n",
      "GD iter. 2691/2999: loss=0.17170328028638848\n",
      "GD iter. 2692/2999: loss=0.17167788007190188\n",
      "GD iter. 2693/2999: loss=0.17165249073292518\n",
      "GD iter. 2694/2999: loss=0.1716271122617281\n",
      "GD iter. 2695/2999: loss=0.17160174465058856\n",
      "GD iter. 2696/2999: loss=0.17157638789179216\n",
      "GD iter. 2697/2999: loss=0.1715510419776327\n",
      "GD iter. 2698/2999: loss=0.17152570690041183\n",
      "GD iter. 2699/2999: loss=0.17150038265243914\n",
      "GD iter. 2700/2999: loss=0.1714750692260321\n",
      "GD iter. 2701/2999: loss=0.1714497666135163\n",
      "GD iter. 2702/2999: loss=0.17142447480722492\n",
      "GD iter. 2703/2999: loss=0.17139919379949914\n",
      "GD iter. 2704/2999: loss=0.17137392358268813\n",
      "GD iter. 2705/2999: loss=0.1713486641491488\n",
      "GD iter. 2706/2999: loss=0.17132341549124594\n",
      "GD iter. 2707/2999: loss=0.17129817760135216\n",
      "GD iter. 2708/2999: loss=0.17127295047184796\n",
      "GD iter. 2709/2999: loss=0.1712477340951216\n",
      "GD iter. 2710/2999: loss=0.17122252846356914\n",
      "GD iter. 2711/2999: loss=0.1711973335695944\n",
      "GD iter. 2712/2999: loss=0.17117214940560913\n",
      "GD iter. 2713/2999: loss=0.17114697596403272\n",
      "GD iter. 2714/2999: loss=0.1711218132372922\n",
      "GD iter. 2715/2999: loss=0.17109666121782258\n",
      "GD iter. 2716/2999: loss=0.17107151989806654\n",
      "GD iter. 2717/2999: loss=0.1710463892704744\n",
      "GD iter. 2718/2999: loss=0.17102126932750428\n",
      "GD iter. 2719/2999: loss=0.17099616006162185\n",
      "GD iter. 2720/2999: loss=0.17097106146530075\n",
      "GD iter. 2721/2999: loss=0.17094597353102198\n",
      "GD iter. 2722/2999: loss=0.1709208962512744\n",
      "GD iter. 2723/2999: loss=0.17089582961855443\n",
      "GD iter. 2724/2999: loss=0.17087077362536626\n",
      "GD iter. 2725/2999: loss=0.1708457282642215\n",
      "GD iter. 2726/2999: loss=0.17082069352763965\n",
      "GD iter. 2727/2999: loss=0.17079566940814747\n",
      "GD iter. 2728/2999: loss=0.1707706558982797\n",
      "GD iter. 2729/2999: loss=0.17074565299057845\n",
      "GD iter. 2730/2999: loss=0.17072066067759337\n",
      "GD iter. 2731/2999: loss=0.17069567895188179\n",
      "GD iter. 2732/2999: loss=0.17067070780600854\n",
      "GD iter. 2733/2999: loss=0.1706457472325459\n",
      "GD iter. 2734/2999: loss=0.17062079722407394\n",
      "GD iter. 2735/2999: loss=0.17059585777318007\n",
      "GD iter. 2736/2999: loss=0.1705709288724591\n",
      "GD iter. 2737/2999: loss=0.17054601051451354\n",
      "GD iter. 2738/2999: loss=0.1705211026919533\n",
      "GD iter. 2739/2999: loss=0.1704962053973958\n",
      "GD iter. 2740/2999: loss=0.17047131862346585\n",
      "GD iter. 2741/2999: loss=0.17044644236279577\n",
      "GD iter. 2742/2999: loss=0.17042157660802534\n",
      "GD iter. 2743/2999: loss=0.17039672135180176\n",
      "GD iter. 2744/2999: loss=0.1703718765867796\n",
      "GD iter. 2745/2999: loss=0.17034704230562092\n",
      "GD iter. 2746/2999: loss=0.17032221850099513\n",
      "GD iter. 2747/2999: loss=0.17029740516557898\n",
      "GD iter. 2748/2999: loss=0.1702726022920568\n",
      "GD iter. 2749/2999: loss=0.17024780987312002\n",
      "GD iter. 2750/2999: loss=0.1702230279014675\n",
      "GD iter. 2751/2999: loss=0.17019825636980568\n",
      "GD iter. 2752/2999: loss=0.17017349527084805\n",
      "GD iter. 2753/2999: loss=0.1701487445973155\n",
      "GD iter. 2754/2999: loss=0.1701240043419363\n",
      "GD iter. 2755/2999: loss=0.17009927449744608\n",
      "GD iter. 2756/2999: loss=0.17007455505658756\n",
      "GD iter. 2757/2999: loss=0.17004984601211082\n",
      "GD iter. 2758/2999: loss=0.1700251473567733\n",
      "GD iter. 2759/2999: loss=0.17000045908333966\n",
      "GD iter. 2760/2999: loss=0.1699757811845818\n",
      "GD iter. 2761/2999: loss=0.16995111365327886\n",
      "GD iter. 2762/2999: loss=0.1699264564822172\n",
      "GD iter. 2763/2999: loss=0.16990180966419047\n",
      "GD iter. 2764/2999: loss=0.16987717319199935\n",
      "GD iter. 2765/2999: loss=0.16985254705845199\n",
      "GD iter. 2766/2999: loss=0.1698279312563635\n",
      "GD iter. 2767/2999: loss=0.16980332577855625\n",
      "GD iter. 2768/2999: loss=0.16977873061785975\n",
      "GD iter. 2769/2999: loss=0.16975414576711076\n",
      "GD iter. 2770/2999: loss=0.16972957121915314\n",
      "GD iter. 2771/2999: loss=0.16970500696683785\n",
      "GD iter. 2772/2999: loss=0.169680453003023\n",
      "GD iter. 2773/2999: loss=0.16965590932057378\n",
      "GD iter. 2774/2999: loss=0.16963137591236266\n",
      "GD iter. 2775/2999: loss=0.16960685277126888\n",
      "GD iter. 2776/2999: loss=0.16958233989017912\n",
      "GD iter. 2777/2999: loss=0.16955783726198692\n",
      "GD iter. 2778/2999: loss=0.169533344879593\n",
      "GD iter. 2779/2999: loss=0.16950886273590504\n",
      "GD iter. 2780/2999: loss=0.16948439082383787\n",
      "GD iter. 2781/2999: loss=0.16945992913631322\n",
      "GD iter. 2782/2999: loss=0.16943547766626005\n",
      "GD iter. 2783/2999: loss=0.16941103640661404\n",
      "GD iter. 2784/2999: loss=0.16938660535031821\n",
      "GD iter. 2785/2999: loss=0.16936218449032236\n",
      "GD iter. 2786/2999: loss=0.16933777381958343\n",
      "GD iter. 2787/2999: loss=0.1693133733310652\n",
      "GD iter. 2788/2999: loss=0.1692889830177383\n",
      "GD iter. 2789/2999: loss=0.16926460287258074\n",
      "GD iter. 2790/2999: loss=0.16924023288857704\n",
      "GD iter. 2791/2999: loss=0.16921587305871902\n",
      "GD iter. 2792/2999: loss=0.16919152337600513\n",
      "GD iter. 2793/2999: loss=0.16916718383344093\n",
      "GD iter. 2794/2999: loss=0.16914285442403873\n",
      "GD iter. 2795/2999: loss=0.16911853514081795\n",
      "GD iter. 2796/2999: loss=0.16909422597680468\n",
      "GD iter. 2797/2999: loss=0.16906992692503206\n",
      "GD iter. 2798/2999: loss=0.16904563797854005\n",
      "GD iter. 2799/2999: loss=0.16902135913037544\n",
      "GD iter. 2800/2999: loss=0.1689970903735918\n",
      "GD iter. 2801/2999: loss=0.1689728317012498\n",
      "GD iter. 2802/2999: loss=0.16894858310641664\n",
      "GD iter. 2803/2999: loss=0.1689243445821665\n",
      "GD iter. 2804/2999: loss=0.16890011612158043\n",
      "GD iter. 2805/2999: loss=0.16887589771774614\n",
      "GD iter. 2806/2999: loss=0.1688516893637582\n",
      "GD iter. 2807/2999: loss=0.168827491052718\n",
      "GD iter. 2808/2999: loss=0.16880330277773364\n",
      "GD iter. 2809/2999: loss=0.16877912453192007\n",
      "GD iter. 2810/2999: loss=0.16875495630839887\n",
      "GD iter. 2811/2999: loss=0.16873079810029856\n",
      "GD iter. 2812/2999: loss=0.16870664990075418\n",
      "GD iter. 2813/2999: loss=0.16868251170290763\n",
      "GD iter. 2814/2999: loss=0.1686583834999075\n",
      "GD iter. 2815/2999: loss=0.16863426528490913\n",
      "GD iter. 2816/2999: loss=0.16861015705107452\n",
      "GD iter. 2817/2999: loss=0.16858605879157232\n",
      "GD iter. 2818/2999: loss=0.1685619704995779\n",
      "GD iter. 2819/2999: loss=0.1685378921682734\n",
      "GD iter. 2820/2999: loss=0.1685138237908475\n",
      "GD iter. 2821/2999: loss=0.1684897653604955\n",
      "GD iter. 2822/2999: loss=0.16846571687041959\n",
      "GD iter. 2823/2999: loss=0.16844167831382822\n",
      "GD iter. 2824/2999: loss=0.16841764968393683\n",
      "GD iter. 2825/2999: loss=0.16839363097396726\n",
      "GD iter. 2826/2999: loss=0.168369622177148\n",
      "GD iter. 2827/2999: loss=0.16834562328671424\n",
      "GD iter. 2828/2999: loss=0.16832163429590757\n",
      "GD iter. 2829/2999: loss=0.1682976551979764\n",
      "GD iter. 2830/2999: loss=0.16827368598617554\n",
      "GD iter. 2831/2999: loss=0.16824972665376633\n",
      "GD iter. 2832/2999: loss=0.16822577719401688\n",
      "GD iter. 2833/2999: loss=0.1682018376002017\n",
      "GD iter. 2834/2999: loss=0.1681779078656018\n",
      "GD iter. 2835/2999: loss=0.16815398798350478\n",
      "GD iter. 2836/2999: loss=0.1681300779472048\n",
      "GD iter. 2837/2999: loss=0.16810617775000247\n",
      "GD iter. 2838/2999: loss=0.16808228738520486\n",
      "GD iter. 2839/2999: loss=0.16805840684612566\n",
      "GD iter. 2840/2999: loss=0.1680345361260849\n",
      "GD iter. 2841/2999: loss=0.1680106752184093\n",
      "GD iter. 2842/2999: loss=0.16798682411643176\n",
      "GD iter. 2843/2999: loss=0.16796298281349187\n",
      "GD iter. 2844/2999: loss=0.16793915130293552\n",
      "GD iter. 2845/2999: loss=0.16791532957811514\n",
      "GD iter. 2846/2999: loss=0.16789151763238958\n",
      "GD iter. 2847/2999: loss=0.16786771545912405\n",
      "GD iter. 2848/2999: loss=0.1678439230516902\n",
      "GD iter. 2849/2999: loss=0.16782014040346618\n",
      "GD iter. 2850/2999: loss=0.16779636750783639\n",
      "GD iter. 2851/2999: loss=0.16777260435819163\n",
      "GD iter. 2852/2999: loss=0.16774885094792932\n",
      "GD iter. 2853/2999: loss=0.16772510727045284\n",
      "GD iter. 2854/2999: loss=0.16770137331917231\n",
      "GD iter. 2855/2999: loss=0.16767764908750404\n",
      "GD iter. 2856/2999: loss=0.16765393456887065\n",
      "GD iter. 2857/2999: loss=0.1676302297567011\n",
      "GD iter. 2858/2999: loss=0.16760653464443084\n",
      "GD iter. 2859/2999: loss=0.16758284922550146\n",
      "GD iter. 2860/2999: loss=0.1675591734933609\n",
      "GD iter. 2861/2999: loss=0.1675355074414635\n",
      "GD iter. 2862/2999: loss=0.16751185106326974\n",
      "GD iter. 2863/2999: loss=0.16748820435224662\n",
      "GD iter. 2864/2999: loss=0.167464567301867\n",
      "GD iter. 2865/2999: loss=0.16744093990561051\n",
      "GD iter. 2866/2999: loss=0.16741732215696278\n",
      "GD iter. 2867/2999: loss=0.16739371404941555\n",
      "GD iter. 2868/2999: loss=0.16737011557646714\n",
      "GD iter. 2869/2999: loss=0.16734652673162184\n",
      "GD iter. 2870/2999: loss=0.16732294750839036\n",
      "GD iter. 2871/2999: loss=0.1672993779002894\n",
      "GD iter. 2872/2999: loss=0.16727581790084217\n",
      "GD iter. 2873/2999: loss=0.16725226750357777\n",
      "GD iter. 2874/2999: loss=0.16722872670203168\n",
      "GD iter. 2875/2999: loss=0.16720519548974566\n",
      "GD iter. 2876/2999: loss=0.16718167386026736\n",
      "GD iter. 2877/2999: loss=0.16715816180715082\n",
      "GD iter. 2878/2999: loss=0.16713465932395605\n",
      "GD iter. 2879/2999: loss=0.16711116640424956\n",
      "GD iter. 2880/2999: loss=0.1670876830416036\n",
      "GD iter. 2881/2999: loss=0.16706420922959686\n",
      "GD iter. 2882/2999: loss=0.16704074496181398\n",
      "GD iter. 2883/2999: loss=0.1670172902318458\n",
      "GD iter. 2884/2999: loss=0.16699384503328926\n",
      "GD iter. 2885/2999: loss=0.16697040935974736\n",
      "GD iter. 2886/2999: loss=0.16694698320482926\n",
      "GD iter. 2887/2999: loss=0.16692356656215018\n",
      "GD iter. 2888/2999: loss=0.16690015942533148\n",
      "GD iter. 2889/2999: loss=0.16687676178800048\n",
      "GD iter. 2890/2999: loss=0.16685337364379055\n",
      "GD iter. 2891/2999: loss=0.16682999498634132\n",
      "GD iter. 2892/2999: loss=0.16680662580929828\n",
      "GD iter. 2893/2999: loss=0.16678326610631297\n",
      "GD iter. 2894/2999: loss=0.16675991587104302\n",
      "GD iter. 2895/2999: loss=0.16673657509715223\n",
      "GD iter. 2896/2999: loss=0.16671324377830996\n",
      "GD iter. 2897/2999: loss=0.1666899219081921\n",
      "GD iter. 2898/2999: loss=0.1666666094804803\n",
      "GD iter. 2899/2999: loss=0.16664330648886205\n",
      "GD iter. 2900/2999: loss=0.1666200129270312\n",
      "GD iter. 2901/2999: loss=0.16659672878868734\n",
      "GD iter. 2902/2999: loss=0.16657345406753596\n",
      "GD iter. 2903/2999: loss=0.16655018875728872\n",
      "GD iter. 2904/2999: loss=0.16652693285166303\n",
      "GD iter. 2905/2999: loss=0.16650368634438245\n",
      "GD iter. 2906/2999: loss=0.16648044922917618\n",
      "GD iter. 2907/2999: loss=0.1664572214997798\n",
      "GD iter. 2908/2999: loss=0.16643400314993434\n",
      "GD iter. 2909/2999: loss=0.16641079417338703\n",
      "GD iter. 2910/2999: loss=0.16638759456389102\n",
      "GD iter. 2911/2999: loss=0.16636440431520513\n",
      "GD iter. 2912/2999: loss=0.16634122342109436\n",
      "GD iter. 2913/2999: loss=0.16631805187532933\n",
      "GD iter. 2914/2999: loss=0.16629488967168668\n",
      "GD iter. 2915/2999: loss=0.16627173680394902\n",
      "GD iter. 2916/2999: loss=0.1662485932659045\n",
      "GD iter. 2917/2999: loss=0.1662254590513475\n",
      "GD iter. 2918/2999: loss=0.16620233415407804\n",
      "GD iter. 2919/2999: loss=0.16617921856790188\n",
      "GD iter. 2920/2999: loss=0.16615611228663077\n",
      "GD iter. 2921/2999: loss=0.16613301530408234\n",
      "GD iter. 2922/2999: loss=0.16610992761407992\n",
      "GD iter. 2923/2999: loss=0.16608684921045266\n",
      "GD iter. 2924/2999: loss=0.16606378008703546\n",
      "GD iter. 2925/2999: loss=0.1660407202376692\n",
      "GD iter. 2926/2999: loss=0.16601766965620038\n",
      "GD iter. 2927/2999: loss=0.1659946283364812\n",
      "GD iter. 2928/2999: loss=0.16597159627237\n",
      "GD iter. 2929/2999: loss=0.1659485734577304\n",
      "GD iter. 2930/2999: loss=0.16592555988643215\n",
      "GD iter. 2931/2999: loss=0.1659025555523505\n",
      "GD iter. 2932/2999: loss=0.1658795604493667\n",
      "GD iter. 2933/2999: loss=0.16585657457136743\n",
      "GD iter. 2934/2999: loss=0.16583359791224536\n",
      "GD iter. 2935/2999: loss=0.16581063046589878\n",
      "GD iter. 2936/2999: loss=0.16578767222623153\n",
      "GD iter. 2937/2999: loss=0.16576472318715352\n",
      "GD iter. 2938/2999: loss=0.16574178334258\n",
      "GD iter. 2939/2999: loss=0.16571885268643208\n",
      "GD iter. 2940/2999: loss=0.16569593121263654\n",
      "GD iter. 2941/2999: loss=0.16567301891512587\n",
      "GD iter. 2942/2999: loss=0.16565011578783806\n",
      "GD iter. 2943/2999: loss=0.16562722182471704\n",
      "GD iter. 2944/2999: loss=0.165604337019712\n",
      "GD iter. 2945/2999: loss=0.1655814613667783\n",
      "GD iter. 2946/2999: loss=0.1655585948598764\n",
      "GD iter. 2947/2999: loss=0.16553573749297273\n",
      "GD iter. 2948/2999: loss=0.16551288926003935\n",
      "GD iter. 2949/2999: loss=0.1654900501550537\n",
      "GD iter. 2950/2999: loss=0.16546722017199897\n",
      "GD iter. 2951/2999: loss=0.16544439930486407\n",
      "GD iter. 2952/2999: loss=0.1654215875476433\n",
      "GD iter. 2953/2999: loss=0.16539878489433665\n",
      "GD iter. 2954/2999: loss=0.16537599133894967\n",
      "GD iter. 2955/2999: loss=0.16535320687549357\n",
      "GD iter. 2956/2999: loss=0.16533043149798504\n",
      "GD iter. 2957/2999: loss=0.1653076652004462\n",
      "GD iter. 2958/2999: loss=0.16528490797690507\n",
      "GD iter. 2959/2999: loss=0.16526215982139483\n",
      "GD iter. 2960/2999: loss=0.1652394207279545\n",
      "GD iter. 2961/2999: loss=0.16521669069062847\n",
      "GD iter. 2962/2999: loss=0.1651939697034667\n",
      "GD iter. 2963/2999: loss=0.16517125776052466\n",
      "GD iter. 2964/2999: loss=0.16514855485586336\n",
      "GD iter. 2965/2999: loss=0.16512586098354926\n",
      "GD iter. 2966/2999: loss=0.16510317613765435\n",
      "GD iter. 2967/2999: loss=0.16508050031225616\n",
      "GD iter. 2968/2999: loss=0.16505783350143766\n",
      "GD iter. 2969/2999: loss=0.1650351756992872\n",
      "GD iter. 2970/2999: loss=0.16501252689989873\n",
      "GD iter. 2971/2999: loss=0.16498988709737158\n",
      "GD iter. 2972/2999: loss=0.16496725628581071\n",
      "GD iter. 2973/2999: loss=0.16494463445932628\n",
      "GD iter. 2974/2999: loss=0.16492202161203404\n",
      "GD iter. 2975/2999: loss=0.16489941773805514\n",
      "GD iter. 2976/2999: loss=0.1648768228315161\n",
      "GD iter. 2977/2999: loss=0.16485423688654907\n",
      "GD iter. 2978/2999: loss=0.16483165989729137\n",
      "GD iter. 2979/2999: loss=0.1648090918578858\n",
      "GD iter. 2980/2999: loss=0.16478653276248068\n",
      "GD iter. 2981/2999: loss=0.1647639826052295\n",
      "GD iter. 2982/2999: loss=0.16474144138029137\n",
      "GD iter. 2983/2999: loss=0.16471890908183068\n",
      "GD iter. 2984/2999: loss=0.16469638570401712\n",
      "GD iter. 2985/2999: loss=0.16467387124102587\n",
      "GD iter. 2986/2999: loss=0.16465136568703737\n",
      "GD iter. 2987/2999: loss=0.16462886903623758\n",
      "GD iter. 2988/2999: loss=0.16460638128281754\n",
      "GD iter. 2989/2999: loss=0.1645839024209739\n",
      "GD iter. 2990/2999: loss=0.1645614324449085\n",
      "GD iter. 2991/2999: loss=0.16453897134882842\n",
      "GD iter. 2992/2999: loss=0.16451651912694637\n",
      "GD iter. 2993/2999: loss=0.16449407577348002\n",
      "GD iter. 2994/2999: loss=0.1644716412826526\n",
      "GD iter. 2995/2999: loss=0.1644492156486925\n",
      "GD iter. 2996/2999: loss=0.16442679886583345\n",
      "GD iter. 2997/2999: loss=0.16440439092831444\n",
      "GD iter. 2998/2999: loss=0.16438199183037983\n",
      "GD iter. 2999/2999: loss=0.1643596015662791\n",
      "The Accuracy is: 0.8818\n",
      "The F1 score is: 0.2941\n",
      "The precision is: 0.3571\n",
      "The recall is: 0.2500\n",
      "Average accuracy score is:  0.8817733990147784\n",
      "Average f1 score is:  0.2941176470588235\n"
     ]
    }
   ],
   "source": [
    "## PCA feature selection \n",
    "pre_train_data = x_train_processed.copy()\n",
    "x_pca = kernel_pca.transform(pre_train_data)\n",
    "# logistic regression using pca feature selection #\n",
    "x_pca_t = add_bias(x_pca)\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_pca_t), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(1):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    # x_t, y_t, x_v, y_v = split_data(x_pca_t, y_train_processed, 0.9)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = logistic_regression(y_t, x_t, initial_w, max_iters=3000, gamma=5)\n",
    "    y_pred = sigmoid(x_v @ w)\n",
    "    y_pred = (y_pred >= 0.75).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/499: loss=1.1319681734575158\n",
      "GD iter. 1/499: loss=1.035188801268022\n",
      "GD iter. 2/499: loss=0.9694526428189921\n",
      "GD iter. 3/499: loss=0.92434361321341\n",
      "GD iter. 4/499: loss=0.8930435237072751\n",
      "GD iter. 5/499: loss=0.8710672179764988\n",
      "GD iter. 6/499: loss=0.855446983551322\n",
      "GD iter. 7/499: loss=0.8442057145132248\n",
      "GD iter. 8/499: loss=0.8360158471240554\n",
      "GD iter. 9/499: loss=0.8299779678311574\n",
      "GD iter. 10/499: loss=0.8254766337526389\n",
      "GD iter. 11/499: loss=0.8220861068775358\n",
      "GD iter. 12/499: loss=0.8195084312255893\n",
      "GD iter. 13/499: loss=0.8175325280468386\n",
      "GD iter. 14/499: loss=0.8160069974032347\n",
      "GD iter. 15/499: loss=0.8148218957039651\n",
      "GD iter. 16/499: loss=0.8138964209465421\n",
      "GD iter. 17/499: loss=0.8131705093658792\n",
      "GD iter. 18/499: loss=0.812599039765525\n",
      "GD iter. 19/499: loss=0.8121477902703146\n",
      "GD iter. 20/499: loss=0.8117905834325928\n",
      "GD iter. 21/499: loss=0.8115072453278209\n",
      "GD iter. 22/499: loss=0.8112821283578635\n",
      "GD iter. 23/499: loss=0.8111030290307675\n",
      "GD iter. 24/499: loss=0.8109603858883679\n",
      "GD iter. 25/499: loss=0.8108466786187852\n",
      "GD iter. 26/499: loss=0.810755973439495\n",
      "GD iter. 27/499: loss=0.8106835761033434\n",
      "GD iter. 28/499: loss=0.8106257649910815\n",
      "GD iter. 29/499: loss=0.8105795844255343\n",
      "GD iter. 30/499: loss=0.8105426837006241\n",
      "GD iter. 31/499: loss=0.8105131911063138\n",
      "GD iter. 32/499: loss=0.810489614941661\n",
      "GD iter. 33/499: loss=0.8104707654728478\n",
      "GD iter. 34/499: loss=0.810455693234059\n",
      "GD iter. 35/499: loss=0.8104436401382353\n",
      "GD iter. 36/499: loss=0.8104340006665095\n",
      "GD iter. 37/499: loss=0.8104262910122855\n",
      "GD iter. 38/499: loss=0.810420124519727\n",
      "GD iter. 39/499: loss=0.8104151921134052\n",
      "GD iter. 40/499: loss=0.8104112466924921\n",
      "GD iter. 41/499: loss=0.8104080906784136\n",
      "GD iter. 42/499: loss=0.8104055660736481\n",
      "GD iter. 43/499: loss=0.8104035465219942\n",
      "GD iter. 44/499: loss=0.8104019309652524\n",
      "GD iter. 45/499: loss=0.8104006385739911\n",
      "GD iter. 46/499: loss=0.8103996046956266\n",
      "GD iter. 47/499: loss=0.8103987776151071\n",
      "GD iter. 48/499: loss=0.8103981159648823\n",
      "GD iter. 49/499: loss=0.8103975866537843\n",
      "GD iter. 50/499: loss=0.8103971632107182\n",
      "GD iter. 51/499: loss=0.8103968244599852\n",
      "GD iter. 52/499: loss=0.8103965534617795\n",
      "GD iter. 53/499: loss=0.8103963366647388\n",
      "GD iter. 54/499: loss=0.8103961632280814\n",
      "GD iter. 55/499: loss=0.8103960244793795\n",
      "GD iter. 56/499: loss=0.8103959134808172\n",
      "GD iter. 57/499: loss=0.8103958246822233\n",
      "GD iter. 58/499: loss=0.8103957536435117\n",
      "GD iter. 59/499: loss=0.8103956968126471\n",
      "GD iter. 60/499: loss=0.8103956513480225\n",
      "GD iter. 61/499: loss=0.8103956149763656\n",
      "GD iter. 62/499: loss=0.8103955858790673\n",
      "GD iter. 63/499: loss=0.8103955626012467\n",
      "GD iter. 64/499: loss=0.8103955439790012\n",
      "GD iter. 65/499: loss=0.810395529081212\n",
      "GD iter. 66/499: loss=0.8103955171629853\n",
      "GD iter. 67/499: loss=0.810395507628407\n",
      "GD iter. 68/499: loss=0.810395500000746\n",
      "GD iter. 69/499: loss=0.8103954938986184\n",
      "GD iter. 70/499: loss=0.8103954890169174\n",
      "GD iter. 71/499: loss=0.8103954851115568\n",
      "GD iter. 72/499: loss=0.8103954819872687\n",
      "GD iter. 73/499: loss=0.8103954794878385\n",
      "GD iter. 74/499: loss=0.8103954774882944\n",
      "GD iter. 75/499: loss=0.8103954758886592\n",
      "GD iter. 76/499: loss=0.8103954746089513\n",
      "GD iter. 77/499: loss=0.8103954735851847\n",
      "GD iter. 78/499: loss=0.8103954727661717\n",
      "GD iter. 79/499: loss=0.8103954721109611\n",
      "GD iter. 80/499: loss=0.8103954715867927\n",
      "GD iter. 81/499: loss=0.8103954711674579\n",
      "GD iter. 82/499: loss=0.8103954708319903\n",
      "GD iter. 83/499: loss=0.8103954705636162\n",
      "GD iter. 84/499: loss=0.8103954703489167\n",
      "GD iter. 85/499: loss=0.8103954701771573\n",
      "GD iter. 86/499: loss=0.8103954700397497\n",
      "GD iter. 87/499: loss=0.8103954699298237\n",
      "GD iter. 88/499: loss=0.8103954698418828\n",
      "GD iter. 89/499: loss=0.8103954697715301\n",
      "GD iter. 90/499: loss=0.810395469715248\n",
      "GD iter. 91/499: loss=0.8103954696702222\n",
      "GD iter. 92/499: loss=0.8103954696342018\n",
      "GD iter. 93/499: loss=0.8103954696053852\n",
      "GD iter. 94/499: loss=0.810395469582332\n",
      "GD iter. 95/499: loss=0.8103954695638895\n",
      "GD iter. 96/499: loss=0.8103954695491354\n",
      "GD iter. 97/499: loss=0.8103954695373323\n",
      "GD iter. 98/499: loss=0.8103954695278895\n",
      "GD iter. 99/499: loss=0.8103954695203355\n",
      "GD iter. 100/499: loss=0.8103954695142924\n",
      "GD iter. 101/499: loss=0.8103954695094577\n",
      "GD iter. 102/499: loss=0.81039546950559\n",
      "GD iter. 103/499: loss=0.810395469502496\n",
      "GD iter. 104/499: loss=0.8103954695000205\n",
      "GD iter. 105/499: loss=0.8103954694980404\n",
      "GD iter. 106/499: loss=0.8103954694964562\n",
      "GD iter. 107/499: loss=0.8103954694951887\n",
      "GD iter. 108/499: loss=0.810395469494175\n",
      "GD iter. 109/499: loss=0.8103954694933638\n",
      "GD iter. 110/499: loss=0.8103954694927149\n",
      "GD iter. 111/499: loss=0.8103954694921958\n",
      "GD iter. 112/499: loss=0.8103954694917804\n",
      "GD iter. 113/499: loss=0.8103954694914484\n",
      "GD iter. 114/499: loss=0.8103954694911825\n",
      "GD iter. 115/499: loss=0.81039546949097\n",
      "GD iter. 116/499: loss=0.8103954694907998\n",
      "GD iter. 117/499: loss=0.8103954694906635\n",
      "GD iter. 118/499: loss=0.8103954694905549\n",
      "GD iter. 119/499: loss=0.8103954694904677\n",
      "GD iter. 120/499: loss=0.8103954694903981\n",
      "GD iter. 121/499: loss=0.8103954694903424\n",
      "GD iter. 122/499: loss=0.8103954694902976\n",
      "GD iter. 123/499: loss=0.810395469490262\n",
      "GD iter. 124/499: loss=0.8103954694902336\n",
      "GD iter. 125/499: loss=0.8103954694902106\n",
      "GD iter. 126/499: loss=0.8103954694901924\n",
      "GD iter. 127/499: loss=0.8103954694901778\n",
      "GD iter. 128/499: loss=0.8103954694901662\n",
      "GD iter. 129/499: loss=0.8103954694901568\n",
      "GD iter. 130/499: loss=0.8103954694901493\n",
      "GD iter. 131/499: loss=0.8103954694901432\n",
      "GD iter. 132/499: loss=0.8103954694901385\n",
      "GD iter. 133/499: loss=0.8103954694901346\n",
      "GD iter. 134/499: loss=0.8103954694901316\n",
      "GD iter. 135/499: loss=0.8103954694901292\n",
      "GD iter. 136/499: loss=0.8103954694901271\n",
      "GD iter. 137/499: loss=0.8103954694901256\n",
      "GD iter. 138/499: loss=0.8103954694901243\n",
      "GD iter. 139/499: loss=0.8103954694901233\n",
      "GD iter. 140/499: loss=0.8103954694901226\n",
      "GD iter. 141/499: loss=0.8103954694901219\n",
      "GD iter. 142/499: loss=0.8103954694901214\n",
      "GD iter. 143/499: loss=0.810395469490121\n",
      "GD iter. 144/499: loss=0.8103954694901205\n",
      "GD iter. 145/499: loss=0.8103954694901203\n",
      "GD iter. 146/499: loss=0.8103954694901201\n",
      "GD iter. 147/499: loss=0.8103954694901201\n",
      "GD iter. 148/499: loss=0.8103954694901199\n",
      "GD iter. 149/499: loss=0.8103954694901198\n",
      "GD iter. 150/499: loss=0.8103954694901198\n",
      "GD iter. 151/499: loss=0.8103954694901195\n",
      "GD iter. 152/499: loss=0.8103954694901195\n",
      "GD iter. 153/499: loss=0.8103954694901195\n",
      "GD iter. 154/499: loss=0.8103954694901194\n",
      "GD iter. 155/499: loss=0.8103954694901195\n",
      "GD iter. 156/499: loss=0.8103954694901194\n",
      "GD iter. 157/499: loss=0.8103954694901194\n",
      "GD iter. 158/499: loss=0.8103954694901192\n",
      "GD iter. 159/499: loss=0.8103954694901192\n",
      "GD iter. 160/499: loss=0.8103954694901192\n",
      "GD iter. 161/499: loss=0.8103954694901193\n",
      "GD iter. 162/499: loss=0.8103954694901193\n",
      "GD iter. 163/499: loss=0.8103954694901193\n",
      "GD iter. 164/499: loss=0.8103954694901193\n",
      "GD iter. 165/499: loss=0.8103954694901193\n",
      "GD iter. 166/499: loss=0.8103954694901193\n",
      "GD iter. 167/499: loss=0.8103954694901193\n",
      "GD iter. 168/499: loss=0.8103954694901193\n",
      "GD iter. 169/499: loss=0.8103954694901193\n",
      "GD iter. 170/499: loss=0.8103954694901193\n",
      "GD iter. 171/499: loss=0.8103954694901193\n",
      "GD iter. 172/499: loss=0.8103954694901193\n",
      "GD iter. 173/499: loss=0.8103954694901193\n",
      "GD iter. 174/499: loss=0.8103954694901193\n",
      "GD iter. 175/499: loss=0.8103954694901193\n",
      "GD iter. 176/499: loss=0.8103954694901193\n",
      "GD iter. 177/499: loss=0.8103954694901193\n",
      "GD iter. 178/499: loss=0.8103954694901193\n",
      "GD iter. 179/499: loss=0.8103954694901193\n",
      "GD iter. 180/499: loss=0.8103954694901193\n",
      "GD iter. 181/499: loss=0.8103954694901193\n",
      "GD iter. 182/499: loss=0.8103954694901193\n",
      "GD iter. 183/499: loss=0.8103954694901193\n",
      "GD iter. 184/499: loss=0.8103954694901193\n",
      "GD iter. 185/499: loss=0.8103954694901193\n",
      "GD iter. 186/499: loss=0.8103954694901193\n",
      "GD iter. 187/499: loss=0.8103954694901193\n",
      "GD iter. 188/499: loss=0.8103954694901193\n",
      "GD iter. 189/499: loss=0.8103954694901193\n",
      "GD iter. 190/499: loss=0.8103954694901193\n",
      "GD iter. 191/499: loss=0.8103954694901193\n",
      "GD iter. 192/499: loss=0.8103954694901193\n",
      "GD iter. 193/499: loss=0.8103954694901193\n",
      "GD iter. 194/499: loss=0.8103954694901193\n",
      "GD iter. 195/499: loss=0.8103954694901193\n",
      "GD iter. 196/499: loss=0.8103954694901193\n",
      "GD iter. 197/499: loss=0.8103954694901193\n",
      "GD iter. 198/499: loss=0.8103954694901193\n",
      "GD iter. 199/499: loss=0.8103954694901193\n",
      "GD iter. 200/499: loss=0.8103954694901193\n",
      "GD iter. 201/499: loss=0.8103954694901193\n",
      "GD iter. 202/499: loss=0.8103954694901193\n",
      "GD iter. 203/499: loss=0.8103954694901193\n",
      "GD iter. 204/499: loss=0.8103954694901193\n",
      "GD iter. 205/499: loss=0.8103954694901193\n",
      "GD iter. 206/499: loss=0.8103954694901193\n",
      "GD iter. 207/499: loss=0.8103954694901193\n",
      "GD iter. 208/499: loss=0.8103954694901193\n",
      "GD iter. 209/499: loss=0.8103954694901193\n",
      "GD iter. 210/499: loss=0.8103954694901193\n",
      "GD iter. 211/499: loss=0.8103954694901193\n",
      "GD iter. 212/499: loss=0.8103954694901193\n",
      "GD iter. 213/499: loss=0.8103954694901193\n",
      "GD iter. 214/499: loss=0.8103954694901193\n",
      "GD iter. 215/499: loss=0.8103954694901193\n",
      "GD iter. 216/499: loss=0.8103954694901193\n",
      "GD iter. 217/499: loss=0.8103954694901193\n",
      "GD iter. 218/499: loss=0.8103954694901193\n",
      "GD iter. 219/499: loss=0.8103954694901193\n",
      "GD iter. 220/499: loss=0.8103954694901193\n",
      "GD iter. 221/499: loss=0.8103954694901193\n",
      "GD iter. 222/499: loss=0.8103954694901193\n",
      "GD iter. 223/499: loss=0.8103954694901193\n",
      "GD iter. 224/499: loss=0.8103954694901193\n",
      "GD iter. 225/499: loss=0.8103954694901193\n",
      "GD iter. 226/499: loss=0.8103954694901193\n",
      "GD iter. 227/499: loss=0.8103954694901193\n",
      "GD iter. 228/499: loss=0.8103954694901193\n",
      "GD iter. 229/499: loss=0.8103954694901193\n",
      "GD iter. 230/499: loss=0.8103954694901193\n",
      "GD iter. 231/499: loss=0.8103954694901193\n",
      "GD iter. 232/499: loss=0.8103954694901193\n",
      "GD iter. 233/499: loss=0.8103954694901193\n",
      "GD iter. 234/499: loss=0.8103954694901193\n",
      "GD iter. 235/499: loss=0.8103954694901193\n",
      "GD iter. 236/499: loss=0.8103954694901193\n",
      "GD iter. 237/499: loss=0.8103954694901193\n",
      "GD iter. 238/499: loss=0.8103954694901193\n",
      "GD iter. 239/499: loss=0.8103954694901193\n",
      "GD iter. 240/499: loss=0.8103954694901193\n",
      "GD iter. 241/499: loss=0.8103954694901193\n",
      "GD iter. 242/499: loss=0.8103954694901193\n",
      "GD iter. 243/499: loss=0.8103954694901193\n",
      "GD iter. 244/499: loss=0.8103954694901193\n",
      "GD iter. 245/499: loss=0.8103954694901193\n",
      "GD iter. 246/499: loss=0.8103954694901193\n",
      "GD iter. 247/499: loss=0.8103954694901193\n",
      "GD iter. 248/499: loss=0.8103954694901193\n",
      "GD iter. 249/499: loss=0.8103954694901193\n",
      "GD iter. 250/499: loss=0.8103954694901193\n",
      "GD iter. 251/499: loss=0.8103954694901193\n",
      "GD iter. 252/499: loss=0.8103954694901193\n",
      "GD iter. 253/499: loss=0.8103954694901193\n",
      "GD iter. 254/499: loss=0.8103954694901193\n",
      "GD iter. 255/499: loss=0.8103954694901193\n",
      "GD iter. 256/499: loss=0.8103954694901193\n",
      "GD iter. 257/499: loss=0.8103954694901193\n",
      "GD iter. 258/499: loss=0.8103954694901193\n",
      "GD iter. 259/499: loss=0.8103954694901193\n",
      "GD iter. 260/499: loss=0.8103954694901193\n",
      "GD iter. 261/499: loss=0.8103954694901193\n",
      "GD iter. 262/499: loss=0.8103954694901193\n",
      "GD iter. 263/499: loss=0.8103954694901193\n",
      "GD iter. 264/499: loss=0.8103954694901193\n",
      "GD iter. 265/499: loss=0.8103954694901193\n",
      "GD iter. 266/499: loss=0.8103954694901193\n",
      "GD iter. 267/499: loss=0.8103954694901193\n",
      "GD iter. 268/499: loss=0.8103954694901193\n",
      "GD iter. 269/499: loss=0.8103954694901193\n",
      "GD iter. 270/499: loss=0.8103954694901193\n",
      "GD iter. 271/499: loss=0.8103954694901193\n",
      "GD iter. 272/499: loss=0.8103954694901193\n",
      "GD iter. 273/499: loss=0.8103954694901193\n",
      "GD iter. 274/499: loss=0.8103954694901193\n",
      "GD iter. 275/499: loss=0.8103954694901193\n",
      "GD iter. 276/499: loss=0.8103954694901193\n",
      "GD iter. 277/499: loss=0.8103954694901193\n",
      "GD iter. 278/499: loss=0.8103954694901193\n",
      "GD iter. 279/499: loss=0.8103954694901193\n",
      "GD iter. 280/499: loss=0.8103954694901193\n",
      "GD iter. 281/499: loss=0.8103954694901193\n",
      "GD iter. 282/499: loss=0.8103954694901193\n",
      "GD iter. 283/499: loss=0.8103954694901193\n",
      "GD iter. 284/499: loss=0.8103954694901193\n",
      "GD iter. 285/499: loss=0.8103954694901193\n",
      "GD iter. 286/499: loss=0.8103954694901193\n",
      "GD iter. 287/499: loss=0.8103954694901193\n",
      "GD iter. 288/499: loss=0.8103954694901193\n",
      "GD iter. 289/499: loss=0.8103954694901193\n",
      "GD iter. 290/499: loss=0.8103954694901193\n",
      "GD iter. 291/499: loss=0.8103954694901193\n",
      "GD iter. 292/499: loss=0.8103954694901193\n",
      "GD iter. 293/499: loss=0.8103954694901193\n",
      "GD iter. 294/499: loss=0.8103954694901193\n",
      "GD iter. 295/499: loss=0.8103954694901193\n",
      "GD iter. 296/499: loss=0.8103954694901193\n",
      "GD iter. 297/499: loss=0.8103954694901193\n",
      "GD iter. 298/499: loss=0.8103954694901193\n",
      "GD iter. 299/499: loss=0.8103954694901193\n",
      "GD iter. 300/499: loss=0.8103954694901193\n",
      "GD iter. 301/499: loss=0.8103954694901193\n",
      "GD iter. 302/499: loss=0.8103954694901193\n",
      "GD iter. 303/499: loss=0.8103954694901193\n",
      "GD iter. 304/499: loss=0.8103954694901193\n",
      "GD iter. 305/499: loss=0.8103954694901193\n",
      "GD iter. 306/499: loss=0.8103954694901193\n",
      "GD iter. 307/499: loss=0.8103954694901193\n",
      "GD iter. 308/499: loss=0.8103954694901193\n",
      "GD iter. 309/499: loss=0.8103954694901193\n",
      "GD iter. 310/499: loss=0.8103954694901193\n",
      "GD iter. 311/499: loss=0.8103954694901193\n",
      "GD iter. 312/499: loss=0.8103954694901193\n",
      "GD iter. 313/499: loss=0.8103954694901193\n",
      "GD iter. 314/499: loss=0.8103954694901193\n",
      "GD iter. 315/499: loss=0.8103954694901193\n",
      "GD iter. 316/499: loss=0.8103954694901193\n",
      "GD iter. 317/499: loss=0.8103954694901193\n",
      "GD iter. 318/499: loss=0.8103954694901193\n",
      "GD iter. 319/499: loss=0.8103954694901193\n",
      "GD iter. 320/499: loss=0.8103954694901193\n",
      "GD iter. 321/499: loss=0.8103954694901193\n",
      "GD iter. 322/499: loss=0.8103954694901193\n",
      "GD iter. 323/499: loss=0.8103954694901193\n",
      "GD iter. 324/499: loss=0.8103954694901193\n",
      "GD iter. 325/499: loss=0.8103954694901193\n",
      "GD iter. 326/499: loss=0.8103954694901193\n",
      "GD iter. 327/499: loss=0.8103954694901193\n",
      "GD iter. 328/499: loss=0.8103954694901193\n",
      "GD iter. 329/499: loss=0.8103954694901193\n",
      "GD iter. 330/499: loss=0.8103954694901193\n",
      "GD iter. 331/499: loss=0.8103954694901193\n",
      "GD iter. 332/499: loss=0.8103954694901193\n",
      "GD iter. 333/499: loss=0.8103954694901193\n",
      "GD iter. 334/499: loss=0.8103954694901193\n",
      "GD iter. 335/499: loss=0.8103954694901193\n",
      "GD iter. 336/499: loss=0.8103954694901193\n",
      "GD iter. 337/499: loss=0.8103954694901193\n",
      "GD iter. 338/499: loss=0.8103954694901193\n",
      "GD iter. 339/499: loss=0.8103954694901193\n",
      "GD iter. 340/499: loss=0.8103954694901193\n",
      "GD iter. 341/499: loss=0.8103954694901193\n",
      "GD iter. 342/499: loss=0.8103954694901193\n",
      "GD iter. 343/499: loss=0.8103954694901193\n",
      "GD iter. 344/499: loss=0.8103954694901193\n",
      "GD iter. 345/499: loss=0.8103954694901193\n",
      "GD iter. 346/499: loss=0.8103954694901193\n",
      "GD iter. 347/499: loss=0.8103954694901193\n",
      "GD iter. 348/499: loss=0.8103954694901193\n",
      "GD iter. 349/499: loss=0.8103954694901193\n",
      "GD iter. 350/499: loss=0.8103954694901193\n",
      "GD iter. 351/499: loss=0.8103954694901193\n",
      "GD iter. 352/499: loss=0.8103954694901193\n",
      "GD iter. 353/499: loss=0.8103954694901193\n",
      "GD iter. 354/499: loss=0.8103954694901193\n",
      "GD iter. 355/499: loss=0.8103954694901193\n",
      "GD iter. 356/499: loss=0.8103954694901193\n",
      "GD iter. 357/499: loss=0.8103954694901193\n",
      "GD iter. 358/499: loss=0.8103954694901193\n",
      "GD iter. 359/499: loss=0.8103954694901193\n",
      "GD iter. 360/499: loss=0.8103954694901193\n",
      "GD iter. 361/499: loss=0.8103954694901193\n",
      "GD iter. 362/499: loss=0.8103954694901193\n",
      "GD iter. 363/499: loss=0.8103954694901193\n",
      "GD iter. 364/499: loss=0.8103954694901193\n",
      "GD iter. 365/499: loss=0.8103954694901193\n",
      "GD iter. 366/499: loss=0.8103954694901193\n",
      "GD iter. 367/499: loss=0.8103954694901193\n",
      "GD iter. 368/499: loss=0.8103954694901193\n",
      "GD iter. 369/499: loss=0.8103954694901193\n",
      "GD iter. 370/499: loss=0.8103954694901193\n",
      "GD iter. 371/499: loss=0.8103954694901193\n",
      "GD iter. 372/499: loss=0.8103954694901193\n",
      "GD iter. 373/499: loss=0.8103954694901193\n",
      "GD iter. 374/499: loss=0.8103954694901193\n",
      "GD iter. 375/499: loss=0.8103954694901193\n",
      "GD iter. 376/499: loss=0.8103954694901193\n",
      "GD iter. 377/499: loss=0.8103954694901193\n",
      "GD iter. 378/499: loss=0.8103954694901193\n",
      "GD iter. 379/499: loss=0.8103954694901193\n",
      "GD iter. 380/499: loss=0.8103954694901193\n",
      "GD iter. 381/499: loss=0.8103954694901193\n",
      "GD iter. 382/499: loss=0.8103954694901193\n",
      "GD iter. 383/499: loss=0.8103954694901193\n",
      "GD iter. 384/499: loss=0.8103954694901193\n",
      "GD iter. 385/499: loss=0.8103954694901193\n",
      "GD iter. 386/499: loss=0.8103954694901193\n",
      "GD iter. 387/499: loss=0.8103954694901193\n",
      "GD iter. 388/499: loss=0.8103954694901193\n",
      "GD iter. 389/499: loss=0.8103954694901193\n",
      "GD iter. 390/499: loss=0.8103954694901193\n",
      "GD iter. 391/499: loss=0.8103954694901193\n",
      "GD iter. 392/499: loss=0.8103954694901193\n",
      "GD iter. 393/499: loss=0.8103954694901193\n",
      "GD iter. 394/499: loss=0.8103954694901193\n",
      "GD iter. 395/499: loss=0.8103954694901193\n",
      "GD iter. 396/499: loss=0.8103954694901193\n",
      "GD iter. 397/499: loss=0.8103954694901193\n",
      "GD iter. 398/499: loss=0.8103954694901193\n",
      "GD iter. 399/499: loss=0.8103954694901193\n",
      "GD iter. 400/499: loss=0.8103954694901193\n",
      "GD iter. 401/499: loss=0.8103954694901193\n",
      "GD iter. 402/499: loss=0.8103954694901193\n",
      "GD iter. 403/499: loss=0.8103954694901193\n",
      "GD iter. 404/499: loss=0.8103954694901193\n",
      "GD iter. 405/499: loss=0.8103954694901193\n",
      "GD iter. 406/499: loss=0.8103954694901193\n",
      "GD iter. 407/499: loss=0.8103954694901193\n",
      "GD iter. 408/499: loss=0.8103954694901193\n",
      "GD iter. 409/499: loss=0.8103954694901193\n",
      "GD iter. 410/499: loss=0.8103954694901193\n",
      "GD iter. 411/499: loss=0.8103954694901193\n",
      "GD iter. 412/499: loss=0.8103954694901193\n",
      "GD iter. 413/499: loss=0.8103954694901193\n",
      "GD iter. 414/499: loss=0.8103954694901193\n",
      "GD iter. 415/499: loss=0.8103954694901193\n",
      "GD iter. 416/499: loss=0.8103954694901193\n",
      "GD iter. 417/499: loss=0.8103954694901193\n",
      "GD iter. 418/499: loss=0.8103954694901193\n",
      "GD iter. 419/499: loss=0.8103954694901193\n",
      "GD iter. 420/499: loss=0.8103954694901193\n",
      "GD iter. 421/499: loss=0.8103954694901193\n",
      "GD iter. 422/499: loss=0.8103954694901193\n",
      "GD iter. 423/499: loss=0.8103954694901193\n",
      "GD iter. 424/499: loss=0.8103954694901193\n",
      "GD iter. 425/499: loss=0.8103954694901193\n",
      "GD iter. 426/499: loss=0.8103954694901193\n",
      "GD iter. 427/499: loss=0.8103954694901193\n",
      "GD iter. 428/499: loss=0.8103954694901193\n",
      "GD iter. 429/499: loss=0.8103954694901193\n",
      "GD iter. 430/499: loss=0.8103954694901193\n",
      "GD iter. 431/499: loss=0.8103954694901193\n",
      "GD iter. 432/499: loss=0.8103954694901193\n",
      "GD iter. 433/499: loss=0.8103954694901193\n",
      "GD iter. 434/499: loss=0.8103954694901193\n",
      "GD iter. 435/499: loss=0.8103954694901193\n",
      "GD iter. 436/499: loss=0.8103954694901193\n",
      "GD iter. 437/499: loss=0.8103954694901193\n",
      "GD iter. 438/499: loss=0.8103954694901193\n",
      "GD iter. 439/499: loss=0.8103954694901193\n",
      "GD iter. 440/499: loss=0.8103954694901193\n",
      "GD iter. 441/499: loss=0.8103954694901193\n",
      "GD iter. 442/499: loss=0.8103954694901193\n",
      "GD iter. 443/499: loss=0.8103954694901193\n",
      "GD iter. 444/499: loss=0.8103954694901193\n",
      "GD iter. 445/499: loss=0.8103954694901193\n",
      "GD iter. 446/499: loss=0.8103954694901193\n",
      "GD iter. 447/499: loss=0.8103954694901193\n",
      "GD iter. 448/499: loss=0.8103954694901193\n",
      "GD iter. 449/499: loss=0.8103954694901193\n",
      "GD iter. 450/499: loss=0.8103954694901193\n",
      "GD iter. 451/499: loss=0.8103954694901193\n",
      "GD iter. 452/499: loss=0.8103954694901193\n",
      "GD iter. 453/499: loss=0.8103954694901193\n",
      "GD iter. 454/499: loss=0.8103954694901193\n",
      "GD iter. 455/499: loss=0.8103954694901193\n",
      "GD iter. 456/499: loss=0.8103954694901193\n",
      "GD iter. 457/499: loss=0.8103954694901193\n",
      "GD iter. 458/499: loss=0.8103954694901193\n",
      "GD iter. 459/499: loss=0.8103954694901193\n",
      "GD iter. 460/499: loss=0.8103954694901193\n",
      "GD iter. 461/499: loss=0.8103954694901193\n",
      "GD iter. 462/499: loss=0.8103954694901193\n",
      "GD iter. 463/499: loss=0.8103954694901193\n",
      "GD iter. 464/499: loss=0.8103954694901193\n",
      "GD iter. 465/499: loss=0.8103954694901193\n",
      "GD iter. 466/499: loss=0.8103954694901193\n",
      "GD iter. 467/499: loss=0.8103954694901193\n",
      "GD iter. 468/499: loss=0.8103954694901193\n",
      "GD iter. 469/499: loss=0.8103954694901193\n",
      "GD iter. 470/499: loss=0.8103954694901193\n",
      "GD iter. 471/499: loss=0.8103954694901193\n",
      "GD iter. 472/499: loss=0.8103954694901193\n",
      "GD iter. 473/499: loss=0.8103954694901193\n",
      "GD iter. 474/499: loss=0.8103954694901193\n",
      "GD iter. 475/499: loss=0.8103954694901193\n",
      "GD iter. 476/499: loss=0.8103954694901193\n",
      "GD iter. 477/499: loss=0.8103954694901193\n",
      "GD iter. 478/499: loss=0.8103954694901193\n",
      "GD iter. 479/499: loss=0.8103954694901193\n",
      "GD iter. 480/499: loss=0.8103954694901193\n",
      "GD iter. 481/499: loss=0.8103954694901193\n",
      "GD iter. 482/499: loss=0.8103954694901193\n",
      "GD iter. 483/499: loss=0.8103954694901193\n",
      "GD iter. 484/499: loss=0.8103954694901193\n",
      "GD iter. 485/499: loss=0.8103954694901193\n",
      "GD iter. 486/499: loss=0.8103954694901193\n",
      "GD iter. 487/499: loss=0.8103954694901193\n",
      "GD iter. 488/499: loss=0.8103954694901193\n",
      "GD iter. 489/499: loss=0.8103954694901193\n",
      "GD iter. 490/499: loss=0.8103954694901193\n",
      "GD iter. 491/499: loss=0.8103954694901193\n",
      "GD iter. 492/499: loss=0.8103954694901193\n",
      "GD iter. 493/499: loss=0.8103954694901193\n",
      "GD iter. 494/499: loss=0.8103954694901193\n",
      "GD iter. 495/499: loss=0.8103954694901193\n",
      "GD iter. 496/499: loss=0.8103954694901193\n",
      "GD iter. 497/499: loss=0.8103954694901193\n",
      "GD iter. 498/499: loss=0.8103954694901193\n",
      "GD iter. 499/499: loss=0.8103954694901193\n",
      "The Accuracy is: 0.0854\n",
      "The F1 score is: 0.1573\n",
      "The precision is: 0.0854\n",
      "The recall is: 1.0000\n",
      "Average accuracy score is:  0.08538587848932677\n",
      "Average f1 score is:  0.15733736762481093\n"
     ]
    }
   ],
   "source": [
    "## PCA feature selection \n",
    "pre_train_data = x_train_processed.copy()\n",
    "x_pca = kernel_pca.transform(pre_train_data)\n",
    "# hinge regression using pca feature selection #\n",
    "x_pca_t = add_bias(x_pca)\n",
    "sub_x, sub_y = split_cross_validation(add_bias(x_pca_t), y_train_processed, 10)\n",
    "accs = []\n",
    "f1s = []\n",
    "# cross-validation\n",
    "for i in range(1):\n",
    "    sub_cur_x = sub_x.copy()\n",
    "    sub_cur_y = sub_y.copy()\n",
    "    x_v, y_v = sub_cur_x.pop(i), sub_cur_y.pop(i)\n",
    "    x_t, y_t = np.vstack(sub_cur_x), np.hstack(sub_cur_y)\n",
    "    x_t, y_t = data_augmentation(x_t, y_t)\n",
    "    initial_w = np.random.randn(x_t.shape[1]) * 0.01\n",
    "    w, loss = hinge_regression(y_t, x_t, initial_w, max_iters=500, gamma=0.1, lambda_= 1)\n",
    "    y_pred = ((x_v @ w) > 0).astype(int)\n",
    "    accs.append(predict_acc_pure(y_pred, y_v))\n",
    "    f1s.append(predict_f1_pure(y_pred, y_v))\n",
    "print(\"Average accuracy score is: \", np.mean(accs))\n",
    "print(\"Average f1 score is: \", np.mean(f1s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
